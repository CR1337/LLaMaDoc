{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a34240cc-aa83-4e01-ad78-8768d9b78750",
   "metadata": {},
   "source": [
    "# Fine-tuning Example\n",
    "\n",
    "This notebook demonstrates how to fine-tune a code-generating LLM to observe a specific prompt and output format.\n",
    "We want Python functions to be enclosed in XML tags:\n",
    "\n",
    "```python\n",
    "# <function name=\"foo\">\n",
    "def foo():\n",
    "    # content\n",
    "# </function>\n",
    "```\n",
    "\n",
    "We will obtain training data by extracting functions from a repository using GitPython and tree-sitter.\n",
    "\n",
    "## ⚠️ Warnings\n",
    "\n",
    "⚡ Fine-tuning happens in-place, which means the **original model is lost** once the PEFT-Adapter is wrapped around it. If you need to test the original model again or re-start with different parameters, **restart the notebook kernel**.\n",
    "\n",
    "⚡ If your loss tensor reports **NaN**, the training did not converge. Try reducing learning rate, introduce warm-up, change the order of training data, or other parameters (like LoRA rank or alpha). Make sure your model is loaded in **bfloat16**, because float16 does not always have sufficient range to cover all optimization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83284a93-f14e-4d92-8bff-fa6db4c993aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "from transformers import GemmaTokenizer, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32325982-1146-4e53-97e7-8942e2139230",
   "metadata": {},
   "source": [
    "# Load an LLM\n",
    "* This example uses [CodeGemma 1.1](https://huggingface.co/google/codegemma-1.1-2b)\n",
    "* We use the small 2B variant, note that fine-tuning larger models is much more memory-intensive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3806a2f6-572d-44a5-87e4-d4c48d65a71c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu_pytorch_tanh`, edit the `model.config` to set `hidden_activation=gelu_pytorch_tanh`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca1e22af742493f95b7aad3ad6beec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpu = torch.device('cuda:0')\n",
    "model_id = \"google/codegemma-1.1-2b\"\n",
    "tokenizer = GemmaTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=gpu, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70d0ebf-5287-4e92-b2af-14144fa43cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A standard greedy generation helper\n",
    "def generate(prompt, max_new_tokens=200):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(gpu)\n",
    "    outputs = model.generate(inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "debfc182-4ef7-4038-bdd5-1bef0a8b66b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos># <function name=\"test_http_404\">\n",
      "# <function name=\"test_http_404\">\n",
      "# <function name=\"test_http_404\">\n",
      "# <function name=\"test_http_404\">\n",
      "# <function name=\"test_http_404\">\n",
      "# <function name=\"test_http_404\">\n",
      "# <function name=\"test_http_404\">\n",
      "# <function name=\"test_http_404\">\n",
      "# <function name=\"test_http_404\">\n",
      "# <function name=\"test_http_404\">\n",
      "# <function name=\"test_http_404\">\n",
      "# <function name=\"test_http_404\">\n",
      "# <function name=\"test_http_404\">\n",
      "# <function name=\"test_http_404\">\n",
      "# <function name=\"test_http_404\">\n",
      "# <function name\n"
     ]
    }
   ],
   "source": [
    "# Test how the model responds to the desired prompt (probably not so well)\n",
    "print(generate('# <function name=\"test_http_404\">\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc1608-4055-4480-81a9-57bf78f70b52",
   "metadata": {},
   "source": [
    "# Data Procurement\n",
    "\n",
    "## Getting Raw Data\n",
    "This part requires the packages `autopep8`, `tree-sitter-python`, and `GitPython`.\n",
    "\n",
    "* We will parse all `.py` files from the current commit in the [Flask](https://github.com/pallets/flask) repository\n",
    "* Using a tree-sitter query, we extract all function definitions\n",
    "* We wrap them in the desired prompt format with XML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d8921f5-5e7a-4bf0-b5e6-6fa5c67daa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: autopep8 in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
      "Requirement already satisfied: tree-sitter in /usr/local/lib/python3.10/dist-packages (0.21.3)\n",
      "Requirement already satisfied: tree-sitter-python in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
      "Requirement already satisfied: GitPython in /usr/local/lib/python3.10/dist-packages (3.1.43)\n",
      "Requirement already satisfied: pycodestyle>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from autopep8) (2.11.1)\n",
      "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from autopep8) (2.0.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython) (4.0.11)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython) (5.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install autopep8 tree-sitter tree-sitter-python GitPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf3ec871-93c5-486b-a511-0828af25327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import autopep8\n",
    "import tree_sitter_python as tspython\n",
    "from tree_sitter import Language, Parser\n",
    "\n",
    "PY_LANGUAGE = Language(tspython.language(), \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e76a790f-321a-4bb2-9577-c5d6cdb16737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'flask.git' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone --bare https://github.com/pallets/flask.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "609a897b-ae0c-4d8d-9a67-1f703b3bdd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = git.Repo('./flask.git')\n",
    "tree = repo.head.commit.tree\n",
    "\n",
    "parser = Parser()\n",
    "parser.set_language(PY_LANGUAGE)\n",
    "query = PY_LANGUAGE.query('''(function_definition) @func''')\n",
    "\n",
    "files = [item.data_stream.read()\n",
    "         for item in tree.list_traverse()\n",
    "         if item.type == 'blob'\n",
    "         and item.name.endswith('.py')]\n",
    "\n",
    "def format_node(node):\n",
    "    code = autopep8.fix_code(node.text.decode('utf-8'))\n",
    "    name = node.child_by_field_name('name').text.decode('utf-8')\n",
    "    return f'# <function name=\"{name}\">\\n{code}# </function>'\n",
    "\n",
    "functions = [format_node(node)\n",
    "             for file in files\n",
    "             for node, _ in query.captures(parser.parse(file).root_node)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6125f6b2-db16-4d9b-a6b3-e5b9b8485e41",
   "metadata": {},
   "source": [
    "## Format Data for Training\n",
    "\n",
    "Next, we need to process the dataset into a format consumable by an LLM training procedure.\n",
    "* We demonstrate the use of the `datasets` library to deal with (possibly large) datasets\n",
    "* We **tokenize** our training examples\n",
    "* To evaluate whether training improves something, we split of a small **test set**, the remaining data is our **train set**\n",
    "* Training happens in **blocks of the same size**, so we split our training data into blocks of equal token numbers, possibly **padding** the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54b758f7-6bb3-4194-8ad5-a0aad7ec5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "816c9b02-740f-4477-adee-1c2b69d23fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.Dataset.from_dict({'source': functions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1bb2fd0-e46c-447a-beb2-566be8f4829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# <function name=\"test_multi_route_class_views\">\n",
      "def test_multi_route_class_views(app, client):\n",
      "    class View:\n",
      "        def __init__(self, app):\n",
      "            app.add_url_rule(\"/\", \"index\", self.index)\n",
      "            app.add_url_rule(\"/<test>/\", \"index\", self.index)\n",
      "\n",
      "        def index(self, test=\"a\"):\n",
      "            return test\n",
      "\n",
      "    _ = View(app)\n",
      "    rv = client.open(\"/\")\n",
      "    assert rv.data == b\"a\"\n",
      "    rv = client.open(\"/b/\")\n",
      "    assert rv.data == b\"b\"\n",
      "# </function>\n"
     ]
    }
   ],
   "source": [
    "print(data[300]['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "979059d6-eb46-47b2-84cf-f521b59676b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataset_row):\n",
    "    source = dataset_row['source']\n",
    "    input_ids = tokenizer.encode(source) + [tokenizer.eos_token_id]\n",
    "    labels = input_ids.copy()\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "489152d4-d28f-4a96-8e62-d2015b320960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11cfa56fbcf499a9dbd9ccdfb76af64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1406 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 1406\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = data.map(tokenize, remove_columns=['source'])\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b0f20ae-c05d-4ed0-9c4e-ce203c224da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(data, block_size=128):\n",
    "    '''Arranges a batch into blocks of given token number'''\n",
    "\n",
    "    # concatenate all items\n",
    "    concatenated = sum(data['input_ids'], [])\n",
    "    length = len(concatenated)\n",
    "\n",
    "    # shape \"n / block_size\" blocks\n",
    "    truncated_length = (length // block_size) * block_size\n",
    "    blocked_ids = [concatenated[i : i + block_size] for i in range(0, truncated_length, block_size)]\n",
    "\n",
    "    # add last block with padding\n",
    "    pad_length = block_size - (length % block_size)  # remaining tokens to fill\n",
    "    if pad_length != block_size:\n",
    "        blocked_ids += [concatenated[truncated_length:] + [tokenizer.eos_token_id] * pad_length]\n",
    "\n",
    "    # format as transformers-friendly model input\n",
    "    assert len(blocked_ids) > 0\n",
    "    return {\n",
    "        'input_ids': blocked_ids,\n",
    "        'labels': blocked_ids.copy()}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b78efe2-9869-4003-a1fa-77f64544852c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 235345, 968, 1929, 1503, 627, 12973, 235298, 3840, 768, 108, 1293, 23426, 235298, 3840, 235278, 1053, 235292, 34295, 11073, 1358, 235275, 3978, 1295, 235292, 108, 141, 773, 7426, 235298, 7242, 235278, 1053, 235265, 7242, 235298, 1067, 235275, 108, 235345, 2692, 1929, 235313, 1], [2, 235345, 968, 1929, 1503, 627, 2195, 235298, 2222, 235298, 502, 235298, 2457, 768, 108, 1293, 2121, 235298, 2222, 235298, 502, 235298, 2457, 235278, 1217, 1245, 108, 141, 1256, 589, 47895, 4980, 235278, 1217, 235269, 664, 2468, 1336, 7310, 235265, 872, 110007, 108, 141, 2994, 44431, 235265, 2457, 235298, 51294, 1159, 664, 2468, 235281, 108, 141, 2994, 44431, 235265, 5153, 1159, 664, 7310, 235265, 872, 235281, 108, 141, 2994, 44431, 235265, 4195, 235298, 4851, 1159, 4973, 108, 141, 2994, 44431, 235265, 2222, 1159, 50998, 108, 235345, 2692, 1929, 235313, 1]]\n",
      "44\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "split_dataset = tokenized_data.train_test_split(\n",
    "    test_size = 0.1,\n",
    "    shuffle = True,\n",
    "    seed = 421337)\n",
    "test_data = split_dataset['test']\n",
    "train_data = split_dataset['train']\n",
    "print(test_data['input_ids'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40742267-fc6d-43c3-92aa-a68e64f129c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16eefcab7fd438e9e433393b6bfdfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/141 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99ea82ac68142debde595e427d06704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1265 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 235345, 968, 1929, 1503, 627, 12973, 235298, 3840, 768, 108, 1293, 23426, 235298, 3840, 235278, 1053, 235292, 34295, 11073, 1358, 235275, 3978, 1295, 235292, 108, 141, 773, 7426, 235298, 7242, 235278, 1053, 235265, 7242, 235298, 1067, 235275, 108, 235345, 2692, 1929, 235313, 1, 2, 235345, 968, 1929, 1503, 627, 2195, 235298, 2222, 235298, 502, 235298, 2457, 768, 108, 1293, 2121, 235298, 2222, 235298, 502, 235298, 2457, 235278, 1217, 1245, 108, 141, 1256, 589, 47895, 4980, 235278, 1217, 235269, 664, 2468, 1336, 7310, 235265, 872, 110007, 108, 141, 2994, 44431, 235265, 2457, 235298, 51294, 1159, 664, 2468, 235281, 108, 141, 2994, 44431, 235265, 5153, 1159, 664, 7310, 235265, 872, 235281, 108, 141, 2994, 44431, 235265, 4195, 235298, 4851, 1159, 4973, 108, 141, 2994, 44431, 235265, 2222, 1159, 50998], [108, 235345, 2692, 1929, 235313, 1, 2, 235345, 968, 1929, 1503, 627, 3389, 235298, 1217, 768, 108, 1293, 8128, 235298, 1217, 235278, 1217, 1245, 108, 141, 1676, 11881, 8746, 7257, 675, 573, 57155, 1059, 235265, 1417, 603, 3151, 731, 108, 141, 1175, 4724, 11902, 235265, 108, 141, 1676, 108, 141, 1217, 235265, 197555, 235298, 1217, 3480, 235278, 3446, 235298, 2949, 235275, 108, 141, 1217, 235265, 10220, 235265, 1254, 235298, 6000, 235278, 3389, 235298, 2949, 235298, 6000, 235275, 108, 235345, 2692, 1929, 235313, 1, 2, 235345, 968, 1929, 1503, 31253, 2222, 235298, 502, 235298, 199935, 768, 108, 1293, 1762, 2222, 235298, 502, 235298, 199935, 235278, 2222, 235292, 1295, 235269, 1156, 235292, 1295, 235275, 3978, 6535, 235292, 108, 141, 1676, 12296, 30554, 2143, 1848, 578, 6504, 573, 4191, 576, 30554]]\n"
     ]
    }
   ],
   "source": [
    "test_data_blocks = test_data.map(block, batched=True)\n",
    "train_data_blocks = train_data.map(block, batched=True)\n",
    "print(test_data_blocks['input_ids'][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c75087-263e-43f3-8bb6-ec53d8584d77",
   "metadata": {},
   "source": [
    "# Fine-tuning using LoRA\n",
    "\n",
    "## Configuring the Training Procedure\n",
    "\n",
    "To perform training, we have to decide on a method, which will be LoRA in our case.\n",
    "* We configure a **LoRA adapter** which insertes the additional matrices into the model\n",
    "* We configure a **Collator** which takes care of correctly aligning our data in (GPU) memory\n",
    "* We configure a **DataLoader** which creates training batches from our collated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61f38b39-3486-4ab9-9f93-21a9cad39534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import get_linear_schedule_with_warmup, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0499b9a8-253e-4de8-8030-fe5eca218200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all model layers. We are looking for the names of the attention matrices (Q and V)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5a096-94d0-499c-a376-5b79f397aa08",
   "metadata": {},
   "source": [
    "### The LoRA Parameters\n",
    "For this experiment, we will \"factorize\" each matrix into rank 8 (`r = 8`), which means the diff to each m * n matrix is represented as the product of an n * 8 and an 8 * m matrix.\n",
    "We can also configure `alpha`, which is the degree to which the diff overrides the original matrix.\n",
    "**Note that `r` and `alpha` are first guesses. Depending on your task you may need to adjust them and see if performance improves!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e745c2d-3136-40b2-ae19-68eec2ecd9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # We want to predict the next token\n",
    "    inference_mode=False,\n",
    "    target_modules=['q_proj', 'v_proj'],   # this is specific to each model! Look up the exact names in the model\n",
    "    r=8,  # rank\n",
    "    lora_alpha=16,  # how strong does it override old values\n",
    "    lora_dropout=0.05,  # to help with generalization, 5% of updates to the LoRA weights are discarded\n",
    "    bias=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4634d0b5-bbcc-4554-a33e-f9410671ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This wraps our LLM into the adapter model\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0f70669-8bcd-47e6-8cc1-999b12963d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 921,600 || all params: 2,507,094,016 || trainable%: 0.036759690467068624\n"
     ]
    }
   ],
   "source": [
    "# Check how many paramaters we need to train (should be orders of magnitude fewer)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6fd8f-b4ec-428c-b197-401908a54284",
   "metadata": {},
   "source": [
    "### Formatting for Training\n",
    "\n",
    "* We need to decide on a **batch size**, the number of blocks we show and train in parallel each step. To save GPU memory, we stick to a low number. If you have space, increase that value.\n",
    "* We decide on a number of training **epochs**, how often we show all data to the model. As long as we see improvements, we can increase the number. For fine-tuning, a small number (1 - 3) is often okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f2f7812-009e-4ab6-84f8-ed4e88c4c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "816885de-f4fd-4b04-9964-fd26449397b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some book-keeping to make sure any padding happens with <EOS> tokens (not all tokenizers are meant to be used in fine-tuning and don't always have this right)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# special collator that takes care of correctly offsetting our data (so that token n predicts n+1) \n",
    "collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer,\n",
    "            mlm=False,  # we could also \"mask\" random tokens to introduce some noise, but we don't need that here\n",
    "            pad_to_multiple_of=8,\n",
    "            return_tensors=\"pt\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "379c5497-f0eb-4eb3-96a8-c7710f17b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data_blocks, collate_fn=collator, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(test_data_blocks, collate_fn=collator, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384dc586-e484-4e7f-b5b8-8bddc7430d54",
   "metadata": {},
   "source": [
    "### Configuring the Optimizer\n",
    "\n",
    "The optimizer is the core of our training. It updates all parameters according to their gradient and the loss they incurred.\n",
    "* We need to set a **learning rate**, which is the proportion of the gradient that gets added each step. This value tends to be low and needs experimentation to set right. We guess a value between 1/1000 and 1/10000 to start with.\n",
    "* The learning rate is changed over time by a **scheduler**. This ensures we \"converge\" over time by learning in smaller and smaller steps.\n",
    "* We could configure a **warmup** in which learning rate increases before it decreases again to make sure the model can \"settle\" a bit before full updates happen. We don't do this here, but if the training data is extremely different than what the model has been pre-trained on, we can \"stabilize\" training this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aeac76a4-b718-4bfe-a32a-12010d5e4e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)  # The AdamW optimizer is well-suited for LLMs\n",
    "\n",
    "# our schedule will decrease learning rate linearly with time\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cf0740-a0de-4901-9115-a74f52e6ae69",
   "metadata": {},
   "source": [
    "## The Training Loop\n",
    "\n",
    "Here, we rely on all the bits we configured above:\n",
    "* We set the model to **training mode**\n",
    "* Our **DataLoader** yields training batches\n",
    "* We **run** the batches through the model, the model remembers its gradients since it is in training mode and we obtain a **loss**\n",
    "* We **back-propagate the loss** through the model. Now each weight knows how much it contributed to the error in the model output.\n",
    "* The **Optimizer** updates the weights based on this information\n",
    "* The **Scheduler** updates the optimizer's learning rate\n",
    "* We repeat the above for every batch in the training set\n",
    "* To check whether we converge on the test data, we run each test batch through the model and average the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4c8e1da-e1e3-4b64-b9b9-32e7aa801483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 10/301 [00:06<03:08,  1.55it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch\u001b[38;5;241m.\u001b[39mto(gpu))  \u001b[38;5;66;03m# run batch through model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m----> 8\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()      \u001b[38;5;66;03m# propagate loss back\u001b[39;00m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()     \u001b[38;5;66;03m# update weights\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()  # set to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        outputs = model(**batch.to(gpu))  # run batch through model\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().cpu().float()\n",
    "        loss.backward()      # propagate loss back\n",
    "        optimizer.step()     # update weights\n",
    "        lr_scheduler.step()  # update learning rate\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()  # set to evaluation mode\n",
    "    eval_loss = 0\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch.to(gpu))\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().cpu().float()\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"{epoch=}: {train_epoch_loss=} {eval_epoch_loss=}\")\n",
    "\n",
    "    # save adapter to be loaded later\n",
    "    peft_model.save_pretrained(f'./my-checkpoint-ep{epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8817742-aeb2-4938-8ec1-c4a647a9d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate('# <function name=\"test_http_404\">\\n'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
