[{"e": "<|fim_prefix|>def start_detached(executable, *args):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        reader, writer = multiprocessing.Pipe(False)\n\n\n    multiprocessing.Process(\n        target=_start_detached,\n        args=(executable, *args),\n        kwargs={\"writer\": writer},\n        daemon=True,\n    ).start()\n\n    pid = reader.recv()\n    REGISTERED.append(pid)\n\n    writer.close()\n    reader.close()\n\n    return pid<|fim_middle|>Starts a fully independent subprocess (with no parent)\n:param executable: executable\n:param args: arguments to the executable, eg: ['--param1_key=param1_val', '-vvv' ...]\n:return: pid of the grandchild process<|file_separator|>", "l": true, "c": "def start_detached(executable, *args):\n    reader, writer = multiprocessing.Pipe(False)\n\n\n    multiprocessing.Process(\n        target=_start_detached,\n        args=(executable, *args),\n        kwargs={\"writer\": writer},\n        daemon=True,\n    ).start()\n\n    pid = reader.recv()\n    REGISTERED.append(pid)\n\n    writer.close()\n    reader.close()\n\n    return pid", "d": "Starts a fully independent subprocess (with no parent)\n:param executable: executable\n:param args: arguments to the executable, eg: ['--param1_key=param1_val', '-vvv' ...]\n:return: pid of the grandchild process"}, {"e": "<|fim_prefix|>def update_locations():\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if not Settings.log_location:\n        Settings.log_location = localize_path(\"logs\")\n\n\n    if not Settings.database_location:\n        Settings.database_location = localize_path(\"db\", \"instapy.db\")<|fim_middle|>As workspace has changed, locations also should be updated\n\nIf the user already has set a location, do not alter it<|file_separator|>", "l": true, "c": "def update_locations():\n    if not Settings.log_location:\n        Settings.log_location = localize_path(\"logs\")\n\n\n    if not Settings.database_location:\n        Settings.database_location = localize_path(\"db\", \"instapy.db\")", "d": "As workspace has changed, locations also should be updated\n\nIf the user already has set a location, do not alter it"}, {"e": "<|fim_prefix|>    def get_current_task(self) -> \"Task | None\":\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            for subtask in self.subtasks:\n            if subtask.state == IN_PROGRESS_STATE:\n                return subtask.get_current_task()\n        if self.state == IN_PROGRESS_STATE:\n            return self\n        return None<|fim_middle|>Retrieves the current task in progress.\n\nReturns:\n    The current task in progress, or None if no task is in progress.<|file_separator|>", "l": true, "c": "    def get_current_task(self) -> \"Task | None\":\n        for subtask in self.subtasks:\n            if subtask.state == IN_PROGRESS_STATE:\n                return subtask.get_current_task()\n        if self.state == IN_PROGRESS_STATE:\n            return self\n        return None", "d": "Retrieves the current task in progress.\n\nReturns:\n    The current task in progress, or None if no task is in progress."}, {"e": "<|fim_prefix|>def __init__(self, model_fpath: Path, verbose=True):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        self.model_fpath = model_fpath\n    self.verbose = verbose\n\n\n    if torch.cuda.is_available():\n        self.device = torch.device(\"cuda\")\n    else:\n        self.device = torch.device(\"cpu\")\n    if self.verbose:\n        print(\"Synthesizer using device:\", self.device)\n\n\n    self._model = None<|fim_middle|>The model isn't instantiated and loaded in memory until needed or until load() is called.\n\n:param model_fpath: path to the trained model file\n:param verbose: if False, prints less information when using the model<|file_separator|>", "l": true, "c": "def __init__(self, model_fpath: Path, verbose=True):\n    self.model_fpath = model_fpath\n    self.verbose = verbose\n\n\n    if torch.cuda.is_available():\n        self.device = torch.device(\"cuda\")\n    else:\n        self.device = torch.device(\"cpu\")\n    if self.verbose:\n        print(\"Synthesizer using device:\", self.device)\n\n\n    self._model = None", "d": "The model isn't instantiated and loaded in memory until needed or until load() is called.\n\n:param model_fpath: path to the trained model file\n:param verbose: if False, prints less information when using the model"}, {"e": "<|fim_prefix|>def extract_valid_urls(inputs: Union[str, list[str]]) -> Union[str, list[str], None]:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        url_pattern = re.compile(r\"https?://\\S+\")\n\n\n    if isinstance(inputs, str):\n        match = url_pattern.search(inputs)\n        return match.group(0) if match else None\n\n\n    elif isinstance(inputs, list):\n        valid_urls = []\n\n        for input_str in inputs:\n            matches = url_pattern.findall(input_str)\n            if matches:\n                valid_urls.extend(matches)\n\n        return valid_urls<|fim_middle|>\u4ece\u8f93\u5165\u4e2d\u63d0\u53d6\u6709\u6548\u7684URL (Extract valid URLs from input)\n\nArgs:\n    inputs (Union[str, list[str]]): \u8f93\u5165\u7684\u5b57\u7b26\u4e32\u6216\u5b57\u7b26\u4e32\u5217\u8868 (Input string or list of strings)\n\nReturns:\n    Union[str, list[str]]: \u63d0\u53d6\u51fa\u7684\u6709\u6548URL\u6216URL\u5217\u8868 (Extracted valid URL or list of URLs)<|file_separator|>", "l": true, "c": "def extract_valid_urls(inputs: Union[str, list[str]]) -> Union[str, list[str], None]:\n    url_pattern = re.compile(r\"https?://\\S+\")\n\n\n    if isinstance(inputs, str):\n        match = url_pattern.search(inputs)\n        return match.group(0) if match else None\n\n\n    elif isinstance(inputs, list):\n        valid_urls = []\n\n        for input_str in inputs:\n            matches = url_pattern.findall(input_str)\n            if matches:\n                valid_urls.extend(matches)\n\n        return valid_urls", "d": "\u4ece\u8f93\u5165\u4e2d\u63d0\u53d6\u6709\u6548\u7684URL (Extract valid URLs from input)\n\nArgs:\n    inputs (Union[str, list[str]]): \u8f93\u5165\u7684\u5b57\u7b26\u4e32\u6216\u5b57\u7b26\u4e32\u5217\u8868 (Input string or list of strings)\n\nReturns:\n    Union[str, list[str]]: \u63d0\u53d6\u51fa\u7684\u6709\u6548URL\u6216URL\u5217\u8868 (Extracted valid URL or list of URLs)"}, {"e": "<|fim_prefix|>def fbresnet18(pretrained=False, **kwargs):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        model = FBResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n    return model<|fim_middle|>Constructs a ResNet-18 model.\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet<|file_separator|>", "l": true, "c": "def fbresnet18(pretrained=False, **kwargs):\n    model = FBResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n    return model", "d": "Constructs a ResNet-18 model.\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet"}, {"e": "<|fim_prefix|>def test_conf_dict():\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        tpot_obj = TPOTClassifier(config_dict='TPOT light')\n    assert tpot_obj.config_dict == classifier_config_dict_light\n\n    tpot_obj = TPOTClassifier(config_dict='TPOT MDR')\n    assert tpot_obj.config_dict == tpot_mdr_classifier_config_dict\n\n    tpot_obj = TPOTRegressor(config_dict='TPOT light')\n    assert tpot_obj.config_dict == regressor_config_dict_light\n\n    assert_raises(TypeError, TPOTRegressor, config_dict='TPOT MDR')<|fim_middle|>Assert that TPOT uses the pre-configured dictionary of operators when config_dict is 'TPOT light' or 'TPOT MDR'.<|file_separator|>", "l": true, "c": "def test_conf_dict():\n    tpot_obj = TPOTClassifier(config_dict='TPOT light')\n    assert tpot_obj.config_dict == classifier_config_dict_light\n\n    tpot_obj = TPOTClassifier(config_dict='TPOT MDR')\n    assert tpot_obj.config_dict == tpot_mdr_classifier_config_dict\n\n    tpot_obj = TPOTRegressor(config_dict='TPOT light')\n    assert tpot_obj.config_dict == regressor_config_dict_light\n\n    assert_raises(TypeError, TPOTRegressor, config_dict='TPOT MDR')", "d": "Assert that TPOT uses the pre-configured dictionary of operators when config_dict is 'TPOT light' or 'TPOT MDR'."}, {"e": "<|fim_prefix|>def _log(self, data):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if self.log_manager is not None:\n        self.log_manager.log(data)<|fim_middle|>Log data using self.log_manager. <|file_separator|>", "l": true, "c": "def _log(self, data):\n    if self.log_manager is not None:\n        self.log_manager.log(data)", "d": "Log data using self.log_manager. "}, {"e": "<|fim_prefix|>def decrypt(config: iface.Config, ctext: Any) -> List[SearchLevel]:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        res: iface.SearchResult = config.objs[\"searcher\"].search(ctext)\n    if config.verbosity < 0:\n        return res.path[-1].result.value\n    else:\n        return iface.pretty_search_results(res)<|fim_middle|>A simple alias for searching a ctext and makes the answer pretty<|file_separator|>", "l": true, "c": "def decrypt(config: iface.Config, ctext: Any) -> List[SearchLevel]:\n    res: iface.SearchResult = config.objs[\"searcher\"].search(ctext)\n    if config.verbosity < 0:\n        return res.path[-1].result.value\n    else:\n        return iface.pretty_search_results(res)", "d": "A simple alias for searching a ctext and makes the answer pretty"}, {"e": "<|fim_prefix|>def check_existing_pt_files(opt):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        for t in ['train', 'valid', 'vocab']:\n        pattern = opt.save_data + '.' + t + '*.pt'\n        if glob.glob(pattern):\n            sys.stderr.write(\"Please backup existing pt file: %s, \"\n                             \"to avoid tampering!\\n\" % pattern)\n            sys.exit(1)<|fim_middle|>Checking if there are existing .pt files to avoid tampering <|file_separator|>", "l": true, "c": "def check_existing_pt_files(opt):\n    for t in ['train', 'valid', 'vocab']:\n        pattern = opt.save_data + '.' + t + '*.pt'\n        if glob.glob(pattern):\n            sys.stderr.write(\"Please backup existing pt file: %s, \"\n                             \"to avoid tampering!\\n\" % pattern)\n            sys.exit(1)", "d": "Checking if there are existing .pt files to avoid tampering "}, {"e": "<|fim_prefix|>def plot_arrow(x, y, yaw, length=1.0, width=0.5, fc=\"r\", ec=\"k\"):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        u\n    if not isinstance(x, float):\n        for (ix, iy, iyaw) in zip(x, y, yaw):\n            plot_arrow(ix, iy, iyaw)\n    else:\n        plt.arrow(x, y, length * math.cos(yaw), length * math.sin(yaw),\n                  fc=fc, ec=ec, head_width=width, head_length=width)\n        plt.plot(x, y)<|fim_middle|>Plot arrow<|file_separator|>", "l": true, "c": "def plot_arrow(x, y, yaw, length=1.0, width=0.5, fc=\"r\", ec=\"k\"):\n    u\n    if not isinstance(x, float):\n        for (ix, iy, iyaw) in zip(x, y, yaw):\n            plot_arrow(ix, iy, iyaw)\n    else:\n        plt.arrow(x, y, length * math.cos(yaw), length * math.sin(yaw),\n                  fc=fc, ec=ec, head_width=width, head_length=width)\n        plt.plot(x, y)", "d": "Plot arrow"}, {"e": "<|fim_prefix|>def parse_tag_name(v):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        base = v.split('+')[0].split('v')[-1]\n    int_parts = [int(part) for part in base.split('.')]\n    return int_parts<|fim_middle|>parses a version tag string formatted like 'vx.x.x'<|file_separator|>", "l": true, "c": "def parse_tag_name(v):\n    base = v.split('+')[0].split('v')[-1]\n    int_parts = [int(part) for part in base.split('.')]\n    return int_parts", "d": "parses a version tag string formatted like 'vx.x.x'"}, {"e": "<|fim_prefix|>def run_migrations_online() -> None:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        connectable = engine_from_config(\n        config.get_section(config.config_ini_section),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n\n    with connectable.connect() as connection:\n        context.configure(connection=connection, target_metadata=target_metadata)\n\n        with context.begin_transaction():\n            context.run_migrations()<|fim_middle|>Run migrations in 'online' mode.\n\nIn this scenario we need to create an Engine\nand associate a connection with the context.<|file_separator|>", "l": true, "c": "def run_migrations_online() -> None:\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n\n    with connectable.connect() as connection:\n        context.configure(connection=connection, target_metadata=target_metadata)\n\n        with context.begin_transaction():\n            context.run_migrations()", "d": "Run migrations in 'online' mode.\n\nIn this scenario we need to create an Engine\nand associate a connection with the context."}, {"e": "<|fim_prefix|>    def to_dict(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            output = asdict(self)\n        output[\"structure_module\"] = self.structure_module.to_dict()\n        return output<|fim_middle|>Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].\n\nReturns:\n    `Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,<|file_separator|>", "l": true, "c": "    def to_dict(self):\n        output = asdict(self)\n        output[\"structure_module\"] = self.structure_module.to_dict()\n        return output", "d": "Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].\n\nReturns:\n    `Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,"}, {"e": "<|fim_prefix|>def serving_method(self, images: list, top_k: int, **kwargs):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        top_k = int(top_k)\n    images_decode = [base64_to_cv2(image) for image in images]\n    resdicts = self.predict(images=images_decode, top_k=top_k, **kwargs)\n    final = {}\n    for resdict in resdicts:\n        for key, value in resdict.items():\n            resdict[key] = float(value)\n    final['data'] = resdicts\n    return final<|fim_middle|>Run as a service.<|file_separator|>", "l": true, "c": "def serving_method(self, images: list, top_k: int, **kwargs):\n    top_k = int(top_k)\n    images_decode = [base64_to_cv2(image) for image in images]\n    resdicts = self.predict(images=images_decode, top_k=top_k, **kwargs)\n    final = {}\n    for resdict in resdicts:\n        for key, value in resdict.items():\n            resdict[key] = float(value)\n    final['data'] = resdicts\n    return final", "d": "Run as a service."}, {"e": "<|fim_prefix|>def walk(df: pd.DataFrame, gid: tp.Union[int, str]=None, **kwargs):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        global global_gid\n    gid = kwargs.get('gid', None)\n    if gid is None:\n        gid = global_gid\n        global_gid += 1\n    props = get_props(df, **kwargs)\n    html = render_gwalker_html(gid, props)\n\n\n    display(HTML(html))<|fim_middle|>walk through pandas.DataFrame df with Graphic Walker\n\nArgs:\n    df (pd.DataFrame, optional): dataframe.\n    gid (tp.Union[int, str], optional): GraphicWalker container div's id ('gwalker-{gid}')\n    hideDataSourceConfig (bool, optional): Hide DataSource import and export button (True) or not (False). Default to True<|file_separator|>", "l": true, "c": "def walk(df: pd.DataFrame, gid: tp.Union[int, str]=None, **kwargs):\n    global global_gid\n    gid = kwargs.get('gid', None)\n    if gid is None:\n        gid = global_gid\n        global_gid += 1\n    props = get_props(df, **kwargs)\n    html = render_gwalker_html(gid, props)\n\n\n    display(HTML(html))", "d": "walk through pandas.DataFrame df with Graphic Walker\n\nArgs:\n    df (pd.DataFrame, optional): dataframe.\n    gid (tp.Union[int, str], optional): GraphicWalker container div's id ('gwalker-{gid}')\n    hideDataSourceConfig (bool, optional): Hide DataSource import and export button (True) or not (False). Default to True"}, {"e": "<|fim_prefix|>    def setModelTypeAsYOLOv3(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            self.__modelType = \"yolov3\"<|fim_middle|>'setModelTypeAsYOLOv3()' is used to set the model type to the YOLOv3 model\nfor the video object detection instance instance object .\n:return:<|file_separator|>", "l": true, "c": "    def setModelTypeAsYOLOv3(self):\n        self.__modelType = \"yolov3\"", "d": "'setModelTypeAsYOLOv3()' is used to set the model type to the YOLOv3 model\nfor the video object detection instance instance object .\n:return:"}, {"e": "<|fim_prefix|>def isident(s):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        first = string.ascii_uppercase + '_'\n    body = string.digits + first\n    if not s:\n        return False\n    if s[0] not in first:\n        return False\n    if not all(c in body for c in s[1:]):\n        return False\n    return True<|fim_middle|>Helper function to check whether a string is a valid identifier,\nas passed in on the command-line.<|file_separator|>", "l": true, "c": "def isident(s):\n    first = string.ascii_uppercase + '_'\n    body = string.digits + first\n    if not s:\n        return False\n    if s[0] not in first:\n        return False\n    if not all(c in body for c in s[1:]):\n        return False\n    return True", "d": "Helper function to check whether a string is a valid identifier,\nas passed in on the command-line."}, {"e": "<|fim_prefix|>def isAllowedToReexecute():\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return options.allow_reexecute<|fim_middle|>*bool* = **not** \"--must-not-re-execute\" <|file_separator|>", "l": true, "c": "def isAllowedToReexecute():\n    return options.allow_reexecute", "d": "*bool* = **not** \"--must-not-re-execute\" "}, {"e": "<|fim_prefix|>def fix_soup(soup, url, session=None, cw=None):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        print_ = get_print(cw)\n    if soup.find('div', class_='logo'):\n        return soup\n    print_('invalid soup: {}'.format(url))\n\n    res = clf2.solve(url, session=session, cw=cw)\n\n    return Soup(res['html'])<|fim_middle|>fix_soup<|file_separator|>", "l": true, "c": "def fix_soup(soup, url, session=None, cw=None):\n    print_ = get_print(cw)\n    if soup.find('div', class_='logo'):\n        return soup\n    print_('invalid soup: {}'.format(url))\n\n    res = clf2.solve(url, session=session, cw=cw)\n\n    return Soup(res['html'])", "d": "fix_soup"}, {"e": "<|fim_prefix|>    def __get_all_func_name__(cls) -> list[str]:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            return [name for name in dir(cls)\n        if not str(name).startswith('_') and callable(getattr(cls, name))]<|fim_middle|>Get all the function names of the class, excluding methods starting with '_' character.\n\nReturns:\n    list[str]: A list that contains function names.<|file_separator|>", "l": true, "c": "    def __get_all_func_name__(cls) -> list[str]:\n        return [name for name in dir(cls)\n        if not str(name).startswith('_') and callable(getattr(cls, name))]", "d": "Get all the function names of the class, excluding methods starting with '_' character.\n\nReturns:\n    list[str]: A list that contains function names."}, {"e": "<|fim_prefix|>def exception(_self, _message, *args, **kwargs):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        logger = _self.opt(exception=True, record=_self._record,\n                       lazy=_self._lazy, backframe=_self._backframe + 1)\n    logger._make_log_function(\"ERROR\")(logger, _message, *args, **kwargs)<|fim_middle|>Convenience method for logging an 'ERROR' with exception information.<|file_separator|>", "l": true, "c": "def exception(_self, _message, *args, **kwargs):\n    logger = _self.opt(exception=True, record=_self._record,\n                       lazy=_self._lazy, backframe=_self._backframe + 1)\n    logger._make_log_function(\"ERROR\")(logger, _message, *args, **kwargs)", "d": "Convenience method for logging an 'ERROR' with exception information."}, {"e": "<|fim_prefix|>    def get_download_url(self, video_url, watermark_flag):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            if watermark_flag == True:\n            download_url = video_url\n\n        else:\n            download_url = video_url.replace('playwm', 'play')\n\n        return download_url<|fim_middle|>\u83b7\u5f97\u5e26\u6c34\u5370\u7684\u89c6\u9891\u64ad\u653e\u5730\u5740\nParameters:\n    video_url\uff1a\u5e26\u6c34\u5370\u7684\u89c6\u9891\u64ad\u653e\u5730\u5740\nReturns:\n    download_url: \u5e26\u6c34\u5370\u7684\u89c6\u9891\u4e0b\u8f7d\u5730\u5740<|file_separator|>", "l": true, "c": "    def get_download_url(self, video_url, watermark_flag):\n        if watermark_flag == True:\n            download_url = video_url\n\n        else:\n            download_url = video_url.replace('playwm', 'play')\n\n        return download_url", "d": "\u83b7\u5f97\u5e26\u6c34\u5370\u7684\u89c6\u9891\u64ad\u653e\u5730\u5740\nParameters:\n    video_url\uff1a\u5e26\u6c34\u5370\u7684\u89c6\u9891\u64ad\u653e\u5730\u5740\nReturns:\n    download_url: \u5e26\u6c34\u5370\u7684\u89c6\u9891\u4e0b\u8f7d\u5730\u5740"}, {"e": "<|fim_prefix|>def imwrite(filename, img):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if PY2:\n        filename = filename.encode(sys.getfilesystemencoding())\n    pil_img = cv2_2_pil(img)\n    pil_img.save(filename, quality=10, optimize=True)<|fim_middle|>\u5199\u51fa\u56fe\u7247\u5230\u672c\u5730\u8def\u5f84\uff0c\u538b\u7f29<|file_separator|>", "l": true, "c": "def imwrite(filename, img):\n    if PY2:\n        filename = filename.encode(sys.getfilesystemencoding())\n    pil_img = cv2_2_pil(img)\n    pil_img.save(filename, quality=10, optimize=True)", "d": "\u5199\u51fa\u56fe\u7247\u5230\u672c\u5730\u8def\u5f84\uff0c\u538b\u7f29"}, {"e": "<|fim_prefix|>def get_token_gtk(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        self.session.get(self.root_url)\n    resp = self.session.get(self.root_url)\n    html_str = resp.content.decode()\n    token = re.findall(r\"token: '(.*?)'\", html_str)[0]\n    gtk = re.findall(r\"window.gtk = '(.*?)'\", html_str)[0]\n\n    return token,gtk<|fim_middle|>\u83b7\u53d6token\u548cgtk(\u7528\u4e8e\u5408\u6210Sign)<|file_separator|>", "l": true, "c": "def get_token_gtk(self):\n    self.session.get(self.root_url)\n    resp = self.session.get(self.root_url)\n    html_str = resp.content.decode()\n    token = re.findall(r\"token: '(.*?)'\", html_str)[0]\n    gtk = re.findall(r\"window.gtk = '(.*?)'\", html_str)[0]\n\n    return token,gtk", "d": "\u83b7\u53d6token\u548cgtk(\u7528\u4e8e\u5408\u6210Sign)"}, {"e": "<|fim_prefix|>def forward(self, x):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        self.extend_pe(x)\n    x = x + self.alpha * self.pe[:, : x.size(1)]\n    return self.dropout(x)<|fim_middle|>Add positional encoding.\nArgs:\n    x (torch.Tensor): Input tensor (batch, time, `*`).\nReturns:\n    torch.Tensor: Encoded tensor (batch, time, `*`).<|file_separator|>", "l": true, "c": "def forward(self, x):\n    self.extend_pe(x)\n    x = x + self.alpha * self.pe[:, : x.size(1)]\n    return self.dropout(x)", "d": "Add positional encoding.\nArgs:\n    x (torch.Tensor): Input tensor (batch, time, `*`).\nReturns:\n    torch.Tensor: Encoded tensor (batch, time, `*`)."}, {"e": "<|fim_prefix|>def loadDecryptView(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        self.viewDecrypt = decrypt.DecryptControl()\n    self.viewDecrypt.DecryptSignal.connect(self.loadMainWinView)\n    self.viewDecrypt.registerSignal.connect(self.loadRegisterView)\n    self.viewDecrypt.show()\n    self.viewDecrypt.db_exist()<|fim_middle|>\u767b\u5f55\u754c\u9762\n:return:<|file_separator|>", "l": true, "c": "def loadDecryptView(self):\n    self.viewDecrypt = decrypt.DecryptControl()\n    self.viewDecrypt.DecryptSignal.connect(self.loadMainWinView)\n    self.viewDecrypt.registerSignal.connect(self.loadRegisterView)\n    self.viewDecrypt.show()\n    self.viewDecrypt.db_exist()", "d": "\u767b\u5f55\u754c\u9762\n:return:"}, {"e": "<|fim_prefix|>    def __iter__(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            all_flows = self.metadata.get_object(\"root\", \"flow\", None, None)\n        all_flows = all_flows if all_flows else []\n        for flow in all_flows:\n            try:\n                v = Flow(_object=flow)\n                yield v\n            except MetaflowNamespaceMismatch:\n                continue<|fim_middle|>Iterator over all flows present.\n\nOnly flows present in the set namespace are returned. A flow is present in a namespace if\nit has at least one run that is in the namespace.\n\nYields\n-------\nFlow\n    A Flow present in the Metaflow universe.<|file_separator|>", "l": true, "c": "    def __iter__(self):\n        all_flows = self.metadata.get_object(\"root\", \"flow\", None, None)\n        all_flows = all_flows if all_flows else []\n        for flow in all_flows:\n            try:\n                v = Flow(_object=flow)\n                yield v\n            except MetaflowNamespaceMismatch:\n                continue", "d": "Iterator over all flows present.\n\nOnly flows present in the set namespace are returned. A flow is present in a namespace if\nit has at least one run that is in the namespace.\n\nYields\n-------\nFlow\n    A Flow present in the Metaflow universe."}, {"e": "<|fim_prefix|>def collect_static():\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        logging.info(\"Collect static files\")\n    try:\n        management.call_command('collectstatic', '--no-input', '-c', verbosity=0, interactive=False)\n        logging.info(\"Collect static files done\")\n    except:\n        pass<|fim_middle|> \u6536\u96c6\u9759\u6001\u6587\u4ef6\u5230\u6307\u5b9a\u76ee\u5f55\n \u672c\u9879\u76ee\u4e3b\u8981\u662f\u5c06\u524d\u7aefvue/dist\u7684\u524d\u6bb5\u9879\u76ee\u653e\u5230\u9759\u6001\u76ee\u5f55\u4e0b\u9762\n:return:<|file_separator|>", "l": true, "c": "def collect_static():\n    logging.info(\"Collect static files\")\n    try:\n        management.call_command('collectstatic', '--no-input', '-c', verbosity=0, interactive=False)\n        logging.info(\"Collect static files done\")\n    except:\n        pass", "d": " \u6536\u96c6\u9759\u6001\u6587\u4ef6\u5230\u6307\u5b9a\u76ee\u5f55\n \u672c\u9879\u76ee\u4e3b\u8981\u662f\u5c06\u524d\u7aefvue/dist\u7684\u524d\u6bb5\u9879\u76ee\u653e\u5230\u9759\u6001\u76ee\u5f55\u4e0b\u9762\n:return:"}, {"e": "<|fim_prefix|>    def cursor(self, *args, prefetch=None,\n               timeout=None) -> cursor.CursorInterface:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            self.__check_open()\n        return cursor.CursorInterface(self._connection, self._query,\n                                      self._state, args, prefetch,\n                                      timeout)<|fim_middle|>Return a *cursor interface* for the prepared statement.\n\n:param args: Query arguments.\n:param int prefetch: The number of rows the *cursor iterator*\n                     will prefetch (defaults to ``50``.)\n:param float timeout: Optional timeout in seconds.\n\n:return: A :class:`~cursor.CursorInterface` object.<|file_separator|>", "l": true, "c": "    def cursor(self, *args, prefetch=None,\n               timeout=None) -> cursor.CursorInterface:\n        self.__check_open()\n        return cursor.CursorInterface(self._connection, self._query,\n                                      self._state, args, prefetch,\n                                      timeout)", "d": "Return a *cursor interface* for the prepared statement.\n\n:param args: Query arguments.\n:param int prefetch: The number of rows the *cursor iterator*\n                     will prefetch (defaults to ``50``.)\n:param float timeout: Optional timeout in seconds.\n\n:return: A :class:`~cursor.CursorInterface` object."}, {"e": "<|fim_prefix|>def clean_dir(path: str) -> None:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        try:\n        if not os.path.exists(path):\n            os.mkdir(path)\n            logger.info(f\"Created directory: {path}\")\n\n        for file in os.listdir(path):\n            file_path = os.path.join(path, file)\n            os.remove(file_path)\n            logger.info(f\"Removed file: {file_path}\")\n\n        logger.info(colored(f\"Cleaned {path} directory\", \"green\"))\n    except Exception as e:\n        logger.error(f\"Error occurred while cleaning directory {path}: {str(e)}\")<|fim_middle|>Removes every file in a directory.\n\nArgs:\n    path (str): Path to directory.\n\nReturns:\n    None<|file_separator|>", "l": true, "c": "def clean_dir(path: str) -> None:\n    try:\n        if not os.path.exists(path):\n            os.mkdir(path)\n            logger.info(f\"Created directory: {path}\")\n\n        for file in os.listdir(path):\n            file_path = os.path.join(path, file)\n            os.remove(file_path)\n            logger.info(f\"Removed file: {file_path}\")\n\n        logger.info(colored(f\"Cleaned {path} directory\", \"green\"))\n    except Exception as e:\n        logger.error(f\"Error occurred while cleaning directory {path}: {str(e)}\")", "d": "Removes every file in a directory.\n\nArgs:\n    path (str): Path to directory.\n\nReturns:\n    None"}, {"e": "<|fim_prefix|>def get_batch(neox_args, context_tokens: torch.Tensor):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        tokens = context_tokens.contiguous().cuda()\n\n    attention_mask, _, position_ids = get_ltor_masks_and_position_ids(\n        tokens,\n        neox_args.tokenizer.eod_id,\n        neox_args.reset_position_ids,\n        neox_args.reset_attention_mask,\n        neox_args.eod_mask_loss)\n    return tokens, attention_mask, position_ids<|fim_middle|>Generate batch from context tokens. Attention mask and position ids are created. Returned tensors will be on CUDA.\n\nneox_args: instantiated NeoXArgs with tokenizer instantiated and reset_position_ids, reset_attention_mask and eod_mask_loss defined\ncontext_tokens: torch tensor with dimensions [batch, context_size]\n\nreturns: tuple of torch tensors (tokens, attention_mask, position_ids) on CUDA<|file_separator|>", "l": true, "c": "def get_batch(neox_args, context_tokens: torch.Tensor):\n    tokens = context_tokens.contiguous().cuda()\n\n    attention_mask, _, position_ids = get_ltor_masks_and_position_ids(\n        tokens,\n        neox_args.tokenizer.eod_id,\n        neox_args.reset_position_ids,\n        neox_args.reset_attention_mask,\n        neox_args.eod_mask_loss)\n    return tokens, attention_mask, position_ids", "d": "Generate batch from context tokens. Attention mask and position ids are created. Returned tensors will be on CUDA.\n\nneox_args: instantiated NeoXArgs with tokenizer instantiated and reset_position_ids, reset_attention_mask and eod_mask_loss defined\ncontext_tokens: torch tensor with dimensions [batch, context_size]\n\nreturns: tuple of torch tensors (tokens, attention_mask, position_ids) on CUDA"}, {"e": "<|fim_prefix|>def _load_from_state_dict(self, state_dict: Dict, prefix: str, *args: Any, **kwargs: Any) -> None:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        mapping = {\"gate.weight\": \"gate.linear.weight\"}\n    state_dict = map_old_state_dict_weights(state_dict, mapping, prefix)\n    super()._load_from_state_dict(state_dict, prefix, *args, **kwargs)<|fim_middle|>For compatibility with base checkpoints.<|file_separator|>", "l": true, "c": "def _load_from_state_dict(self, state_dict: Dict, prefix: str, *args: Any, **kwargs: Any) -> None:\n    mapping = {\"gate.weight\": \"gate.linear.weight\"}\n    state_dict = map_old_state_dict_weights(state_dict, mapping, prefix)\n    super()._load_from_state_dict(state_dict, prefix, *args, **kwargs)", "d": "For compatibility with base checkpoints."}, {"e": "<|fim_prefix|>    def _load_data(self, default: bool = False):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            self.settings.beginGroup('mainwindow')\n        if default:\n\n            self.settings.remove('mainwindow')\n\n\n\n\n\n        self.settings.endGroup()\n        self._late_update()<|fim_middle|>Load the data for this window\n\n(Only run right after window initialization or to reset settings)\n\nParameters:\n    default(bool):\n        Reset to the default settings<|file_separator|>", "l": true, "c": "    def _load_data(self, default: bool = False):\n        self.settings.beginGroup('mainwindow')\n        if default:\n\n            self.settings.remove('mainwindow')\n\n\n\n\n\n        self.settings.endGroup()\n        self._late_update()", "d": "Load the data for this window\n\n(Only run right after window initialization or to reset settings)\n\nParameters:\n    default(bool):\n        Reset to the default settings"}, {"e": "<|fim_prefix|>def index_modules(a_dict: object, keys: List[str]) -> object:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if len(keys) == 0:\n        return a_dict\n    return index_modules(a_dict=a_dict.__dict__[keys[0]], keys=keys[1:])<|fim_middle|>Recursively find a syft module from its path\n\nThis is the recursive inner function of index_syft_by_module_name.\nSee that method for a full description.\n\n:param a_dict: a module we're traversing\n:type a_dict: object\n:param keys: the list of string attributes we're using to traverse the module\n:type keys: List[str]\n:returns: a reference to the final object\n:rtype: object<|file_separator|>", "l": true, "c": "def index_modules(a_dict: object, keys: List[str]) -> object:\n    if len(keys) == 0:\n        return a_dict\n    return index_modules(a_dict=a_dict.__dict__[keys[0]], keys=keys[1:])", "d": "Recursively find a syft module from its path\n\nThis is the recursive inner function of index_syft_by_module_name.\nSee that method for a full description.\n\n:param a_dict: a module we're traversing\n:type a_dict: object\n:param keys: the list of string attributes we're using to traverse the module\n:type keys: List[str]\n:returns: a reference to the final object\n:rtype: object"}, {"e": "<|fim_prefix|>def standRegres(xArr,yArr):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        xMat = np.mat(xArr); yMat = np.mat(yArr).T\n    xTx = xMat.T * xMat\n    if np.linalg.det(xTx) == 0.0:\n        print(\"\u77e9\u9635\u4e3a\u5947\u5f02\u77e9\u9635,\u4e0d\u80fd\u8f6c\u7f6e\")\n        return\n    ws = xTx.I * (xMat.T*yMat)\n    return ws<|fim_middle|>\u51fd\u6570\u8bf4\u660e:\u8ba1\u7b97\u56de\u5f52\u7cfb\u6570w\nParameters:\n    xArr - x\u6570\u636e\u96c6\n    yArr - y\u6570\u636e\u96c6\nReturns:\n    ws - \u56de\u5f52\u7cfb\u6570\nWebsite:\n    http://www.cuijiahua.com/\nModify:\n    2017-11-19<|file_separator|>", "l": true, "c": "def standRegres(xArr,yArr):\n    xMat = np.mat(xArr); yMat = np.mat(yArr).T\n    xTx = xMat.T * xMat\n    if np.linalg.det(xTx) == 0.0:\n        print(\"\u77e9\u9635\u4e3a\u5947\u5f02\u77e9\u9635,\u4e0d\u80fd\u8f6c\u7f6e\")\n        return\n    ws = xTx.I * (xMat.T*yMat)\n    return ws", "d": "\u51fd\u6570\u8bf4\u660e:\u8ba1\u7b97\u56de\u5f52\u7cfb\u6570w\nParameters:\n    xArr - x\u6570\u636e\u96c6\n    yArr - y\u6570\u636e\u96c6\nReturns:\n    ws - \u56de\u5f52\u7cfb\u6570\nWebsite:\n    http://www.cuijiahua.com/\nModify:\n    2017-11-19"}, {"e": "<|fim_prefix|>def is_pixel_equal(self, img1, img2, x, y):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        pixel1 = img1.load()[x-1, y]\n    pixel2 = img2.load()[x-1, y]\n    threshold = 100\n    if abs(pixel1[0] - pixel2[0]) < threshold and abs(pixel1[1] - pixel2[1]) < threshold and abs(\n            pixel1[2] - pixel2[2]) < threshold:\n        return True\n    else:\n        return False<|fim_middle|>\u5224\u65ad\u4e24\u4e2a\u50cf\u7d20\u662f\u5426\u76f8\u540c\n:param img1: \u539f\u59cb\u9a8c\u8bc1\u7801\n:param img2: \u7f3a\u5757\u9a8c\u8bc1\u7801\n:param x: \u50cf\u7d20\u70b9\u7684x\u5750\u6807\n:param y: \u50cf\u7d20\u70b9\u7684y\u5750\u6807\n:return: \u50cf\u7d20\u662f\u5426\u76f8\u540c<|file_separator|>", "l": true, "c": "def is_pixel_equal(self, img1, img2, x, y):\n    pixel1 = img1.load()[x-1, y]\n    pixel2 = img2.load()[x-1, y]\n    threshold = 100\n    if abs(pixel1[0] - pixel2[0]) < threshold and abs(pixel1[1] - pixel2[1]) < threshold and abs(\n            pixel1[2] - pixel2[2]) < threshold:\n        return True\n    else:\n        return False", "d": "\u5224\u65ad\u4e24\u4e2a\u50cf\u7d20\u662f\u5426\u76f8\u540c\n:param img1: \u539f\u59cb\u9a8c\u8bc1\u7801\n:param img2: \u7f3a\u5757\u9a8c\u8bc1\u7801\n:param x: \u50cf\u7d20\u70b9\u7684x\u5750\u6807\n:param y: \u50cf\u7d20\u70b9\u7684y\u5750\u6807\n:return: \u50cf\u7d20\u662f\u5426\u76f8\u540c"}, {"e": "<|fim_prefix|>def update_master_grads(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if self.dynamic_loss_scale:\n        self._check_overflow()\n        if self.overflow: return\n    self._model_grads_to_master_grads()\n    self._downscale_master()<|fim_middle|>Copy the ``.grad`` attribute from stored references to fp16 parameters to \nthe ``.grad`` attribute of the fp32 master parameters that are directly \nupdated by the optimizer.  :attr:`update_master_grads` only needs to be called if\n``fp16_optimizer_obj.backward`` was called with ``update_master_grads=False``.<|file_separator|>", "l": true, "c": "def update_master_grads(self):\n    if self.dynamic_loss_scale:\n        self._check_overflow()\n        if self.overflow: return\n    self._model_grads_to_master_grads()\n    self._downscale_master()", "d": "Copy the ``.grad`` attribute from stored references to fp16 parameters to \nthe ``.grad`` attribute of the fp32 master parameters that are directly \nupdated by the optimizer.  :attr:`update_master_grads` only needs to be called if\n``fp16_optimizer_obj.backward`` was called with ``update_master_grads=False``."}, {"e": "<|fim_prefix|>    def load_model():\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            shared.sd_model\n\n        if modules.sd_hijack.current_optimizer is None:\n            modules.sd_hijack.apply_optimizations()<|fim_middle|>Accesses shared.sd_model property to load model.\nAfter it's available, if it has been loaded before this access by some extension,\nits optimization may be None because the list of optimizaers has neet been filled\nby that time, so we apply optimization again.<|file_separator|>", "l": true, "c": "    def load_model():\n        shared.sd_model\n\n        if modules.sd_hijack.current_optimizer is None:\n            modules.sd_hijack.apply_optimizations()", "d": "Accesses shared.sd_model property to load model.\nAfter it's available, if it has been loaded before this access by some extension,\nits optimization may be None because the list of optimizaers has neet been filled\nby that time, so we apply optimization again."}, {"e": "<|fim_prefix|>def test_completion(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        prompter = NoSystemPrompter()\n    strat = AlpacaPromptTokenizingStrategy(\n        prompter,\n        self.tokenizer,\n        False,\n        2048,\n    )\n    sample = {\n        \"instruction\": \"hello cruel. lorem ipsum dolor sit amet.\",\n        \"output\": \"world!\",\n    }\n    example = strat.tokenize_prompt(sample)\n    world_idx = example[\"input_ids\"].index(3186)\n    assert example[\"labels\"][world_idx] == 3186\n    assert example[\"labels\"][world_idx - 1] == -100<|fim_middle|>tests the interface between the user and assistant parts<|file_separator|>", "l": true, "c": "def test_completion(self):\n    prompter = NoSystemPrompter()\n    strat = AlpacaPromptTokenizingStrategy(\n        prompter,\n        self.tokenizer,\n        False,\n        2048,\n    )\n    sample = {\n        \"instruction\": \"hello cruel. lorem ipsum dolor sit amet.\",\n        \"output\": \"world!\",\n    }\n    example = strat.tokenize_prompt(sample)\n    world_idx = example[\"input_ids\"].index(3186)\n    assert example[\"labels\"][world_idx] == 3186\n    assert example[\"labels\"][world_idx - 1] == -100", "d": "tests the interface between the user and assistant parts"}, {"e": "<|fim_prefix|>def add_default_pipeline(cls, tasks_configs: Union[TaskConfig, List[TaskConfig]], **properties):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        pipeline_config = PipelineConfig(_Config.DEFAULT_KEY, tasks_configs, **properties)\n    cls._python_config.pipelines[_Config.DEFAULT_KEY] = pipeline_config\n    cls.__compile_configs()\n    return cls._applied_config.pipelines[_Config.DEFAULT_KEY]<|fim_middle|>Configures the default pipeline configuration.<|file_separator|>", "l": true, "c": "def add_default_pipeline(cls, tasks_configs: Union[TaskConfig, List[TaskConfig]], **properties):\n    pipeline_config = PipelineConfig(_Config.DEFAULT_KEY, tasks_configs, **properties)\n    cls._python_config.pipelines[_Config.DEFAULT_KEY] = pipeline_config\n    cls.__compile_configs()\n    return cls._applied_config.pipelines[_Config.DEFAULT_KEY]", "d": "Configures the default pipeline configuration."}, {"e": "<|fim_prefix|>    def forward(self, pred, gt, mask, weights=None):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            assert pred.shape == gt.shape\n        assert pred.shape == mask.shape\n        if weights is not None:\n            assert weights.shape == mask.shape\n            mask = weights * mask\n        intersection = paddle.sum(pred * gt * mask)\n\n        union = paddle.sum(pred * mask) + paddle.sum(gt * mask) + self.eps\n        loss = 1 - 2.0 * intersection / union\n        assert loss <= 1\n        return loss<|fim_middle|>DiceLoss function.<|file_separator|>", "l": true, "c": "    def forward(self, pred, gt, mask, weights=None):\n        assert pred.shape == gt.shape\n        assert pred.shape == mask.shape\n        if weights is not None:\n            assert weights.shape == mask.shape\n            mask = weights * mask\n        intersection = paddle.sum(pred * gt * mask)\n\n        union = paddle.sum(pred * mask) + paddle.sum(gt * mask) + self.eps\n        loss = 1 - 2.0 * intersection / union\n        assert loss <= 1\n        return loss", "d": "DiceLoss function."}, {"e": "<|fim_prefix|>def load_proxies() -> list:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        proxy_data = []\n    proxy_path = pathlib.Path(path, 'proxy.txt')\n    for line in open(proxy_path):\n        le = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n        if le == '':\n            continue\n        proxy_one = {\n            'all://': 'http://' + le\n        }\n        proxy_data.append(proxy_one)\n    return proxy_data<|fim_middle|>load proxies for proxy.txt\n:return: proxies list<|file_separator|>", "l": true, "c": "def load_proxies() -> list:\n    proxy_data = []\n    proxy_path = pathlib.Path(path, 'proxy.txt')\n    for line in open(proxy_path):\n        le = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n        if le == '':\n            continue\n        proxy_one = {\n            'all://': 'http://' + le\n        }\n        proxy_data.append(proxy_one)\n    return proxy_data", "d": "load proxies for proxy.txt\n:return: proxies list"}, {"e": "<|fim_prefix|>    def log_metrics(self, metrics, step=None):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            for k, v in metrics.items():\n            if isinstance(v, torch.Tensor):\n                metrics[k] = v.item()\n\n        if step is not None:\n            self.run.log(metrics, step=step)\n        else:\n            self.run.log(metrics)<|fim_middle|>Args:\n    metrics (dict): metrics dict.\n    step (int): step number.<|file_separator|>", "l": true, "c": "    def log_metrics(self, metrics, step=None):\n        for k, v in metrics.items():\n            if isinstance(v, torch.Tensor):\n                metrics[k] = v.item()\n\n        if step is not None:\n            self.run.log(metrics, step=step)\n        else:\n            self.run.log(metrics)", "d": "Args:\n    metrics (dict): metrics dict.\n    step (int): step number."}, {"e": "<|fim_prefix|>    def forward(self, enc_input, attn_bias):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            for i in range(self._n_layer):\n            enc_output = self._encoder_sublayers[i](enc_input, attn_bias)\n            enc_input = enc_output\n\n        return self._preprocess_layer(None, enc_output, self._preprocess_cmd,\n                                      self._prepostprocess_dropout)<|fim_middle|>forward\n:param enc_input:\n:param attn_bias:\n:return:<|file_separator|>", "l": true, "c": "    def forward(self, enc_input, attn_bias):\n        for i in range(self._n_layer):\n            enc_output = self._encoder_sublayers[i](enc_input, attn_bias)\n            enc_input = enc_output\n\n        return self._preprocess_layer(None, enc_output, self._preprocess_cmd,\n                                      self._prepostprocess_dropout)", "d": "forward\n:param enc_input:\n:param attn_bias:\n:return:"}, {"e": "<|fim_prefix|>def count_token(self, user_input):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        logging.warning(\"token count not implemented, using default\")\n    return len(user_input)<|fim_middle|>get token count from input, implement if needed<|file_separator|>", "l": true, "c": "def count_token(self, user_input):\n    logging.warning(\"token count not implemented, using default\")\n    return len(user_input)", "d": "get token count from input, implement if needed"}, {"e": "<|fim_prefix|>    def print_current_losses(self, epoch, iters, losses, t_comp, t_data, dataset='train'):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            message = '(dataset: %s, epoch: %d, iters: %d, time: %.3f, data: %.3f) ' % (\n            dataset, epoch, iters, t_comp, t_data)\n        for k, v in losses.items():\n            message += '%s: %.3f ' % (k, v)\n\n        print(message)\n        with open(self.log_name, \"a\") as log_file:\n            log_file.write('%s\\n' % message)<|fim_middle|>print current losses on console; also save the losses to the disk\n\nParameters:\n    epoch (int) -- current epoch\n    iters (int) -- current training iteration during this epoch (reset to 0 at the end of every epoch)\n    losses (OrderedDict) -- training losses stored in the format of (name, float) pairs\n    t_comp (float) -- computational time per data point (normalized by batch_size)\n    t_data (float) -- data loading time per data point (normalized by batch_size)<|file_separator|>", "l": true, "c": "    def print_current_losses(self, epoch, iters, losses, t_comp, t_data, dataset='train'):\n        message = '(dataset: %s, epoch: %d, iters: %d, time: %.3f, data: %.3f) ' % (\n            dataset, epoch, iters, t_comp, t_data)\n        for k, v in losses.items():\n            message += '%s: %.3f ' % (k, v)\n\n        print(message)\n        with open(self.log_name, \"a\") as log_file:\n            log_file.write('%s\\n' % message)", "d": "print current losses on console; also save the losses to the disk\n\nParameters:\n    epoch (int) -- current epoch\n    iters (int) -- current training iteration during this epoch (reset to 0 at the end of every epoch)\n    losses (OrderedDict) -- training losses stored in the format of (name, float) pairs\n    t_comp (float) -- computational time per data point (normalized by batch_size)\n    t_data (float) -- data loading time per data point (normalized by batch_size)"}, {"e": "<|fim_prefix|>def train(self, **kwargs):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        raise NotImplementedError(\"FastSAM models don't support training\")<|fim_middle|>Function trains models but raises an error as FastSAM models do not support training.<|file_separator|>", "l": true, "c": "def train(self, **kwargs):\n    raise NotImplementedError(\"FastSAM models don't support training\")", "d": "Function trains models but raises an error as FastSAM models do not support training."}, {"e": "<|fim_prefix|>    def __init__(self, max_val=1.0, use_mask=True):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            self.__max_val = max_val\n        self.__use_mask = use_mask<|fim_middle|>Init.\n\nArgs:\n    max_val (float, optional): Max output value. Defaults to 1.0.\n    use_mask (bool, optional): Only operate on valid pixels (mask == True). Defaults to True.<|file_separator|>", "l": true, "c": "    def __init__(self, max_val=1.0, use_mask=True):\n        self.__max_val = max_val\n        self.__use_mask = use_mask", "d": "Init.\n\nArgs:\n    max_val (float, optional): Max output value. Defaults to 1.0.\n    use_mask (bool, optional): Only operate on valid pixels (mask == True). Defaults to True."}, {"e": "<|fim_prefix|>def _count_token(self, messages) -> int:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        model = \"gpt-3.5-turbo-0301\"\n    tokens_per_message = (\n        4\n    )\n    tokens_per_name = -1\n    encoding = tiktoken.encoding_for_model(model)\n    num_tokens = 0\n    for message in messages:\n        num_tokens += tokens_per_message\n        for key, value in message.items():\n            num_tokens += len(encoding.encode(value))\n            if key == \"name\":\n                num_tokens += tokens_per_name\n    num_tokens += 3\n    return num_tokens<|fim_middle|>Count the number of tokens in the messages\nParameters\n----------\n    messages: a list of messages\nReturns\n-------\n    num_tokens: int<|file_separator|>", "l": true, "c": "def _count_token(self, messages) -> int:\n    model = \"gpt-3.5-turbo-0301\"\n    tokens_per_message = (\n        4\n    )\n    tokens_per_name = -1\n    encoding = tiktoken.encoding_for_model(model)\n    num_tokens = 0\n    for message in messages:\n        num_tokens += tokens_per_message\n        for key, value in message.items():\n            num_tokens += len(encoding.encode(value))\n            if key == \"name\":\n                num_tokens += tokens_per_name\n    num_tokens += 3\n    return num_tokens", "d": "Count the number of tokens in the messages\nParameters\n----------\n    messages: a list of messages\nReturns\n-------\n    num_tokens: int"}, {"e": "<|fim_prefix|>def _parse_num_range(s):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        range_re = re.compile(r'^(\\d+)-(\\d+)$')\n    m = range_re.match(s)\n    if m:\n        return range(int(m.group(1)), int(m.group(2))+1)\n    vals = s.split(',')\n    return [int(x) for x in vals]<|fim_middle|>Accept either a comma separated list of numbers 'a,b,c' or a range 'a-c' and return as a list of ints.<|file_separator|>", "l": true, "c": "def _parse_num_range(s):\n    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n    m = range_re.match(s)\n    if m:\n        return range(int(m.group(1)), int(m.group(2))+1)\n    vals = s.split(',')\n    return [int(x) for x in vals]", "d": "Accept either a comma separated list of numbers 'a,b,c' or a range 'a-c' and return as a list of ints."}, {"e": "<|fim_prefix|>def check_css(css):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        h1_rule, = css.matcher.lower_local_name_selectors['h1']\n    assert h1_rule[3] == 'before'\n    assert h1_rule[4][0][0] == 'content'\n    assert h1_rule[4][0][1][0][1] == 'I\u00a0l\u00f8v\u00eb Unicode'\n    assert h1_rule[4][1][0] == 'background_image'\n    assert h1_rule[4][1][1][0][0] == 'url'\n    assert h1_rule[4][1][1][0][1].startswith('file:')\n    assert h1_rule[4][1][1][0][1].endswith(\n        'weasyprint/tests/resources/pattern.png')<|fim_middle|>Check that a parsed stylsheet looks like resources/utf8-test.css<|file_separator|>", "l": true, "c": "def check_css(css):\n    h1_rule, = css.matcher.lower_local_name_selectors['h1']\n    assert h1_rule[3] == 'before'\n    assert h1_rule[4][0][0] == 'content'\n    assert h1_rule[4][0][1][0][1] == 'I\u00a0l\u00f8v\u00eb Unicode'\n    assert h1_rule[4][1][0] == 'background_image'\n    assert h1_rule[4][1][1][0][0] == 'url'\n    assert h1_rule[4][1][1][0][1].startswith('file:')\n    assert h1_rule[4][1][1][0][1].endswith(\n        'weasyprint/tests/resources/pattern.png')", "d": "Check that a parsed stylsheet looks like resources/utf8-test.css"}, {"e": "<|fim_prefix|>def _get_data(self, cmd: str) -> dict:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        model = (\n        self.openapi.get(\"paths\", {})\n        .get(f\"/api/v1{cmd}\", {})\n        .get(\"get\", {})\n        .get(\"model\")\n    )\n    if model:\n        schema = self.openapi[\"components\"][\"schemas\"][model][\"properties\"]\n        data = {}\n        for name, info in schema.items():\n            data[name] = {\n                \"description\": info.get(\"description\", \"\").replace(\"\\n\", \" \"),\n            }\n        return data\n    return {}<|fim_middle|>Get the data of the command from the openapi.<|file_separator|>", "l": true, "c": "def _get_data(self, cmd: str) -> dict:\n    model = (\n        self.openapi.get(\"paths\", {})\n        .get(f\"/api/v1{cmd}\", {})\n        .get(\"get\", {})\n        .get(\"model\")\n    )\n    if model:\n        schema = self.openapi[\"components\"][\"schemas\"][model][\"properties\"]\n        data = {}\n        for name, info in schema.items():\n            data[name] = {\n                \"description\": info.get(\"description\", \"\").replace(\"\\n\", \" \"),\n            }\n        return data\n    return {}", "d": "Get the data of the command from the openapi."}, {"e": "<|fim_prefix|>def prettify(self, elem):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        rough_string = ElementTree.tostring(elem, 'utf8')\n    root = etree.fromstring(rough_string)\n    return etree.tostring(root, pretty_print=True, encoding=ENCODE_METHOD).replace('  ', '\\t')\n\n    '''reparsed = minidom.parseString(rough_string)\n    return reparsed.toprettyxml(indent=\"\\t\", encoding=ENCODE_METHOD)'''<|fim_middle|>Return a pretty-printed XML string for the Element.<|file_separator|>", "l": true, "c": "def prettify(self, elem):\n    rough_string = ElementTree.tostring(elem, 'utf8')\n    root = etree.fromstring(rough_string)\n    return etree.tostring(root, pretty_print=True, encoding=ENCODE_METHOD).replace('  ', '\\t')\n\n    '''reparsed = minidom.parseString(rough_string)\n    return reparsed.toprettyxml(indent=\"\\t\", encoding=ENCODE_METHOD)'''", "d": "Return a pretty-printed XML string for the Element."}, {"e": "<|fim_prefix|>    def generate(self, x: Tensor) -> Tensor:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            return self.forward(x)[0]<|fim_middle|>Given an input image x, returns the reconstructed image\n:param x: (Tensor) [B x C x H x W]\n:return: (Tensor) [B x C x H x W]<|file_separator|>", "l": true, "c": "    def generate(self, x: Tensor) -> Tensor:\n        return self.forward(x)[0]", "d": "Given an input image x, returns the reconstructed image\n:param x: (Tensor) [B x C x H x W]\n:return: (Tensor) [B x C x H x W]"}, {"e": "<|fim_prefix|>def save_checkpoint(program, ckpt_name):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        ckpt_dir = os.path.join(cfg.TRAIN.MODEL_SAVE_DIR, str(ckpt_name))\n    print(\"Save model checkpoint to {}\".format(ckpt_dir))\n    if not os.path.isdir(ckpt_dir):\n        os.makedirs(ckpt_dir)\n\n    static.save(program, os.path.join(ckpt_dir, 'model'))\n\n    return ckpt_dir<|fim_middle|>Save checkpoint for evaluation or resume training<|file_separator|>", "l": true, "c": "def save_checkpoint(program, ckpt_name):\n    ckpt_dir = os.path.join(cfg.TRAIN.MODEL_SAVE_DIR, str(ckpt_name))\n    print(\"Save model checkpoint to {}\".format(ckpt_dir))\n    if not os.path.isdir(ckpt_dir):\n        os.makedirs(ckpt_dir)\n\n    static.save(program, os.path.join(ckpt_dir, 'model'))\n\n    return ckpt_dir", "d": "Save checkpoint for evaluation or resume training"}, {"e": "<|fim_prefix|>def unpad_input(hidden_states, attention_mask):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.torch.int32), (1, 0))\n\n\n\n\n\n    return (index_first_axis(rearrange(hidden_states, 'b s d -> (b s) d'), indices), indices,\n            cu_seqlens, max_seqlen_in_batch)<|fim_middle|>Arguments:\n    hidden_states: (batch, seqlen, dim)\n    attention_mask: (batch, seqlen), bool / int, 1 means valid and 0 means not valid.\nReturn:\n    hidden_states: (total_nnz, dim), where total_nnz = number of tokens in selected in attention_mask.\n    cu_seqlens: (batch + 1), the cumulative sequence lengths, used to index into hidden_states.\n    max_seqlen_in_batch: int<|file_separator|>", "l": true, "c": "def unpad_input(hidden_states, attention_mask):\n    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.torch.int32), (1, 0))\n\n\n\n\n\n    return (index_first_axis(rearrange(hidden_states, 'b s d -> (b s) d'), indices), indices,\n            cu_seqlens, max_seqlen_in_batch)", "d": "Arguments:\n    hidden_states: (batch, seqlen, dim)\n    attention_mask: (batch, seqlen), bool / int, 1 means valid and 0 means not valid.\nReturn:\n    hidden_states: (total_nnz, dim), where total_nnz = number of tokens in selected in attention_mask.\n    cu_seqlens: (batch + 1), the cumulative sequence lengths, used to index into hidden_states.\n    max_seqlen_in_batch: int"}, {"e": "<|fim_prefix|>def delete_album_thing_photo(cursor):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        SQL = \"\"\"\n        with {}\n        delete from api_albumthing_photos\n        where (albumthing_id,photo_id) not in ( select albumthing_id, photo_id from api_albumthing_photos_sql)\n    \"\"\".replace(\n        \"{}\", view_api_album_thing_photos_sql\n    )\n    cursor.execute(SQL)<|fim_middle|>This function delete photos form albums thing where thing disapears<|file_separator|>", "l": true, "c": "def delete_album_thing_photo(cursor):\n    SQL = \"\"\"\n        with {}\n        delete from api_albumthing_photos\n        where (albumthing_id,photo_id) not in ( select albumthing_id, photo_id from api_albumthing_photos_sql)\n    \"\"\".replace(\n        \"{}\", view_api_album_thing_photos_sql\n    )\n    cursor.execute(SQL)", "d": "This function delete photos form albums thing where thing disapears"}, {"e": "<|fim_prefix|>def world_size(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return self._world_size<|fim_middle|>Property returns the total number of GPUs.\nReturns:\n    Total number of GPUs.<|file_separator|>", "l": true, "c": "def world_size(self):\n    return self._world_size", "d": "Property returns the total number of GPUs.\nReturns:\n    Total number of GPUs."}, {"e": "<|fim_prefix|>def __call__(self, clip):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        clip_resize = resize_scale(clip=clip, target_size=self.size, interpolation_mode=self.interpolation_mode)\n    clip_center_crop = center_crop(clip_resize, self.size)\n    return clip_center_crop<|fim_middle|>Args:\n    clip (torch.tensor): Video clip to be cropped. Size is (T, C, H, W)\nReturns:\n    torch.tensor: scale resized / center cropped video clip.\n        size is (T, C, crop_size, crop_size)<|file_separator|>", "l": true, "c": "def __call__(self, clip):\n    clip_resize = resize_scale(clip=clip, target_size=self.size, interpolation_mode=self.interpolation_mode)\n    clip_center_crop = center_crop(clip_resize, self.size)\n    return clip_center_crop", "d": "Args:\n    clip (torch.tensor): Video clip to be cropped. Size is (T, C, H, W)\nReturns:\n    torch.tensor: scale resized / center cropped video clip.\n        size is (T, C, crop_size, crop_size)"}, {"e": "<|fim_prefix|>def wrap_details(title, body, wraplines=5):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        out = f\"\\n\\n\n    if body.count(\"\\n\") > wraplines:\n        out += \"\\n<details><summary>Click to expand</summary>\"\n    out += f\"\\n<p>\\n\\n{body.strip()}\\n\\n</p>\\n\"\n    if body.count(\"\\n\") > wraplines:\n        out += \"</details>\"\n    return out<|fim_middle|>Wrap lines into a <details> element if body is longer than `wraplines`<|file_separator|>", "l": true, "c": "def wrap_details(title, body, wraplines=5):\n    out = f\"\\n\\n\n    if body.count(\"\\n\") > wraplines:\n        out += \"\\n<details><summary>Click to expand</summary>\"\n    out += f\"\\n<p>\\n\\n{body.strip()}\\n\\n</p>\\n\"\n    if body.count(\"\\n\") > wraplines:\n        out += \"</details>\"\n    return out", "d": "Wrap lines into a <details> element if body is longer than `wraplines`"}, {"e": "<|fim_prefix|>  def detect(self, text):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        reliable, index, top_3_choices = cld2.detect(text, bestEffort=False)\n\n    if not reliable:\n      self.reliable = False\n      reliable, index, top_3_choices = cld2.detect(text, bestEffort=True)\n\n      if not self.quiet and not reliable:\n        raise UnknownLanguage(\"Try passing a longer snippet of text\")\n\n    self.languages = [Language(x) for x in top_3_choices]\n    self.language = self.languages[0]\n    return self.language<|fim_middle|>Decide which language is used to write the text.\nThe method tries first to detect the language with high reliability. If\nthat is not possible, the method switches to best effort strategy.\nArgs:\n  text (string): A snippet of text, the longer it is the more reliable we\n                 can detect the language used to write the text.<|file_separator|>", "l": true, "c": "  def detect(self, text):\n    reliable, index, top_3_choices = cld2.detect(text, bestEffort=False)\n\n    if not reliable:\n      self.reliable = False\n      reliable, index, top_3_choices = cld2.detect(text, bestEffort=True)\n\n      if not self.quiet and not reliable:\n        raise UnknownLanguage(\"Try passing a longer snippet of text\")\n\n    self.languages = [Language(x) for x in top_3_choices]\n    self.language = self.languages[0]\n    return self.language", "d": "Decide which language is used to write the text.\nThe method tries first to detect the language with high reliability. If\nthat is not possible, the method switches to best effort strategy.\nArgs:\n  text (string): A snippet of text, the longer it is the more reliable we\n                 can detect the language used to write the text."}, {"e": "<|fim_prefix|>    def encrypt(self, data, pad=None, padmode=None):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            data = self._guardAgainstUnicode(data)\n        if pad is not None:\n            pad = self._guardAgainstUnicode(pad)\n        data = self._padData(data, pad, padmode)\n        return self.crypt(data, des.ENCRYPT)<|fim_middle|>encrypt(data, [pad], [padmode]) -> bytes\n\ndata : Bytes to be encrypted\npad  : Optional argument for encryption padding. Must only be one byte\npadmode : Optional argument for overriding the padding mode.\n\nThe data must be a multiple of 8 bytes and will be encrypted\nwith the already specified key. Data does not have to be a\nmultiple of 8 bytes if the padding character is supplied, or\nthe padmode is set to PAD_PKCS5, as bytes will then added to\nensure the be padded data is a multiple of 8 bytes.<|file_separator|>", "l": true, "c": "    def encrypt(self, data, pad=None, padmode=None):\n        data = self._guardAgainstUnicode(data)\n        if pad is not None:\n            pad = self._guardAgainstUnicode(pad)\n        data = self._padData(data, pad, padmode)\n        return self.crypt(data, des.ENCRYPT)", "d": "encrypt(data, [pad], [padmode]) -> bytes\n\ndata : Bytes to be encrypted\npad  : Optional argument for encryption padding. Must only be one byte\npadmode : Optional argument for overriding the padding mode.\n\nThe data must be a multiple of 8 bytes and will be encrypted\nwith the already specified key. Data does not have to be a\nmultiple of 8 bytes if the padding character is supplied, or\nthe padmode is set to PAD_PKCS5, as bytes will then added to\nensure the be padded data is a multiple of 8 bytes."}, {"e": "<|fim_prefix|>def char_token(s: Text) -> List[Text]:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return list(s)<|fim_middle|>chinese charactor\nArgs:\n    s (Text): \"\u6211\u7231\u4e2d\u56fd\u201c\n\nReturns:\n    List[Text]: ['\u6211', '\u7231', '\u4e2d', '\u56fd']<|file_separator|>", "l": true, "c": "def char_token(s: Text) -> List[Text]:\n    return list(s)", "d": "chinese charactor\nArgs:\n    s (Text): \"\u6211\u7231\u4e2d\u56fd\u201c\n\nReturns:\n    List[Text]: ['\u6211', '\u7231', '\u4e2d', '\u56fd']"}, {"e": "<|fim_prefix|>def visible_pos(mesh, nd, ns):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        i = mtf.range(mesh, nd, tf.int32)[:, None]\n    j = mtf.range(mesh, ns, tf.int32)\n    m = i >= j - ns + nd\n    return m<|fim_middle|>1's in the lower triangle, counting from the lower right corner.\n\nSame as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n\nUPDATE: modified for mtf<|file_separator|>", "l": true, "c": "def visible_pos(mesh, nd, ns):\n    i = mtf.range(mesh, nd, tf.int32)[:, None]\n    j = mtf.range(mesh, ns, tf.int32)\n    m = i >= j - ns + nd\n    return m", "d": "1's in the lower triangle, counting from the lower right corner.\n\nSame as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n\nUPDATE: modified for mtf"}, {"e": "<|fim_prefix|>def uint82bin(n, count=8):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return ''.join([str((n >> y) & 1) for y in range(count-1, -1, -1)])<|fim_middle|>returns the binary of integer n, count refers to amount of bits<|file_separator|>", "l": true, "c": "def uint82bin(n, count=8):\n    return ''.join([str((n >> y) & 1) for y in range(count-1, -1, -1)])", "d": "returns the binary of integer n, count refers to amount of bits"}, {"e": "<|fim_prefix|>def preprocess_javascript(code):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        lines = code.split(\"\\n\")\n    processed_lines = []\n\n    for i, line in enumerate(lines, 1):\n\n        processed_lines.append(f'console.log(\"\n        processed_lines.append(line)\n\n\n    processed_code = \"\\n\".join(processed_lines)\n\n\n    processed_code = f\"\"\"\ntry {{\n{processed_code}\n}} catch (e) {{\n    console.log(e);\n}}\nconsole.log(\"\n\"\"\"\n\n    return processed_code<|fim_middle|>Add active line markers\nWrap in a try catch\nAdd end of execution marker<|file_separator|>", "l": true, "c": "def preprocess_javascript(code):\n    lines = code.split(\"\\n\")\n    processed_lines = []\n\n    for i, line in enumerate(lines, 1):\n\n        processed_lines.append(f'console.log(\"\n        processed_lines.append(line)\n\n\n    processed_code = \"\\n\".join(processed_lines)\n\n\n    processed_code = f\"\"\"\ntry {{\n{processed_code}\n}} catch (e) {{\n    console.log(e);\n}}\nconsole.log(\"\n\"\"\"\n\n    return processed_code", "d": "Add active line markers\nWrap in a try catch\nAdd end of execution marker"}, {"e": "<|fim_prefix|>def get_single_unit_from(script_args: List[Any], index: int=0) -> Optional[ControlNetUnit]:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        i = 0\n    while i < len(script_args) and index >= 0:\n        if type(script_args[i]) is bool:\n            if index == 0:\n                return ControlNetUnit(*script_args[i:i + PARAM_COUNT])\n            i += PARAM_COUNT\n\n        else:\n            if index == 0 and script_args[i] is not None:\n                return to_processing_unit(script_args[i])\n            i += 1\n\n        index -= 1\n\n    return None<|fim_middle|>Fetch a single ControlNet processing unit from ControlNet script arguments.\nThe list must not contain script positional arguments. It must only contain processing units.<|file_separator|>", "l": true, "c": "def get_single_unit_from(script_args: List[Any], index: int=0) -> Optional[ControlNetUnit]:\n    i = 0\n    while i < len(script_args) and index >= 0:\n        if type(script_args[i]) is bool:\n            if index == 0:\n                return ControlNetUnit(*script_args[i:i + PARAM_COUNT])\n            i += PARAM_COUNT\n\n        else:\n            if index == 0 and script_args[i] is not None:\n                return to_processing_unit(script_args[i])\n            i += 1\n\n        index -= 1\n\n    return None", "d": "Fetch a single ControlNet processing unit from ControlNet script arguments.\nThe list must not contain script positional arguments. It must only contain processing units."}, {"e": "<|fim_prefix|>def is_email_available() -> bool:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return bool(settings.EMAIL_HOST)<|fim_middle|>Returns whether email services are available on this instance (i.e. settings are in place).<|file_separator|>", "l": true, "c": "def is_email_available() -> bool:\n    return bool(settings.EMAIL_HOST)", "d": "Returns whether email services are available on this instance (i.e. settings are in place)."}, {"e": "<|fim_prefix|>def _get_cache_logic(self, cached_response: Any):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if cached_response is None:\n        return cached_response\n\n    cached_response = cached_response.decode(\"utf-8\")\n    try:\n        cached_response = json.loads(\n            cached_response\n        )\n    except:\n        cached_response = ast.literal_eval(cached_response)\n    return cached_response<|fim_middle|>Common 'get_cache_logic' across sync + async redis client implementations<|file_separator|>", "l": true, "c": "def _get_cache_logic(self, cached_response: Any):\n    if cached_response is None:\n        return cached_response\n\n    cached_response = cached_response.decode(\"utf-8\")\n    try:\n        cached_response = json.loads(\n            cached_response\n        )\n    except:\n        cached_response = ast.literal_eval(cached_response)\n    return cached_response", "d": "Common 'get_cache_logic' across sync + async redis client implementations"}, {"e": "<|fim_prefix|>    def copy(self) -> \"ManimConfig\":\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            return copy.deepcopy(self)<|fim_middle|>Deepcopy the contents of this ManimConfig.\n\nReturns\n-------\n:class:`ManimConfig`\n    A copy of this object containing no shared references.\n\nSee Also\n--------\n:func:`tempconfig`\n\nNotes\n-----\nThis is the main mechanism behind :func:`tempconfig`.<|file_separator|>", "l": true, "c": "    def copy(self) -> \"ManimConfig\":\n        return copy.deepcopy(self)", "d": "Deepcopy the contents of this ManimConfig.\n\nReturns\n-------\n:class:`ManimConfig`\n    A copy of this object containing no shared references.\n\nSee Also\n--------\n:func:`tempconfig`\n\nNotes\n-----\nThis is the main mechanism behind :func:`tempconfig`."}, {"e": "<|fim_prefix|>def vector_jacobian_product(fun, argnum=0):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        def vector_dot_fun(*args, **kwargs):\n        args, vector = args[:-1], args[-1]\n        return np.tensordot(fun(*args, **kwargs), vector, axes=vector.ndim)\n    return grad(vector_dot_fun, argnum)<|fim_middle|>Builds a function that returns the exact vector-Jacobian product, that\nis the Jacobian matrix left multiplied by vector. The returned function\nhas arguments (*args, vector, **kwargs).<|file_separator|>", "l": true, "c": "def vector_jacobian_product(fun, argnum=0):\n    def vector_dot_fun(*args, **kwargs):\n        args, vector = args[:-1], args[-1]\n        return np.tensordot(fun(*args, **kwargs), vector, axes=vector.ndim)\n    return grad(vector_dot_fun, argnum)", "d": "Builds a function that returns the exact vector-Jacobian product, that\nis the Jacobian matrix left multiplied by vector. The returned function\nhas arguments (*args, vector, **kwargs)."}, {"e": "<|fim_prefix|>def just(value):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        from hypothesis.searchstrategy.misc import JustStrategy\n\n    return JustStrategy(value)<|fim_middle|>Return a strategy which only generates ``value``.\n\nNote: ``value`` is not copied. Be wary of using mutable values.\n\nIf ``value`` is the result of a callable, you can use\n:func:`builds(callable) <hypothesis.strategies.builds>` instead\nof ``just(callable())`` to get a fresh value each time.\n\nExamples from this strategy do not shrink (because there is only one).<|file_separator|>", "l": true, "c": "def just(value):\n    from hypothesis.searchstrategy.misc import JustStrategy\n\n    return JustStrategy(value)", "d": "Return a strategy which only generates ``value``.\n\nNote: ``value`` is not copied. Be wary of using mutable values.\n\nIf ``value`` is the result of a callable, you can use\n:func:`builds(callable) <hypothesis.strategies.builds>` instead\nof ``just(callable())`` to get a fresh value each time.\n\nExamples from this strategy do not shrink (because there is only one)."}, {"e": "<|fim_prefix|>    def callback(self, position):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            callbacks = self.stage_config.get('callbacks', {})\n        callback = callbacks.get(position)\n\n        if callback:\n            (mod_name, cb_func) = callback.rsplit('.', 1)\n\n            module_ = importlib.import_module(mod_name)\n            getattr(module_, cb_func)(self)<|fim_middle|>Allows the execution of custom code between creation of the zip file and deployment to AWS.\n\n:return: None<|file_separator|>", "l": true, "c": "    def callback(self, position):\n        callbacks = self.stage_config.get('callbacks', {})\n        callback = callbacks.get(position)\n\n        if callback:\n            (mod_name, cb_func) = callback.rsplit('.', 1)\n\n            module_ = importlib.import_module(mod_name)\n            getattr(module_, cb_func)(self)", "d": "Allows the execution of custom code between creation of the zip file and deployment to AWS.\n\n:return: None"}, {"e": "<|fim_prefix|>def pos(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if 'pos' not in self.annotators:\n        return None\n    return [t[self.POS] for t in self.data]<|fim_middle|>Returns a list of part-of-speech tags of each token.\nReturns None if this annotation was not included.<|file_separator|>", "l": true, "c": "def pos(self):\n    if 'pos' not in self.annotators:\n        return None\n    return [t[self.POS] for t in self.data]", "d": "Returns a list of part-of-speech tags of each token.\nReturns None if this annotation was not included."}, {"e": "<|fim_prefix|>def is_subscriber(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return self.get_subscription().get('@type') != 'free'<|fim_middle|>status of subscription. True if device is connected to a paying\nsubscriber.<|file_separator|>", "l": true, "c": "def is_subscriber(self):\n    return self.get_subscription().get('@type') != 'free'", "d": "status of subscription. True if device is connected to a paying\nsubscriber."}, {"e": "<|fim_prefix|>def torch_to_im(img):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        img = to_numpy(img)\n    img = np.transpose(img, (1, 2, 0))\n    return img<|fim_middle|>Transform torch tensor to ndarray image.\nParameters\n----------\nimg: torch.Tensor\n    A tensor with shape: `(3, H, W)`.\nReturns\n-------\nnumpy.ndarray\n    An ndarray with shape: `(H, W, 3)`.<|file_separator|>", "l": true, "c": "def torch_to_im(img):\n    img = to_numpy(img)\n    img = np.transpose(img, (1, 2, 0))\n    return img", "d": "Transform torch tensor to ndarray image.\nParameters\n----------\nimg: torch.Tensor\n    A tensor with shape: `(3, H, W)`.\nReturns\n-------\nnumpy.ndarray\n    An ndarray with shape: `(H, W, 3)`."}, {"e": "<|fim_prefix|>def decode(self, input, *args, **kwargs ) -> Union[str, List[str]]:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if isinstance(input, list) and input and isinstance(input[0], list):\n        output = []\n        for single_input in input:\n            single_output = self.decode(single_input, *args, **kwargs)\n            output.append(single_output)\n        return output\n    else:\n\n        return self.tokenizer.decode(input, *args, **kwargs)<|fim_middle|>Perform decoding process of the tokenizer.\n\nParameters\n------------\ninputs : list.\n    The token sequence.\n    \nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.    \n\nReturns\n------------\noutputs :\n    The text decoded from the token inputs.<|file_separator|>", "l": true, "c": "def decode(self, input, *args, **kwargs ) -> Union[str, List[str]]:\n    if isinstance(input, list) and input and isinstance(input[0], list):\n        output = []\n        for single_input in input:\n            single_output = self.decode(single_input, *args, **kwargs)\n            output.append(single_output)\n        return output\n    else:\n\n        return self.tokenizer.decode(input, *args, **kwargs)", "d": "Perform decoding process of the tokenizer.\n\nParameters\n------------\ninputs : list.\n    The token sequence.\n    \nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.    \n\nReturns\n------------\noutputs :\n    The text decoded from the token inputs."}, {"e": "<|fim_prefix|>    def forward(self, x, step=None):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            x = self.proj(x).squeeze(2)\n        if self.position_encoding:\n            x = self.pe(x, step=step)\n\n        return x<|fim_middle|>Args:\n    x (FloatTensor): input, ``(len, batch, 1, vec_feats)``.\n\nReturns:\n    FloatTensor: embedded vecs ``(len, batch, embedding_size)``.<|file_separator|>", "l": true, "c": "    def forward(self, x, step=None):\n        x = self.proj(x).squeeze(2)\n        if self.position_encoding:\n            x = self.pe(x, step=step)\n\n        return x", "d": "Args:\n    x (FloatTensor): input, ``(len, batch, 1, vec_feats)``.\n\nReturns:\n    FloatTensor: embedded vecs ``(len, batch, embedding_size)``."}, {"e": "<|fim_prefix|>def __call__(self, im, im_info):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        im = im.transpose((2, 0, 1)).copy()\n    return im, im_info<|fim_middle|>Args:\n    im (np.ndarray): image (np.ndarray)\n    im_info (dict): info of image\nReturns:\n    im (np.ndarray):  processed image (np.ndarray)\n    im_info (dict): info of processed image<|file_separator|>", "l": true, "c": "def __call__(self, im, im_info):\n    im = im.transpose((2, 0, 1)).copy()\n    return im, im_info", "d": "Args:\n    im (np.ndarray): image (np.ndarray)\n    im_info (dict): info of image\nReturns:\n    im (np.ndarray):  processed image (np.ndarray)\n    im_info (dict): info of processed image"}]