[{"e": "<|fim_prefix|>def load_requirements(path_dir: str, file_name: str = \"base.txt\", unfreeze: str = \"all\") -> List[str]:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        assert unfreeze in {\"none\", \"major\", \"all\"}\n    path = Path(path_dir) / file_name\n    assert path.exists(), (path_dir, file_name, path)\n    text = path.read_text()\n    return [req.adjust(unfreeze) for req in _parse_requirements(text)]<|fim_middle|>Loading requirements from a file.\n\n>>> path_req = os.path.join(_PROJECT_ROOT, \"requirements\")\n>>> load_requirements(path_req, \"docs.txt\", unfreeze=\"major\")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n['sphinx<6.0,>=4.0', ...]<|file_separator|>", "l": true, "c": "def load_requirements(path_dir: str, file_name: str = \"base.txt\", unfreeze: str = \"all\") -> List[str]:\n    assert unfreeze in {\"none\", \"major\", \"all\"}\n    path = Path(path_dir) / file_name\n    assert path.exists(), (path_dir, file_name, path)\n    text = path.read_text()\n    return [req.adjust(unfreeze) for req in _parse_requirements(text)]", "d": "Loading requirements from a file.\n\n>>> path_req = os.path.join(_PROJECT_ROOT, \"requirements\")\n>>> load_requirements(path_req, \"docs.txt\", unfreeze=\"major\")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n['sphinx<6.0,>=4.0', ...]"}, {"e": "<|fim_prefix|>    def make_save_file_name(self, store_type: str) -> str:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            return f\"{self.csv_store_path}/{crawler_type_var.get()}_{store_type}_{utils.get_current_date()}.csv\"<|fim_middle|>make save file name by store type\nArgs:\n    store_type: contents or comments\n\nReturns: eg: data/xhs/search_comments_20240114.csv ...<|file_separator|>", "l": true, "c": "    def make_save_file_name(self, store_type: str) -> str:\n        return f\"{self.csv_store_path}/{crawler_type_var.get()}_{store_type}_{utils.get_current_date()}.csv\"", "d": "make save file name by store type\nArgs:\n    store_type: contents or comments\n\nReturns: eg: data/xhs/search_comments_20240114.csv ..."}, {"e": "<|fim_prefix|>    def __init__(self, cfg):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            BaseDataset.__init__(self, cfg)\n        self.dir_AB = os.path.join(cfg.dataroot, cfg.phase)\n        self.AB_paths = sorted(make_dataset(self.dir_AB, cfg.max_dataset_size))\n        assert(self.cfg.transform.load_size >= self.cfg.transform.crop_size)\n        self.input_nc = self.cfg.output_nc if self.cfg.direction == 'BtoA' else self.cfg.input_nc\n        self.output_nc = self.cfg.input_nc if self.cfg.direction == 'BtoA' else self.cfg.output_nc<|fim_middle|>Initialize this dataset class.\n\nArgs:\n    cfg (dict) -- stores all the experiment flags<|file_separator|>", "l": true, "c": "    def __init__(self, cfg):\n        BaseDataset.__init__(self, cfg)\n        self.dir_AB = os.path.join(cfg.dataroot, cfg.phase)\n        self.AB_paths = sorted(make_dataset(self.dir_AB, cfg.max_dataset_size))\n        assert(self.cfg.transform.load_size >= self.cfg.transform.crop_size)\n        self.input_nc = self.cfg.output_nc if self.cfg.direction == 'BtoA' else self.cfg.input_nc\n        self.output_nc = self.cfg.input_nc if self.cfg.direction == 'BtoA' else self.cfg.output_nc", "d": "Initialize this dataset class.\n\nArgs:\n    cfg (dict) -- stores all the experiment flags"}, {"e": "<|fim_prefix|>    def __init__(self, priors, iou_threshold, neg_pos_ratio,\n                 center_variance, size_variance, device):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            super(MultiboxLoss, self).__init__()\n        self.iou_threshold = iou_threshold\n        self.neg_pos_ratio = neg_pos_ratio\n        self.center_variance = center_variance\n        self.size_variance = size_variance\n        self.priors = priors\n        self.priors.to(device)<|fim_middle|>Implement SSD Multibox Loss.\n\nBasically, Multibox loss combines classification loss\n and Smooth L1 regression loss.<|file_separator|>", "l": true, "c": "    def __init__(self, priors, iou_threshold, neg_pos_ratio,\n                 center_variance, size_variance, device):\n        super(MultiboxLoss, self).__init__()\n        self.iou_threshold = iou_threshold\n        self.neg_pos_ratio = neg_pos_ratio\n        self.center_variance = center_variance\n        self.size_variance = size_variance\n        self.priors = priors\n        self.priors.to(device)", "d": "Implement SSD Multibox Loss.\n\nBasically, Multibox loss combines classification loss\n and Smooth L1 regression loss."}, {"e": "<|fim_prefix|>def data_shapes(self, val):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        self._data_shapes = val<|fim_middle|>doc<|file_separator|>", "l": true, "c": "def data_shapes(self, val):\n    self._data_shapes = val", "d": "doc"}, {"e": "<|fim_prefix|>def get_author_data(data):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        data = data[\"user\"]\n    uid = data[\"uid\"]\n    sec_uid = data[\"sec_uid\"]\n    short_id = data.get(\"short_id\", \"\")\n    unique_id = data.get(\"unique_id\", \"\")\n    user_age = data.get(\"user_age\", \"-1\")\n    signature = data.get(\"signature\", \"\")\n    nickname = data.get(\"nickname\", \"\u5df2\u6ce8\u9500\u8d26\u53f7\")\n    return uid, sec_uid, short_id, unique_id, user_age, signature, nickname<|fim_middle|>\u90e8\u5206\u6570\u636e\u9700\u8981\u5df2\u767b\u5f55\u7684 Cookie \u624d\u53ef\u83b7\u53d6<|file_separator|>", "l": true, "c": "def get_author_data(data):\n    data = data[\"user\"]\n    uid = data[\"uid\"]\n    sec_uid = data[\"sec_uid\"]\n    short_id = data.get(\"short_id\", \"\")\n    unique_id = data.get(\"unique_id\", \"\")\n    user_age = data.get(\"user_age\", \"-1\")\n    signature = data.get(\"signature\", \"\")\n    nickname = data.get(\"nickname\", \"\u5df2\u6ce8\u9500\u8d26\u53f7\")\n    return uid, sec_uid, short_id, unique_id, user_age, signature, nickname", "d": "\u90e8\u5206\u6570\u636e\u9700\u8981\u5df2\u767b\u5f55\u7684 Cookie \u624d\u53ef\u83b7\u53d6"}, {"e": "<|fim_prefix|>def test_rmse():\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        predictions = [pred(0, 0), pred(1, 1), pred(2, 2), pred(100, 100)]\n    assert recsys.accuracy.rmse(predictions) == 0\n\n    predictions = [pred(0, 0), pred(0, 2)]\n    assert recsys.accuracy.rmse(predictions) == sqrt((0 - 2)**2 / 2)\n\n    predictions = [pred(2, 0), pred(3, 4)]\n    assert recsys.accuracy.rmse(predictions) == sqrt(((2 - 0)**2 + (3 - 4)**2) / 2)\n\n    with pytest.raises(ValueError):\n        recsys.accuracy.rmse([])<|fim_middle|>Tests for the RMSE function.<|file_separator|>", "l": true, "c": "def test_rmse():\n    predictions = [pred(0, 0), pred(1, 1), pred(2, 2), pred(100, 100)]\n    assert recsys.accuracy.rmse(predictions) == 0\n\n    predictions = [pred(0, 0), pred(0, 2)]\n    assert recsys.accuracy.rmse(predictions) == sqrt((0 - 2)**2 / 2)\n\n    predictions = [pred(2, 0), pred(3, 4)]\n    assert recsys.accuracy.rmse(predictions) == sqrt(((2 - 0)**2 + (3 - 4)**2) / 2)\n\n    with pytest.raises(ValueError):\n        recsys.accuracy.rmse([])", "d": "Tests for the RMSE function."}, {"e": "<|fim_prefix|>def model_provider():\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        print_rank_0('building BERT model ...')\n\n    model = BertModel(\n        num_tokentypes=2,\n        add_binary_head=True,\n        parallel_output=True)\n\n    return model<|fim_middle|>Build the model.<|file_separator|>", "l": true, "c": "def model_provider():\n    print_rank_0('building BERT model ...')\n\n    model = BertModel(\n        num_tokentypes=2,\n        add_binary_head=True,\n        parallel_output=True)\n\n    return model", "d": "Build the model."}, {"e": "<|fim_prefix|>def make_ipaddress_url(raw_url: str):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        dot_count = raw_url.count(\".\")\n    if dot_count > 1:\n        raw_url_list = raw_url.split(\".\")\n        tmp_url = raw_url_list[-2] + \".\" + raw_url_list[-1]\n        ipaddress_url = \"https://\" + tmp_url + IPADDRESS_PREFIX + \"/\" + raw_url\n    else:\n        ipaddress_url = \"https://\" + raw_url + IPADDRESS_PREFIX\n    return ipaddress_url<|fim_middle|>\u751f\u6210 ipaddress \u5bf9\u5e94\u7684 url\n:param raw_url: \u539f\u59cb url\n:return: ipaddress \u7684 url<|file_separator|>", "l": true, "c": "def make_ipaddress_url(raw_url: str):\n    dot_count = raw_url.count(\".\")\n    if dot_count > 1:\n        raw_url_list = raw_url.split(\".\")\n        tmp_url = raw_url_list[-2] + \".\" + raw_url_list[-1]\n        ipaddress_url = \"https://\" + tmp_url + IPADDRESS_PREFIX + \"/\" + raw_url\n    else:\n        ipaddress_url = \"https://\" + raw_url + IPADDRESS_PREFIX\n    return ipaddress_url", "d": "\u751f\u6210 ipaddress \u5bf9\u5e94\u7684 url\n:param raw_url: \u539f\u59cb url\n:return: ipaddress \u7684 url"}, {"e": "<|fim_prefix|>def get_tree_hash(self, *args: List[bytes32]) -> bytes32:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        tmp = sexp_from_stream(io.BytesIO(self._buf), SExp.to)\n    return _tree_hash(tmp, set(args))<|fim_middle|>Any values in `args` that appear in the tree\nare presumed to have been hashed already.<|file_separator|>", "l": true, "c": "def get_tree_hash(self, *args: List[bytes32]) -> bytes32:\n    tmp = sexp_from_stream(io.BytesIO(self._buf), SExp.to)\n    return _tree_hash(tmp, set(args))", "d": "Any values in `args` that appear in the tree\nare presumed to have been hashed already."}, {"e": "<|fim_prefix|>    def daily(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            from module.reward.reward import Reward\n        az = Reward(self.config, device=self.device)\n        az.daily_wrapper_run()\n\n        self.reward_when_finished()<|fim_middle|>Method to run daily missions.<|file_separator|>", "l": true, "c": "    def daily(self):\n        from module.reward.reward import Reward\n        az = Reward(self.config, device=self.device)\n        az.daily_wrapper_run()\n\n        self.reward_when_finished()", "d": "Method to run daily missions."}, {"e": "<|fim_prefix|>def openHttp(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        self.gw.write_port(\"80\")\n    self.gw.write_domain(self.domain)<|fim_middle|>cloudfare cdn proxy 80 port<|file_separator|>", "l": true, "c": "def openHttp(self):\n    self.gw.write_port(\"80\")\n    self.gw.write_domain(self.domain)", "d": "cloudfare cdn proxy 80 port"}, {"e": "<|fim_prefix|>    def decode_greedy(self, text_index, length):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            texts = []\n        index = 0\n        for l in length:\n            t = text_index[index:index + l]\n\n            char_list = []\n            for i in range(l):\n\n                if t[i] not in self.ignore_idx and (not (i > 0 and t[i - 1] == t[i])):\n                    char_list.append(self.character[t[i]])\n            text = ''.join(char_list)\n\n            texts.append(text)\n            index += l\n        return texts<|fim_middle|>convert text-index into text-label. <|file_separator|>", "l": true, "c": "    def decode_greedy(self, text_index, length):\n        texts = []\n        index = 0\n        for l in length:\n            t = text_index[index:index + l]\n\n            char_list = []\n            for i in range(l):\n\n                if t[i] not in self.ignore_idx and (not (i > 0 and t[i - 1] == t[i])):\n                    char_list.append(self.character[t[i]])\n            text = ''.join(char_list)\n\n            texts.append(text)\n            index += l\n        return texts", "d": "convert text-index into text-label. "}, {"e": "<|fim_prefix|>def forward(self, x: Tensor) -> Tensor:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if torch.jit.is_scripting() or torch.jit.is_tracing():\n        return x * torch.sigmoid(x - 1.0)\n    return DoubleSwishFunction.apply(x)<|fim_middle|>Return double-swish activation function which is an approximation to Swish(Swish(x)),\nthat we approximate closely with x * sigmoid(x-1).<|file_separator|>", "l": true, "c": "def forward(self, x: Tensor) -> Tensor:\n    if torch.jit.is_scripting() or torch.jit.is_tracing():\n        return x * torch.sigmoid(x - 1.0)\n    return DoubleSwishFunction.apply(x)", "d": "Return double-swish activation function which is an approximation to Swish(Swish(x)),\nthat we approximate closely with x * sigmoid(x-1)."}, {"e": "<|fim_prefix|>def __getitem__(self, key):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        x, y = key\n    x -= self._x\n    y -= self._y\n    if x < 0 or x >= self._w or y < 0 or y >= self._h:\n        return 0\n    return self.__data[x + y * self._w]<|fim_middle|>Gets a pixel at (x, y).\n\nDefaults to 0 if out of bounds.<|file_separator|>", "l": true, "c": "def __getitem__(self, key):\n    x, y = key\n    x -= self._x\n    y -= self._y\n    if x < 0 or x >= self._w or y < 0 or y >= self._h:\n        return 0\n    return self.__data[x + y * self._w]", "d": "Gets a pixel at (x, y).\n\nDefaults to 0 if out of bounds."}, {"e": "<|fim_prefix|>def encode(matched, priors, variances):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]\n\n    g_cxcy /= (variances[0] * priors[:, 2:])\n\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = torch.log(g_wh) / variances[1]\n\n    return torch.cat([g_cxcy, g_wh], 1)<|fim_middle|>Encode the variances from the priorbox layers into the ground truth boxes\nwe have matched (based on jaccard overlap) with the prior boxes.\nArgs:\n    matched: (tensor) Coords of ground truth for each prior in point-form\n        Shape: [num_priors, 4].\n    priors: (tensor) Prior boxes in center-offset form\n        Shape: [num_priors,4].\n    variances: (list[float]) Variances of priorboxes\nReturn:\n    encoded boxes (tensor), Shape: [num_priors, 4]<|file_separator|>", "l": true, "c": "def encode(matched, priors, variances):\n    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]\n\n    g_cxcy /= (variances[0] * priors[:, 2:])\n\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = torch.log(g_wh) / variances[1]\n\n    return torch.cat([g_cxcy, g_wh], 1)", "d": "Encode the variances from the priorbox layers into the ground truth boxes\nwe have matched (based on jaccard overlap) with the prior boxes.\nArgs:\n    matched: (tensor) Coords of ground truth for each prior in point-form\n        Shape: [num_priors, 4].\n    priors: (tensor) Prior boxes in center-offset form\n        Shape: [num_priors,4].\n    variances: (list[float]) Variances of priorboxes\nReturn:\n    encoded boxes (tensor), Shape: [num_priors, 4]"}, {"e": "<|fim_prefix|>    def update_child_locals(self, locals_: Dict[str, Any]) -> None:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            for callback in self.callbacks:\n            callback.update_locals(locals_)<|fim_middle|>Update the references to the local variables.\n\n:param locals_: the local variables during rollout collection<|file_separator|>", "l": true, "c": "    def update_child_locals(self, locals_: Dict[str, Any]) -> None:\n        for callback in self.callbacks:\n            callback.update_locals(locals_)", "d": "Update the references to the local variables.\n\n:param locals_: the local variables during rollout collection"}, {"e": "<|fim_prefix|>def update_mobjects(self, dt):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        for mob in self.get_all_mobjects():\n        mob.update(dt)<|fim_middle|>Updates things like starting_mobject, and (for\nTransforms) target_mobject.  Note, since typically\n(always?) self.mobject will have its updating\nsuspended during the animation, this will do\nnothing to self.mobject.<|file_separator|>", "l": true, "c": "def update_mobjects(self, dt):\n    for mob in self.get_all_mobjects():\n        mob.update(dt)", "d": "Updates things like starting_mobject, and (for\nTransforms) target_mobject.  Note, since typically\n(always?) self.mobject will have its updating\nsuspended during the animation, this will do\nnothing to self.mobject."}, {"e": "<|fim_prefix|>    def _addSdFile(self, filename):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            self._sdFiles.append(filename)\n        self._sdFileAdded(filename)<|fim_middle|>Adds a file to the SD file list\n\n:param filename: the filename to add to the list<|file_separator|>", "l": true, "c": "    def _addSdFile(self, filename):\n        self._sdFiles.append(filename)\n        self._sdFileAdded(filename)", "d": "Adds a file to the SD file list\n\n:param filename: the filename to add to the list"}, {"e": "<|fim_prefix|>def run_test_case(objective, guideline):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        subprocess.run(['operate', '--prompt', f'\"{objective}\"'], stdout=subprocess.DEVNULL)\n\n    try:\n        result = evaluate_summary_screenshot(guideline)\n    except(OSError):\n        print(\"Couldn't open the summary screenshot\")\n        return False\n\n    return result<|fim_middle|>Returns True if the result of the test with the given prompt meets the given guideline.<|file_separator|>", "l": true, "c": "def run_test_case(objective, guideline):\n    subprocess.run(['operate', '--prompt', f'\"{objective}\"'], stdout=subprocess.DEVNULL)\n\n    try:\n        result = evaluate_summary_screenshot(guideline)\n    except(OSError):\n        print(\"Couldn't open the summary screenshot\")\n        return False\n\n    return result", "d": "Returns True if the result of the test with the given prompt meets the given guideline."}, {"e": "<|fim_prefix|>def load_requirements(path_dir: str, file_name: str = \"base.txt\", unfreeze: str = \"all\") -> List[str]:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        assert unfreeze in {\"none\", \"major\", \"all\"}\n    path = Path(path_dir) / file_name\n    assert path.exists(), (path_dir, file_name, path)\n    text = path.read_text()\n    return [req.adjust(unfreeze) for req in _parse_requirements(text)]<|fim_middle|>Encode the variances from the priorbox layers into the ground truth boxes\nwe have matched (based on jaccard overlap) with the prior boxes.\nArgs:\n    matched: (tensor) Coords of ground truth for each prior in point-form\n        Shape: [num_priors, 4].\n    priors: (tensor) Prior boxes in center-offset form\n        Shape: [num_priors,4].\n    variances: (list[float]) Variances of priorboxes\nReturn:\n    encoded boxes (tensor), Shape: [num_priors, 4]<|file_separator|>", "l": false, "c": "def load_requirements(path_dir: str, file_name: str = \"base.txt\", unfreeze: str = \"all\") -> List[str]:\n    assert unfreeze in {\"none\", \"major\", \"all\"}\n    path = Path(path_dir) / file_name\n    assert path.exists(), (path_dir, file_name, path)\n    text = path.read_text()\n    return [req.adjust(unfreeze) for req in _parse_requirements(text)]", "d": "Encode the variances from the priorbox layers into the ground truth boxes\nwe have matched (based on jaccard overlap) with the prior boxes.\nArgs:\n    matched: (tensor) Coords of ground truth for each prior in point-form\n        Shape: [num_priors, 4].\n    priors: (tensor) Prior boxes in center-offset form\n        Shape: [num_priors,4].\n    variances: (list[float]) Variances of priorboxes\nReturn:\n    encoded boxes (tensor), Shape: [num_priors, 4]"}, {"e": "<|fim_prefix|>    def make_save_file_name(self, store_type: str) -> str:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            return f\"{self.csv_store_path}/{crawler_type_var.get()}_{store_type}_{utils.get_current_date()}.csv\"<|fim_middle|>Gets a pixel at (x, y).\n\nDefaults to 0 if out of bounds.<|file_separator|>", "l": false, "c": "    def make_save_file_name(self, store_type: str) -> str:\n        return f\"{self.csv_store_path}/{crawler_type_var.get()}_{store_type}_{utils.get_current_date()}.csv\"", "d": "Gets a pixel at (x, y).\n\nDefaults to 0 if out of bounds."}, {"e": "<|fim_prefix|>    def __init__(self, cfg):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            BaseDataset.__init__(self, cfg)\n        self.dir_AB = os.path.join(cfg.dataroot, cfg.phase)\n        self.AB_paths = sorted(make_dataset(self.dir_AB, cfg.max_dataset_size))\n        assert(self.cfg.transform.load_size >= self.cfg.transform.crop_size)\n        self.input_nc = self.cfg.output_nc if self.cfg.direction == 'BtoA' else self.cfg.input_nc\n        self.output_nc = self.cfg.input_nc if self.cfg.direction == 'BtoA' else self.cfg.output_nc<|fim_middle|>Any values in `args` that appear in the tree\nare presumed to have been hashed already.<|file_separator|>", "l": false, "c": "    def __init__(self, cfg):\n        BaseDataset.__init__(self, cfg)\n        self.dir_AB = os.path.join(cfg.dataroot, cfg.phase)\n        self.AB_paths = sorted(make_dataset(self.dir_AB, cfg.max_dataset_size))\n        assert(self.cfg.transform.load_size >= self.cfg.transform.crop_size)\n        self.input_nc = self.cfg.output_nc if self.cfg.direction == 'BtoA' else self.cfg.input_nc\n        self.output_nc = self.cfg.input_nc if self.cfg.direction == 'BtoA' else self.cfg.output_nc", "d": "Any values in `args` that appear in the tree\nare presumed to have been hashed already."}, {"e": "<|fim_prefix|>    def __init__(self, priors, iou_threshold, neg_pos_ratio,\n                 center_variance, size_variance, device):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            super(MultiboxLoss, self).__init__()\n        self.iou_threshold = iou_threshold\n        self.neg_pos_ratio = neg_pos_ratio\n        self.center_variance = center_variance\n        self.size_variance = size_variance\n        self.priors = priors\n        self.priors.to(device)<|fim_middle|>Initialize this dataset class.\n\nArgs:\n    cfg (dict) -- stores all the experiment flags<|file_separator|>", "l": false, "c": "    def __init__(self, priors, iou_threshold, neg_pos_ratio,\n                 center_variance, size_variance, device):\n        super(MultiboxLoss, self).__init__()\n        self.iou_threshold = iou_threshold\n        self.neg_pos_ratio = neg_pos_ratio\n        self.center_variance = center_variance\n        self.size_variance = size_variance\n        self.priors = priors\n        self.priors.to(device)", "d": "Initialize this dataset class.\n\nArgs:\n    cfg (dict) -- stores all the experiment flags"}, {"e": "<|fim_prefix|>def data_shapes(self, val):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        self._data_shapes = val<|fim_middle|>\u90e8\u5206\u6570\u636e\u9700\u8981\u5df2\u767b\u5f55\u7684 Cookie \u624d\u53ef\u83b7\u53d6<|file_separator|>", "l": false, "c": "def data_shapes(self, val):\n    self._data_shapes = val", "d": "\u90e8\u5206\u6570\u636e\u9700\u8981\u5df2\u767b\u5f55\u7684 Cookie \u624d\u53ef\u83b7\u53d6"}, {"e": "<|fim_prefix|>def get_author_data(data):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        data = data[\"user\"]\n    uid = data[\"uid\"]\n    sec_uid = data[\"sec_uid\"]\n    short_id = data.get(\"short_id\", \"\")\n    unique_id = data.get(\"unique_id\", \"\")\n    user_age = data.get(\"user_age\", \"-1\")\n    signature = data.get(\"signature\", \"\")\n    nickname = data.get(\"nickname\", \"\u5df2\u6ce8\u9500\u8d26\u53f7\")\n    return uid, sec_uid, short_id, unique_id, user_age, signature, nickname<|fim_middle|>cloudfare cdn proxy 80 port<|file_separator|>", "l": false, "c": "def get_author_data(data):\n    data = data[\"user\"]\n    uid = data[\"uid\"]\n    sec_uid = data[\"sec_uid\"]\n    short_id = data.get(\"short_id\", \"\")\n    unique_id = data.get(\"unique_id\", \"\")\n    user_age = data.get(\"user_age\", \"-1\")\n    signature = data.get(\"signature\", \"\")\n    nickname = data.get(\"nickname\", \"\u5df2\u6ce8\u9500\u8d26\u53f7\")\n    return uid, sec_uid, short_id, unique_id, user_age, signature, nickname", "d": "cloudfare cdn proxy 80 port"}, {"e": "<|fim_prefix|>def test_rmse():\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        predictions = [pred(0, 0), pred(1, 1), pred(2, 2), pred(100, 100)]\n    assert recsys.accuracy.rmse(predictions) == 0\n\n    predictions = [pred(0, 0), pred(0, 2)]\n    assert recsys.accuracy.rmse(predictions) == sqrt((0 - 2)**2 / 2)\n\n    predictions = [pred(2, 0), pred(3, 4)]\n    assert recsys.accuracy.rmse(predictions) == sqrt(((2 - 0)**2 + (3 - 4)**2) / 2)\n\n    with pytest.raises(ValueError):\n        recsys.accuracy.rmse([])<|fim_middle|>Adds a file to the SD file list\n\n:param filename: the filename to add to the list<|file_separator|>", "l": false, "c": "def test_rmse():\n    predictions = [pred(0, 0), pred(1, 1), pred(2, 2), pred(100, 100)]\n    assert recsys.accuracy.rmse(predictions) == 0\n\n    predictions = [pred(0, 0), pred(0, 2)]\n    assert recsys.accuracy.rmse(predictions) == sqrt((0 - 2)**2 / 2)\n\n    predictions = [pred(2, 0), pred(3, 4)]\n    assert recsys.accuracy.rmse(predictions) == sqrt(((2 - 0)**2 + (3 - 4)**2) / 2)\n\n    with pytest.raises(ValueError):\n        recsys.accuracy.rmse([])", "d": "Adds a file to the SD file list\n\n:param filename: the filename to add to the list"}, {"e": "<|fim_prefix|>def model_provider():\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        print_rank_0('building BERT model ...')\n\n    model = BertModel(\n        num_tokentypes=2,\n        add_binary_head=True,\n        parallel_output=True)\n\n    return model<|fim_middle|>make save file name by store type\nArgs:\n    store_type: contents or comments\n\nReturns: eg: data/xhs/search_comments_20240114.csv ...<|file_separator|>", "l": false, "c": "def model_provider():\n    print_rank_0('building BERT model ...')\n\n    model = BertModel(\n        num_tokentypes=2,\n        add_binary_head=True,\n        parallel_output=True)\n\n    return model", "d": "make save file name by store type\nArgs:\n    store_type: contents or comments\n\nReturns: eg: data/xhs/search_comments_20240114.csv ..."}, {"e": "<|fim_prefix|>def make_ipaddress_url(raw_url: str):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        dot_count = raw_url.count(\".\")\n    if dot_count > 1:\n        raw_url_list = raw_url.split(\".\")\n        tmp_url = raw_url_list[-2] + \".\" + raw_url_list[-1]\n        ipaddress_url = \"https://\" + tmp_url + IPADDRESS_PREFIX + \"/\" + raw_url\n    else:\n        ipaddress_url = \"https://\" + raw_url + IPADDRESS_PREFIX\n    return ipaddress_url<|fim_middle|>Tests for the RMSE function.<|file_separator|>", "l": false, "c": "def make_ipaddress_url(raw_url: str):\n    dot_count = raw_url.count(\".\")\n    if dot_count > 1:\n        raw_url_list = raw_url.split(\".\")\n        tmp_url = raw_url_list[-2] + \".\" + raw_url_list[-1]\n        ipaddress_url = \"https://\" + tmp_url + IPADDRESS_PREFIX + \"/\" + raw_url\n    else:\n        ipaddress_url = \"https://\" + raw_url + IPADDRESS_PREFIX\n    return ipaddress_url", "d": "Tests for the RMSE function."}, {"e": "<|fim_prefix|>def get_tree_hash(self, *args: List[bytes32]) -> bytes32:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        tmp = sexp_from_stream(io.BytesIO(self._buf), SExp.to)\n    return _tree_hash(tmp, set(args))<|fim_middle|>convert text-index into text-label. <|file_separator|>", "l": false, "c": "def get_tree_hash(self, *args: List[bytes32]) -> bytes32:\n    tmp = sexp_from_stream(io.BytesIO(self._buf), SExp.to)\n    return _tree_hash(tmp, set(args))", "d": "convert text-index into text-label. "}, {"e": "<|fim_prefix|>    def daily(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            from module.reward.reward import Reward\n        az = Reward(self.config, device=self.device)\n        az.daily_wrapper_run()\n\n        self.reward_when_finished()<|fim_middle|>Build the model.<|file_separator|>", "l": false, "c": "    def daily(self):\n        from module.reward.reward import Reward\n        az = Reward(self.config, device=self.device)\n        az.daily_wrapper_run()\n\n        self.reward_when_finished()", "d": "Build the model."}, {"e": "<|fim_prefix|>def openHttp(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        self.gw.write_port(\"80\")\n    self.gw.write_domain(self.domain)<|fim_middle|>\u751f\u6210 ipaddress \u5bf9\u5e94\u7684 url\n:param raw_url: \u539f\u59cb url\n:return: ipaddress \u7684 url<|file_separator|>", "l": false, "c": "def openHttp(self):\n    self.gw.write_port(\"80\")\n    self.gw.write_domain(self.domain)", "d": "\u751f\u6210 ipaddress \u5bf9\u5e94\u7684 url\n:param raw_url: \u539f\u59cb url\n:return: ipaddress \u7684 url"}, {"e": "<|fim_prefix|>    def decode_greedy(self, text_index, length):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            texts = []\n        index = 0\n        for l in length:\n            t = text_index[index:index + l]\n\n            char_list = []\n            for i in range(l):\n\n                if t[i] not in self.ignore_idx and (not (i > 0 and t[i - 1] == t[i])):\n                    char_list.append(self.character[t[i]])\n            text = ''.join(char_list)\n\n            texts.append(text)\n            index += l\n        return texts<|fim_middle|>Method to run daily missions.<|file_separator|>", "l": false, "c": "    def decode_greedy(self, text_index, length):\n        texts = []\n        index = 0\n        for l in length:\n            t = text_index[index:index + l]\n\n            char_list = []\n            for i in range(l):\n\n                if t[i] not in self.ignore_idx and (not (i > 0 and t[i - 1] == t[i])):\n                    char_list.append(self.character[t[i]])\n            text = ''.join(char_list)\n\n            texts.append(text)\n            index += l\n        return texts", "d": "Method to run daily missions."}, {"e": "<|fim_prefix|>def forward(self, x: Tensor) -> Tensor:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if torch.jit.is_scripting() or torch.jit.is_tracing():\n        return x * torch.sigmoid(x - 1.0)\n    return DoubleSwishFunction.apply(x)<|fim_middle|>Update the references to the local variables.\n\n:param locals_: the local variables during rollout collection<|file_separator|>", "l": false, "c": "def forward(self, x: Tensor) -> Tensor:\n    if torch.jit.is_scripting() or torch.jit.is_tracing():\n        return x * torch.sigmoid(x - 1.0)\n    return DoubleSwishFunction.apply(x)", "d": "Update the references to the local variables.\n\n:param locals_: the local variables during rollout collection"}, {"e": "<|fim_prefix|>def __getitem__(self, key):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        x, y = key\n    x -= self._x\n    y -= self._y\n    if x < 0 or x >= self._w or y < 0 or y >= self._h:\n        return 0\n    return self.__data[x + y * self._w]<|fim_middle|>doc<|file_separator|>", "l": false, "c": "def __getitem__(self, key):\n    x, y = key\n    x -= self._x\n    y -= self._y\n    if x < 0 or x >= self._w or y < 0 or y >= self._h:\n        return 0\n    return self.__data[x + y * self._w]", "d": "doc"}, {"e": "<|fim_prefix|>def encode(matched, priors, variances):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]\n\n    g_cxcy /= (variances[0] * priors[:, 2:])\n\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = torch.log(g_wh) / variances[1]\n\n    return torch.cat([g_cxcy, g_wh], 1)<|fim_middle|>Loading requirements from a file.\n\n>>> path_req = os.path.join(_PROJECT_ROOT, \"requirements\")\n>>> load_requirements(path_req, \"docs.txt\", unfreeze=\"major\")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n['sphinx<6.0,>=4.0', ...]<|file_separator|>", "l": false, "c": "def encode(matched, priors, variances):\n    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]\n\n    g_cxcy /= (variances[0] * priors[:, 2:])\n\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = torch.log(g_wh) / variances[1]\n\n    return torch.cat([g_cxcy, g_wh], 1)", "d": "Loading requirements from a file.\n\n>>> path_req = os.path.join(_PROJECT_ROOT, \"requirements\")\n>>> load_requirements(path_req, \"docs.txt\", unfreeze=\"major\")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n['sphinx<6.0,>=4.0', ...]"}, {"e": "<|fim_prefix|>    def update_child_locals(self, locals_: Dict[str, Any]) -> None:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            for callback in self.callbacks:\n            callback.update_locals(locals_)<|fim_middle|>Returns True if the result of the test with the given prompt meets the given guideline.<|file_separator|>", "l": false, "c": "    def update_child_locals(self, locals_: Dict[str, Any]) -> None:\n        for callback in self.callbacks:\n            callback.update_locals(locals_)", "d": "Returns True if the result of the test with the given prompt meets the given guideline."}, {"e": "<|fim_prefix|>def update_mobjects(self, dt):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        for mob in self.get_all_mobjects():\n        mob.update(dt)<|fim_middle|>Return double-swish activation function which is an approximation to Swish(Swish(x)),\nthat we approximate closely with x * sigmoid(x-1).<|file_separator|>", "l": false, "c": "def update_mobjects(self, dt):\n    for mob in self.get_all_mobjects():\n        mob.update(dt)", "d": "Return double-swish activation function which is an approximation to Swish(Swish(x)),\nthat we approximate closely with x * sigmoid(x-1)."}, {"e": "<|fim_prefix|>    def _addSdFile(self, filename):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            self._sdFiles.append(filename)\n        self._sdFileAdded(filename)<|fim_middle|>Updates things like starting_mobject, and (for\nTransforms) target_mobject.  Note, since typically\n(always?) self.mobject will have its updating\nsuspended during the animation, this will do\nnothing to self.mobject.<|file_separator|>", "l": false, "c": "    def _addSdFile(self, filename):\n        self._sdFiles.append(filename)\n        self._sdFileAdded(filename)", "d": "Updates things like starting_mobject, and (for\nTransforms) target_mobject.  Note, since typically\n(always?) self.mobject will have its updating\nsuspended during the animation, this will do\nnothing to self.mobject."}, {"e": "<|fim_prefix|>def run_test_case(objective, guideline):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        subprocess.run(['operate', '--prompt', f'\"{objective}\"'], stdout=subprocess.DEVNULL)\n\n    try:\n        result = evaluate_summary_screenshot(guideline)\n    except(OSError):\n        print(\"Couldn't open the summary screenshot\")\n        return False\n\n    return result<|fim_middle|>Implement SSD Multibox Loss.\n\nBasically, Multibox loss combines classification loss\n and Smooth L1 regression loss.<|file_separator|>", "l": false, "c": "def run_test_case(objective, guideline):\n    subprocess.run(['operate', '--prompt', f'\"{objective}\"'], stdout=subprocess.DEVNULL)\n\n    try:\n        result = evaluate_summary_screenshot(guideline)\n    except(OSError):\n        print(\"Couldn't open the summary screenshot\")\n        return False\n\n    return result", "d": "Implement SSD Multibox Loss.\n\nBasically, Multibox loss combines classification loss\n and Smooth L1 regression loss."}]