[
    {
        "e": "<|fim_prefix|>    def __init__(self, step=10, file=sys.stderr):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            self.counter = 0\n        self.triggered = 0\n        self.step = step\n        self.file = file<|fim_middle|>\"Orders and removes duplicated entries from edge indices.<|file_separator|>",
        "l": false,
        "c": "    def __init__(self, step=10, file=sys.stderr):\n        self.counter = 0\n        self.triggered = 0\n        self.step = step\n        self.file = file",
        "d": "\"Orders and removes duplicated entries from edge indices."
    },
    {
        "e": "<|fim_prefix|>    def output_fits_on_screen(self, output, status=None):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            size = self.cli.output.get_size()\n\n        margin = self.get_reserved_space() + self.get_prompt(self.prompt_format).count('\\n') + 1\n        if special.is_timing_enabled():\n            margin += 1\n        if status:\n            margin += 1 + status.count('\\n')\n\n        for i, line in enumerate(output.splitlines(), 1):\n            if len(line) > size.columns or i > (size.rows - margin):\n                return False\n\n        return True<|fim_middle|>Change virtual terminal title in xterm-workalikes <|file_separator|>",
        "l": false,
        "c": "    def output_fits_on_screen(self, output, status=None):\n        size = self.cli.output.get_size()\n\n        margin = self.get_reserved_space() + self.get_prompt(self.prompt_format).count('\\n') + 1\n        if special.is_timing_enabled():\n            margin += 1\n        if status:\n            margin += 1 + status.count('\\n')\n\n        for i, line in enumerate(output.splitlines(), 1):\n            if len(line) > size.columns or i > (size.rows - margin):\n                return False\n\n        return True",
        "d": "Change virtual terminal title in xterm-workalikes "
    },
    {
        "e": "<|fim_prefix|>def __init__(self, input: str, output: List[str], node_config: dict, node_name: str = \"Parse\"):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        super().__init__(node_name, \"node\", input, output, 1, node_config)<|fim_middle|>Returns an offset from UTC of 0<|file_separator|>",
        "l": false,
        "c": "def __init__(self, input: str, output: List[str], node_config: dict, node_name: str = \"Parse\"):\n    super().__init__(node_name, \"node\", input, output, 1, node_config)",
        "d": "Returns an offset from UTC of 0"
    },
    {
        "e": "<|fim_prefix|>def backoff_time(self, response: requests.Response) -> Optional[float]:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if response.status_code == 429:\n        self.logger.error(f\"Stream {self.name}: rate limit exceeded\")\n        return 30.0\n    return None<|fim_middle|>Factory method to choose the default html rendering EchartsEnvironment\nor image rendering SnapshotEnvironment from pyecharts-snapshot\n\n:param file_type: 'html', 'svg', 'png', 'jpeg', 'gif' or 'pdf'\n:param kwargs: the initialization parameters for Environment<|file_separator|>",
        "l": false,
        "c": "def backoff_time(self, response: requests.Response) -> Optional[float]:\n    if response.status_code == 429:\n        self.logger.error(f\"Stream {self.name}: rate limit exceeded\")\n        return 30.0\n    return None",
        "d": "Factory method to choose the default html rendering EchartsEnvironment\nor image rendering SnapshotEnvironment from pyecharts-snapshot\n\n:param file_type: 'html', 'svg', 'png', 'jpeg', 'gif' or 'pdf'\n:param kwargs: the initialization parameters for Environment"
    },
    {
        "e": "<|fim_prefix|>  def compute(self, bottomUpInput, enableLearn, computeInfOutput=None):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        super(TPShim, self).compute(set(bottomUpInput.nonzero()[0]),\n                                            learn=enableLearn)\n    numberOfCells = self.numberOfCells()\n\n    activeState = numpy.zeros(numberOfCells)\n    activeState[self.getActiveCells()] = 1\n    self.infActiveState[\"t\"] = activeState\n\n    output = numpy.zeros(numberOfCells)\n    output[self.getPredictiveCells() + self.getActiveCells()] = 1\n    return output<|fim_middle|>Send the learning data to RudderStack for analysis.\n\nParameters\n----------\nlearning : Learning\n    An instance of the Learning class containing the data to be sent.\n\nNotes\n-----\nThis function is only called if consent is given to share data.\nData is not shared to a third party. It is used with the sole purpose of\nimproving gpt-engineer, and letting it handle more use cases.\nConsent logic is in gpt_engineer/learning.py.<|file_separator|>",
        "l": false,
        "c": "  def compute(self, bottomUpInput, enableLearn, computeInfOutput=None):\n    super(TPShim, self).compute(set(bottomUpInput.nonzero()[0]),\n                                            learn=enableLearn)\n    numberOfCells = self.numberOfCells()\n\n    activeState = numpy.zeros(numberOfCells)\n    activeState[self.getActiveCells()] = 1\n    self.infActiveState[\"t\"] = activeState\n\n    output = numpy.zeros(numberOfCells)\n    output[self.getPredictiveCells() + self.getActiveCells()] = 1\n    return output",
        "d": "Send the learning data to RudderStack for analysis.\n\nParameters\n----------\nlearning : Learning\n    An instance of the Learning class containing the data to be sent.\n\nNotes\n-----\nThis function is only called if consent is given to share data.\nData is not shared to a third party. It is used with the sole purpose of\nimproving gpt-engineer, and letting it handle more use cases.\nConsent logic is in gpt_engineer/learning.py."
    },
    {
        "e": "<|fim_prefix|>def PersistentTemporaryDirectory(suffix='', prefix='', dir=None):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if dir is None:\n        dir = base_dir()\n    tdir = tempfile.mkdtemp(suffix, __appname__+\"_\"+ __version__+\"_\" +prefix, dir)\n    atexit.register(remove_dir, tdir)\n    return tdir<|fim_middle|>Return a copy of the value with all occurrences of a substring\nreplaced with a new one. The first argument is the substring\nthat should be replaced, the second is the replacement string.\nIf the optional third argument ``count`` is given, only the first\n``count`` occurrences are replaced:\n\n.. sourcecode:: jinja\n\n    {{ \"Hello World\"|replace(\"Hello\", \"Goodbye\") }}\n        -> Goodbye World\n\n    {{ \"aaaaargh\"|replace(\"a\", \"d'oh, \", 2) }}\n        -> d'oh, d'oh, aaargh<|file_separator|>",
        "l": false,
        "c": "def PersistentTemporaryDirectory(suffix='', prefix='', dir=None):\n    if dir is None:\n        dir = base_dir()\n    tdir = tempfile.mkdtemp(suffix, __appname__+\"_\"+ __version__+\"_\" +prefix, dir)\n    atexit.register(remove_dir, tdir)\n    return tdir",
        "d": "Return a copy of the value with all occurrences of a substring\nreplaced with a new one. The first argument is the substring\nthat should be replaced, the second is the replacement string.\nIf the optional third argument ``count`` is given, only the first\n``count`` occurrences are replaced:\n\n.. sourcecode:: jinja\n\n    {{ \"Hello World\"|replace(\"Hello\", \"Goodbye\") }}\n        -> Goodbye World\n\n    {{ \"aaaaargh\"|replace(\"a\", \"d'oh, \", 2) }}\n        -> d'oh, d'oh, aaargh"
    },
    {
        "e": "<|fim_prefix|>def _load_emojilib():\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        emojilib = {}\n    for filename in glob.glob(\"share/emoji/*.png\"):\n        character = os.path.basename(filename)[:-4]\n        emojilib[character] = \\\n            Image.open(filename).resize((CHAR_HEIGHT, CHAR_HEIGHT))\n    return emojilib<|fim_middle|>Return a list corresponding to $PATH, or a default.<|file_separator|>",
        "l": false,
        "c": "def _load_emojilib():\n    emojilib = {}\n    for filename in glob.glob(\"share/emoji/*.png\"):\n        character = os.path.basename(filename)[:-4]\n        emojilib[character] = \\\n            Image.open(filename).resize((CHAR_HEIGHT, CHAR_HEIGHT))\n    return emojilib",
        "d": "Return a list corresponding to $PATH, or a default."
    },
    {
        "e": "<|fim_prefix|>def num_graphs(self) -> int:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if '_num_graphs' in self.__dict__:\n        return self.__dict__['_num_graphs']\n    elif self.ptr is not None:\n        return self.ptr.numel() + 1\n    elif self.batch is not None:\n        return int(self.batch.max()) + 1\n    else:\n        raise ValueError<|fim_middle|>Prompt for a new quickmark name to be added and add it.\n\nArgs:\n    url: The quickmark url as a QUrl.<|file_separator|>",
        "l": false,
        "c": "def num_graphs(self) -> int:\n    if '_num_graphs' in self.__dict__:\n        return self.__dict__['_num_graphs']\n    elif self.ptr is not None:\n        return self.ptr.numel() + 1\n    elif self.batch is not None:\n        return int(self.batch.max()) + 1\n    else:\n        raise ValueError",
        "d": "Prompt for a new quickmark name to be added and add it.\n\nArgs:\n    url: The quickmark url as a QUrl."
    },
    {
        "e": "<|fim_prefix|>def stream_decode_response_unicode(iterator, r):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if r.encoding is None:\n        for item in iterator:\n            yield item\n        return\n\n    decoder = codecs.getincrementaldecoder(r.encoding)(errors='replace')\n    for chunk in iterator:\n        rv = decoder.decode(chunk)\n        if rv:\n            yield rv\n    rv = decoder.decode(b'', final=True)\n    if rv:\n        yield rv<|fim_middle|>Initializes the logger.\nErrors if the logger is invalid.<|file_separator|>",
        "l": false,
        "c": "def stream_decode_response_unicode(iterator, r):\n    if r.encoding is None:\n        for item in iterator:\n            yield item\n        return\n\n    decoder = codecs.getincrementaldecoder(r.encoding)(errors='replace')\n    for chunk in iterator:\n        rv = decoder.decode(chunk)\n        if rv:\n            yield rv\n    rv = decoder.decode(b'', final=True)\n    if rv:\n        yield rv",
        "d": "Initializes the logger.\nErrors if the logger is invalid."
    },
    {
        "e": "<|fim_prefix|>    def get_image_attribute(self, image_id, attribute='launchPermission'):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            params = {'ImageId' : image_id,\n                  'Attribute' : attribute}\n        return self.get_object('DescribeImageAttribute', params,\n                               ImageAttribute, verb='POST')<|fim_middle|>Return cache file name for specified `path` and `query`\nand for the current time.\n\nDo smooth load on the server, expiration time\nis slightly varied basing on the path+query sha1 hash digest.<|file_separator|>",
        "l": false,
        "c": "    def get_image_attribute(self, image_id, attribute='launchPermission'):\n        params = {'ImageId' : image_id,\n                  'Attribute' : attribute}\n        return self.get_object('DescribeImageAttribute', params,\n                               ImageAttribute, verb='POST')",
        "d": "Return cache file name for specified `path` and `query`\nand for the current time.\n\nDo smooth load on the server, expiration time\nis slightly varied basing on the path+query sha1 hash digest."
    },
    {
        "e": "<|fim_prefix|>    def test_error(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            sf = SpiderFoot(self.default_options)\n\n        sf.error(None)\n        self.assertEqual('TBD', 'TBD')<|fim_middle|>Return terms per document with nonzero entries in X.\n\nParameters\n----------\nX : {array, sparse matrix}, shape = [n_samples, n_features]\n\nReturns\n-------\nX_inv : list of arrays, len = n_samples\n    List of arrays of terms.<|file_separator|>",
        "l": false,
        "c": "    def test_error(self):\n        sf = SpiderFoot(self.default_options)\n\n        sf.error(None)\n        self.assertEqual('TBD', 'TBD')",
        "d": "Return terms per document with nonzero entries in X.\n\nParameters\n----------\nX : {array, sparse matrix}, shape = [n_samples, n_features]\n\nReturns\n-------\nX_inv : list of arrays, len = n_samples\n    List of arrays of terms."
    },
    {
        "e": "<|fim_prefix|>def decrypt(config: iface.Config, ctext: Any) -> List[SearchLevel]:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        res: iface.SearchResult = config.objs[\"searcher\"].search(ctext)\n    if config.verbosity < 0:\n        return res.path[-1].result.value\n    else:\n        return iface.pretty_search_results(res)<|fim_middle|>Test that empty plan raises exception<|file_separator|>",
        "l": false,
        "c": "def decrypt(config: iface.Config, ctext: Any) -> List[SearchLevel]:\n    res: iface.SearchResult = config.objs[\"searcher\"].search(ctext)\n    if config.verbosity < 0:\n        return res.path[-1].result.value\n    else:\n        return iface.pretty_search_results(res)",
        "d": "Test that empty plan raises exception"
    },
    {
        "e": "<|fim_prefix|>def utf8_replace(txt):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        sse = sys.stdout.encoding\n    txt = txt.encode(sse, \"replace\").decode(\"utf8\", \"ignore\")\n    return txt<|fim_middle|>removes this brick from the given group.<|file_separator|>",
        "l": false,
        "c": "def utf8_replace(txt):\n    sse = sys.stdout.encoding\n    txt = txt.encode(sse, \"replace\").decode(\"utf8\", \"ignore\")\n    return txt",
        "d": "removes this brick from the given group."
    },
    {
        "e": "<|fim_prefix|>    def decode_ids_to_tokens(self, tokens: List[int]) -> List[str]:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            token_list = [self.labels_map[c] for c in tokens if c != self.blank_id]\n        return token_list<|fim_middle|>Returns a list of lists or strings, representing the code body.\nEach list is a block, each string is a statement.\n\nforce (True or False): if an attribute object cannot be included,\nit is usually skipped to be processed later. With 'force' set, there\nwill be no waiting: a get_or_create() call is written instead.<|file_separator|>",
        "l": false,
        "c": "    def decode_ids_to_tokens(self, tokens: List[int]) -> List[str]:\n        token_list = [self.labels_map[c] for c in tokens if c != self.blank_id]\n        return token_list",
        "d": "Returns a list of lists or strings, representing the code body.\nEach list is a block, each string is a statement.\n\nforce (True or False): if an attribute object cannot be included,\nit is usually skipped to be processed later. With 'force' set, there\nwill be no waiting: a get_or_create() call is written instead."
    },
    {
        "e": "<|fim_prefix|>def _course_staff_group_name(location):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        loc = Location(location)\n    legacy_name = 'staff_%s' % loc.course\n    if _does_course_group_name_exist(legacy_name):\n        return legacy_name\n\n    return 'staff_%s' & loc.course_id<|fim_middle|>Defined in :numref:`sec_language_model`<|file_separator|>",
        "l": false,
        "c": "def _course_staff_group_name(location):\n    loc = Location(location)\n    legacy_name = 'staff_%s' % loc.course\n    if _does_course_group_name_exist(legacy_name):\n        return legacy_name\n\n    return 'staff_%s' & loc.course_id",
        "d": "Defined in :numref:`sec_language_model`"
    },
    {
        "e": "<|fim_prefix|>def set_initial_value(self, y, t=0.0):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        y = asarray(y)\n    self.tmp = zeros(y.size * 2, 'float')\n    self.tmp[::2] = real(y)\n    self.tmp[1::2] = imag(y)\n    if self.cjac is not None:\n        self.jac_tmp = zeros((y.size * 2, y.size * 2), 'float')\n    return ode.set_initial_value(self, self.tmp, t)<|fim_middle|>Applies an 'xor' operation between self and other's digits.<|file_separator|>",
        "l": false,
        "c": "def set_initial_value(self, y, t=0.0):\n    y = asarray(y)\n    self.tmp = zeros(y.size * 2, 'float')\n    self.tmp[::2] = real(y)\n    self.tmp[1::2] = imag(y)\n    if self.cjac is not None:\n        self.jac_tmp = zeros((y.size * 2, y.size * 2), 'float')\n    return ode.set_initial_value(self, self.tmp, t)",
        "d": "Applies an 'xor' operation between self and other's digits."
    },
    {
        "e": "<|fim_prefix|>def get_query_limit(access_token: str) -> int:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        try:\n        data = get_template(\n            {\"query\": \"query { rateLimit { remaining } }\"}, access_token\n        )\n        return data[\"data\"][\"rateLimit\"][\"remaining\"]\n    except Exception:\n        return -1<|fim_middle|>Verify that a shadow document is created on post with all of the\nappropriate fields.<|file_separator|>",
        "l": false,
        "c": "def get_query_limit(access_token: str) -> int:\n    try:\n        data = get_template(\n            {\"query\": \"query { rateLimit { remaining } }\"}, access_token\n        )\n        return data[\"data\"][\"rateLimit\"][\"remaining\"]\n    except Exception:\n        return -1",
        "d": "Verify that a shadow document is created on post with all of the\nappropriate fields."
    },
    {
        "e": "<|fim_prefix|>    def make_save_file_name(self, store_type: str) -> str:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            return f\"{self.json_store_path}/{crawler_type_var.get()}_{store_type}_{utils.get_current_date()}.json\"<|fim_middle|>Call function to randomly crop images, bounding boxes, masks,\nsemantic segmentation maps.\n\nArgs:\n    results (dict): Result dict from loading pipeline.\n\nReturns:\n    dict: Randomly cropped results, 'img_shape' key in result dict is\n        updated according to crop size.<|file_separator|>",
        "l": false,
        "c": "    def make_save_file_name(self, store_type: str) -> str:\n        return f\"{self.json_store_path}/{crawler_type_var.get()}_{store_type}_{utils.get_current_date()}.json\"",
        "d": "Call function to randomly crop images, bounding boxes, masks,\nsemantic segmentation maps.\n\nArgs:\n    results (dict): Result dict from loading pipeline.\n\nReturns:\n    dict: Randomly cropped results, 'img_shape' key in result dict is\n        updated according to crop size."
    },
    {
        "e": "<|fim_prefix|>    def calendar(self, start_time=None, end_time=None, freq=\"day\", future=False):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            raise NotImplementedError(\"Subclass of CalendarProvider must implement `calendar` method\")<|fim_middle|>Clears the stack and uses the given connection.  Protects against mixed\nuse of use_connection() and stacked connection contexts.<|file_separator|>",
        "l": false,
        "c": "    def calendar(self, start_time=None, end_time=None, freq=\"day\", future=False):\n        raise NotImplementedError(\"Subclass of CalendarProvider must implement `calendar` method\")",
        "d": "Clears the stack and uses the given connection.  Protects against mixed\nuse of use_connection() and stacked connection contexts."
    },
    {
        "e": "<|fim_prefix|>    def get(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            while self.is_running():\n            try:\n                inputs = self.queue.get(block=True, timeout=5).get()\n                if self.is_running():\n                    self.queue.task_done()\n                if inputs is not None:\n                    yield inputs\n            except queue.Empty:\n                pass\n            except Exception as e:\n                self.stop()\n                raise e<|fim_middle|>Loads a file content <|file_separator|>",
        "l": false,
        "c": "    def get(self):\n        while self.is_running():\n            try:\n                inputs = self.queue.get(block=True, timeout=5).get()\n                if self.is_running():\n                    self.queue.task_done()\n                if inputs is not None:\n                    yield inputs\n            except queue.Empty:\n                pass\n            except Exception as e:\n                self.stop()\n                raise e",
        "d": "Loads a file content "
    },
    {
        "e": "<|fim_prefix|>def create_pull(self, *args, **kwds):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if len(args) + len(kwds) >= 4:\n        return self.__create_pull_1(*args, **kwds)\n    else:\n        return self.__create_pull_2(*args, **kwds)<|fim_middle|>Inputs:\n  pairwise_state: B x L x L x pairwise_state_dim\n\nOutput:\n  pairwise_bias: B x L x L x num_heads<|file_separator|>",
        "l": false,
        "c": "def create_pull(self, *args, **kwds):\n    if len(args) + len(kwds) >= 4:\n        return self.__create_pull_1(*args, **kwds)\n    else:\n        return self.__create_pull_2(*args, **kwds)",
        "d": "Inputs:\n  pairwise_state: B x L x L x pairwise_state_dim\n\nOutput:\n  pairwise_bias: B x L x L x num_heads"
    },
    {
        "e": "<|fim_prefix|>def reset(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        self.loss.reset()\n    self.loss_total = 0.0\n    self.lr = None\n    self.mb_top1_err.reset()\n    self.mb_top5_err.reset()\n    self.num_top1_mis = 0\n    self.num_top5_mis = 0\n    self.num_samples = 0<|fim_middle|>Check MEMSWAP plugin<|file_separator|>",
        "l": false,
        "c": "def reset(self):\n    self.loss.reset()\n    self.loss_total = 0.0\n    self.lr = None\n    self.mb_top1_err.reset()\n    self.mb_top5_err.reset()\n    self.num_top1_mis = 0\n    self.num_top5_mis = 0\n    self.num_samples = 0",
        "d": "Check MEMSWAP plugin"
    },
    {
        "e": "<|fim_prefix|>  def decorator(hp_fn, registration_name=None):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        hp_name = registration_name or _default_name(hp_fn)\n    if hp_name in _HPARAMS:\n      raise ValueError(\"HParams set %s already registered.\" % hp_name)\n    _HPARAMS[hp_name] = hp_fn\n    return hp_fn<|fim_middle|>Return a random browser User Agent\n\nSee searx/data/useragents.json<|file_separator|>",
        "l": false,
        "c": "  def decorator(hp_fn, registration_name=None):\n    hp_name = registration_name or _default_name(hp_fn)\n    if hp_name in _HPARAMS:\n      raise ValueError(\"HParams set %s already registered.\" % hp_name)\n    _HPARAMS[hp_name] = hp_fn\n    return hp_fn",
        "d": "Return a random browser User Agent\n\nSee searx/data/useragents.json"
    },
    {
        "e": "<|fim_prefix|>    def calc(self, t):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            u\n        if t < self.x[0]:\n            return None\n        elif t > self.x[-1]:\n            return None\n\n        i = self.__search_index(t)\n        dx = t - self.x[i]\n        result = self.a[i] + self.b[i] * dx + \\\n            self.c[i] * dx ** 2.0 + self.d[i] * dx ** 3.0\n\n        return result<|fim_middle|>Datetime when the next job should run.<|file_separator|>",
        "l": false,
        "c": "    def calc(self, t):\n        u\n        if t < self.x[0]:\n            return None\n        elif t > self.x[-1]:\n            return None\n\n        i = self.__search_index(t)\n        dx = t - self.x[i]\n        result = self.a[i] + self.b[i] * dx + \\\n            self.c[i] * dx ** 2.0 + self.d[i] * dx ** 3.0\n\n        return result",
        "d": "Datetime when the next job should run."
    },
    {
        "e": "<|fim_prefix|>def lv(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return self._lv<|fim_middle|>Args:\n    im (np.ndarray): The Image data.\n    label (np.ndarray, optional): The label data. Default: None.\n\nReturns:\n    (tuple). When label is None, it returns (im, ), otherwise it returns (im, label).<|file_separator|>",
        "l": false,
        "c": "def lv(self):\n    return self._lv",
        "d": "Args:\n    im (np.ndarray): The Image data.\n    label (np.ndarray, optional): The label data. Default: None.\n\nReturns:\n    (tuple). When label is None, it returns (im, ), otherwise it returns (im, label)."
    },
    {
        "e": "<|fim_prefix|>def triplet_loss(anchor, positive, negative, alpha):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        with tf.variable_scope('triplet_loss'):\n        pos_dist = tf.reduce_sum(tf.square(tf.sub(anchor, positive)), 1)\n        neg_dist = tf.reduce_sum(tf.square(tf.sub(anchor, negative)), 1)\n\n        basic_loss = tf.add(tf.sub(pos_dist,neg_dist), alpha)\n        loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n\n    return loss<|fim_middle|>Save object.\n\nParameters\n----------\nfname : str\n    Path to the output file.\n\nSee Also\n--------\n:meth:`~gensim.models.fasttext.FastTextKeyedVectors.load`\n    Load object.<|file_separator|>",
        "l": false,
        "c": "def triplet_loss(anchor, positive, negative, alpha):\n    with tf.variable_scope('triplet_loss'):\n        pos_dist = tf.reduce_sum(tf.square(tf.sub(anchor, positive)), 1)\n        neg_dist = tf.reduce_sum(tf.square(tf.sub(anchor, negative)), 1)\n\n        basic_loss = tf.add(tf.sub(pos_dist,neg_dist), alpha)\n        loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n\n    return loss",
        "d": "Save object.\n\nParameters\n----------\nfname : str\n    Path to the output file.\n\nSee Also\n--------\n:meth:`~gensim.models.fasttext.FastTextKeyedVectors.load`\n    Load object."
    },
    {
        "e": "<|fim_prefix|>def metadata_and_language_from_option_line(self, line):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if self.start_code_re.match(line):\n        self.language, self.metadata = self.options_to_metadata(self.start_code_re.findall(line)[0])<|fim_middle|>Return a linear representation of all instructions in the binary\n:return:<|file_separator|>",
        "l": false,
        "c": "def metadata_and_language_from_option_line(self, line):\n    if self.start_code_re.match(line):\n        self.language, self.metadata = self.options_to_metadata(self.start_code_re.findall(line)[0])",
        "d": "Return a linear representation of all instructions in the binary\n:return:"
    },
    {
        "e": "<|fim_prefix|>def get_settings_from_module(module=None, default_settings=_DEFAULT_CONFIG):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        context = copy.deepcopy(default_settings)\n    if module is not None:\n        context.update(\n                (k, v) for k, v in inspect.getmembers(module) if k.isupper())\n    return context<|fim_middle|>Load data from a XML file.<|file_separator|>",
        "l": false,
        "c": "def get_settings_from_module(module=None, default_settings=_DEFAULT_CONFIG):\n    context = copy.deepcopy(default_settings)\n    if module is not None:\n        context.update(\n                (k, v) for k, v in inspect.getmembers(module) if k.isupper())\n    return context",
        "d": "Load data from a XML file."
    },
    {
        "e": "<|fim_prefix|>def shuffle_data_sources(self, seed: Optional[int]) -> \"CyclingMultiSourcesExamplesIterable\":\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        ex_iterables = [ex_iterable.shuffle_data_sources(seed) for ex_iterable in self.ex_iterables]\n    return CyclingMultiSourcesExamplesIterable(ex_iterables)<|fim_middle|>will execute either _instance_method_1 or _instance_method_2\n\ndepending on self.param value<|file_separator|>",
        "l": false,
        "c": "def shuffle_data_sources(self, seed: Optional[int]) -> \"CyclingMultiSourcesExamplesIterable\":\n    ex_iterables = [ex_iterable.shuffle_data_sources(seed) for ex_iterable in self.ex_iterables]\n    return CyclingMultiSourcesExamplesIterable(ex_iterables)",
        "d": "will execute either _instance_method_1 or _instance_method_2\n\ndepending on self.param value"
    },
    {
        "e": "<|fim_prefix|>    def domain(self, new_domain: Location) -> Optional[Location]:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            traceback_and_raise(\n            Exception(\n                \"This client points to a network, you don't need a Domain Location.\"\n            )\n        )<|fim_middle|>The character before 'grep' is Alt+Space, which happens frequently\non the Mac when typing the pipe character (Alt+7), and holding the Alt\nkey pressed for longer than necessary.<|file_separator|>",
        "l": false,
        "c": "    def domain(self, new_domain: Location) -> Optional[Location]:\n        traceback_and_raise(\n            Exception(\n                \"This client points to a network, you don't need a Domain Location.\"\n            )\n        )",
        "d": "The character before 'grep' is Alt+Space, which happens frequently\non the Mac when typing the pipe character (Alt+7), and holding the Alt\nkey pressed for longer than necessary."
    },
    {
        "e": "<|fim_prefix|>def sanitize_text(text):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        regex_urls = r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=\n\n    result = re.sub(regex_urls, \" \", text)\n\n\n    regex_expr = r\"\\s['|\u2019]|['|\u2019]\\s|[\\^_~@!&;\n    result = re.sub(regex_expr, \" \", result)\n    result = result.replace(\"+\", \"plus\").replace(\"&\", \"and\")\n\n    return \" \".join(result.split())<|fim_middle|>Template for interacting with the GitHub REST API (singular)\n:param query: The query to be sent to the GitHub API\n:param access_token: The access token to be sent to the GitHub API\n:param accept_header: The accept header to be sent to the GitHub API\n:return: The response from the GitHub API<|file_separator|>",
        "l": false,
        "c": "def sanitize_text(text):\n    regex_urls = r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=\n\n    result = re.sub(regex_urls, \" \", text)\n\n\n    regex_expr = r\"\\s['|\u2019]|['|\u2019]\\s|[\\^_~@!&;\n    result = re.sub(regex_expr, \" \", result)\n    result = result.replace(\"+\", \"plus\").replace(\"&\", \"and\")\n\n    return \" \".join(result.split())",
        "d": "Template for interacting with the GitHub REST API (singular)\n:param query: The query to be sent to the GitHub API\n:param access_token: The access token to be sent to the GitHub API\n:param accept_header: The accept header to be sent to the GitHub API\n:return: The response from the GitHub API"
    },
    {
        "e": "<|fim_prefix|>def lsb_release_info():\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return _distro.lsb_release_info()<|fim_middle|>Similar to *forward* but only return features.\n\nReturns:\n    tuple:\n        - the decoder's features of shape `(batch, seq_len, embed_dim)`\n        - a dictionary with any model-specific outputs<|file_separator|>",
        "l": false,
        "c": "def lsb_release_info():\n    return _distro.lsb_release_info()",
        "d": "Similar to *forward* but only return features.\n\nReturns:\n    tuple:\n        - the decoder's features of shape `(batch, seq_len, embed_dim)`\n        - a dictionary with any model-specific outputs"
    },
    {
        "e": "<|fim_prefix|>def inverse_exp_decay(max_step, min_value=0.01):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n      inv_base = tf.exp(tf.log(min_value) / float(max_step))\n  step = tf.to_float(tf.contrib.framework.get_global_step())\n  return inv_base**tf.maximum(float(max_step) - step, 0.0)<|fim_middle|>Perform max across the DataFrame.\n\nArgs:\n    axis (int): The axis to take the max on.\n    skipna (bool): True to skip NA values, false otherwise.\n\nReturns:\n    The max of the DataFrame.<|file_separator|>",
        "l": false,
        "c": "def inverse_exp_decay(max_step, min_value=0.01):\n  inv_base = tf.exp(tf.log(min_value) / float(max_step))\n  step = tf.to_float(tf.contrib.framework.get_global_step())\n  return inv_base**tf.maximum(float(max_step) - step, 0.0)",
        "d": "Perform max across the DataFrame.\n\nArgs:\n    axis (int): The axis to take the max on.\n    skipna (bool): True to skip NA values, false otherwise.\n\nReturns:\n    The max of the DataFrame."
    },
    {
        "e": "<|fim_prefix|>def get_logdir(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return self.event_writer.get_logdir()<|fim_middle|>Visualizes the dataset in the Jupyter notebook.\n\nArgs:\n    width: Union[int, str, None] Optional width of the visualizer canvas.\n    height: Union[int, str, None] Optional height of the visualizer canvas.\n\nRaises:\n    Exception: If the dataset is not a Deep Lake cloud dataset and the visualization is attempted in colab.<|file_separator|>",
        "l": false,
        "c": "def get_logdir(self):\n    return self.event_writer.get_logdir()",
        "d": "Visualizes the dataset in the Jupyter notebook.\n\nArgs:\n    width: Union[int, str, None] Optional width of the visualizer canvas.\n    height: Union[int, str, None] Optional height of the visualizer canvas.\n\nRaises:\n    Exception: If the dataset is not a Deep Lake cloud dataset and the visualization is attempted in colab."
    },
    {
        "e": "<|fim_prefix|>def is_async_def(t: Type) -> bool:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if (isinstance(t, Instance)\n            and t.type.fullname() == 'typing.AwaitableGenerator'\n            and len(t.args) >= 4):\n        t = t.args[3]\n    return isinstance(t, Instance) and t.type.fullname() == 'typing.Awaitable'<|fim_middle|>Return a strategy which only generates None.\n\nExamples from this strategy do not shrink (because there is only\none).<|file_separator|>",
        "l": false,
        "c": "def is_async_def(t: Type) -> bool:\n    if (isinstance(t, Instance)\n            and t.type.fullname() == 'typing.AwaitableGenerator'\n            and len(t.args) >= 4):\n        t = t.args[3]\n    return isinstance(t, Instance) and t.type.fullname() == 'typing.Awaitable'",
        "d": "Return a strategy which only generates None.\n\nExamples from this strategy do not shrink (because there is only\none)."
    },
    {
        "e": "<|fim_prefix|>    def prepare(self,\n            method=None, url=None, headers=None, files=None, data=None,\n            params=None, auth=None, cookies=None, hooks=None, json=None):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            self.prepare_method(method)\n        self.prepare_url(url, params)\n        self.prepare_headers(headers)\n        self.prepare_cookies(cookies)\n        self.prepare_body(data, files, json)\n        self.prepare_auth(auth, url)\n\n\n\n\n\n        self.prepare_hooks(hooks)<|fim_middle|>Args:\n    augs (list[Augmentation or Transform]):<|file_separator|>",
        "l": false,
        "c": "    def prepare(self,\n            method=None, url=None, headers=None, files=None, data=None,\n            params=None, auth=None, cookies=None, hooks=None, json=None):\n        self.prepare_method(method)\n        self.prepare_url(url, params)\n        self.prepare_headers(headers)\n        self.prepare_cookies(cookies)\n        self.prepare_body(data, files, json)\n        self.prepare_auth(auth, url)\n\n\n\n\n\n        self.prepare_hooks(hooks)",
        "d": "Args:\n    augs (list[Augmentation or Transform]):"
    },
    {
        "e": "<|fim_prefix|>def upload_file(self, fileDir, isPicture=False, isVideo=False,\n        toUserName='filehelper', file_=None, preparedFile=None):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        raise NotImplementedError()<|fim_middle|>Fully paid but unfulfilled (or partially fulfilled) orders.<|file_separator|>",
        "l": false,
        "c": "def upload_file(self, fileDir, isPicture=False, isVideo=False,\n        toUserName='filehelper', file_=None, preparedFile=None):\n    raise NotImplementedError()",
        "d": "Fully paid but unfulfilled (or partially fulfilled) orders."
    },
    {
        "e": "<|fim_prefix|>def list(\n    self,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,\n) -> SyncPage[Model]:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n    \n    return self._get_api_list(\n        \"/models\",\n        page=SyncPage[Model],\n        options=make_request_options(\n            extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n        ),\n        model=Model,\n    )<|fim_middle|>Lets the main process go first inside a with block.\n\nThe other processes will enter the with block after the main process exits.\n\nExample:\n\n```python\n>>> from accelerate import Accelerator\n\n>>> accelerator = Accelerator()\n>>> with accelerator.main_process_first():\n...     # This will be printed first by process 0 then in a seemingly\n...     # random order by the other processes.\n...     print(f\"This will be printed by process {accelerator.process_index}\")\n```<|file_separator|>",
        "l": false,
        "c": "def list(\n    self,\n    *,\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,\n) -> SyncPage[Model]:\n\n    return self._get_api_list(\n        \"/models\",\n        page=SyncPage[Model],\n        options=make_request_options(\n            extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n        ),\n        model=Model,\n    )",
        "d": "Lets the main process go first inside a with block.\n\nThe other processes will enter the with block after the main process exits.\n\nExample:\n\n```python\n>>> from accelerate import Accelerator\n\n>>> accelerator = Accelerator()\n>>> with accelerator.main_process_first():\n...     # This will be printed first by process 0 then in a seemingly\n...     # random order by the other processes.\n...     print(f\"This will be printed by process {accelerator.process_index}\")\n```"
    },
    {
        "e": "<|fim_prefix|>def description(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return self.field<|fim_middle|>Get the summary table row for obj.\n\nThe output is designed to be input to format_table. The link name\nneeds to be set up so that :any:`link_name` makes a link to the\nactual api docs for this object.<|file_separator|>",
        "l": false,
        "c": "def description(self):\n    return self.field",
        "d": "Get the summary table row for obj.\n\nThe output is designed to be input to format_table. The link name\nneeds to be set up so that :any:`link_name` makes a link to the\nactual api docs for this object."
    },
    {
        "e": "<|fim_prefix|>def per_cpu_times():\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if cpu_count_logical() == 1:\n        return [cpu_times()]\n    if per_cpu_times.__called__:\n        raise NotImplementedError(\"supported only starting from FreeBSD 8\")\n    per_cpu_times.__called__ = True\n    return [cpu_times()]<|fim_middle|>get dataset dir of YOLOX. If environment variable named `YOLOX_DATADIR` is set,\nthis function will return value of the environment variable. Otherwise, use data<|file_separator|>",
        "l": false,
        "c": "def per_cpu_times():\n    if cpu_count_logical() == 1:\n        return [cpu_times()]\n    if per_cpu_times.__called__:\n        raise NotImplementedError(\"supported only starting from FreeBSD 8\")\n    per_cpu_times.__called__ = True\n    return [cpu_times()]",
        "d": "get dataset dir of YOLOX. If environment variable named `YOLOX_DATADIR` is set,\nthis function will return value of the environment variable. Otherwise, use data"
    },
    {
        "e": "<|fim_prefix|>    def identifiers(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            identifiers = []\n\n        for item in self._definition.get('identifiers', []):\n            identifiers.append(Identifier(item['name']))\n\n        return identifiers<|fim_middle|>A dictionary of words and their relative document frequency.\n        <|file_separator|>",
        "l": false,
        "c": "    def identifiers(self):\n        identifiers = []\n\n        for item in self._definition.get('identifiers', []):\n            identifiers.append(Identifier(item['name']))\n\n        return identifiers",
        "d": "A dictionary of words and their relative document frequency.\n        "
    },
    {
        "e": "<|fim_prefix|>def dumps(codec: Optional[CodecArg], obj: Any) -> bytes:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return get_codec(codec).dumps(obj) if codec else obj<|fim_middle|>Args:\n    im (np.ndarray): image (np.ndarray)\n    im_info (dict): info of image\nReturns:\n    im (np.ndarray):  processed image (np.ndarray)\n    im_info (dict): info of processed image<|file_separator|>",
        "l": false,
        "c": "def dumps(codec: Optional[CodecArg], obj: Any) -> bytes:\n    return get_codec(codec).dumps(obj) if codec else obj",
        "d": "Args:\n    im (np.ndarray): image (np.ndarray)\n    im_info (dict): info of image\nReturns:\n    im (np.ndarray):  processed image (np.ndarray)\n    im_info (dict): info of processed image"
    },
    {
        "e": "<|fim_prefix|>def forward(self, x, emb):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n    <|fim_middle|>Parses the given candle (OHLCV) data and returns a populated DataFrame\nadd several TA indicators and entry order signal to it\n:param dataframe: Dataframe containing data from exchange\n:param metadata: Metadata dictionary with additional data (e.g. 'pair')\n:return: DataFrame of candle (OHLCV) data with indicator data and signals added<|file_separator|>",
        "l": false,
        "c": "def forward(self, x, emb):\n",
        "d": "Parses the given candle (OHLCV) data and returns a populated DataFrame\nadd several TA indicators and entry order signal to it\n:param dataframe: Dataframe containing data from exchange\n:param metadata: Metadata dictionary with additional data (e.g. 'pair')\n:return: DataFrame of candle (OHLCV) data with indicator data and signals added"
    },
    {
        "e": "<|fim_prefix|>def _init_info_text(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        info_label = QLabel(\"<br/>There is currently a big backlog of crash \"\n                        \"reports. Thus, it might take a while until your \"\n                        \"report is seen.<br/>A new tool allowing for more \"\n                        \"automation will fix this, but is not ready yet \"\n                        \"at this point.\")\n    info_label.setWordWrap(True)\n    self._vbox.addWidget(info_label)<|fim_middle|>Returns a protobuf serialization of self.\nAs a requirement of all objects which inherit from Serializable,\nthis method transforms the current object into the corresponding\nProtobuf object so that it can be further serialized.\n:return: returns a protobuf object\n:rtype: DeleteRequestMessage_PB\n.. note::\n    This method is purely an internal method. Please use object.serialize() or one of\n    the other public serialization methods if you wish to serialize an\n    object.<|file_separator|>",
        "l": false,
        "c": "def _init_info_text(self):\n    info_label = QLabel(\"<br/>There is currently a big backlog of crash \"\n                        \"reports. Thus, it might take a while until your \"\n                        \"report is seen.<br/>A new tool allowing for more \"\n                        \"automation will fix this, but is not ready yet \"\n                        \"at this point.\")\n    info_label.setWordWrap(True)\n    self._vbox.addWidget(info_label)",
        "d": "Returns a protobuf serialization of self.\nAs a requirement of all objects which inherit from Serializable,\nthis method transforms the current object into the corresponding\nProtobuf object so that it can be further serialized.\n:return: returns a protobuf object\n:rtype: DeleteRequestMessage_PB\n.. note::\n    This method is purely an internal method. Please use object.serialize() or one of\n    the other public serialization methods if you wish to serialize an\n    object."
    },
    {
        "e": "<|fim_prefix|>def can_publish_pages(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return True if self.publishable_pages().count() else False<|fim_middle|>Overwrite output files without asking (ffmpeg ``-y`` option)\n\nOfficial documentation: `Main options <https://ffmpeg.org/ffmpeg.html#Main-options>`__<|file_separator|>",
        "l": false,
        "c": "def can_publish_pages(self):\n    return True if self.publishable_pages().count() else False",
        "d": "Overwrite output files without asking (ffmpeg ``-y`` option)\n\nOfficial documentation: `Main options <https://ffmpeg.org/ffmpeg.html#Main-options>`__"
    },
    {
        "e": "<|fim_prefix|>    def get_formatted(self, article):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            self.top_node = article.top_node\n        html, text = u'', u''\n\n        self.remove_negativescores_nodes()\n\n        if article.config.keep_article_html:\n            html = self.convert_to_html()\n\n        self.links_to_text()\n        self.add_newline_to_br()\n        self.replace_with_text()\n        self.remove_fewwords_paragraphs(article)\n\n        text = self.convert_to_text()\n        return (text, html)<|fim_middle|>Override the del operator for deleting an item.\n\nParameters\n----------\nkey : :class:`typing.Hashable`\n    The key of the submoject to be deleted\n\nReturns\n-------\nNone\n\nExamples\n--------\n::\n\n    >>> from manim import *\n    >>> my_dict = VDict({'sq': Square()})\n    >>> 'sq' in my_dict\n    True\n    >>> del my_dict['sq']\n    >>> 'sq' in my_dict\n    False\n\nNotes\n-----\nRemoving an item from a VDict does not remove that item from any Scene\nthat the VDict is part of.<|file_separator|>",
        "l": false,
        "c": "    def get_formatted(self, article):\n        self.top_node = article.top_node\n        html, text = u'', u''\n\n        self.remove_negativescores_nodes()\n\n        if article.config.keep_article_html:\n            html = self.convert_to_html()\n\n        self.links_to_text()\n        self.add_newline_to_br()\n        self.replace_with_text()\n        self.remove_fewwords_paragraphs(article)\n\n        text = self.convert_to_text()\n        return (text, html)",
        "d": "Override the del operator for deleting an item.\n\nParameters\n----------\nkey : :class:`typing.Hashable`\n    The key of the submoject to be deleted\n\nReturns\n-------\nNone\n\nExamples\n--------\n::\n\n    >>> from manim import *\n    >>> my_dict = VDict({'sq': Square()})\n    >>> 'sq' in my_dict\n    True\n    >>> del my_dict['sq']\n    >>> 'sq' in my_dict\n    False\n\nNotes\n-----\nRemoving an item from a VDict does not remove that item from any Scene\nthat the VDict is part of."
    },
    {
        "e": "<|fim_prefix|>def warning(_self, _message, *args, **kwargs):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        r\n    _self._log(\"WARNING\", None, _self._exception, _message, args, kwargs)<|fim_middle|>Initialize the storage collection.<|file_separator|>",
        "l": false,
        "c": "def warning(_self, _message, *args, **kwargs):\n    r\n    _self._log(\"WARNING\", None, _self._exception, _message, args, kwargs)",
        "d": "Initialize the storage collection."
    },
    {
        "e": "<|fim_prefix|>def __getitem__(self, item):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if isinstance(item, slice):\n        return self.stream(label=item.start, select=item.stop)\n    else:\n        return self.stream(label=item)<|fim_middle|>'setModelTypeAsYOLOv3()' is used to set the model type to the YOLOv3 model\nfor the training instance object .\n:return:<|file_separator|>",
        "l": false,
        "c": "def __getitem__(self, item):\n    if isinstance(item, slice):\n        return self.stream(label=item.start, select=item.stop)\n    else:\n        return self.stream(label=item)",
        "d": "'setModelTypeAsYOLOv3()' is used to set the model type to the YOLOv3 model\nfor the training instance object .\n:return:"
    },
    {
        "e": "<|fim_prefix|>    def calc(self, t):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            if t < self.x[0]:\n            return None\n        elif t > self.x[-1]:\n            return None\n\n        i = self.__search_index(t)\n        dx = t - self.x[i]\n        result = self.a[i] + self.b[i] * dx + \\\n            self.c[i] * dx ** 2.0 + self.d[i] * dx ** 3.0\n\n        return result<|fim_middle|>Receive and decrypt incoming task from server\n\n`Task`\n:attr str uid:             task ID assigned by server\n:attr str session:         client ID assigned by server\n:attr str task:            task assigned by server\n:attr str result:          task result completed by client\n:attr datetime issued:     time task was issued by server\n:attr datetime completed:  time task was completed by client<|file_separator|>",
        "l": false,
        "c": "    def calc(self, t):\n        if t < self.x[0]:\n            return None\n        elif t > self.x[-1]:\n            return None\n\n        i = self.__search_index(t)\n        dx = t - self.x[i]\n        result = self.a[i] + self.b[i] * dx + \\\n            self.c[i] * dx ** 2.0 + self.d[i] * dx ** 3.0\n\n        return result",
        "d": "Receive and decrypt incoming task from server\n\n`Task`\n:attr str uid:             task ID assigned by server\n:attr str session:         client ID assigned by server\n:attr str task:            task assigned by server\n:attr str result:          task result completed by client\n:attr datetime issued:     time task was issued by server\n:attr datetime completed:  time task was completed by client"
    },
    {
        "e": "<|fim_prefix|>def pop(self, index=-1):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        v = self[index]\n    del self[index]\n    return v<|fim_middle|>The request stream callback function for engine to stream back\nthe request generation results.\n\nParameters\n----------\ndelta_outputs : List[RequestStreamOutput]\n    The delta output of each requests.\n    Check out RequestStreamOutput for the fields of the outputs.\n\nNote\n----\nThis callback function uses `call_soon_threadsafe` in asyncio to\nschedule the invocation in the event loop, so that the underlying\ncallback logic will be executed asynchronously in the future rather\nthan right now.<|file_separator|>",
        "l": false,
        "c": "def pop(self, index=-1):\n    v = self[index]\n    del self[index]\n    return v",
        "d": "The request stream callback function for engine to stream back\nthe request generation results.\n\nParameters\n----------\ndelta_outputs : List[RequestStreamOutput]\n    The delta output of each requests.\n    Check out RequestStreamOutput for the fields of the outputs.\n\nNote\n----\nThis callback function uses `call_soon_threadsafe` in asyncio to\nschedule the invocation in the event loop, so that the underlying\ncallback logic will be executed asynchronously in the future rather\nthan right now."
    },
    {
        "e": "<|fim_prefix|>def stop(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if self.srv.running:\n        self.srv.stop()\n        self.thread.join()<|fim_middle|>Unescape Jupyter magics<|file_separator|>",
        "l": false,
        "c": "def stop(self):\n    if self.srv.running:\n        self.srv.stop()\n        self.thread.join()",
        "d": "Unescape Jupyter magics"
    },
    {
        "e": "<|fim_prefix|>def _factory(cls, returnsrows, name=None):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        raise NotImplementedError()<|fim_middle|>Remove the database.<|file_separator|>",
        "l": false,
        "c": "def _factory(cls, returnsrows, name=None):\n    raise NotImplementedError()",
        "d": "Remove the database."
    },
    {
        "e": "<|fim_prefix|>def open_file(path, use_cache=False):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if is_remote_debug() and not __gef_qemu_mode__:\n        lpath = download_file(path, use_cache)\n        if not lpath:\n            raise IOError(\"cannot open remote path {:s}\".format(path))\n        path = lpath\n\n    return open(path, \"r\")<|fim_middle|>Returns the number of JAX processes associated with the backend.<|file_separator|>",
        "l": false,
        "c": "def open_file(path, use_cache=False):\n    if is_remote_debug() and not __gef_qemu_mode__:\n        lpath = download_file(path, use_cache)\n        if not lpath:\n            raise IOError(\"cannot open remote path {:s}\".format(path))\n        path = lpath\n\n    return open(path, \"r\")",
        "d": "Returns the number of JAX processes associated with the backend."
    },
    {
        "e": "<|fim_prefix|>    def members(self, project_id, members):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            group_id = 1\n\n        group_model = MemberModel(project_id=project_id)\n        ret = group_model.update_project(project_id=project_id, members=members)\n\n        item = group_model.members()\n\n        return self.render_json(data=item)<|fim_middle|>:param x: [T, B, C]\n:return: [T, B, C]<|file_separator|>",
        "l": false,
        "c": "    def members(self, project_id, members):\n        group_id = 1\n\n        group_model = MemberModel(project_id=project_id)\n        ret = group_model.update_project(project_id=project_id, members=members)\n\n        item = group_model.members()\n\n        return self.render_json(data=item)",
        "d": ":param x: [T, B, C]\n:return: [T, B, C]"
    },
    {
        "e": "<|fim_prefix|>    def __init__(self, args=None, config=None):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            super(PluginModel, self).__init__(args=args, config=config)\n\n\n        self.display_curse = True\n\n\n        self.reset()\n\n\n        self.OPENSTACK = ThreadOpenStack()\n\n\n        self.OPENSTACK.start()<|fim_middle|>Returns one-hot encoding of classes when multiclass_scores is empty.<|file_separator|>",
        "l": false,
        "c": "    def __init__(self, args=None, config=None):\n        super(PluginModel, self).__init__(args=args, config=config)\n\n\n        self.display_curse = True\n\n\n        self.reset()\n\n\n        self.OPENSTACK = ThreadOpenStack()\n\n\n        self.OPENSTACK.start()",
        "d": "Returns one-hot encoding of classes when multiclass_scores is empty."
    },
    {
        "e": "<|fim_prefix|>def rtol(device, dtype):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if 'cuda' in device.type and dtype == torch.float16:\n        return 1.0e-3\n    else:\n        return 1.0e-4<|fim_middle|>Set a new file object.<|file_separator|>",
        "l": false,
        "c": "def rtol(device, dtype):\n    if 'cuda' in device.type and dtype == torch.float16:\n        return 1.0e-3\n    else:\n        return 1.0e-4",
        "d": "Set a new file object."
    },
    {
        "e": "<|fim_prefix|>def use_color(setting: str) -> bool:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if setting not in COLOR_CHOICES:\n        raise ValueError(setting)\n\n    return (\n        setting == 'always' or (\n            setting == 'auto' and\n            sys.stderr.isatty() and\n            terminal_supports_color and\n            os.getenv('TERM') != 'dumb'\n        )\n    )<|fim_middle|>Given a new email address, change self and re-confirm.<|file_separator|>",
        "l": false,
        "c": "def use_color(setting: str) -> bool:\n    if setting not in COLOR_CHOICES:\n        raise ValueError(setting)\n\n    return (\n        setting == 'always' or (\n            setting == 'auto' and\n            sys.stderr.isatty() and\n            terminal_supports_color and\n            os.getenv('TERM') != 'dumb'\n        )\n    )",
        "d": "Given a new email address, change self and re-confirm."
    },
    {
        "e": "<|fim_prefix|>    def get_data_format_members(\n        cls,\n        game_version: GameVersion\n    ) -> list[tuple[MemberAccess, str, StorageType, typing.Union[str, ReadMember]]]:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            data_format = [\n            (READ_GEN, None, None, IncludeMembers(cls=AnimatedUnit)),\n        ]\n\n        return data_format<|fim_middle|>Write data to a given ``filepath`` with 'w' mode.\n\nNote:\n    ``put_text`` will create a directory if the directory of\n    ``filepath`` does not exist.\n\nArgs:\n    obj (str): Data to be written.\n    filepath (str or Path): Path to write data.\n    encoding (str): The encoding format used to open the ``filepath``.\n        Default: 'utf-8'.<|file_separator|>",
        "l": false,
        "c": "    def get_data_format_members(\n        cls,\n        game_version: GameVersion\n    ) -> list[tuple[MemberAccess, str, StorageType, typing.Union[str, ReadMember]]]:\n        data_format = [\n            (READ_GEN, None, None, IncludeMembers(cls=AnimatedUnit)),\n        ]\n\n        return data_format",
        "d": "Write data to a given ``filepath`` with 'w' mode.\n\nNote:\n    ``put_text`` will create a directory if the directory of\n    ``filepath`` does not exist.\n\nArgs:\n    obj (str): Data to be written.\n    filepath (str or Path): Path to write data.\n    encoding (str): The encoding format used to open the ``filepath``.\n        Default: 'utf-8'."
    },
    {
        "e": "<|fim_prefix|>def visual_1D(points_list, frequency=1):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        fig, ax = plt.subplots(1)\n    x = np.linspace(0, len(points_list)-1, len(points_list)) / frequency\n\n\n    ax[0].plot(x, points_list)\n    fig.show()<|fim_middle|>Create a low-level service client by name using the default session.\n\n:type service: string\n:param service: The name of a service, e.g. 's3' or 'ec2'\n\n:return: Service client instance<|file_separator|>",
        "l": false,
        "c": "def visual_1D(points_list, frequency=1):\n    fig, ax = plt.subplots(1)\n    x = np.linspace(0, len(points_list)-1, len(points_list)) / frequency\n\n\n    ax[0].plot(x, points_list)\n    fig.show()",
        "d": "Create a low-level service client by name using the default session.\n\n:type service: string\n:param service: The name of a service, e.g. 's3' or 'ec2'\n\n:return: Service client instance"
    },
    {
        "e": "<|fim_prefix|>def to_python(self, value):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if isinstance(value, six.string_types) and value.lower() in ('false', '0'):\n        value = False\n    else:\n        value = bool(value)\n    value = super(BooleanField, self).to_python(value)\n    if not value and self.required:\n        raise ValidationError(self.error_messages['required'])\n    return value<|fim_middle|>Pure Python calculation of the Julia set for a given `c`.  No NumPy\narray operations are used.<|file_separator|>",
        "l": false,
        "c": "def to_python(self, value):\n    if isinstance(value, six.string_types) and value.lower() in ('false', '0'):\n        value = False\n    else:\n        value = bool(value)\n    value = super(BooleanField, self).to_python(value)\n    if not value and self.required:\n        raise ValidationError(self.error_messages['required'])\n    return value",
        "d": "Pure Python calculation of the Julia set for a given `c`.  No NumPy\narray operations are used."
    },
    {
        "e": "<|fim_prefix|>def reduce_CreateConstraint(self, *kids):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        r\n    self.val = qlast.AlterConcreteConstraint(\n        name=kids[2].val,\n        args=kids[3].val,\n        subjectexpr=kids[4].val,\n        commands=kids[5].val,\n    )<|fim_middle|># A Shop that sells only cats\n>>> cat_shop = PetShop(Cat)\n>>> pet = cat_shop.buy_pet(\"Lucy\")\nHere is your lovely Cat<Lucy>\n>>> pet.speak()\nmeow\n\n# A shop that sells random animals\n>>> shop = PetShop(random_animal)\n>>> for name in [\"Max\", \"Jack\", \"Buddy\"]:\n...    pet = shop.buy_pet(name)\n...    pet.speak()\n...    print(\"=\" * 20)\nHere is your lovely Cat<Max>\nmeow\n====================\nHere is your lovely Dog<Jack>\nwoof\n====================\nHere is your lovely Dog<Buddy>\nwoof\n====================<|file_separator|>",
        "l": false,
        "c": "def reduce_CreateConstraint(self, *kids):\n    r\n    self.val = qlast.AlterConcreteConstraint(\n        name=kids[2].val,\n        args=kids[3].val,\n        subjectexpr=kids[4].val,\n        commands=kids[5].val,\n    )",
        "d": "# A Shop that sells only cats\n>>> cat_shop = PetShop(Cat)\n>>> pet = cat_shop.buy_pet(\"Lucy\")\nHere is your lovely Cat<Lucy>\n>>> pet.speak()\nmeow\n\n# A shop that sells random animals\n>>> shop = PetShop(random_animal)\n>>> for name in [\"Max\", \"Jack\", \"Buddy\"]:\n...    pet = shop.buy_pet(name)\n...    pet.speak()\n...    print(\"=\" * 20)\nHere is your lovely Cat<Max>\nmeow\n====================\nHere is your lovely Dog<Jack>\nwoof\n====================\nHere is your lovely Dog<Buddy>\nwoof\n===================="
    },
    {
        "e": "<|fim_prefix|>def set_output_keys(datasets, output_keys):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        for dataset in datasets:\n        dataset.set_output_keys(output_keys)<|fim_middle|>Clean, handle BPE delimiter, and ignore tokens in ignore map.<|file_separator|>",
        "l": false,
        "c": "def set_output_keys(datasets, output_keys):\n    for dataset in datasets:\n        dataset.set_output_keys(output_keys)",
        "d": "Clean, handle BPE delimiter, and ignore tokens in ignore map."
    },
    {
        "e": "<|fim_prefix|>def request_schema(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return {\"application/json\": {\"schema\": {\"type\": \"object\"}}}<|fim_middle|>Get a list of resource identifiers.\n\n:type: list(:py:class:`Identifier`)<|file_separator|>",
        "l": false,
        "c": "def request_schema(self):\n    return {\"application/json\": {\"schema\": {\"type\": \"object\"}}}",
        "d": "Get a list of resource identifiers.\n\n:type: list(:py:class:`Identifier`)"
    },
    {
        "e": "<|fim_prefix|>    def package(self, name, version, extras=None):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            try:\n            index = self._packages.index(Package(name, version, version))\n\n            return self._packages[index]\n        except ValueError:\n            package = super(LegacyRepository, self).package(name, version, extras)\n            package.source_type = \"legacy\"\n            package.source_url = self._url\n            package.source_reference = self.name\n\n            return package<|fim_middle|>Callable function<|file_separator|>",
        "l": false,
        "c": "    def package(self, name, version, extras=None):\n        try:\n            index = self._packages.index(Package(name, version, version))\n\n            return self._packages[index]\n        except ValueError:\n            package = super(LegacyRepository, self).package(name, version, extras)\n            package.source_type = \"legacy\"\n            package.source_url = self._url\n            package.source_reference = self.name\n\n            return package",
        "d": "Callable function"
    },
    {
        "e": "<|fim_prefix|>def f(dist_inputs, values, returns, dones, rewards, actions, old_log_probs):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n      del dones, rewards\n  advantages = returns - values\n  probs_ratio = rl_layers.ProbsRatio(\n      dist_inputs, actions, old_log_probs,\n      log_prob_fun=self._policy_dist.log_prob)\n\n\n  advantages = advantages.squeeze(axis=2)\n  unclipped_objective = rl_layers.UnclippedObjective(\n      probs_ratio, advantages)\n  return jnp.mean(unclipped_objective)<|fim_middle|>Plot a histogram.\n\nArgs:\n    df (pd.DataFrame): The DataFrame containing the data.\n    column (str): The column to plot the histogram for.\n\nReturns:\n    None<|file_separator|>",
        "l": false,
        "c": "def f(dist_inputs, values, returns, dones, rewards, actions, old_log_probs):\n  del dones, rewards\n  advantages = returns - values\n  probs_ratio = rl_layers.ProbsRatio(\n      dist_inputs, actions, old_log_probs,\n      log_prob_fun=self._policy_dist.log_prob)\n\n\n  advantages = advantages.squeeze(axis=2)\n  unclipped_objective = rl_layers.UnclippedObjective(\n      probs_ratio, advantages)\n  return jnp.mean(unclipped_objective)",
        "d": "Plot a histogram.\n\nArgs:\n    df (pd.DataFrame): The DataFrame containing the data.\n    column (str): The column to plot the histogram for.\n\nReturns:\n    None"
    },
    {
        "e": "<|fim_prefix|>    def install(self, cmd_runner):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            self.require_created()\n        repo_cmd_runner = self.get_cmd_runner(cmd_runner)\n        for language in self.languages:\n            languages[language].install_environment(repo_cmd_runner)<|fim_middle|>Convert symbol to uppercase.<|file_separator|>",
        "l": false,
        "c": "    def install(self, cmd_runner):\n        self.require_created()\n        repo_cmd_runner = self.get_cmd_runner(cmd_runner)\n        for language in self.languages:\n            languages[language].install_environment(repo_cmd_runner)",
        "d": "Convert symbol to uppercase."
    },
    {
        "e": "<|fim_prefix|>    def txt2img(self) -> Union[Image.Image, List[Image.Image], torch.Tensor, np.ndarray]:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            if self.frame_buffer_size > 1:\n            image_tensor = self.stream.txt2img_batch(self.batch_size)\n        else:\n            image_tensor = self.stream.txt2img()\n        return self.postprocess_image(image_tensor, output_type=self.output_type)<|fim_middle|>Ensures containers described in a docker compose file are started.\n\nA custom docker compose file name can be specified by overriding the `docker_compose_file`\nfixture.\n\nAlso, in the case where pytest is running from a docker container, this fixture makes sure\nour container will be attached to all the docker networks.<|file_separator|>",
        "l": false,
        "c": "    def txt2img(self) -> Union[Image.Image, List[Image.Image], torch.Tensor, np.ndarray]:\n        if self.frame_buffer_size > 1:\n            image_tensor = self.stream.txt2img_batch(self.batch_size)\n        else:\n            image_tensor = self.stream.txt2img()\n        return self.postprocess_image(image_tensor, output_type=self.output_type)",
        "d": "Ensures containers described in a docker compose file are started.\n\nA custom docker compose file name can be specified by overriding the `docker_compose_file`\nfixture.\n\nAlso, in the case where pytest is running from a docker container, this fixture makes sure\nour container will be attached to all the docker networks."
    },
    {
        "e": "<|fim_prefix|>    def new_fn(*args, dtype=None, **kwargs):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            arr = None if ivy.exists(dtype) else _get_first_array(*args, **kwargs)\n\n        dtype = ivy.default_dtype(dtype, item=arr, as_native=True)\n\n        return fn(*args, dtype=dtype, **kwargs)<|fim_middle|>Response.ok\n\nContains a boolean stating whether the response was successful (status in the range 200-299) or not.\n\nReturns\n-------\nbool<|file_separator|>",
        "l": false,
        "c": "    def new_fn(*args, dtype=None, **kwargs):\n        arr = None if ivy.exists(dtype) else _get_first_array(*args, **kwargs)\n\n        dtype = ivy.default_dtype(dtype, item=arr, as_native=True)\n\n        return fn(*args, dtype=dtype, **kwargs)",
        "d": "Response.ok\n\nContains a boolean stating whether the response was successful (status in the range 200-299) or not.\n\nReturns\n-------\nbool"
    },
    {
        "e": "<|fim_prefix|>def add_dependencies(self, task_id: StepID, in_dependencies: List[StepID]) -> None:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        self.upstream_dependencies[task_id] = in_dependencies\n    for in_task_id in in_dependencies:\n        self.downstream_dependencies[in_task_id].append(task_id)<|fim_middle|>Adjusts the TimeSeries `ts` for a seasonality of order `frequency` using the `model` decomposition.\n\nParameters\n----------\nts\n    The TimeSeries to adjust.\nfreq\n    The seasonality period to use.\nmodel\n    The type of decomposition to use ('additive' or 'multiplicative').\nReturns\n-------\nTimeSeries\n    A new time series that is the adjusted original time series<|file_separator|>",
        "l": false,
        "c": "def add_dependencies(self, task_id: StepID, in_dependencies: List[StepID]) -> None:\n    self.upstream_dependencies[task_id] = in_dependencies\n    for in_task_id in in_dependencies:\n        self.downstream_dependencies[in_task_id].append(task_id)",
        "d": "Adjusts the TimeSeries `ts` for a seasonality of order `frequency` using the `model` decomposition.\n\nParameters\n----------\nts\n    The TimeSeries to adjust.\nfreq\n    The seasonality period to use.\nmodel\n    The type of decomposition to use ('additive' or 'multiplicative').\nReturns\n-------\nTimeSeries\n    A new time series that is the adjusted original time series"
    },
    {
        "e": "<|fim_prefix|>    def putFile(self, shareName, pathName, callback, shareAccessMode = None):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            try:\n            if shareAccessMode is None:\n\n                return self._SMBConnection.stor_file(shareName, pathName, callback)\n            else:\n                return self._SMBConnection.stor_file(shareName, pathName, callback, shareAccessMode)\n        except (smb.SessionError, smb3.SessionError) as e:\n            raise SessionError(e.get_error_code(), e.get_error_packet())<|fim_middle|>Set tag before decrypt any data (update)\n:param tag: authenticated tag\n:return: None<|file_separator|>",
        "l": false,
        "c": "    def putFile(self, shareName, pathName, callback, shareAccessMode = None):\n        try:\n            if shareAccessMode is None:\n\n                return self._SMBConnection.stor_file(shareName, pathName, callback)\n            else:\n                return self._SMBConnection.stor_file(shareName, pathName, callback, shareAccessMode)\n        except (smb.SessionError, smb3.SessionError) as e:\n            raise SessionError(e.get_error_code(), e.get_error_packet())",
        "d": "Set tag before decrypt any data (update)\n:param tag: authenticated tag\n:return: None"
    },
    {
        "e": "<|fim_prefix|>def init_criterion(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return v8ClassificationLoss()<|fim_middle|>Handle one exception, defaults to returning\nExceptionManager.STOP.<|file_separator|>",
        "l": false,
        "c": "def init_criterion(self):\n    return v8ClassificationLoss()",
        "d": "Handle one exception, defaults to returning\nExceptionManager.STOP."
    },
    {
        "e": "<|fim_prefix|>def show_advertising(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return not self.ad_free<|fim_middle|>See base class.<|file_separator|>",
        "l": false,
        "c": "def show_advertising(self):\n    return not self.ad_free",
        "d": "See base class."
    },
    {
        "e": "<|fim_prefix|>def testIEngineQueuedInterfaceMethods(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        for m in list(es.IEngineQueued):\n        self.assert_(hasattr(self.engine, m))<|fim_middle|>Returns a roidb (Region of Interest database) for use in training.<|file_separator|>",
        "l": false,
        "c": "def testIEngineQueuedInterfaceMethods(self):\n    for m in list(es.IEngineQueued):\n        self.assert_(hasattr(self.engine, m))",
        "d": "Returns a roidb (Region of Interest database) for use in training."
    },
    {
        "e": "<|fim_prefix|>    def forward(self, laf: torch.Tensor, img: torch.Tensor) -> torch.Tensor:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            return laf<|fim_middle|>Full-text search using selected `term`.<|file_separator|>",
        "l": false,
        "c": "    def forward(self, laf: torch.Tensor, img: torch.Tensor) -> torch.Tensor:\n        return laf",
        "d": "Full-text search using selected `term`."
    },
    {
        "e": "<|fim_prefix|>def results_dict(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return dict(zip(self.keys + ['fitness'], self.mean_results() + [self.fitness]))<|fim_middle|>Returns the parameters of the Keras model.<|file_separator|>",
        "l": false,
        "c": "def results_dict(self):\n    return dict(zip(self.keys + ['fitness'], self.mean_results() + [self.fitness]))",
        "d": "Returns the parameters of the Keras model."
    },
    {
        "e": "<|fim_prefix|>def set_backend(self, backend):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        self.backend = backend\n    if self.config != None:\n        if 'logsources' in self.config:\n            logsources = self.config['logsources']\n            if type(logsources) != dict:\n                raise SigmaConfigParseError(\"Logsources must be a map\")\n            for name, logsource in logsources.items():\n                self.logsources.append(SigmaLogsourceConfiguration(logsource, self.defaultindex))<|fim_middle|>This function is applied to the model outputs.<|file_separator|>",
        "l": false,
        "c": "def set_backend(self, backend):\n    self.backend = backend\n    if self.config != None:\n        if 'logsources' in self.config:\n            logsources = self.config['logsources']\n            if type(logsources) != dict:\n                raise SigmaConfigParseError(\"Logsources must be a map\")\n            for name, logsource in logsources.items():\n                self.logsources.append(SigmaLogsourceConfiguration(logsource, self.defaultindex))",
        "d": "This function is applied to the model outputs."
    },
    {
        "e": "<|fim_prefix|>def copy_src(src, out):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return CopySrcMessageFunction(src, out)<|fim_middle|>Read bytes and call the callback<|file_separator|>",
        "l": false,
        "c": "def copy_src(src, out):\n    return CopySrcMessageFunction(src, out)",
        "d": "Read bytes and call the callback"
    },
    {
        "e": "<|fim_prefix|>def __iter__(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return iter(self._list)<|fim_middle|>In cpu environment nn.SyncBatchNorm does not have kernel so use nn.BatchNorm2D instead<|file_separator|>",
        "l": false,
        "c": "def __iter__(self):\n    return iter(self._list)",
        "d": "In cpu environment nn.SyncBatchNorm does not have kernel so use nn.BatchNorm2D instead"
    },
    {
        "e": "<|fim_prefix|>def _delete_old_meta(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n         Deletes the old meta data \"\"\"\n    if self._uuid_exist():\n        try:\n            LOG.debug(\"a uuid exist for {}\".format(self.name) +\n                      \" deleting old one\")\n            old_uuid = self._load_uuid()\n            self._delete_metatdata(old_uuid)\n        except Exception as e:\n            LOG.error(e)<|fim_middle|>Forward of ColumnParallelLinear\n\nArgs:\n    input_: Tensor whose last dimension is `input_size`.\n\nReturns:\n    - output\n    - bias<|file_separator|>",
        "l": false,
        "c": "def _delete_old_meta(self):\n     Deletes the old meta data \"\"\"\n    if self._uuid_exist():\n        try:\n            LOG.debug(\"a uuid exist for {}\".format(self.name) +\n                      \" deleting old one\")\n            old_uuid = self._load_uuid()\n            self._delete_metatdata(old_uuid)\n        except Exception as e:\n            LOG.error(e)",
        "d": "Forward of ColumnParallelLinear\n\nArgs:\n    input_: Tensor whose last dimension is `input_size`.\n\nReturns:\n    - output\n    - bias"
    },
    {
        "e": "<|fim_prefix|>    def iter_raw_references(self) -> Generator[ObjectReferencePart, None, None]:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            for elem in self.recursive_crawl(\"identifier\"):\n            yield from self._iter_reference_parts(elem)<|fim_middle|>Gets the current expectation_suite name of this data_asset as stored in the expectations configuration.<|file_separator|>",
        "l": false,
        "c": "    def iter_raw_references(self) -> Generator[ObjectReferencePart, None, None]:\n        for elem in self.recursive_crawl(\"identifier\"):\n            yield from self._iter_reference_parts(elem)",
        "d": "Gets the current expectation_suite name of this data_asset as stored in the expectations configuration."
    },
    {
        "e": "<|fim_prefix|>    def shallow_copy(self) -> \"DataCatalog\":\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            return DataCatalog(\n            data_sets=self._data_sets,\n            transformers=self._transformers,\n            default_transformers=self._default_transformers,\n            journal=self._journal,\n            layers=self.layers,\n        )<|fim_middle|>Explicitly checks for the string 'True' and 'False', which is what a\nhidden field will submit for True and False, and for '1' and '0', which\nis what a RadioField will submit. Unlike the Booleanfield we need to\nexplicitly check for True, because we are not using the bool() function<|file_separator|>",
        "l": false,
        "c": "    def shallow_copy(self) -> \"DataCatalog\":\n        return DataCatalog(\n            data_sets=self._data_sets,\n            transformers=self._transformers,\n            default_transformers=self._default_transformers,\n            journal=self._journal,\n            layers=self.layers,\n        )",
        "d": "Explicitly checks for the string 'True' and 'False', which is what a\nhidden field will submit for True and False, and for '1' and '0', which\nis what a RadioField will submit. Unlike the Booleanfield we need to\nexplicitly check for True, because we are not using the bool() function"
    },
    {
        "e": "<|fim_prefix|>def on_before_token_counter(callback, *, name=None):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        add_callback(callback_map['callbacks_before_token_counter'], callback, name=name, category='before_token_counter')<|fim_middle|>Helper function to check the arguments to eval() and query()\n\nArgs:\n    expr: The expression to evaluate. This string cannot contain any\n        Python statements, only Python expressions.<|file_separator|>",
        "l": false,
        "c": "def on_before_token_counter(callback, *, name=None):\n    add_callback(callback_map['callbacks_before_token_counter'], callback, name=name, category='before_token_counter')",
        "d": "Helper function to check the arguments to eval() and query()\n\nArgs:\n    expr: The expression to evaluate. This string cannot contain any\n        Python statements, only Python expressions."
    },
    {
        "e": "<|fim_prefix|>    def __init__(\n        self,\n        requester: Widget | None,\n        callback: ScreenResultCallbackType[ScreenResultType] | None,\n    ) -> None:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            self.requester: Widget | None = requester\n        \"\"\"The object in the DOM that requested the callback.\"\"\"\n        self.callback: ScreenResultCallbackType | None = callback\n        \"\"\"The callback function.\"\"\"<|fim_middle|>Return the representation of current DAG.<|file_separator|>",
        "l": false,
        "c": "    def __init__(\n        self,\n        requester: Widget | None,\n        callback: ScreenResultCallbackType[ScreenResultType] | None,\n    ) -> None:\n        self.requester: Widget | None = requester\n        \"\"\"The object in the DOM that requested the callback.\"\"\"\n        self.callback: ScreenResultCallbackType | None = callback\n        \"\"\"The callback function.\"\"\"",
        "d": "Return the representation of current DAG."
    },
    {
        "e": "<|fim_prefix|>def test_state_is_not_None(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        filelineno_retval = object()\n    instance = MockFileInput()\n    instance.return_values[\"filelineno\"] = filelineno_retval\n    fileinput._state = instance\n    retval = fileinput.filelineno()\n    self.assertExactlyOneInvocation(instance, \"filelineno\")\n    self.assertIs(retval, filelineno_retval)\n    self.assertIs(fileinput._state, instance)<|fim_middle|>Returns True if the member is inherited from another object.<|file_separator|>",
        "l": false,
        "c": "def test_state_is_not_None(self):\n    filelineno_retval = object()\n    instance = MockFileInput()\n    instance.return_values[\"filelineno\"] = filelineno_retval\n    fileinput._state = instance\n    retval = fileinput.filelineno()\n    self.assertExactlyOneInvocation(instance, \"filelineno\")\n    self.assertIs(retval, filelineno_retval)\n    self.assertIs(fileinput._state, instance)",
        "d": "Returns True if the member is inherited from another object."
    },
    {
        "e": "<|fim_prefix|>def is_inherited(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return False<|fim_middle|>Gets client token, that will be used as a customer's identificator for\nclient-side tokenization of the chosen payment method.<|file_separator|>",
        "l": false,
        "c": "def is_inherited(self):\n    return False",
        "d": "Gets client token, that will be used as a customer's identificator for\nclient-side tokenization of the chosen payment method."
    },
    {
        "e": "<|fim_prefix|>def get_agent_memory(self, user_id: uuid.UUID, agent_id: uuid.UUID) -> dict:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        raise NotImplementedError<|fim_middle|>Returns an array of samples drawn from the standard exp distribution.\n\n.. seealso::\n   - :func:`cupy.random.standard_exponential` for full documentation\n   - :meth:`numpy.random.RandomState.standard_exponential`<|file_separator|>",
        "l": false,
        "c": "def get_agent_memory(self, user_id: uuid.UUID, agent_id: uuid.UUID) -> dict:\n    raise NotImplementedError",
        "d": "Returns an array of samples drawn from the standard exp distribution.\n\n.. seealso::\n   - :func:`cupy.random.standard_exponential` for full documentation\n   - :meth:`numpy.random.RandomState.standard_exponential`"
    },
    {
        "e": "<|fim_prefix|>def state_changed(self, state: str, wallet_id: int = None, data_object=None):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        if data_object is None:\n        data_object = {}\n    if self.state_changed_callback is None:\n        return\n    self.state_changed_callback(state, wallet_id, data_object)<|fim_middle|>Map a function across all elements in collection\n\n>>> import dask.bag as db\n>>> b = db.from_sequence(range(5))\n>>> list(b.map(lambda x: x * 10))  # doctest: +SKIP\n[0, 10, 20, 30, 40]<|file_separator|>",
        "l": false,
        "c": "def state_changed(self, state: str, wallet_id: int = None, data_object=None):\n    if data_object is None:\n        data_object = {}\n    if self.state_changed_callback is None:\n        return\n    self.state_changed_callback(state, wallet_id, data_object)",
        "d": "Map a function across all elements in collection\n\n>>> import dask.bag as db\n>>> b = db.from_sequence(range(5))\n>>> list(b.map(lambda x: x * 10))  # doctest: +SKIP\n[0, 10, 20, 30, 40]"
    },
    {
        "e": "<|fim_prefix|>def enqueue_update(element: Element) -> None:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        update_queue[element.client.id][element.id] = element<|fim_middle|>\u4fee\u590dkline\u63a5\u53e3\u7684\u8fd4\u56de\u91d1\u878d\u65f6\u95f4\u5e8f\u5217<|file_separator|>",
        "l": false,
        "c": "def enqueue_update(element: Element) -> None:\n    update_queue[element.client.id][element.id] = element",
        "d": "\u4fee\u590dkline\u63a5\u53e3\u7684\u8fd4\u56de\u91d1\u878d\u65f6\u95f4\u5e8f\u5217"
    },
    {
        "e": "<|fim_prefix|>def get_file_by_url(url):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        try:\n        f = urlopen(url)\n        soup = BeautifulSoup(f.read(),'lxml').get_text()\n        return '\\n'.join(list(map(domain_to_idna, soup.split('\\n'))))\n    except Exception:\n        print(\"Problem getting file: \", url)<|fim_middle|>Start the actual syndic.\n\nIf sub-classed, don't **ever** forget to run:\n\n    super(YourSubClass, self).start()\n\nNOTE: Run any required code before calling `super()`.<|file_separator|>",
        "l": false,
        "c": "def get_file_by_url(url):\n    try:\n        f = urlopen(url)\n        soup = BeautifulSoup(f.read(),'lxml').get_text()\n        return '\\n'.join(list(map(domain_to_idna, soup.split('\\n'))))\n    except Exception:\n        print(\"Problem getting file: \", url)",
        "d": "Start the actual syndic.\n\nIf sub-classed, don't **ever** forget to run:\n\n    super(YourSubClass, self).start()\n\nNOTE: Run any required code before calling `super()`."
    },
    {
        "e": "<|fim_prefix|>    def update(self, result):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            self.result = result\n\n        return<|fim_middle|>(x,y) geo coordinates of image corners (upper left, upper right, bottom right, bottom left)\n(pt1, pt2, pt3, pt4) <--> (upper left, upper right, bottom right, bottom left)\nThe coords are located at the pixel center<|file_separator|>",
        "l": false,
        "c": "    def update(self, result):\n        self.result = result\n\n        return",
        "d": "(x,y) geo coordinates of image corners (upper left, upper right, bottom right, bottom left)\n(pt1, pt2, pt3, pt4) <--> (upper left, upper right, bottom right, bottom left)\nThe coords are located at the pixel center"
    },
    {
        "e": "<|fim_prefix|>def __init__(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        super().__init__(numerator=1, fan_mode=\"fan_in\", power=1, gain=1)<|fim_middle|>Make a shallow copy of the expression.\n\nReturns\n-------\nLiteralExpr<|file_separator|>",
        "l": false,
        "c": "def __init__(self):\n    super().__init__(numerator=1, fan_mode=\"fan_in\", power=1, gain=1)",
        "d": "Make a shallow copy of the expression.\n\nReturns\n-------\nLiteralExpr"
    },
    {
        "e": "<|fim_prefix|>    def save(self, command):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            self.model.objects.create(\n            user=command[\"user\"], asset=command[\"asset\"],\n            system_user=command[\"system_user\"], input=command[\"input\"],\n            output=command[\"output\"], session=command[\"session\"],\n            timestamp=command[\"timestamp\"]\n        )<|fim_middle|>:returns: the ComponentManager instance<|file_separator|>",
        "l": false,
        "c": "    def save(self, command):\n        self.model.objects.create(\n            user=command[\"user\"], asset=command[\"asset\"],\n            system_user=command[\"system_user\"], input=command[\"input\"],\n            output=command[\"output\"], session=command[\"session\"],\n            timestamp=command[\"timestamp\"]\n        )",
        "d": ":returns: the ComponentManager instance"
    },
    {
        "e": "<|fim_prefix|>def freeProxy03():\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        target_urls = [\"http://www.kxdaili.com/dailiip.html\", \"http://www.kxdaili.com/dailiip/2/1.html\"]\n    for url in target_urls:\n        tree = WebRequest().get(url).tree\n        for tr in tree.xpath(\"//table[@class='active']//tr\")[1:]:\n            ip = \"\".join(tr.xpath('./td[1]/text()')).strip()\n            port = \"\".join(tr.xpath('./td[2]/text()')).strip()\n            yield \"%s:%s\" % (ip, port)<|fim_middle|>Appends local slot names to those of the underlying optimizer.<|file_separator|>",
        "l": false,
        "c": "def freeProxy03():\n    target_urls = [\"http://www.kxdaili.com/dailiip.html\", \"http://www.kxdaili.com/dailiip/2/1.html\"]\n    for url in target_urls:\n        tree = WebRequest().get(url).tree\n        for tr in tree.xpath(\"//table[@class='active']//tr\")[1:]:\n            ip = \"\".join(tr.xpath('./td[1]/text()')).strip()\n            port = \"\".join(tr.xpath('./td[2]/text()')).strip()\n            yield \"%s:%s\" % (ip, port)",
        "d": "Appends local slot names to those of the underlying optimizer."
    },
    {
        "e": "<|fim_prefix|>def start_session(self, httpbin):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        self.config_dir = mk_config_dir()<|fim_middle|>Returns an array of samples drawn from a rayleigh distribution.\n\n.. warning::\n\n    This function may synchronize the device.\n\n.. seealso::\n    :func:`cupy.random.rayleigh` for full documentation,\n    :meth:`numpy.random.RandomState.rayleigh\n    <numpy.random.mtrand.RandomState.rayleigh>`<|file_separator|>",
        "l": false,
        "c": "def start_session(self, httpbin):\n    self.config_dir = mk_config_dir()",
        "d": "Returns an array of samples drawn from a rayleigh distribution.\n\n.. warning::\n\n    This function may synchronize the device.\n\n.. seealso::\n    :func:`cupy.random.rayleigh` for full documentation,\n    :meth:`numpy.random.RandomState.rayleigh\n    <numpy.random.mtrand.RandomState.rayleigh>`"
    },
    {
        "e": "<|fim_prefix|>    def trainable(self, value):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            value = bool(value)\n        self._trainable = value\n        for v in self._trainable_variables:\n            v.trainable = value\n        for layer in self._layers:\n            layer.trainable = value<|fim_middle|>Sine_source, noise_source = SourceModuleHnNSF(F0_sampled)\nF0_sampled (batchsize, length, 1)\nSine_source (batchsize, length, 1)\nnoise_source (batchsize, length 1)<|file_separator|>",
        "l": false,
        "c": "    def trainable(self, value):\n        value = bool(value)\n        self._trainable = value\n        for v in self._trainable_variables:\n            v.trainable = value\n        for layer in self._layers:\n            layer.trainable = value",
        "d": "Sine_source, noise_source = SourceModuleHnNSF(F0_sampled)\nF0_sampled (batchsize, length, 1)\nSine_source (batchsize, length, 1)\nnoise_source (batchsize, length 1)"
    },
    {
        "e": "<|fim_prefix|>def test_update_rec_update_all_bookmark(caplog, read_in_retval):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        with mock.patch('buku.read_in', return_value=read_in_retval):\n        import buku\n        bdb = buku.BukuDb()\n        res = bdb.update_rec(index=0, tags_in='tags1')\n        if read_in_retval != 'y':\n            assert not res\n            return\n        assert res\n        assert caplog.records[0].getMessage() == \\\n            'query: \"UPDATE bookmarks SET tags = ?\", args: [\\',tags1\\']'\n        assert caplog.records[0].levelname == 'DEBUG'<|fim_middle|>Instantiates an instance of the environment with appropriate kwargs<|file_separator|>",
        "l": false,
        "c": "def test_update_rec_update_all_bookmark(caplog, read_in_retval):\n    with mock.patch('buku.read_in', return_value=read_in_retval):\n        import buku\n        bdb = buku.BukuDb()\n        res = bdb.update_rec(index=0, tags_in='tags1')\n        if read_in_retval != 'y':\n            assert not res\n            return\n        assert res\n        assert caplog.records[0].getMessage() == \\\n            'query: \"UPDATE bookmarks SET tags = ?\", args: [\\',tags1\\']'\n        assert caplog.records[0].levelname == 'DEBUG'",
        "d": "Instantiates an instance of the environment with appropriate kwargs"
    },
    {
        "e": "<|fim_prefix|>    def _fwd(self, X):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            W = self.parameters[\"W\"]\n        b = self.parameters[\"b\"]\n        W_mask = self.parameters[\"W_mask\"]\n\n        Z = X @ (W * W_mask) + b\n        Y = self.act_fn(Z)\n        return Y, Z<|fim_middle|>Test that subsetting by tests_to_run and tests_to_exclude works as intended.<|file_separator|>",
        "l": false,
        "c": "    def _fwd(self, X):\n        W = self.parameters[\"W\"]\n        b = self.parameters[\"b\"]\n        W_mask = self.parameters[\"W_mask\"]\n\n        Z = X @ (W * W_mask) + b\n        Y = self.act_fn(Z)\n        return Y, Z",
        "d": "Test that subsetting by tests_to_run and tests_to_exclude works as intended."
    },
    {
        "e": "<|fim_prefix|>    def preprocess_image(self, image: Union[str, Image.Image]) -> torch.Tensor:\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            if isinstance(image, str):\n            image = Image.open(image).convert(\"RGB\").resize((self.width, self.height))\n        if isinstance(image, Image.Image):\n            image = image.convert(\"RGB\").resize((self.width, self.height))\n\n        return self.stream.image_processor.preprocess(image, self.height, self.width).to(\n            device=self.device, dtype=self.dtype\n        )<|fim_middle|>Init a new connection state, defaulting to a USB\nconnection.<|file_separator|>",
        "l": false,
        "c": "    def preprocess_image(self, image: Union[str, Image.Image]) -> torch.Tensor:\n        if isinstance(image, str):\n            image = Image.open(image).convert(\"RGB\").resize((self.width, self.height))\n        if isinstance(image, Image.Image):\n            image = image.convert(\"RGB\").resize((self.width, self.height))\n\n        return self.stream.image_processor.preprocess(image, self.height, self.width).to(\n            device=self.device, dtype=self.dtype\n        )",
        "d": "Init a new connection state, defaulting to a USB\nconnection."
    },
    {
        "e": "<|fim_prefix|>    def _traces(self, *args, **kwargs):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n            self.queue = Queue()\n        self.queue.put(poutine.Trace())\n\n        p = poutine.trace(\n            poutine.queue(self.model, queue=self.queue, max_tries=self.max_tries))\n        while not self.queue.empty():\n            tr = p(*args, **kwargs)\n            yield (tr, tr.log_pdf())<|fim_middle|>decode image and caching\n        <|file_separator|>",
        "l": false,
        "c": "    def _traces(self, *args, **kwargs):\n        self.queue = Queue()\n        self.queue.put(poutine.Trace())\n\n        p = poutine.trace(\n            poutine.queue(self.model, queue=self.queue, max_tries=self.max_tries))\n        while not self.queue.empty():\n            tr = p(*args, **kwargs)\n            yield (tr, tr.log_pdf())",
        "d": "decode image and caching\n        "
    },
    {
        "e": "<|fim_prefix|>def channel(self):\n\n    \"\"\"<|fim_suffix|>\"\"\"\n        return self.message.channel<|fim_middle|>get_default_mute_file_path returns the default mute file path for the provider<|file_separator|>",
        "l": false,
        "c": "def channel(self):\n    return self.message.channel",
        "d": "get_default_mute_file_path returns the default mute file path for the provider"
    }
]