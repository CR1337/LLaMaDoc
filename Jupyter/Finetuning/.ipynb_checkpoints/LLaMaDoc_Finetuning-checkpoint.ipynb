{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a2ea83-1adc-43fa-ab6b-aa7c9c3fda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import huggingface_hub\n",
    "import datasets\n",
    "from transformers import GemmaTokenizer, AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from accelerate import Accelerator, DataLoaderConfiguration\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9b65e4-443e-45ea-b3f2-691d95307087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu_pytorch_tanh`, edit the `model.config` to set `hidden_activation=gelu_pytorch_tanh`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca8b706a3714b228c0ac0372ef08b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "gpu = torch.device('cuda:0')\n",
    "model_id = \"google/codegemma-1.1-2b\"\n",
    "tokenizer = GemmaTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=gpu, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5202b3ae-06f7-4f91-852a-5532222e1ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_new_tokens=200):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(gpu)\n",
    "    outputs = model.generate(inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa9bf567-7266-470d-ab29-121a9b572b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><|fim_prefix|>def add(x, y) -> int:\n",
      "\n",
      "    \"\"\"<|fim_suffix|>\"\"\"\n",
      "        return x + y<|fim_middle|>\n",
      "    This function adds two numbers and returns the result.\n",
      "\n",
      "    Args:\n",
      "        x (int): The first number to be added.\n",
      "        y (int): The second number to be added.\n",
      "\n",
      "    Returns:\n",
      "        int: The sum of the two input numbers.\n",
      "    <|file_separator|><eos>\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"<|fim_prefix|>def add(x, y) -> int:\\n\\n    \\\"\\\"\\\"<|fim_suffix|>\\\"\\\"\\\"\\n        return x + y<|fim_middle|>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac5a57a-d08d-4631-ae2c-560449bb3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset('json', split='train', data_files='train_data.json')\n",
    "#print(ds[0]['e'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1dff58c-3b78-46f0-8a8d-470592fce246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataset_row):\n",
    "    source = dataset_row['e']\n",
    "    input_ids = tokenizer.encode(source) + [tokenizer.eos_token_id]\n",
    "    labels = input_ids.copy()\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84f5de18-373d-4366-9d2d-9fb11d174916",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = ds.map(tokenize, remove_columns=['l', 'd', 'c', 'e'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60f5db7b-90b8-4a2e-9ac7-cad9fc532f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 40000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90b4174a-7105-4d6d-aeb7-0e902a8406bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(data, block_size=512):\n",
    "\n",
    "    blocked_ids = [[]]\n",
    "    current_block = 0\n",
    "    current_block_capacity = block_size\n",
    "    \n",
    "    for i in range(len(data['input_ids'])):\n",
    "        example = data['input_ids'][i]\n",
    "        if(len(example) > block_size):\n",
    "            continue\n",
    "        if(current_block_capacity - len(example) >= 0):\n",
    "            blocked_ids[current_block] = blocked_ids[current_block] + example\n",
    "            current_block_capacity -= len(example)\n",
    "        else:\n",
    "            while(len(blocked_ids[current_block]) < block_size):\n",
    "                blocked_ids[current_block].append(tokenizer.eos_token_id)\n",
    "            \n",
    "            blocked_ids.append([])\n",
    "            current_block += 1\n",
    "            current_block_capacity = block_size\n",
    "    while(len(blocked_ids[current_block]) < block_size):\n",
    "            blocked_ids[current_block].append(tokenizer.eos_token_id)\n",
    "\n",
    "    # concatenate all items\n",
    "    #concatenated = sum(data['input_ids'], [])\n",
    "    #length = len(concatenated)\n",
    "           \n",
    "    # shape \"n / block_size\" blocks\n",
    "    #truncated_length = (length // block_size) * block_size\n",
    "    #blocked_ids = [concatenated[i : i + block_size] for i in range(0, truncated_length, block_size)]\n",
    "\n",
    "    # add last block with padding\n",
    "    #pad_length = block_size - (length % block_size)  # remaining tokens to fill\n",
    "    #if pad_length != block_size:\n",
    "    #    blocked_ids += [concatenated[truncated_length:] + [tokenizer.eos_token_id] * pad_length]\n",
    "\n",
    "    # format as transformers-friendly model input\n",
    "    assert len(blocked_ids) > 0\n",
    "    return {\n",
    "        'input_ids': blocked_ids,\n",
    "        'labels': blocked_ids.copy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "695c4205-b8aa-4b21-9f26-b7dbd6420e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = tokenized_data.train_test_split(\n",
    "    test_size = 0.1,\n",
    "    shuffle = True,\n",
    "    seed = 421337)\n",
    "test_data = split_dataset['test']\n",
    "train_data = split_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90bd69c2-65cc-4f0e-8356-edfcad36785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_blocks = test_data.map(block, batched=True)\n",
    "train_data_blocks = train_data.map(block, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c0bec15-6fb1-4ec2-a4a4-5fb28d5e4030",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data_blocks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest_data_blocks\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data_blocks' is not defined"
     ]
    }
   ],
   "source": [
    "print(test_data_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce94f6e0-1bb1-4741-a9fd-511ab4921629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import get_linear_schedule_with_warmup, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dbd144f-dff6-4dde-a6d9-a2f6364bd94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # We want to predict the next token\n",
    "    inference_mode=False,\n",
    "    target_modules=['q_proj', 'v_proj'],   # this is specific to each model! Look up the exact names in the model\n",
    "    r=8,  # rank\n",
    "    lora_alpha=16,  # how strong does it override old values\n",
    "    lora_dropout=0.05,  # to help with generalization, 5% of updates to the LoRA weights are discarded\n",
    "    bias=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "835cf93c-01e4-47f5-8881-f3b3bdaa17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This wraps our LLM into the adapter model\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f3000e3-508f-4da2-90c0-9d47345dcc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 921,600 || all params: 2,507,094,016 || trainable%: 0.036759690467068624\n"
     ]
    }
   ],
   "source": [
    "# Check how many paramaters we need to train (should be orders of magnitude fewer)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4293d831-684d-4e84-9294-9490fbe4de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fe6b7b8-39ca-4bf6-8171-82cd934da86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some book-keeping to make sure any padding happens with <EOS> tokens (not all tokenizers are meant to be used in fine-tuning and don't always have this right)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# special collator that takes care of correctly offsetting our data (so that token n predicts n+1) \n",
    "collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer,\n",
    "            mlm=False,  # we could also \"mask\" random tokens to introduce some noise, but we don't need that here\n",
    "            pad_to_multiple_of=8,\n",
    "            return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbacfb9a-3f92-47c2-9a1b-2a28e08d5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data_blocks, shuffle=True, collate_fn=collator, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(test_data_blocks, shuffle=True, collate_fn=collator, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ba4d3a6-30b0-4953-8430-99af7fd31168",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)  # The AdamW optimizer is well-suited for LLMs\n",
    "\n",
    "# our schedule will decrease learning rate linearly with time\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be54d7-11b8-4816-b132-9afee7efa12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 274/1766 [10:17<56:07,  2.26s/it]  "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()  # set to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        outputs = model(**batch.to(gpu))  # run batch through model\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().cpu().float()\n",
    "        loss.backward()      # propagate loss back\n",
    "        optimizer.step()     # update weights\n",
    "        lr_scheduler.step()  # update learning rate\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()  # set to evaluation mode\n",
    "    eval_loss = 0\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch.to(gpu))\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().cpu().float()\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"{epoch=}: {train_epoch_loss=} {eval_epoch_loss=}\")\n",
    "\n",
    "    # save adapter to be loaded later\n",
    "    peft_model.save_pretrained(f'./finetuning_checkpoints/checkpoint-ep{epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa6e8c9-be11-4d52-8c69-08ee6c0c982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(generate(\"<|fim_prefix|>def add(x, y) -> int:\\n\\n    \\\"\\\"\\\"<|fim_suffix|>\\\"\\\"\\\"\\n        return x + y<|fim_middle|>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89ba04c6-e60e-49cc-963c-a9d2c5a1bf99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eff1e08da1c4cd192c6300b81118117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><|fim_prefix|>def add(x, y) -> int:\n",
      "\n",
      "    \"\"\"<|fim_suffix|>\"\"\"\n",
      "        return x + y<|fim_middle|>Add two numbers.<|file_separator|><eos>\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"finetuning_checkpoints/checkpoint-ep9\"\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(checkpoint).to(gpu)\n",
    "\n",
    "def finetuned_generate(prompt, max_new_tokens=200):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(gpu)\n",
    "    outputs = finetuned_model.generate(inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(outputs[0])\n",
    "\n",
    "print(finetuned_generate(\"<|fim_prefix|>def add(x, y) -> int:\\n\\n    \\\"\\\"\\\"<|fim_suffix|>\\\"\\\"\\\"\\n        return x + y<|fim_middle|>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
