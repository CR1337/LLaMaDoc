{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting numpy (from sentence-transformers)\n",
      "  Using cached numpy-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.15.1 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: Pillow in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\n",
      "Collecting filelock (from huggingface-hub>=0.15.1->sentence-transformers)\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.15.1->sentence-transformers)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Collecting requests (from huggingface-hub>=0.15.1->sentence-transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Collecting triton==2.3.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.40)\n",
      "Collecting numpy (from sentence-transformers)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub>=0.15.1->sentence-transformers)\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface-hub>=0.15.1->sentence-transformers)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface-hub>=0.15.1->sentence-transformers)\n",
      "  Downloading certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/cr/Documents/hpi/out-of-date-docstrings/.venv/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Using cached sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "Downloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m502.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Downloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.0/163.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Installing collected packages: nvidia-cusparse-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, joblib, idna, fsspec, filelock, charset-normalizer, certifi, triton, scipy, requests, nvidia-cusolver-cu12, nvidia-cudnn-cu12, jinja2, torch, scikit-learn, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed MarkupSafe-2.1.5 certifi-2024.6.2 charset-normalizer-3.3.2 filelock-3.14.0 fsspec-2024.6.0 huggingface-hub-0.23.3 idna-3.7 jinja2-3.1.4 joblib-1.4.2 networkx-3.3 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cudnn-cu12-8.9.2.26 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 requests-2.32.3 scikit-learn-1.5.0 scipy-1.13.1 sentence-transformers-3.0.1 tokenizers-0.19.1 torch-2.3.1 transformers-4.41.2 triton-2.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentence-transformers matplotlib numpy torch tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import Random\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from typing import List, Tuple\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../cache/updated_docstrings_0.json', 'r') as f:\n",
    "    updated_docstrings_0 = json.load(f)\n",
    "\n",
    "updated_docstrings_0 = {\n",
    "    mid: [list(x) for x in zip(*data)]\n",
    "    for mid, data in updated_docstrings_0.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "sample_size=512\n",
    "\n",
    "with open(\"test_data.json\", 'r') as f:\n",
    "    test_data = Random(seed).sample(\n",
    "        [\n",
    "            {'c': item['c'], 'd': item['d'], 'l': item['l']}\n",
    "            for item in json.load(f)\n",
    "        ],\n",
    "        k=sample_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checkpoints/finetuned_0': [{'c': '    def __init__(self, file_path: str) -> None:\\n        try:\\n            from pdfminer.high_level import extract_text\\n        except ImportError:\\n            raise ImportError(\\n                \"`pdfminer` package not found, please install it with \"\\n                \"`pip install pdfminer.six`\"\\n            )\\n\\n        super().__init__(file_path)\\n        self.parser = PDFMinerParser()',\n",
       "   'd': 'Initialize with file path.',\n",
       "   'l': True,\n",
       "   'g': ['.',\n",
       "    '.',\n",
       "    '_iter_raw_references() -> Generator[ObjectReferencePart, None, None]:\\n    _iter_reference_parts(self) -> Generator[ObjectReferencePart, None, None]:\\n    _iter_reference_parts(self, include_self=False) -> Generator[ObjectReferencePart, None, None]:\\n    _iter_reference_parts(self, include_self=True) -> Generator[ObjectReferencePart, None, None]:\\n    _iter_reference_parts(self, include_self=False, include_self_as_identifier=False) -> Generator[ObjectReferencePart, None, None]:\\n    _iter_reference_parts(self, include_self=True, include_self_as_identifier=False) -> Generator[ObjectReferencePart, None, None]:\\n    _iter_reference_parts(self, include_self=False, include_self_as_identifier=True) -> Generator[ObjectReferencePart, None, None]:\\n    _iter_reference_parts(self, include_self=True, include_self_as_identifier=True) -> Generator[ObjectReferencePart, None, None]:\\n    _iter_reference_parts',\n",
       "    '_iter_raw_references() -> Generator[ObjectReferencePart, None, None]:\\n    _iter_reference_parts(elem) -> Generator[ObjectReferencePart, None, None]:\\n    _iter_reference_parts(elem, include_self=True) -> Generator[ObjectReferencePart, None, None]:\\n    _iter_reference_parts(elem, include_self=False) -> Generator[ObjectReferencePart, None, None]:\\n    _iter_reference_parts(elem, include_self=False, include_self_type=False) -> Generator[ObjectReferencePart, None, None]:\\n    _iter_reference_parts(elem, include_self=False, include_self_type=True) -> Generator[ObjectReferencePart, None, None]:\\n    _iter_reference_parts(elem, include_self=False, include_self_type=True, include_self_type_id=False) -> Generator[ObjectReferencePart, None, None]:\\n    _iter_reference_parts(elem, include_self=False, include_self_type=True, include_self_type_id=True) -> Generator[ObjectReferencePart,']},\n",
       "  {'c': '    def __call__(\\n        self,\\n        inputs: Union[Dict[str, Any], Any],\\n        return_only_outputs: bool = False,\\n        callbacks: Callbacks = None,\\n        *,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        run_id: Optional[UUID] = None,\\n        run_name: Optional[str] = None,\\n        include_run_info: bool = False,\\n    ) -> Dict[str, Any]:\\n        inputs = self.prep_inputs(inputs)\\n        callback_manager = CallbackManager.configure(\\n            callbacks,\\n            self.callbacks,\\n            self.verbose,\\n            tags,\\n            self.tags,\\n            metadata,\\n            self.metadata,\\n        )\\n        new_arg_supported = inspect.signature(self._call).parameters.get(\"run_manager\")\\n        run_manager = callback_manager.on_chain_start(\\n            dumpd(self),\\n            inputs,\\n            run_id=run_id,\\n            name=run_name,\\n        )\\n        try:\\n            outputs = (\\n                self._call(inputs, run_manager=run_manager)\\n                if new_arg_supported\\n                else self._call(inputs)\\n            )\\n        except (KeyboardInterrupt, Exception) as e:\\n            run_manager.on_chain_error(e)\\n            raise e\\n        run_manager.on_chain_end(outputs)\\n        final_outputs: Dict[str, Any] = self.prep_outputs(\\n            inputs, outputs, return_only_outputs\\n        )\\n        if include_run_info:\\n            final_outputs[RUN_KEY] = RunInfo(run_id=run_manager.run_id)\\n        return final_outputs',\n",
       "   'd': \"Execute the chain.\\n\\nArgs:\\n    inputs: Dictionary of inputs, or single input if chain expects\\n        only one param. Should contain all inputs specified in\\n        `Chain.input_keys` except for inputs that will be set by the chain's\\n        memory.\\n    return_only_outputs: Whether to return only outputs in the\\n        response. If True, only new keys generated by this chain will be\\n        returned. If False, both input keys and new keys generated by this\\n        chain will be returned. Defaults to False.\\n    callbacks: Callbacks to use for this chain run. These will be called in\\n        addition to callbacks passed to the chain during construction, but only\\n        these runtime callbacks will propagate to calls to other objects.\\n    tags: List of string tags to pass to all callbacks. These will be passed in\\n        addition to tags passed to the chain during construction, but only\\n        these runtime tags will propagate to calls to other objects.\\n    metadata: Optional metadata associated with the chain. Defaults to None\\n    include_run_info: Whether to include run info in the response. Defaults\\n        to False.\\n\\nReturns:\\n    A dict of named outputs. Should contain all outputs specified in\\n        `Chain.output_keys`.\",\n",
       "   'l': True,\n",
       "   'g': ['(x) -> (self, x)',\n",
       "    '_get_size()\\n\\n    :param x:\\n    :return:',\n",
       "    '(x)',\n",
       "    '.combine(x) -> self.data = x.data + self.data']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        file_path: str,\\n        textract_features: Optional[Sequence[str]] = None,\\n        client: Optional[Any] = None,\\n        credentials_profile_name: Optional[str] = None,\\n        region_name: Optional[str] = None,\\n        endpoint_url: Optional[str] = None,\\n        headers: Optional[Dict] = None,\\n    ) -> None:\\n        super().__init__(file_path, headers=headers)\\n\\n        try:\\n            import textractcaller as tc\\n        except ImportError:\\n            raise ModuleNotFoundError(\\n                \"Could not import amazon-textract-caller python package. \"\\n                \"Please install it with `pip install amazon-textract-caller`.\"\\n            )\\n        if textract_features:\\n            features = [tc.Textract_Features[x] for x in textract_features]\\n        else:\\n            features = []\\n\\n        if credentials_profile_name or region_name or endpoint_url:\\n            try:\\n                import boto3\\n\\n                if credentials_profile_name is not None:\\n                    session = boto3.Session(profile_name=credentials_profile_name)\\n                else:\\n\\n                    session = boto3.Session()\\n\\n                client_params = {}\\n                if region_name:\\n                    client_params[\"region_name\"] = region_name\\n                if endpoint_url:\\n                    client_params[\"endpoint_url\"] = endpoint_url\\n\\n                client = session.client(\"textract\", **client_params)\\n\\n            except ImportError:\\n                raise ModuleNotFoundError(\\n                    \"Could not import boto3 python package. \"\\n                    \"Please install it with `pip install boto3`.\"\\n                )\\n            except Exception as e:\\n                raise ValueError(\\n                    \"Could not load credentials to authenticate with AWS client. \"\\n                    \"Please check that credentials in the specified \"\\n                    \"profile name are valid.\"\\n                ) from e\\n        self.parser = AmazonTextractPDFParser(textract_features=features, client=client)',\n",
       "   'd': 'Return response when agent has been stopped due to max iterations.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n    return self._cache.get((prompt, llm_string), None)',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': ['_sin_gen(x)',\n",
       "    '_sin_gen(x):',\n",
       "    '_sin_gen(x):',\n",
       "    '_sin_gen(self, x):\\n\\n    l_tanh(l_linear(l_sin_gen(x)))']},\n",
       "  {'c': 'def _llm_type(self) -> str:\\n    return \"fireworks-chat\"',\n",
       "   'd': 'Return type of llm.',\n",
       "   'l': True,\n",
       "   'g': ['Stop the server.',\n",
       "    '',\n",
       "    '',\n",
       "    'Stop the server.\\n\\n  :return: None.\\n  :rtype: None.\\n  :raises: :class:`~scalene.server.ServerException`\\n  :raises: :class:`~scalene.server.ServerException`\\n  :raises: :class:`~scalene.server.ServerException`\\n  :raises: :class:`~scalene.server.ServerException`\\n  :raises: :class:`~scalene.server.ServerException`\\n  :raises: :class:`~scalene.server.ServerException`\\n  :raises: :class:`~scalene.server.ServerException`\\n  :raises: :class:`~scalene.server.ServerException`\\n  :raises: :class:`~scalene.server.ServerException`\\n  :raises: :class:`~scalene.server.ServerException`\\n  :raises: :class:`~scalene.server.ServerException`\\n  :raises: :class:`~scalene.server.ServerException`\\n  :raises: :class:`~scalene.server.ServerException`\\n  :raises: :class:`~scalene.server.ServerException']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        messages = self.format_messages(**kwargs)\\n        return get_buffer_string(messages)',\n",
       "   'd': 'Format the prompt with inputs generating a string.\\n\\nUse this method to generate a string representation of a prompt consisting\\nof chat messages.\\n\\nUseful for feeding into a string based completion language model or debugging.\\n\\nArgs:\\n    **kwargs: keyword arguments to use for formatting.\\n\\nReturns:\\n    A string representation of the prompt',\n",
       "   'l': True,\n",
       "   'g': [\"Calculate the ratio and resize the image.\\n    :param img: the image to be resized.\\n    :param width: the width of the image.\\n    :param height: the height of the image.\\n    :param model_height: the height of the model.\\n    :return: the image and the ratio.\\n    :rtype: image,float.\\n    :raises: ValueError if the ratio is not between 1 and 2.\\n    :Example:\\n        >>> img = cv2.imread('test.jpg')\\n        >>> img,ratio = compute_ratio_and_resize(img,100,100,200)\\n        >>> cv2.imwrite('test.jpg',img)\\n        >>> print(ratio)\\n        1.0\\n        >>> cv2.imread('test.jpg')\\n        array([[ 128  128  128  128  128  128  128  128  128  128  128  128  128  128  128  128  128\",\n",
       "    ':param img:\\n    :param width:\\n    :param height:\\n    :param model_height:\\n    :return: \\n    :rtype:',\n",
       "    'Calculates the ratio of the image to the desired width and height and resizes the image to the desired width and height.\\n\\n    Parameters:\\n        img (np.ndarray): The image to be resized.\\n        width (int): The desired width of the image.\\n        height (int): The desired height of the image.\\n        model_height (int): The height of the model.\\n\\n    Returns:\\n        np.ndarray: The resized image.\\n        float: The ratio of the image to the desired width and height.',\n",
       "    ':param img:\\n      :param width:\\n      :param height:\\n      :param model_height:\\n      :return:']},\n",
       "  {'c': '    def __call__(\\n        self,\\n        inputs: Union[Dict[str, Any], Any],\\n        return_only_outputs: bool = False,\\n        callbacks: Callbacks = None,\\n        *,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n    ) -> Dict[str, Any]:\\n        inputs = self.prep_inputs(inputs)\\n        callback_manager = CallbackManager.configure(\\n            callbacks,\\n            self.callbacks,\\n            self.verbose,\\n            tags,\\n            self.tags,\\n            metadata,\\n            self.metadata,\\n        )\\n        new_arg_supported = inspect.signature(self._call).parameters.get(\"run_manager\")\\n        run_manager = callback_manager.on_chain_start(\\n            dumpd(self),\\n            inputs,\\n        )\\n        try:\\n            outputs = (\\n                self._call(inputs, run_manager=run_manager)\\n                if new_arg_supported\\n                else self._call(inputs)\\n            )\\n        except (KeyboardInterrupt, Exception) as e:\\n            run_manager.on_chain_error(e)\\n            raise e\\n        run_manager.on_chain_end(outputs)\\n        final_outputs: Dict[str, Any] = self.prep_outputs(\\n            inputs, outputs, return_only_outputs\\n        )\\n        if include_run_info:\\n            final_outputs[RUN_KEY] = RunInfo(run_id=run_manager.run_id)\\n        return final_outputs',\n",
       "   'd': \"Execute the chain.\\n\\nArgs:\\n    inputs: Dictionary of inputs, or single input if chain expects\\n        only one param. Should contain all inputs specified in\\n        `Chain.input_keys` except for inputs that will be set by the chain's\\n        memory.\\n    return_only_outputs: Whether to return only outputs in the\\n        response. If True, only new keys generated by this chain will be\\n        returned. If False, both input keys and new keys generated by this\\n        chain will be returned. Defaults to False.\\n    callbacks: Callbacks to use for this chain run. These will be called in\\n        addition to callbacks passed to the chain during construction, but only\\n        these runtime callbacks will propagate to calls to other objects.\\n    tags: List of string tags to pass to all callbacks. These will be passed in\\n        addition to tags passed to the chain during construction, but only\\n        these runtime tags will propagate to calls to other objects.\\n    metadata: Optional metadata associated with the chain. Defaults to None\\n    include_run_info: Whether to include run info in the response. Defaults\\n        to False.\\n\\nReturns:\\n    A dict of named outputs. Should contain all outputs specified in\\n        `Chain.output_keys`.\",\n",
       "   'l': True,\n",
       "   'g': ['_predict(self, X):\\n    Predict the class of the input data.\\n    Parameters\\n    ----------\\n    X : array_like, shape = [n_samples, n_features]\\n        The input data.\\n    Returns\\n    -------\\n    y : array_like, shape = [n_samples]\\n        The predicted class of the input data.',\n",
       "    'Predict using the model.',\n",
       "    'Predict using the model.\\n\\n    :param X: The data to predict.\\n    :return: The predictions.\\n    :rtype: np.ndarray\\n    :raises: :class:`~sklearn.exceptions.NotFittedError`\\n    :raises: :class:`~sklearn.exceptions.Data',\n",
       "    '']},\n",
       "  {'c': '    def on_tool_error(\\n        self,\\n        error: Union[Exception, KeyboardInterrupt],\\n        *,\\n        run_id: UUID,\\n        **kwargs: Any,\\n    ) -> None:\\n        if not run_id:\\n            raise TracerException(\"No run_id provided for on_tool_error callback.\")\\n        tool_run = self.run_map.get(str(run_id))\\n        if tool_run is None or tool_run.run_type != \"tool\":\\n            raise TracerException(\"No tool Run found to be traced\")\\n\\n        tool_run.error = repr(error)\\n        tool_run.end_time = datetime.utcnow()\\n        tool_run.events.append({\"name\": \"error\", \"time\": tool_run.end_time})\\n        self._end_trace(tool_run)\\n        self._on_tool_error(tool_run)',\n",
       "   'd': 'Handle an error for a tool run.',\n",
       "   'l': True,\n",
       "   'g': [':param algo_name:\\n    :param score_thresholds:\\n    :return:',\n",
       "    '',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': 'def foo(bar: str, callbacks: Optional[CallbackManagerForToolRun] = None) -> str:\\n    assert callbacks is not None\\n    return \"foo\" + bar',\n",
       "   'd': 'Formats the request according the the chosen api',\n",
       "   'l': False,\n",
       "   'g': ['Register static files.\\n\\n  :param uri: The URI for the static files.\\n  :param file_or_directory: The directory containing the static files.\\n  :param pattern: The pattern to match the static files.\\n  :param use_modified_since: Use the modified-since header to determine if\\n    the static files have changed.\\n  :param use_content_range: Use the content-range header to determine if\\n    the static files have changed.\\n  :param stream_large_files: Stream large files.\\n  :param name: The name of the static files.',\n",
       "    'Registers static files with a given URI.\\n\\n  :param uri: The URI to use for the static files.\\n  :param file_or_directory: The file or directory to serve.\\n  :param pattern: The pattern to match the file or directory\\n    against.\\n  :param use_modified_since: Whether to use the modified_since\\n    header.\\n  :param use_content_range: Whether to use the content_range\\n    header.\\n  :param stream_large_files: Whether to stream large files.\\n  :param name: The name of the static files.',\n",
       "    'Registers static files with the given URI.\\n  :param uri: The URI to register the static files with.\\n  :param file_or_directory: The file or directory to serve.\\n  :param pattern: The pattern to match the static files with.\\n  :param use_modified_since: Whether to use the modified-since header.\\n  :param use_content_range: Whether to use the content-range header.\\n  :param stream_large_files: Whether to stream large files.\\n  :param name: The name of the static files.',\n",
       "    'Registers a directory of static files with the given URI.\\n\\n    :param uri: The URI to which the static files are registered.\\n    :param file_or_directory: The directory of static files to register.\\n    :param pattern: The pattern to match the static files.\\n    :param use_modified_since: Whether to use the modified-since header.\\n    :param use_content_range: Whether to use the content-range header.\\n    :param stream_large_files: Whether to stream large files.\\n    :param name: The name of the static files.']},\n",
       "  {'c': '    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        self.return_each_line = return_each_line\\n\\n\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )',\n",
       "   'd': 'Create a new MarkdownHeaderTextSplitter.\\n\\nArgs:\\n    headers_to_split_on: Headers we want to track\\n    return_each_line: Return each line w/ associated headers',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def __init__(self, text_kwargs: Optional[Mapping[str, Any]] = None) -> None:\\n        self.text_kwargs = text_kwargs or {}',\n",
       "   'd': 'Initialize the parser.\\n\\nArgs:\\n    text_kwargs: Keyword arguments to pass to ``fitz.Page.get_text()``.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def input_keys(self) -> List[str]:\\n        return [self.input_key]',\n",
       "   'd': 'Expect input key.\\n\\n:meta private:',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Returns the coordinate of the current extremum along a dimension.\\n\\n    Parameters\\n    ----------\\n    dim : int\\n        The dimension to get the extremum along.\\n    direction : tuple of int\\n        The direction of the extremum.\\n\\n    Returns\\n    -------\\n    int\\n        The coordinate of the current extremum along the dimension.',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': 'def test_invalid_suffix() -> None:\\n    path = \"lc://chains/path.invalid\"\\n    loader = Mock()\\n    valid_suffixes = {\"json\"}\\n\\n    with pytest.raises(\\n        ValueError, match=f\"Unsupported file type, must be one of {valid_suffixes}.\"\\n    ):\\n        try_load_from_hub(path, loader, \"chains\", valid_suffixes)\\n\\n    loader.assert_not_called()',\n",
       "   'd': 'Test that a hub path with an invalid suffix raises an error.',\n",
       "   'l': True,\n",
       "   'g': ['_init_ -',\n",
       "    '_init_ -',\n",
       "    '_init_\\n    :param result: result of the function\\n    :type result: int or float or string or list or dict or None\\n    :return: None\\n    :rtype: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises: None\\n    :raises',\n",
       "    '_init_ -']},\n",
       "  {'c': 'def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\\n    if self.input_keys:\\n        input_variables = {key: input_variables[key] for key in self.input_keys}\\n    query = \" \".join(sorted_values(input_variables))\\n    example_docs = self.vectorstore.similarity_search(query, k=self.k)\\n\\n\\n    examples = [dict(e.metadata) for e in example_docs]\\n\\n    if self.example_keys:\\n        examples = [{k: eg[k] for k in self.example_keys} for eg in examples]\\n    return examples',\n",
       "   'd': 'Select which examples to use based on semantic similarity.',\n",
       "   'l': True,\n",
       "   'g': ['Convert to native encoding.\\n\\n  Args:\\n    s: The string to convert.\\n    encoding: The encoding to use.\\n    errors: The error handling method to use.\\n\\n  Returns:\\n    The string in native encoding.',\n",
       "    ':param s: string to be converted\\n    :param encoding: encoding of the string\\n    :param errors: error handling method\\n    :return: string in the specified encoding\\n    :raises: :class:`~Unicode`',\n",
       "    '',\n",
       "    'Convert `s` to a string of the specified encoding.\\n\\n    If `s` is already a string of the specified encoding, it is returned\\n    unchanged. Otherwise, `s` is encoded using `encoding` and returned as a\\n    string.']},\n",
       "  {'c': 'def test_json_spec_value() -> None:\\n    spec = JsonSpec(dict_={\"foo\": \"bar\", \"baz\": {\"test\": {\"foo\": [1, 2, 3]}}})\\n    assert spec.value(\"data\") == \"{\\'foo\\': \\'bar\\', \\'baz\\': {\\'test\\': {\\'foo\\': [1, 2, 3]}}}\"\\n    assert spec.value(\\'data[\"foo\"]\\') == \"bar\"\\n    assert spec.value(\\'data[\"baz\"]\\') == \"{\\'test\\': {\\'foo\\': [1, 2, 3]}}\"\\n    assert spec.value(\\'data[\"baz\"][\"test\"]\\') == \"{\\'foo\\': [1, 2, 3]}\"\\n    assert spec.value(\\'data[\"baz\"][\"test\"][\"foo\"]\\') == \"[1, 2, 3]\"\\n    assert spec.value(\"data[\\'foo\\']\") == \"bar\"\\n    assert spec.value(\"data[\\'baz\\']\") == \"{\\'test\\': {\\'foo\\': [1, 2, 3]}}\"\\n    assert spec.value(\"data[\\'baz\\'][\\'test\\']\") == \"{\\'foo\\': [1, 2, 3]}\"\\n    assert spec.value(\"data[\\'baz\\'][\\'test\\'][\\'foo\\']\") == \"[1, 2, 3]\"',\n",
       "   'd': 'Lazily parse the blob.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        return self.format_prompt(**kwargs).to_string()',\n",
       "   'd': 'Format the chat template into a string.\\n\\nArgs:\\n    **kwargs: keyword arguments to use for filling in template variables\\n              in all the template messages in this chat template.\\n\\nReturns:\\n    formatted string',\n",
       "   'l': True,\n",
       "   'g': ['_make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type',\n",
       "    '_make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type',\n",
       "    '_make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type',\n",
       "    '_make_save_file_name(store_type: str) -> str:\\n\\n    _make_save_file_name(store_type: str) -> str:\\n\\n    _make_save_file_name(store_type: str, crawler_type_var: str) -> str:']},\n",
       "  {'c': '    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        all_required_field_names = {field.alias for field in cls.__fields__.values()}\\n\\n        extra = values.get(\"model_kwargs\", {})\\n        for field_name in list(values):\\n            if field_name not in all_required_field_names:\\n                if field_name in extra:\\n                    raise ValueError(f\"Found {field_name} supplied twice.\")\\n                logger.warning(\\n                    f\"\"\"{field_name} was transferred to model_kwargs.\\n                    Please confirm that {field_name} is what you intended.\"\"\"\\n                )\\n                extra[field_name] = values.pop(field_name)\\n        values[\"model_kwargs\"] = extra\\n        return values',\n",
       "   'd': 'Build extra kwargs from additional params that were passed in.',\n",
       "   'l': True,\n",
       "   'g': [':return:  the path of the log file to be used for the TODO_refactor.',\n",
       "    '',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': '    def lazy_parse(self, blob: Blob) -> Iterator[Document]:\\n        import fitz\\n\\n        with blob.as_bytes_io() as file_path:\\n            doc = fitz.open(file_path)\\n\\n            yield from [\\n                Document(\\n                    page_content=page.get_text(**self.text_kwargs),\\n                    metadata=dict(\\n                        {\\n                            \"source\": blob.source,\\n                            \"file_path\": blob.source,\\n                            \"page\": page.number,\\n                            \"total_pages\": len(doc),\\n                        },\\n                        **{\\n                            k: doc.metadata[k]\\n                            for k in doc.metadata\\n                            if type(doc.metadata[k]) in [str, int]\\n                        },\\n                    ),\\n                )\\n                for page in doc\\n            ]',\n",
       "   'd': 'Run when chain errors.\\n\\nArgs:\\n    error (Exception or KeyboardInterrupt): The error.',\n",
       "   'l': False,\n",
       "   'g': ['.encode(self, xs, masks) -> (xs, masks)',\n",
       "    '.',\n",
       "    '.encode(self, xs, masks)\\n\\n    :param xs: [B, T, D]\\n    :param masks: [B, T]',\n",
       "    '.  :param xs: [paddle.Tensor]\\n:param masks: [paddle.Tensor]']},\n",
       "  {'c': '    def format_messages(self, **kwargs: Any) -> List[BaseMessage]:\\n        return [self.format(**kwargs)]',\n",
       "   'd': 'Format messages from kwargs.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    List of BaseMessages.',\n",
       "   'l': True,\n",
       "   'g': [':return: (pt1, pt2, pt3, pt4)',\n",
       "    ':return: (pt1, pt2, pt3, pt4) \\n    :rtype: tuple',\n",
       "    'Returns the corners of the center of the rectangle',\n",
       "    ':return: (pt1, pt2, pt3, pt4) \\n    :rtype: tuple']},\n",
       "  {'c': 'def from_function(\\n    cls,\\n    func: Callable,\\n    name: str,\\n    description: str,\\n    return_direct: bool = False,\\n    args_schema: Optional[Type[BaseModel]] = None,\\n    **kwargs: Any,\\n) -> Tool:\\n    return cls(\\n        name=name,\\n        func=func,\\n        description=description,\\n        return_direct=return_direct,\\n        args_schema=args_schema,\\n        **kwargs,\\n    )',\n",
       "   'd': 'Initialize with file path.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def _llm_type(self) -> str:\\n    return \"azure\"',\n",
       "   'd': 'Clear cache.',\n",
       "   'l': False,\n",
       "   'g': ['Pad data to 64-byte boundary.',\n",
       "    'Pad the data with the necessary padding to make the length a multiple of 64.',\n",
       "    'Padded data to make the length a multiple of 64.\\n\\n  :return:  The padded data.\\n  :rtype:   bytes\\n  :raises:  ValueError if the length is not a multiple of 64.\\n  :Example:',\n",
       "    'Padded data to make it 64-byte aligned.']},\n",
       "  {'c': '    def with_listeners(\\n        self,\\n        *,\\n        on_start: Optional[Listener] = None,\\n        on_end: Optional[Listener] = None,\\n        on_error: Optional[Listener] = None,\\n    ) -> Runnable[Input, Output]:\\n        from langchain.callbacks.tracers.root_listeners import RootListenersTracer\\n\\n        return self.__class__(\\n            bound=self.bound,\\n            kwargs=self.kwargs,\\n            config=self.config,\\n            config_factories=[\\n                lambda config: {\\n                    \"callbacks\": [\\n                        RootListenersTracer(\\n                            config=config,\\n                            on_start=on_start,\\n                            on_end=on_end,\\n                            on_error=on_error,\\n                        )\\n                    ],\\n                }\\n            ],\\n            custom_input_type=self.custom_input_type,\\n            custom_output_type=self.custom_output_type,\\n        )',\n",
       "   'd': 'Bind lifecycle listeners to a Runnable, returning a new Runnable.\\n\\non_start: Called before the runnable starts running, with the Run object.\\non_end: Called after the runnable finishes running, with the Run object.\\non_error: Called if the runnable throws an error, with the Run object.\\n\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.',\n",
       "   'l': True,\n",
       "   'g': ['.is_fully_within_image(self, image):',\n",
       "    '.is_fully_within_image(self, image):',\n",
       "    '_is_fully_within_image(self, image, fully=True, partly=True)',\n",
       "    '.is_fully_within_image(self, image):']},\n",
       "  {'c': 'def _run(\\n    self,\\n    table_names: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> str:\\n    return self.db.get_table_info_no_throw(table_names.split(\", \"))',\n",
       "   'd': 'Get the schema for tables in a comma-separated list.',\n",
       "   'l': True,\n",
       "   'g': ['Show an error bubble.\\n\\n    Args:\\n        error (str): The error message to show.\\n        width (str): The width of the bubble.\\n        pos (tuple): The position of the bubble.\\n        arrow_pos (tuple): The position of the arrow.\\n        exit (bool): Whether to exit the app.\\n        icon (str): The icon to use.\\n        duration (int): The duration of the bubble.\\n\\n    Returns:\\n        None.',\n",
       "    'Show an error bubble with a custom icon.\\n\\n  :param error: The error message to show.\\n  :param width: The width of the bubble.\\n  :param pos: The position of the bubble.\\n  :param arrow_pos: The position of the arrow.\\n  :param exit: If True, the bubble will exit after a delay.\\n  :param icon: The icon to use.\\n  :param duration: The duration of the bubble.',\n",
       "    'Show error message.\\n\\n  Args:\\n    error (str): error message.\\n    width (str): width of the bubble.\\n    pos (tuple): position of the bubble.\\n    arrow_pos (tuple): position of the arrow.\\n    exit (bool): if True, close the bubble when the arrow is clicked.\\n    icon (str): icon of the error bubble.\\n    duration (int): duration of the bubble.',\n",
       "    '']},\n",
       "  {'c': '    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        texts = [t.replace(\"\\\\n\", \" \") for t in texts]\\n        embeddings = self.client.encode(texts, **self.encode_kwargs)\\n        return embeddings.tolist()',\n",
       "   'd': 'Format the prompt template.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    Formatted message.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    ':return: list of parameters for the current model.\\n    :rtype: list of int or float or str or tuple.',\n",
       "    'Returns the parameters of the current transform.',\n",
       "    '']},\n",
       "  {'c': '    def clear(self) -> None:\\n        from botocore.exceptions import ClientError\\n\\n        try:\\n            self.table.delete_item(self.key)\\n        except ClientError as err:\\n            logger.error(err)',\n",
       "   'd': 'Get the schema for tables in a comma-separated list.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'Start a process in a new process.\\n\\n    :param executable: The path to the executable to start.\\n    :param args: The arguments to pass to the executable.\\n    :return: The process ID.',\n",
       "    'Start a new process and return its process id.\\n\\n    :param executable: the path to the executable to run\\n    :param args: the arguments to pass to the executable\\n    :return: the process id of the new process\\n    :raises: :class:`multiprocessing.AuthenticationError` if the process is not running']},\n",
       "  {'c': '    def __call__(\\n        self,\\n        inputs: Union[Dict[str, Any], Any],\\n        return_only_outputs: bool = False,\\n        callbacks: Callbacks = None,\\n        *,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        run_id: Optional[UUID] = None,\\n        run_name: Optional[str] = None,\\n        include_run_info: bool = False,\\n    ) -> Dict[str, Any]:\\n        inputs = self.prep_inputs(inputs)\\n        callback_manager = CallbackManager.configure(\\n            callbacks,\\n            self.callbacks,\\n            self.verbose,\\n            tags,\\n            self.tags,\\n            metadata,\\n            self.metadata,\\n        )\\n        new_arg_supported = inspect.signature(self._call).parameters.get(\"run_manager\")\\n        run_manager = callback_manager.on_chain_start(\\n            dumpd(self),\\n            inputs,\\n            run_id=run_id,\\n            name=run_name,\\n        )\\n        try:\\n            outputs = (\\n                self._call(inputs, run_manager=run_manager)\\n                if new_arg_supported\\n                else self._call(inputs)\\n            )\\n        except (KeyboardInterrupt, Exception) as e:\\n            run_manager.on_chain_error(e)\\n            raise e\\n        run_manager.on_chain_end(outputs)\\n        final_outputs: Dict[str, Any] = self.prep_outputs(\\n            inputs, outputs, return_only_outputs\\n        )\\n        if include_run_info:\\n            final_outputs[RUN_KEY] = RunInfo(run_id=run_manager.run_id)\\n        return final_outputs',\n",
       "   'd': 'Lazily parse the blob.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '()\\n    return', '']},\n",
       "  {'c': '    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        vectors = np.array(\\n            self.embeddings.embed_documents([inputs[\"prediction\"], inputs[\"reference\"]])\\n        )\\n        score = self._compute_score(vectors)\\n        return {\"score\": score}',\n",
       "   'd': 'Compute the score for a prediction and reference.\\n\\nArgs:\\n    inputs (Dict[str, Any]): The input data.\\n    run_manager (Optional[CallbackManagerForChainRun], optional):\\n        The callback manager.\\n\\nReturns:\\n    Dict[str, Any]: The computed score.',\n",
       "   'l': True,\n",
       "   'g': ['_forward(self, x, style, skip=None)',\n",
       "    '_forward(self, x, style, skip=None) -> torch.Tensor:',\n",
       "    '_forward',\n",
       "    '_forward']},\n",
       "  {'c': 'def serialize_chat_messages(self, messages: List[Dict]) -> str:\\n    chat_messages = _get_messages_from_run_dict(messages)\\n    return get_buffer_string(chat_messages)',\n",
       "   'd': 'Extract the input messages from the run.',\n",
       "   'l': True,\n",
       "   'g': ['_init_', '_init_', '_init_', '(']},\n",
       "  {'c': 'def load(self) -> List[Document]:\\n    loader = UnstructuredPDFLoader(str(self.file_path))\\n    return loader.load()',\n",
       "   'd': 'Maps the Run to a dictionary.',\n",
       "   'l': False,\n",
       "   'g': ['(output_file, resume, progress_file) ->',\n",
       "    '(output_file=None, resume=False, progress_file=sys.stderr)',\n",
       "    '(output_file=None, resume=False, progress_file=sys.stderr)\\n\\n    :param output_file: (optional) the file to write the output to.\\n    :param resume: (optional) if True, the output file will be opened in\\n        append mode.  If False, the output file will be opened in write\\n        mode.  (default: False)\\n    :param progress_file: (optional) the file to write the progress to.\\n        (default: sys.stderr)\\n    :return: a new instance of the class.  (default: None)\\n        (see: :func:`__init__` for more information)\\n    :raises: :class:`~exceptions.Exception`\\n        if the output file cannot be opened.  (see: :func:`~exceptions.Exception`\\n        for more information)\\n        if the progress file cannot be opened.  (see: :func:`~exceptions.Exception`\\n        for more information)\\n        if the output file cannot be opened.  (see: :func:`~exceptions.Exception`\\n        for more information)\\n        if the progress file cannot be opened.  (see',\n",
       "    '(output_file=None, resume=False, progress_file=sys.stderr)']},\n",
       "  {'c': '    def clear(self, **kwargs: Any) -> None:\\n        from gptcache import Cache\\n\\n        for gptcache_instance in self.gptcache_dict.values():\\n            gptcache_instance = cast(Cache, gptcache_instance)\\n            gptcache_instance.flush()\\n\\n        self.gptcache_dict.clear()',\n",
       "   'd': 'Clear cache.',\n",
       "   'l': True,\n",
       "   'g': ['.run.log(values, step=step, **kwargs)\\n    \\n    ..note::\\n        This method is a wrapper for the `run.log` method.\\n        \\n        ..code-block:: python\\n            \\n            logger.debug(\"Successfully logged to WandB\")\\n            \\n            wandb.log(values, step=step, **kwargs)\\n            \\n        ..note::\\n            This method is a wrapper for the `run.log` method.',\n",
       "    '(str, dict, int, **kwargs) -> None:',\n",
       "    '.run.log(values, step=step, **kwargs)\\n        logger.debug(\"Successfully logged to WandB\")\\n        return',\n",
       "    '_log()']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        cohere_api_key = get_from_dict_or_env(\\n            values, \"cohere_api_key\", \"COHERE_API_KEY\"\\n        )\\n        try:\\n            import cohere\\n\\n            values[\"client\"] = cohere.Client(cohere_api_key)\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import cohere python package. \"\\n                \"Please install it with `pip install cohere`.\"\\n            )\\n        return values',\n",
       "   'd': 'Validate that api key and python package exists in environment.',\n",
       "   'l': True,\n",
       "   'g': ['.',\n",
       "    '.',\n",
       "    ':return: the json representation of the object.\\n    :rtype: str\\n    :raises: :class:`~django.core.serializers.',\n",
       "    '_to_json']},\n",
       "  {'c': '    def execute(self, query: str, params: dict = {}, retry: int = 0) -> Any:\\n        from nebula3.Exception import IOErrorException, NoValidSessionException\\n        from nebula3.fbthrift.transport.TTransport import TTransportException\\n\\n        try:\\n            result = self.session_pool.execute_parameter(query, params)\\n            if not result.is_succeeded():\\n                logging.warning(\\n                    f\"Error executing query to NebulaGraph. \"\\n                    f\"Error: {result.error_msg()}\\\\n\"\\n                    f\"Query: {query} \\\\n\"\\n                )\\n            return result\\n\\n        except NoValidSessionException:\\n            logging.warning(\\n                f\"No valid session found in session pool. \"\\n                f\"Please consider increasing the session pool size. \"\\n                f\"Current size: {self.session_pool_size}\"\\n            )\\n            raise ValueError(\\n                f\"No valid session found in session pool. \"\\n                f\"Please consider increasing the session pool size. \"\\n                f\"Current size: {self.session_pool_size}\"\\n            )\\n\\n        except RuntimeError as e:\\n            if retry < RETRY_TIMES:\\n                retry += 1\\n                logging.warning(\\n                    f\"Error executing query to NebulaGraph. \"\\n                    f\"Retrying ({retry}/{RETRY_TIMES})...\\\\n\"\\n                    f\"query: {query} \\\\n\"\\n                    f\"Error: {e}\"\\n                )\\n                return self.execute(query, params, retry)\\n            else:\\n                raise ValueError(f\"Error executing query to NebulaGraph. Error: {e}\")\\n\\n        except (TTransportException, IOErrorException):\\n\\n            if retry < RETRY_TIMES:\\n                retry += 1\\n                logging.warning(\\n                    f\"Connection issue with NebulaGraph. \"\\n                    f\"Retrying ({retry}/{RETRY_TIMES})...\\\\n to recreate session pool\"\\n                )\\n                self.session_pool = self._get_session_pool()\\n                return self.execute(query, params, retry)',\n",
       "   'd': 'Split HTML text string\\n\\nArgs:\\n    text: HTML text',\n",
       "   'l': False,\n",
       "   'g': ['(str)', '_call__', '_format_tb_for_exception_report_', '(str)']},\n",
       "  {'c': 'def load(self) -> List[Document]:\\n    loader = UnstructuredPDFLoader(str(self.file_path))\\n    return loader.load()',\n",
       "   'd': 'Load documents.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def delete(\\n        self,\\n        ids: Optional[List[str]] = None,\\n        **kwargs: Any,\\n    ) -> Optional[bool]:\\n        ret: Optional[bool] = None\\n        tmp_res = []\\n        if ids is None or ids.__len__() == 0:\\n            return ret\\n        for _id in ids:\\n            if self.flag:\\n                ret = self.vearch.delete(self.using_db_name, self.using_table_name, _id)\\n            else:\\n                ret = self.vearch.del_doc(_id)\\n            tmp_res.append(ret)\\n        ret = all(i == 0 for i in tmp_res)\\n        return ret',\n",
       "   'd': 'Format the prompt with inputs generating a string.\\n\\nUse this method to generate a string representation of a prompt consisting\\nof chat messages.\\n\\nUseful for feeding into a string based completion language model or debugging.\\n\\nArgs:\\n    **kwargs: keyword arguments to use for formatting.\\n\\nReturns:\\n    A string representation of the prompt',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    '',\n",
       "    'Generates a policy from the app.py file in the project directory.\\n\\n  :param config: The configuration object.\\n  :return: The policy object.']},\n",
       "  {'c': '    def _call(\\n        self,\\n        prompt: str,\\n        stop: Optional[List[str]] = None,\\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\\n        **kwargs: Any,\\n    ) -> str:\\n        try:\\n            from clarifai_grpc.grpc.api import (\\n                resources_pb2,\\n                service_pb2,\\n            )\\n            from clarifai_grpc.grpc.api.status import status_code_pb2\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import clarifai python package. \"\\n                \"Please install it with `pip install clarifai`.\"\\n            )\\n\\n\\n\\n\\n        post_model_outputs_request = service_pb2.PostModelOutputsRequest(\\n            user_app_id=self.userDataObject,\\n            model_id=self.model_id,\\n            version_id=self.model_version_id,\\n            inputs=[\\n                resources_pb2.Input(\\n                    data=resources_pb2.Data(text=resources_pb2.Text(raw=prompt))\\n                )\\n            ],\\n        )\\n        post_model_outputs_response = self.stub.PostModelOutputs(\\n            post_model_outputs_request\\n        )\\n\\n        if post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\\n            logger.error(post_model_outputs_response.status)\\n            first_model_failure = (\\n                post_model_outputs_response.outputs[0].status\\n                if len(post_model_outputs_response.outputs)\\n                else None\\n            )\\n            raise Exception(\\n                f\"Post model outputs failed, status: \"\\n                f\"{post_model_outputs_response.status}, first output failure: \"\\n                f\"{first_model_failure}\"\\n            )\\n\\n        text = post_model_outputs_response.outputs[0].data.text.raw\\n\\n\\n        if stop is not None:\\n            text = enforce_stop_tokens(text, stop)\\n        return text',\n",
       "   'd': '求阶乘',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'Set the backend to use for all the config objects in this\\n    config object.',\n",
       "    '']},\n",
       "  {'c': '    def parse(self, text: str) -> T:\\n',\n",
       "   'd': 'Run the tool.',\n",
       "   'l': False,\n",
       "   'g': ['', '.', '.', '']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        values[\"openai_api_key\"] = get_from_dict_or_env(\\n            values,\\n            \"openai_api_key\",\\n            \"OPENAI_API_KEY\",\\n        )\\n        values[\"openai_api_base\"] = get_from_dict_or_env(\\n            values,\\n            \"openai_api_base\",\\n            \"OPENAI_API_BASE\",\\n        )\\n        values[\"openai_api_version\"] = get_from_dict_or_env(\\n            values,\\n            \"openai_api_version\",\\n            \"OPENAI_API_VERSION\",\\n        )\\n        values[\"openai_api_type\"] = get_from_dict_or_env(\\n            values, \"openai_api_type\", \"OPENAI_API_TYPE\", default=\"azure\"\\n        )\\n        values[\"openai_organization\"] = get_from_dict_or_env(\\n            values,\\n            \"openai_organization\",\\n            \"OPENAI_ORGANIZATION\",\\n            default=\"\",\\n        )\\n        values[\"openai_proxy\"] = get_from_dict_or_env(\\n            values,\\n            \"openai_proxy\",\\n            \"OPENAI_PROXY\",\\n            default=\"\",\\n        )\\n        try:\\n            import openai\\n\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import openai python package. \"\\n                \"Please install it with `pip install openai`.\"\\n            )\\n        try:\\n            values[\"client\"] = openai.ChatCompletion\\n        except AttributeError:\\n            raise ValueError(\\n                \"`openai` has no `ChatCompletion` attribute, this is likely \"\\n                \"due to an old version of the openai package. Try upgrading it \"\\n                \"with `pip install --upgrade openai`.\"\\n            )\\n        if values[\"n\"] < 1:\\n            raise ValueError(\"n must be at least 1.\")\\n        if values[\"n\"] > 1 and values[\"streaming\"]:\\n            raise ValueError(\"n must be 1 when streaming.\")\\n        return values',\n",
       "   'd': 'Validate that api key and python package exists in environment.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    ':param type type: \\n    :param message message: \\n    :param stack_trace stack_trace: \\n    :param inner_exception inner_exception: \\n    :param data data: \\n    :param error_response error_response:',\n",
       "    ':param type: \\n    :param message: \\n    :param stack_trace: \\n    :param inner_exception: \\n    :param data: \\n    :param error_response:',\n",
       "    '']},\n",
       "  {'c': 'def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def format_messages(self, **kwargs: Any) -> List[BaseMessage]:\\n',\n",
       "   'd': 'Format kwargs into a list of messages.',\n",
       "   'l': True,\n",
       "   'g': ['_networkaccessmanager() -> Optional[QNetworkAccessManager]:',\n",
       "    '_networkaccessmanager: Optional[QNetworkAccessManager]',\n",
       "    '_networkaccessmanager() -> QNetworkAccessManager:',\n",
       "    '_networkaccessmanager() -> Optional[QNetworkAccessManager]:']},\n",
       "  {'c': 'def _type(self) -> str:\\n    return \"default\"',\n",
       "   'd': 'Load a prompt template from a template.',\n",
       "   'l': False,\n",
       "   'g': ['Test that command execution works as expected.',\n",
       "    'Test that the command is executed and the output is returned.\\n\\n  :return: None\\n  :rtype: None',\n",
       "    'Test that a command is executed on the target.\\n\\n  :param command: The command to execute.\\n  :param args: The arguments to pass to the command.\\n  :param expected: The expected output of the command.\\n  :param timeout: The timeout in seconds to wait for the command to complete.\\n  :return: The output of the command.',\n",
       "    'Test that command execution works.']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[Dict[Any, Any]]] = None,\\n        model_id: Optional[str] = None,\\n        refresh_indices: bool = False,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        if not self.client.indices.exists(index=self.index_name):\\n            dims = kwargs.get(\"dims\")\\n\\n            if dims is None:\\n                raise ValueError(\"ElasticKnnSearch requires \\'dims\\' parameter\")\\n\\n            similarity = kwargs.get(\"similarity\")\\n            optional_args = {}\\n\\n            if similarity is not None:\\n                optional_args[\"similarity\"] = similarity\\n\\n            mapping = self._default_knn_mapping(dims=dims, **optional_args)\\n            self.create_knn_index(mapping)\\n\\n        embeddings = self.embedding.embed_documents(list(texts))\\n\\n\\n        body: List[Mapping[str, Any]] = []\\n        for text, vector in zip(texts, embeddings):\\n            body.extend(\\n                [\\n                    {\"index\": {\"_index\": self.index_name}},\\n                    {\"text\": text, \"vector\": vector},\\n                ]\\n            )\\n\\n        responses = self.client.bulk(operations=body)\\n\\n        ids = [\\n            item[\"index\"][\"_id\"]\\n            for item in responses[\"items\"]\\n            if item[\"index\"][\"result\"] == \"created\"\\n        ]\\n\\n        if refresh_indices:\\n            self.client.indices.refresh(index=self.index_name)\\n\\n        return ids',\n",
       "   'd': 'Add a list of texts to the Elasticsearch index.\\n\\nArgs:\\n    texts (Iterable[str]): The texts to add to the index.\\n    metadatas (List[Dict[Any, Any]], optional): A list of metadata dictionaries\\n        to associate with the texts.\\n    model_id (str, optional): The ID of the model to use for transforming the\\n        texts into vectors.\\n    refresh_indices (bool, optional): Whether to refresh the Elasticsearch\\n        indices after adding the texts.\\n    **kwargs: Arbitrary keyword arguments.\\n\\nReturns:\\n    A list of IDs for the added texts.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'Returns the prompt embedding for the given adapter_name.\\n\\n    Args:\\n        adapter_name (str): The name of the adapter.\\n\\n    Returns:\\n        torch.Tensor: The prompt embedding.',\n",
       "    'Get the prompt embedding to save.\\n\\n    Args:\\n        adapter_name (str): The name of the adapter.\\n\\n    Returns:\\n        torch.Tensor: The prompt embedding.']},\n",
       "  {'c': 'def _get_combined_score(\\n    self,\\n    document: Document,\\n    vector_relevance: Optional[float],\\n    current_time: datetime.datetime,\\n) -> float:\\n    hours_passed = _get_hours_passed(\\n        current_time,\\n        document.metadata[\"last_accessed_at\"],\\n    )\\n    score = (1.0 - self.decay_rate) ** hours_passed\\n    for key in self.other_score_keys:\\n        if key in document.metadata:\\n            score += document.metadata[key]\\n    if vector_relevance is not None:\\n        score += vector_relevance\\n    return score',\n",
       "   'd': 'Test error is raised when name of input variable is wrong.',\n",
       "   'l': False,\n",
       "   'g': ['.clear()\\n    return self._clear()',\n",
       "    '.clear()\\n    self._clear()\\n        self._update()\\n        return self.draw()\\n\\ndef _clear(self):\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw()\\n    self.draw',\n",
       "    '_clear()\\n\\n    Clear the screen and reset the cursor to the top left.\\n\\n    :return:',\n",
       "    '']},\n",
       "  {'c': '    def load(self) -> List[Document]:\\n        from pdfminer.high_level import extract_text_to_fp\\n        from pdfminer.layout import LAParams\\n        from pdfminer.utils import open_filename\\n\\n        output_string = StringIO()\\n        with open_filename(self.file_path, \"rb\") as fp:\\n            extract_text_to_fp(\\n                fp,\\n                output_string,\\n                codec=\"\",\\n                laparams=LAParams(),\\n                output_type=\"html\",\\n            )\\n        metadata = {\"source\": self.file_path}\\n        return [Document(page_content=output_string.getvalue(), metadata=metadata)]',\n",
       "   'd': 'Lazy load the chat sessions from the iMessage chat.db\\nand yield them in the required format.\\n\\nYields:\\n    ChatSession: Loaded chat session.',\n",
       "   'l': False,\n",
       "   'g': ['ResNeXt-101-32x8d-ssl',\n",
       "    'Returns:\\n    - model (nn.Module):',\n",
       "    '`ssl_resnext101_32x8d` is a PyTorch implementation of the\\n  `ResNeXt-101-32x8d` model from\\n  https://github.com/facebook/fb-ResNet-Python/blob/',\n",
       "    '`ssl_resnext101_32x8d` is a PyTorch implementation of the\\n  ResNeXt-101-32x8d-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl-ssl']},\n",
       "  {'c': '    def input_variables(self) -> List[str]:\\n',\n",
       "   'd': 'Input variables for this prompt template.\\n\\nReturns:\\n    List of input variables.',\n",
       "   'l': True,\n",
       "   'g': ['Returns the center of the arc.',\n",
       "    'Finds the center of an arc by using the 4 points of the arc.\\n    :return: np.array(float) of the center of the arc.',\n",
       "    'Find the center of the arc.\\n\\n    :return: The center of the arc.\\n    :rtype: np.array\\n    :raises: ValueError if the arc is not a circle.\\n    :raises: Exception if the arc is not a circle.\\n    :raises: Exception if the arc is not a circle.\\n    :raises: Exception if the arc is not a circle.\\n    :raises: Exception if the arc is not a circle.\\n    :raises: Exception if the arc is not a circle.\\n    :raises: Exception if the arc is not a circle.\\n    :raises: Exception if the arc is not a circle.\\n    :raises: Exception if the arc is not a circle.\\n    :raises: Exception if the arc is not a circle.\\n    :raises: Exception if the arc is not a circle.\\n    :raises: Exception if the arc is not a circle.\\n    :raises: Exception if the arc is not a circle.\\n    :raises: Exception if the arc is not a circle.\\n    :raises: Exception if the arc is not a circle.\\n    :raises: Exception if the arc is not a circle.\\n    :raises:',\n",
       "    '']},\n",
       "  {'c': '    def input_keys(self) -> List[str]:\\n        return self._input_keys',\n",
       "   'd': 'Return the input keys.\\n\\nReturns:\\n    List of input keys.',\n",
       "   'l': True,\n",
       "   'g': ['_init_', '_init_', '_init_', '_init_']},\n",
       "  {'c': 'def return_stopped_response(\\n    self,\\n    early_stopping_method: str,\\n    intermediate_steps: List[Tuple[AgentAction, str]],\\n    **kwargs: Any,\\n) -> AgentFinish:\\n    if early_stopping_method == \"force\":\\n\\n        return AgentFinish(\\n            {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\\n        )\\n    else:\\n        raise ValueError(\\n            f\"Got unsupported early_stopping_method `{early_stopping_method}`\"\\n        )',\n",
       "   'd': 'Return response when agent has been stopped due to max iterations.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '\"A function that builds a model and a quantize mapping for a given model_config and a given\\n    quantization.\\n    :param model_config: The model_config to use for the model.\\n    :param quantization: The quantization to use for the model.\\n    :return: A tuple of a model and a quantize mapping.\\n    :rtype: Tuple[nn.Module, QuantizeMapping].\\n    :raises ValueError: If the model_config is not a GemmaConfig.\\n    :raises ValueError: If the quantization is not a GroupQuantize.',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': '    def _add_vectors(\\n        client: supabase.client.Client,\\n        table_name: str,\\n        vectors: List[List[float]],\\n        documents: List[Document],\\n        ids: List[str],\\n    ) -> List[str]:\\n        rows: List[Dict[str, Any]] = [\\n            {\\n                \"id\": ids[idx],\\n                \"content\": documents[idx].page_content,\\n                \"embedding\": embedding,\\n                \"metadata\": documents[idx].metadata,\\n            }\\n            for idx, embedding in enumerate(vectors)\\n        ]\\n\\n\\n\\n        chunk_size = 500\\n        id_list: List[str] = []\\n        for i in range(0, len(rows), chunk_size):\\n            chunk = rows[i : i + chunk_size]\\n\\n            result = client.from_(table_name).upsert(chunk).execute()\\n\\n            if len(result.data) == 0:\\n                raise Exception(\"Error inserting: No rows added\")\\n\\n\\n            ids = [str(i.get(\"id\")) for i in result.data if i.get(\"id\")]\\n\\n            id_list.extend(ids)\\n\\n        return id_list',\n",
       "   'd': 'Add vectors to Supabase table.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def input_schema(self) -> Type[BaseModel]:\\n        func = getattr(self, \"func\", None) or getattr(self, \"afunc\")\\n\\n        if isinstance(func, itemgetter):\\n\\n\\n            items = str(func).replace(\"operator.itemgetter(\", \"\")[:-1].split(\", \")\\n            if all(\\n                item[0] == \"\\'\" and item[-1] == \"\\'\" and len(item) > 2 for item in items\\n            ):\\n\\n                return create_model(\\n                    \"RunnableLambdaInput\",\\n                    **{item[1:-1]: (Any, None) for item in items},\\n                )\\n            else:\\n                return create_model(\"RunnableLambdaInput\", __root__=(List[Any], None))\\n\\n        if dict_keys := get_function_first_arg_dict_keys(func):\\n            return create_model(\\n                \"RunnableLambdaInput\",\\n                **{key: (Any, None) for key in dict_keys},\\n            )\\n\\n        return super().input_schema',\n",
       "   'd': 'Construct ElasticVectorSearch wrapper from raw documents.\\n\\nThis is a user-friendly interface that:\\n    1. Embeds documents.\\n    2. Creates a new index for the embeddings in the Elasticsearch instance.\\n    3. Adds the documents to the newly created Elasticsearch index.\\n\\nThis is intended to be a quick way to get started.\\n\\nExample:\\n    .. code-block:: python\\n\\n        from langchain import ElasticVectorSearch\\n        from langchain.embeddings import OpenAIEmbeddings\\n        embeddings = OpenAIEmbeddings()\\n        elastic_vector_search = ElasticVectorSearch.from_texts(\\n            texts,\\n            embeddings,\\n            elasticsearch_url=\"http://localhost:9200\"\\n        )',\n",
       "   'l': False,\n",
       "   'g': [':param target: \\n    :param use_cache: \\n    :return:',\n",
       "    'download file from remote server to local.\\n\\n    :param target: file path\\n    :param use_cache: whether to use cache.\\n    :return: local file path.\\n    :rtype: str',\n",
       "    'download file from remote server to local server\\n\\n  :param target: remote server file path\\n  :param use_cache: use local file if exist\\n  :return: local file path',\n",
       "    ':param target: \\n    :param use_cache: \\n    :return:']},\n",
       "  {'c': 'def _run_chain(\\n    chain: Union[Chain, Runnable],\\n    inputs: Dict[str, Any],\\n    callbacks: Callbacks,\\n    *,\\n    tags: Optional[List[str]] = None,\\n    input_mapper: Optional[Callable[[Dict], Any]] = None,\\n) -> Union[Dict, str]:\\n    inputs_ = inputs if input_mapper is None else input_mapper(inputs)\\n    if isinstance(chain, Chain) and isinstance(inputs_, dict) and len(inputs_) == 1:\\n        val = next(iter(inputs_.values()))\\n        output = chain(val, callbacks=callbacks, tags=tags)\\n    else:\\n        runnable_config = RunnableConfig(tags=tags or [], callbacks=callbacks)\\n        output = chain.invoke(inputs_, config=runnable_config)\\n    return output',\n",
       "   'd': 'Run a chain on inputs.',\n",
       "   'l': True,\n",
       "   'g': [\"(cls, name: str, agent_cls: Type['Agent']) -> None:\\n\\n    Register a new agent class under a given name.\\n\\n    Args:\\n        cls (type): The class to register.\\n        name (str): The name to register the agent under.\\n        agent_cls (Type['Agent']): The type of the agent to register.\\n\\n    Returns:\\n        None: This function does not return a value.\",\n",
       "    \".register(cls, name, agent_cls)\\n\\n    Register an agent class under a given name.\\n\\n    Parameters\\n    ----------\\n    cls: Class\\n        The parent class of the agent.\\n    name: str\\n        The name of the agent.\\n    agent_cls: Type['Agent']\\n        The agent class.\",\n",
       "    \".register(cls, name: str, agent_cls: Type['Agent'])\",\n",
       "    \"(cls, name: str, agent_cls: Type['Agent']) -> None:\\n\\n    Register an agent class for use in the environment.\"]},\n",
       "  {'c': '    def __init__(\\n        self,\\n        file: Union[IO, Sequence[IO]],\\n        mode: str = \"single\",\\n        url: str = \"https://api.unstructured.io/general/v0/general\",\\n        api_key: str = \"\",\\n        **unstructured_kwargs: Any,\\n    ):\\n        if isinstance(file, collections.abc.Sequence):\\n            validate_unstructured_version(min_unstructured_version=\"0.6.3\")\\n        if file:\\n            validate_unstructured_version(min_unstructured_version=\"0.6.2\")\\n\\n        self.url = url\\n        self.api_key = api_key\\n\\n        super().__init__(file=file, mode=mode, **unstructured_kwargs)',\n",
       "   'd': 'Initialize with file path.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Update time every 1/60th of a second.',\n",
       "    'Initializes the clock.\\n\\n  Args:\\n    self: The clock object.',\n",
       "    ':type self: MainScene\\n  :rtype: None\\n  :return: None\\n  :raises: None\\n  :no']},\n",
       "  {'c': '    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        prompt = inputs[self.input_key]\\n\\n        _intent = self.sparql_intent_chain.run({\"prompt\": prompt}, callbacks=callbacks)\\n        intent = _intent.strip()\\n\\n        if \"SELECT\" not in intent and \"UPDATE\" not in intent:\\n            raise ValueError(\\n                \"I am sorry, but this prompt seems to fit none of the currently \"\\n                \"supported SPARQL query types, i.e., SELECT and UPDATE.\"\\n            )\\n        elif intent.find(\"SELECT\") < intent.find(\"UPDATE\"):\\n            sparql_generation_chain = self.sparql_generation_select_chain\\n            intent = \"SELECT\"\\n        else:\\n            sparql_generation_chain = self.sparql_generation_update_chain\\n            intent = \"UPDATE\"\\n\\n        _run_manager.on_text(\"Identified intent:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(intent, color=\"green\", end=\"\\\\n\", verbose=self.verbose)\\n\\n        generated_sparql = sparql_generation_chain.run(\\n            {\"prompt\": prompt, \"schema\": self.graph.get_schema}, callbacks=callbacks\\n        )\\n\\n        _run_manager.on_text(\"Generated SPARQL:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(\\n            generated_sparql, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n\\n        if intent == \"SELECT\":\\n            context = self.graph.query(generated_sparql)\\n\\n            _run_manager.on_text(\"Full Context:\", end=\"\\\\n\", verbose=self.verbose)\\n            _run_manager.on_text(\\n                str(context), color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n            )\\n            result = self.qa_chain(\\n                {\"prompt\": prompt, \"context\": context},\\n                callbacks=callbacks,\\n            )\\n            res = result[self.qa_chain.output_key]\\n        elif intent == \"UPDATE\":\\n            self.graph.update(generated_sparql)\\n            res = \"Successfully inserted triples into the graph.\"\\n        else:\\n            raise ValueError(\"Unsupported SPARQL query type.\")\\n        return {self.output_key: res}',\n",
       "   'd': 'Generate SPARQL query, use it to retrieve a response from the gdb and answer\\nthe question.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '()', '']},\n",
       "  {'c': '    def from_messages(\\n        cls, messages: Sequence[Union[BaseMessagePromptTemplate, BaseMessage]]\\n    ) -> ChatPromptTemplate:\\n        input_vars = set()\\n        for message in messages:\\n            if isinstance(message, BaseMessagePromptTemplate):\\n                input_vars.update(message.input_variables)\\n        return cls(input_variables=list(input_vars), messages=messages)',\n",
       "   'd': 'Return dictionary representation of agent.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def __init__(self, separator: str = \"\\\\n\\\\n\", **kwargs: Any) -> None:\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator',\n",
       "   'd': 'Test PDFMiner loader.',\n",
       "   'l': False,\n",
       "   'g': ['Parse a set expression.\\n\\n  :returns: a :class:`~django.template.base.Node`\\n  :parametrization:\\n    - `name:endset` - endset expression\\n    - `assign` - assign expression\\n    - `filter` - filter expression\\n    - `name:endset` - endset expression\\n    - `assign` - assign expression\\n    - `filter` - filter expression',\n",
       "    'Parse a set expression.\\n\\n  :returns: A :class:`~django.template.base.Node` representing the set\\n            expression.',\n",
       "    '',\n",
       "    'Parse a set expression.\\n\\n  :returns: :class:`~django.template.base.Node`\\n  :parametrized-by: assign_target, filter_node, body\\n\\n  :parametrized-by: assign_target, filter_node, body\\n  :parametrized-by: assign_target, filter_node, body, drop_needle\\n  :parametrized-by: assign_target, filter_node, body, drop_needle\\n  :parametrized-by: assign_target, filter_node, body, drop_needle, with_namespace']},\n",
       "  {'c': 'def _llm_type(self) -> str:\\n    return \"azure\"',\n",
       "   'd': \"Call out to Aleph Alpha's asymmetric, query embedding endpoint\\nArgs:\\n    text: The text to embed.\\n\\nReturns:\\n    Embeddings for the text.\",\n",
       "   'l': False,\n",
       "   'g': ['(path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext',\n",
       "    '(path: str, ext_map: Optional[Dict[str, List[str]]]=None) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]=None) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]=None) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]=None) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]=None) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]=None) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]=None) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]=None) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]=None) -> str:\\n    (path: str, ext_map: Optional[Dict[',\n",
       "    '(path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext_map: Optional[Dict[str, List[str]]]) -> str:\\n    (path: str, ext',\n",
       "    '(str, Optional[Dict[str, List[str]]]) = None) -> str:\\n    code = \"\\n        code += ImportDefinition.build(path=path)\\n        code += ClassDefinition.build(path, ext_map)\\n\\n        return code\\n    \\n    def build(path: str, ext_map: Optional[Dict[str, List[str]]] = None) -> str:']},\n",
       "  {'c': '    def delete(\\n        self,\\n        ids: Optional[List[str]] = None,\\n        refresh_indices: Optional[bool] = True,\\n        **kwargs: Any,\\n    ) -> Optional[bool]:\\n        try:\\n            from elasticsearch.helpers import BulkIndexError, bulk\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import elasticsearch python package. \"\\n                \"Please install it with `pip install elasticsearch`.\"\\n            )\\n\\n        body = []\\n\\n        if ids is None:\\n            raise ValueError(\"ids must be provided.\")\\n\\n        for _id in ids:\\n            body.append({\"_op_type\": \"delete\", \"_index\": self.index_name, \"_id\": _id})\\n\\n        if len(body) > 0:\\n            try:\\n                bulk(self.client, body, refresh=refresh_indices, ignore_status=404)\\n                logger.debug(f\"Deleted {len(body)} texts from index\")\\n\\n                return True\\n            except BulkIndexError as e:\\n                logger.error(f\"Error deleting texts: {e}\")\\n                firstError = e.errors[0].get(\"index\", {}).get(\"error\", {})\\n                logger.error(f\"First error reason: {firstError.get(\\'reason\\')}\")\\n                raise e\\n\\n        else:\\n            logger.debug(\"No texts to delete from index\")\\n            return False',\n",
       "   'd': 'Expect input key.\\n\\n:meta private:',\n",
       "   'l': False,\n",
       "   'g': ['', 'Returns the distance and angle of a point.', '', '']},\n",
       "  {'c': 'def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n    return {self.memory_key: self.buffer}',\n",
       "   'd': 'Return history buffer.',\n",
       "   'l': True,\n",
       "   'g': ['(self, *args, **kwargs) -> None:',\n",
       "    '(self, *args, **kwargs)',\n",
       "    '_init_',\n",
       "    '(self, *args, **kwargs)']},\n",
       "  {'c': '    def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n        from gptcache.adapter.api import get\\n\\n        _gptcache = self._get_gptcache(llm_string)\\n\\n        res = get(prompt, cache_obj=_gptcache)\\n        if res:\\n            return [\\n                Generation(**generation_dict) for generation_dict in json.loads(res)\\n            ]\\n        return None',\n",
       "   'd': 'Initialize a PythonCodeTextSplitter.',\n",
       "   'l': False,\n",
       "   'g': ['_regularizers: List[Tuple[str, Regularizer]] = None',\n",
       "    '_regularizers: List[Tuple[str, Regularizer]] = None',\n",
       "    '_regularizers: List[Tuple[str, Regularizer]] = None',\n",
       "    '_regularizers: List[Tuple[str, Regularizer]] = None']},\n",
       "  {'c': 'def _get_docs(\\n    self,\\n    question: str,\\n    inputs: Dict[str, Any],\\n    *,\\n    run_manager: CallbackManagerForChainRun,\\n) -> List[Document]:\\n',\n",
       "   'd': 'Return consistent embeddings for the text, if seen before, or a constant\\none if the text is unknown.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def buffer(self) -> List[BaseMessage]:\\n    return self.chat_memory.messages',\n",
       "   'd': 'String buffer of memory.',\n",
       "   'l': True,\n",
       "   'g': [':param obj: object to convert to string\\n    :param encoding: encoding to use\\n    :param errors: error handling method\\n    :return: string representation of the object\\n    :raises: :class:`~django.core.exceptions.ImproperlyConfigured`\\n    :',\n",
       "    '',\n",
       "    '',\n",
       "    ':param obj:  str or bytes\\n  :param encoding:  str\\n  :param errors:  str\\n  :return:  str or bytes']},\n",
       "  {'c': 'def serialize_chat_messages(self, messages: List[Dict]) -> str:\\n    chat_messages = _get_messages_from_run_dict(messages)\\n    return get_buffer_string(chat_messages)',\n",
       "   'd': 'An individual iterator of a :py:func:`~.tee`',\n",
       "   'l': False,\n",
       "   'g': ['(int) -> (int)',\n",
       "    '(self):',\n",
       "    '.close()\\n    return self.close_connection()',\n",
       "    '(self):\\n\\n    def __enter__(self):\\n\\n    def __exit__(self, *args, **kwargs):\\n\\n    def __call__(self, *args, **kwargs):\\n\\n    def __repr__(self):\\n\\n    def __str__(self):\\n\\n    def __getattr__(self, attr):\\n\\n    def __setattr__(self, attr, value):\\n\\n    def __delattr__(self, attr):\\n\\n    def __getattribute__(self, attr):\\n\\n    def __setattribute__(self, attr, value):\\n\\n    def __delattribute__(self, attr):\\n\\n    def __getitem__(self, key):\\n\\n    def __setitem__(self, key, value):\\n\\n    def __delitem__(self, key):']},\n",
       "  {'c': '    def clear(self, **kwargs: Any) -> None:\\n        from momento.responses import CacheFlush\\n\\n        flush_response = self.cache_client.flush_cache(self.cache_name)\\n        if isinstance(flush_response, CacheFlush.Success):\\n            pass\\n        elif isinstance(flush_response, CacheFlush.Error):\\n            raise flush_response.inner_exception',\n",
       "   'd': 'Clear the cache.\\n\\nRaises:\\n    SdkException: Momento service or network error',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def lazy_load(self) -> Iterator[chat_loaders.ChatSession]:\\n        import sqlite3\\n\\n        try:\\n            conn = sqlite3.connect(self.db_path)\\n        except sqlite3.OperationalError as e:\\n            raise ValueError(\\n                f\"Could not open iMessage DB file {self.db_path}.\\\\n\"\\n                \"Make sure your terminal emulator has disk access to this file.\\\\n\"\\n                \"   You can either copy the DB file to an accessible location\"\\n                \" or grant full disk access for your terminal emulator.\"\\n                \"  You can grant full disk access for your terminal emulator\"\\n                \" in System Settings > Security and Privacy > Full Disk Access.\"\\n            ) from e\\n        cursor = conn.cursor()\\n\\n\\n        query = \"\"\"SELECT chat_id\\n        FROM message\\n        JOIN chat_message_join ON message.ROWID = chat_message_join.message_id\\n        GROUP BY chat_id\\n        ORDER BY MAX(date) DESC;\"\"\"\\n        cursor.execute(query)\\n        chat_ids = [row[0] for row in cursor.fetchall()]\\n\\n        for chat_id in chat_ids:\\n            yield self._load_single_chat_session(cursor, chat_id)\\n\\n        conn.close()',\n",
       "   'd': 'Lazy load the chat sessions from the iMessage chat.db\\nand yield them in the required format.\\n\\nYields:\\n    ChatSession: Loaded chat session.',\n",
       "   'l': True,\n",
       "   'g': ['Send a table of the status of the RPC client.\\n\\n  :param bot: The bot instance.\\n  :param update: The update instance.',\n",
       "    'Send a table with the status of the RPC server.',\n",
       "    'Send the status table.\\n\\n  :param bot: The bot.\\n  :param update: The update.',\n",
       "    'Show the status table in a message.\\n\\n  :param bot: the bot to send the message to\\n  :param update: the update to send the message to\\n  :return: None\\n  :raises: RPCException if the RPC server is not available']},\n",
       "  {'c': '    def split_text(self, text: str) -> List[Document]:\\n        lines = text.split(\"\\\\n\")\\n\\n        lines_with_metadata: List[LineType] = []\\n\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n\\n\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            for sep, name in self.headers_to_split_on:\\n\\n                if stripped_line.startswith(sep) and (\\n\\n\\n                    len(stripped_line) == len(sep)\\n                    or stripped_line[len(sep)] == \" \"\\n                ):\\n\\n                    if name is not None:\\n\\n                        current_header_level = sep.count(\"\\n\\n\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n\\n\\n                            popped_header = header_stack.pop()\\n\\n\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])\\n\\n\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n\\n                        initial_metadata[name] = header[\"data\"]\\n\\n\\n\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n\\n\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
       "   'd': 'Split markdown file\\nArgs:\\n    text: Markdown file',\n",
       "   'l': True,\n",
       "   'g': ['_get_lr_scale()',\n",
       "    '(opt) -> (current_lr, lr)',\n",
       "    '_get_lr_scale\\n    :param opt:\\n    :return:\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype: float\\n    :rtype:',\n",
       "    '_get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr_scale()\\n    _get_lr']},\n",
       "  {'c': '    def __add__(self, other: Any) -> ChatPromptTemplate:\\n        if isinstance(other, ChatPromptTemplate):\\n            return ChatPromptTemplate(messages=self.messages + other.messages)\\n        elif isinstance(\\n            other, (BaseMessagePromptTemplate, BaseMessage, BaseChatPromptTemplate)\\n        ):\\n            return ChatPromptTemplate(messages=self.messages + [other])\\n        elif isinstance(other, (list, tuple)):\\n            _other = ChatPromptTemplate.from_messages(other)\\n            return ChatPromptTemplate(messages=self.messages + _other.messages)\\n        elif isinstance(other, str):\\n            prompt = HumanMessagePromptTemplate.from_template(other)\\n            return ChatPromptTemplate(messages=self.messages + [prompt])\\n        else:\\n            raise NotImplementedError(f\"Unsupported operand type for +: {type(other)}\")',\n",
       "   'd': 'Combine two prompt templates.\\n\\nArgs:\\n    other: Another prompt template.\\n\\nReturns:\\n    Combined prompt template.',\n",
       "   'l': True,\n",
       "   'g': ['(str|int, str|int) - chat_id and user_id, or None',\n",
       "    '(int, int)',\n",
       "    '(str|int, str|int) ->',\n",
       "    '(str|int, str|int) ->']},\n",
       "  {'c': 'def put(self, url: str, data: Dict[str, Any], **kwargs: Any) -> str:\\n    return self.requests.put(url, data, **kwargs).text',\n",
       "   'd': 'Validate that chains are all single input/output.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '_defaultStack(layer = None, axolotl = False, groups = True, media = True, privacy = True, profiles = True):',\n",
       "    '_defaultStack()',\n",
       "    '']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\\n\\n        examples = self._get_examples(**kwargs)\\n        examples = [\\n            {k: e[k] for k in self.example_prompt.input_variables} for e in examples\\n        ]\\n\\n        example_strings = [\\n            self.example_prompt.format(**example) for example in examples\\n        ]\\n\\n        pieces = [self.prefix, *example_strings, self.suffix]\\n        template = self.example_separator.join([piece for piece in pieces if piece])\\n\\n\\n        return DEFAULT_FORMATTER_MAPPING[self.template_format](template, **kwargs)',\n",
       "   'd': 'Test end to end construction and search without metadata.',\n",
       "   'l': False,\n",
       "   'g': ['_run(self) -> str:', '(str)', '(def)', '(self) -> str:']},\n",
       "  {'c': 'def on_tool_error(\\n    self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\\n) -> None:\\n    self.step += 1\\n    self.errors += 1',\n",
       "   'd': 'Create a class from a string template.\\n\\nArgs:\\n    template: a template.\\n    template_format: format of the template.\\n    **kwargs: keyword arguments to pass to the constructor.\\n\\nReturns:\\n    A new instance of this class.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def _chain_type(self) -> str:\\n    return \"retrieval_qa\"',\n",
       "   'd': 'Return the chain type.',\n",
       "   'l': True,\n",
       "   'g': ['Load mel_filters from npz file.',\n",
       "    'Load the mel_filters from the npz file.\\n\\n  Args:\\n    device: torch.device.\\n    n_mels: int.',\n",
       "    'Generates a Mel-filters.\\n\\n  Args:\\n      device (torch.device): The device to use.\\n      n_mels (int): Number of Mel-filters to generate.\\n\\n  Returns:\\n      torch.Tensor: The Mel-filters.',\n",
       "    'Generate mel filters.\\n\\n  Args:\\n      device (torch.device): Device to load the filters on.\\n      n_mels (int): Number of filters to generate.\\n\\n  Returns:\\n      torch.Tensor: Mel filters.\\n\\n  Note:\\n      This function generates the mel filters from a pre-trained model.']},\n",
       "  {'c': '    def clear(self, **kwargs: Any) -> None:\\n        from momento.responses import CacheFlush\\n\\n        flush_response = self.cache_client.flush_cache(self.cache_name)\\n        if isinstance(flush_response, CacheFlush.Success):\\n            pass\\n        elif isinstance(flush_response, CacheFlush.Error):\\n            raise flush_response.inner_exception',\n",
       "   'd': 'Return default prompt.',\n",
       "   'l': False,\n",
       "   'g': [':param itag: int\\n    :return: dict\\n    :return: dict - the format profile of the itag\\n    :rtype: dict - the format profile of the itag\\n    :rtype: None - if itag is not in ITAGS, or if itag is not in ITAGS, return None\\n    :rtype: dict - the format profile of the itag\\n    :rtype: None - if itag is not in ITAGS, or if itag is not in ITAGS, return None\\n    :rtype: dict - the format profile of the itag\\n    :rtype: None - if itag is not in ITAGS, or if itag is not in ITAGS, return None\\n    :rtype: dict - the format profile of the itag\\n    :rtype: None - if itag is not in ITAGS, or if itag is not in ITAGS, return None\\n    :rtype: dict - the format profile of the itag\\n    :rtype: None - if itag is not in ITAGS, or if itag is not in ITAGS, return None\\n    :rtype: dict - the format profile of the itag\\n    :rtype:',\n",
       "    ':param itag: int, itag of the video\\n  :return: dict, format profile of the video\\n  :rtype: dict',\n",
       "    ':param itag: \\n    :return: \\n    :rtype:',\n",
       "    ':param itag:']},\n",
       "  {'c': 'def __init__(self, database_path: str = \".langchain.db\"):\\n    engine = create_engine(f\"sqlite:///{database_path}\")\\n    super().__init__(engine)',\n",
       "   'd': 'Initialize by creating the engine and all tables.',\n",
       "   'l': True,\n",
       "   'g': [':param word_set:\\n    :param url:\\n    :param line_function:\\n    :return:\\n    :rtype: None\\n    :raises: ValueError\\n    :raises: TypeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises:  AttributeError\\n    :raises',\n",
       "    ':param word_set:\\n    :param url:\\n    :param line_function:\\n    :return:\\n    :rtype: None',\n",
       "    ':param word_set: set of words to add\\n  :param url: url to fetch\\n  :param line_function: function to process each line of the response\\n  :return: None\\n  :raises: ValueError if the line_function does not return a string or None\\n  :raises: ValueError if the url is not a string or None\\n  :raises: ValueError if the word_set is not a set or None\\n  :raises: ValueError if the line_function does not return a string or None\\n  :raises: ValueError if the url is not a string or None\\n  :raises: ValueError if the word_set is not a set or None\\n  :raises: ValueError if the line_function does not return a string or None\\n  :raises: ValueError if the url is not a string or None\\n  :raises: ValueError if the word_set is not a set or None\\n  :raises: ValueError if the line_function does not return a string or None\\n  :raises: ValueError if the url is not a string or None\\n  :raises: ValueError if the line_function does not return a string or None\\n  :raises: ValueError if the url is not a',\n",
       "    ':param word_set: set of words\\n  :param url: url to get words from\\n  :param line_function: function to process each line of the response']},\n",
       "  {'c': 'def _get_docs(\\n    self,\\n    inputs: Dict[str, Any],\\n    *,\\n    run_manager: CallbackManagerForChainRun,\\n) -> List[Document]:\\n',\n",
       "   'd': 'Get docs to run questioning over.',\n",
       "   'l': True,\n",
       "   'g': ['(x, y) -> z',\n",
       "    '_call_',\n",
       "    '_summary_or_description_of_function_or_method_',\n",
       "    '_summary_or_description_of_method_or_function_or_class_or_module_or_package_or_function_or_class_or_module_or_package_or_function_or_class_or_module_or_package_or_function_or_class_or_module_or_package_or_function_or_class_or_module_or_package_or_function_or_class_or_module_or_package_or_function_or_class_or_module_or_package_or_function_or_class_or_module_or_package_or_function_or_class_or_module_or_package_or_function_or_class_or_module_or_package_or_function_or_class_or_module_or_package_or_function_or_class_or_module_or_package_or_function_or_class_or_module_or_package_or_function_or_class_or_module_or_package_or_function_or_class_or_module_or_package_or_function_or']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\\n        embedding_function: Optional[Embeddings] = None,\\n        persist_directory: Optional[str] = None,\\n        client_settings: Optional[chromadb.config.Settings] = None,\\n        collection_metadata: Optional[Dict] = None,\\n        client: Optional[chromadb.Client] = None,\\n        relevance_score_fn: Optional[Callable[[float], float]] = None,\\n    ) -> None:\\n        try:\\n            import chromadb\\n            import chromadb.config\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import chromadb python package. \"\\n                \"Please install it with `pip install chromadb`.\"\\n            )\\n\\n        if client is not None:\\n            self._client_settings = client_settings\\n            self._client = client\\n            self._persist_directory = persist_directory\\n        else:\\n            if client_settings:\\n                _client_settings = client_settings\\n            elif persist_directory:\\n\\n                major, minor, _ = chromadb.__version__.split(\".\")\\n                if int(major) == 0 and int(minor) < 4:\\n                    _client_settings = chromadb.config.Settings(\\n                        chroma_db_impl=\"duckdb+parquet\",\\n                    )\\n                else:\\n                    _client_settings = chromadb.config.Settings(is_persistent=True)\\n                _client_settings.persist_directory = persist_directory\\n            else:\\n                _client_settings = chromadb.config.Settings()\\n            self._client_settings = _client_settings\\n            self._client = chromadb.Client(_client_settings)\\n            self._persist_directory = (\\n                _client_settings.persist_directory or persist_directory\\n            )\\n\\n        self._embedding_function = embedding_function\\n        self._collection = self._client.get_or_create_collection(\\n            name=collection_name,\\n            embedding_function=self._embedding_function.embed_documents\\n            if self._embedding_function is not None\\n            else None,\\n            metadata=collection_metadata,\\n        )\\n        self.override_relevance_score_fn = relevance_score_fn',\n",
       "   'd': 'Initialize with Chroma client.',\n",
       "   'l': True,\n",
       "   'g': ['Lookup the friends of the current user.\\n\\n  :param user_id: The user id of the user to lookup.\\n  :param screen_name: The screen name of the user to lookup.\\n  :return: The list of friends.',\n",
       "    '',\n",
       "    'Lookup a list of users and their relationships.\\n\\n  :param user_id: The user id to lookup.\\n  :param screen_name: The screen name to lookup.\\n\\n  :return: A list of users and their relationships.',\n",
       "    '']},\n",
       "  {'c': 'def type(self) -> str:\\n    return \"function\"',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': False,\n",
       "   'g': ['Configure the ECharts.js library.\\n\\n  :param jshost: The host to use for the ECharts.js library.\\n  :param echarts_template_dir: The directory to use for the ECharts.js\\n    template files.\\n  :param force_js_embed: Whether to force the use of the ECharts.js\\n    library.  This is useful if you want to use the ECharts.js\\n    library in a non-ECharts.js-based environment.  This is\\n    generally a good idea if you are using a non-ECharts.js\\n    library such as the ECharts.js-based library in a\\n    non-ECharts.js-based environment.\\n\\n  :type jshost: str\\n  :type echarts_template_dir: str\\n  :type force_js_embed: bool\\n\\n  :return: None.',\n",
       "    'Configure the ECharts JavaScript host.\\n\\n  :param jshost: The JavaScript host to use.\\n  :param echarts_template_dir: The directory to use for the template.\\n  :param force_js_embed: Force the JavaScript to be embedded.\\n  :return:',\n",
       "    ':param jshost:\\n  :param echarts_template_dir:\\n  :param force_js_embed:\\n  :return:\\n  :rtype: object\\n  :param kwargs:\\n  :return:\\n  :rtype: object',\n",
       "    ':param jshost:\\n  :param echarts_template_dir:\\n  :param force_js_embed:\\n  :return:\\n  :rtype: object\\n  :raises:']},\n",
       "  {'c': 'def _run(\\n    self, text: str, run_manager: Optional[CallbackManagerForToolRun] = None\\n) -> str:\\n    try:\\n        data = _parse_input(text)\\n        return self.requests_wrapper.put(_clean_url(data[\"url\"]), data[\"data\"])\\n    except Exception as e:\\n        return repr(e)',\n",
       "   'd': 'Combine two prompt templates.\\n\\nArgs:\\n    other: Another prompt template.\\n\\nReturns:\\n    Combined prompt template.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    ':param cls: type of the saved search doc to be created\\n  :param search_doc: type of the search doc to be converted\\n  :param db_doc_id: id of the document in the database to be converted\\n  :return: type of the saved search doc to be created',\n",
       "    ':param cls: The class to create the saved search doc from.\\n  :param search_doc: The search doc to create the saved search doc from.\\n  :param db_doc_id: The database document id to use for the saved search doc.\\n  :return: The saved search doc.\\n  :rtype: :class:`SavedSearchDoc`\\n  :raises: :class:`ValueError` if the search doc is not a :class:`SearchDoc`.\\n  :raises: :class:`ValueError` if the search doc is not a :class:`SearchDoc`.\\n  :raises: :class:`ValueError` if the search doc is not a :class:`SearchDoc`.\\n  :raises: :class:`ValueError` if the search doc is not a :class:`SearchDoc`.\\n  :raises: :class:`ValueError` if the search doc is not a :class:`SearchDoc`.\\n  :raises: :class:`ValueError` if the search doc is not a :class:`SearchDoc`.\\n  :raises: :class:`ValueError` if the search doc is not a :class:`SearchDoc`.\\n  :raises: :class:`ValueError` if the search doc is not a']},\n",
       "  {'c': 'def _run(\\n    self, url: str, run_manager: Optional[CallbackManagerForToolRun] = None\\n) -> str:\\n    return self.requests_wrapper.get(_clean_url(url))',\n",
       "   'd': 'Type of the message, used for serialization.',\n",
       "   'l': False,\n",
       "   'g': ['Convert a number to an interval label.\\n\\n  Args:\\n      x: The number to convert.\\n      pos: The position of the interval in the octave.\\n\\n  Returns:\\n      The interval label.',\n",
       "    '',\n",
       "    'Return the interval label for the given x-value.',\n",
       "    '']},\n",
       "  {'c': '    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n        for gen in return_val:\\n            if not isinstance(gen, Generation):\\n                raise ValueError(\\n                    \"RedisCache only supports caching of normal LLM generations, \"\\n                    f\"got {type(gen)}\"\\n                )\\n\\n        key = self._key(prompt, llm_string)\\n\\n        with self.redis.pipeline() as pipe:\\n            pipe.hset(\\n                key,\\n                mapping={\\n                    str(idx): dumps(generation)\\n                    for idx, generation in enumerate(return_val)\\n                },\\n            )\\n            if self.ttl is not None:\\n                pipe.expire(key, self.ttl)\\n\\n            pipe.execute()',\n",
       "   'd': 'Update cache based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': ['_forward(self, inputs) -> torch.Tensor:\\n    raise Exception(\"The forward method must be implemented by inherited class\")',\n",
       "    '_forward(self, inputs) -> torch.Tensor:\\n    raise Exception(\"The forward method must be implemented by inherited class\")',\n",
       "    '',\n",
       "    '_forward(self, inputs):']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        aleph_alpha_api_key = get_from_dict_or_env(\\n            values, \"aleph_alpha_api_key\", \"ALEPH_ALPHA_API_KEY\"\\n        )\\n        try:\\n            import aleph_alpha_client\\n\\n            values[\"client\"] = aleph_alpha_client.Client(token=aleph_alpha_api_key)\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import aleph_alpha_client python package. \"\\n                \"Please install it with `pip install aleph_alpha_client`.\"\\n            )\\n        return values',\n",
       "   'd': 'Validate that api key and python package exists in environment.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def run_on_dataset(\\n    client: Client,\\n    dataset_name: str,\\n    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,\\n    *,\\n    evaluation: Optional[RunEvalConfig] = None,\\n    concurrency_level: int = 5,\\n    project_name: Optional[str] = None,\\n    verbose: bool = False,\\n    tags: Optional[List[str]] = None,\\n    input_mapper: Optional[Callable[[Dict], Any]] = None,\\n    **kwargs: Any,\\n) -> Dict[str, Any]:\\n    if kwargs:\\n        warnings.warn(\\n            \"The following arguments are deprecated and \"\\n            \"will be removed in a future release: \"\\n            f\"{kwargs.keys()}.\",\\n            DeprecationWarning,\\n        )\\n    wrapped_model, project_name, dataset, examples = _prepare_eval_run(\\n        client, dataset_name, llm_or_chain_factory, project_name\\n    )\\n    if concurrency_level in (0, 1):\\n        results = _run_on_examples(\\n            client,\\n            examples,\\n            wrapped_model,\\n            project_name=project_name,\\n            verbose=verbose,\\n            tags=tags,\\n            evaluation=evaluation,\\n            input_mapper=input_mapper,\\n            data_type=dataset.data_type,\\n        )\\n    else:\\n\\n        coro = _arun_on_examples(\\n            client,\\n            examples,\\n            wrapped_model,\\n            concurrency_level=concurrency_level,\\n            project_name=project_name,\\n            verbose=verbose,\\n            tags=tags,\\n            evaluation=evaluation,\\n            input_mapper=input_mapper,\\n            data_type=dataset.data_type,\\n        )\\n        results = _handle_coroutine(coro)\\n    return {\\n        \"project_name\": project_name,\\n        \"results\": results,\\n    }',\n",
       "   'd': 'Convert a raw function/class to an OpenAI function.\\n\\nArgs:\\n    function: Either a dictionary, a pydantic.BaseModel class, or a Python function.\\n        If a dictionary is passed in, it is assumed to already be a valid OpenAI\\n        function.\\n\\nReturns:\\n    A dict version of the passed in function which is compatible with the\\n        OpenAI function-calling API.',\n",
       "   'l': False,\n",
       "   'g': [':param text:  text to be encrypted\\n    :param debug:  debug mode\\n    :return:  decrypted text',\n",
       "    '',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': 'def _chain_type(self) -> str:\\n    return \"vector_db_qa\"',\n",
       "   'd': 'Return the chain type.',\n",
       "   'l': True,\n",
       "   'g': [':param datToClass: \\n    :param classifierArr: \\n    :return:',\n",
       "    '',\n",
       "    '�',\n",
       "    '']},\n",
       "  {'c': '    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        *,\\n        prompt: Optional[PromptTemplate] = None,\\n        criteria: Optional[Union[CRITERIA_TYPE, str]] = None,\\n        **kwargs: Any,\\n    ) -> PairwiseStringEvalChain:\\n        expected_input_vars = {\"prediction\", \"prediction_b\", \"input\", \"criteria\"}\\n        prompt_ = prompt or PROMPT\\n        if expected_input_vars != set(prompt_.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt_.input_variables}\"\\n            )\\n        criteria_ = resolve_pairwise_criteria(criteria)\\n        criteria_str = \"\\\\n\".join(f\"{k}: {v}\" if v else k for k, v in criteria_.items())\\n        return cls(llm=llm, prompt=prompt_.partial(criteria=criteria_str), **kwargs)',\n",
       "   'd': 'Expect input key.\\n\\n:meta private:',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def generate(\\n    self,\\n    prompts: List[str],\\n    stop: Optional[List[str]] = None,\\n    callbacks: Callbacks = None,\\n    *,\\n    tags: Optional[List[str]] = None,\\n    metadata: Optional[Dict[str, Any]] = None,\\n    **kwargs: Any,\\n) -> LLMResult:\\n    if not isinstance(prompts, list):\\n        raise ValueError(\\n            \"Argument \\'prompts\\' is expected to be of type List[str], received\"\\n            f\" argument of type {type(prompts)}.\"\\n        )\\n    params = self.dict()\\n    params[\"stop\"] = stop\\n    options = {\"stop\": stop}\\n    (\\n        existing_prompts,\\n        llm_string,\\n        missing_prompt_idxs,\\n        missing_prompts,\\n    ) = get_prompts(params, prompts)\\n    disregard_cache = self.cache is not None and not self.cache\\n    callback_manager = CallbackManager.configure(\\n        callbacks,\\n        self.callbacks,\\n        self.verbose,\\n        tags,\\n        self.tags,\\n        metadata,\\n        self.metadata,\\n    )\\n    new_arg_supported = inspect.signature(self._generate).parameters.get(\\n        \"run_manager\"\\n    )\\n    if langchain.llm_cache is None or disregard_cache:\\n        if self.cache is not None and self.cache:\\n            raise ValueError(\\n                \"Asked to cache, but no cache found at `langchain.cache`.\"\\n            )\\n        run_managers = callback_manager.on_llm_start(\\n            dumpd(self), prompts, invocation_params=params, options=options\\n        )\\n        output = self._generate_helper(\\n            prompts, stop, run_managers, bool(new_arg_supported), **kwargs\\n        )\\n        return output\\n    if len(missing_prompts) > 0:\\n        run_managers = callback_manager.on_llm_start(\\n            dumpd(self), missing_prompts, invocation_params=params, options=options\\n        )\\n        new_results = self._generate_helper(\\n            missing_prompts, stop, run_managers, bool(new_arg_supported), **kwargs\\n        )\\n        llm_output = update_cache(\\n            existing_prompts, llm_string, missing_prompt_idxs, new_results, prompts\\n        )\\n        run_info = (\\n            [RunInfo(run_id=run_manager.run_id) for run_manager in run_managers]\\n            if run_managers\\n            else None\\n        )\\n    else:\\n        llm_output = {}\\n        run_info = None\\n    generations = [existing_prompts[i] for i in range(len(prompts))]\\n    return LLMResult(generations=generations, llm_output=llm_output, run=run_info)',\n",
       "   'd': 'The type of output this runnable produces specified as a type annotation.',\n",
       "   'l': False,\n",
       "   'g': [':param cls:\\n  :param v:\\n  :return:',\n",
       "    ':param v: string or list of string\\n    :return: string or list of string',\n",
       "    ':param cls:\\n  :param v:\\n  :return:',\n",
       "    ':param cls:\\n  :param v:\\n  :return:']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: List[str],\\n        metadatas: Optional[List[dict]] = None,\\n        doc_metadata: Optional[dict] = None,\\n    ) -> None:\\n        self.vectorstore.add_texts(texts, metadatas, doc_metadata or {})',\n",
       "   'd': 'Lazily parse the blob.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    \"Returns the group of a token.\\n\\n    >>> ungroup('(a b c)')\\n    'abc'\\n    >>> ungroup('(a b c d e f)')\\n    'abcdef'\\n    >>> ungroup('(a b c d e f g h i j k l m n o p q r s t u v w x y z)')\\n    'abcdefghijklmnopqrstuvwxyz'\\n    >>> ungroup('(a b c d e f g h i j k l m n o p q r s t u v w x y z (a b c d e f g h i j k l m n o p q r s t u v w x y z))')\\n    'abcdefghijklmnopqrstuvwxyz'\\n    >>> ungroup('(a b c d e f g h i j k l m n o p q r s t u v w x y z (a b c d e f g h i j k l m n o p q r s t u v w x y z (a b c d e f g h i j k l m n o p q r s t u v w x y z)))')\\n    'abcdefghijklmnopqrstuvwxyz'\\n    >>> ungroup('(a b c\",\n",
       "    ':param expr:  A list of tokens.\\n    :return:  A list of tokens.',\n",
       "    'This function is used to convert a group to a single token.\\n    :param expr: the expression to be converted\\n    :return: the converted expression']},\n",
       "  {'c': '    def on_tool_error(\\n        self,\\n        error: BaseException,\\n        *,\\n        run_id: UUID,\\n        **kwargs: Any,\\n    ) -> None:\\n        if not run_id:\\n            raise TracerException(\"No run_id provided for on_tool_error callback.\")\\n        tool_run = self.run_map.get(str(run_id))\\n        if tool_run is None or tool_run.run_type != \"tool\":\\n            raise TracerException(f\"No tool Run found to be traced for {run_id}\")\\n\\n        tool_run.error = repr(error)\\n        tool_run.end_time = datetime.utcnow()\\n        tool_run.events.append({\"name\": \"error\", \"time\": tool_run.end_time})\\n        self._end_trace(tool_run)\\n        self._on_tool_error(tool_run)',\n",
       "   'd': 'Handle an error for a tool run.',\n",
       "   'l': True,\n",
       "   'g': ['_dispatchers: list of functions to dispatch to\\n_dp: the function to use to dispatch to the functions in _dispatchers\\n_ep: the function to use to concatenate the results of the functions in _dispatchers\\n_dp: the function to use to dispatch to the functions in _dispatchers\\n_ep: the function to use to concatenate the results of the functions in _dispatchers\\n_ep: the function to use to convert the gradient to a tensor\\n_dp: the function to use to dispatch to the functions in _dispatchers\\n_ep: the function to use to concatenate the results of the functions in _dispatchers\\n_ep: the function to use to convert the gradient to a tensor\\n_dp: the function to use to dispatch to the functions in _dispatchers\\n_ep: the function to use to concatenate the results of the functions in _dispatchers\\n_ep: the function to use to convert the gradient to a tensor\\n_dp: the function to use to dispatch to the functions in _dispatchers\\n_ep: the function to use to concatenate the results of the functions in _dispatchers\\n_ep: the function to use to convert the gradient to a tensor\\n_dp: the function',\n",
       "    '_dispatchers = [\\n  (tf.float32, tf.float32),\\n  (tf.float32, tf.float32),\\n  (tf.float32, tf.float32),\\n  (tf.float32, tf.float32),\\n  (tf.float32, tf.float32),\\n  (tf.float32, tf.float32),\\n  (tf.float32, tf.float32),\\n  (tf.float32, tf.float32),\\n  (tf.float32, tf.float32),\\n  (tf.float32, tf.float32),\\n  (tf.float32, tf.float32),\\n  (tf.float32, tf.float32),\\n  (tf.float32, tf.float32),\\n  (tf.float32, tf.float32),\\n  (tf.float32, tf.float32),\\n  (tf.float32, tf.float32),\\n  (tf.float32, tf.',\n",
       "    '.  :param inp:  :return:  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :',\n",
       "    '_dispatchers:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:\\n    _dp:\\n    _ep:']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\\n\\n        examples = self._get_examples(**kwargs)\\n        examples = [\\n            {k: e[k] for k in self.example_prompt.input_variables} for e in examples\\n        ]\\n\\n        example_strings = [\\n            self.example_prompt.format(**example) for example in examples\\n        ]\\n\\n        pieces = [self.prefix, *example_strings, self.suffix]\\n        template = self.example_separator.join([piece for piece in pieces if piece])\\n\\n\\n        return DEFAULT_FORMATTER_MAPPING[self.template_format](template, **kwargs)',\n",
       "   'd': 'Construct FAISS wrapper from raw documents.\\n\\nThis is a user friendly interface that:\\n    1. Embeds documents.\\n    2. Creates an in memory docstore\\n    3. Initializes the FAISS database\\n\\nThis is intended to be a quick way to get started.\\n\\nExample:\\n    .. code-block:: python\\n\\n        from langchain import FAISS\\n        from langchain.embeddings import OpenAIEmbeddings\\n\\n        embeddings = OpenAIEmbeddings()\\n        text_embeddings = embeddings.embed_documents(texts)\\n        text_embedding_pairs = zip(texts, text_embeddings)\\n        faiss = FAISS.from_embeddings(text_embedding_pairs, embeddings)',\n",
       "   'l': False,\n",
       "   'g': ['Build a backbone from a config.\\n    Args:\\n        cfg (ConfigDict): A config dict.\\n        default_args (dict, optional): A dict of default arguments.\\n    Returns:\\n        str: The name of the backbone.',\n",
       "    '',\n",
       "    'Builds a backbone from a config.',\n",
       "    'Build a backbone from a config.\\n\\n    Args:\\n        cfg (ConfigDict): Config to build from.\\n        default_args (dict, optional): Default args to pass to the backbone.\\n\\n    Returns:\\n        str: The backbone name.']},\n",
       "  {'c': '    def lazy_parse(self, blob: Blob) -> Iterator[Document]:\\n        url_parse_result = urlparse(str(blob.path)) if blob.path else None\\n\\n        if (\\n            url_parse_result\\n            and url_parse_result.scheme == \"s3\"\\n            and url_parse_result.netloc\\n        ):\\n            textract_response_json = self.tc.call_textract(\\n                input_document=str(blob.path),\\n                features=self.textract_features,\\n                boto3_textract_client=self.boto3_textract_client,\\n            )\\n        else:\\n            textract_response_json = self.tc.call_textract(\\n                input_document=blob.as_bytes(),\\n                features=self.textract_features,\\n                call_mode=self.tc.Textract_Call_Mode.FORCE_SYNC,\\n                boto3_textract_client=self.boto3_textract_client,\\n            )\\n\\n        current_text = \"\"\\n        current_page = 1\\n        for block in textract_response_json[\"Blocks\"]:\\n            if \"Page\" in block and not (int(block[\"Page\"]) == current_page):\\n                yield Document(\\n                    page_content=current_text,\\n                    metadata={\"source\": blob.source, \"page\": current_page},\\n                )\\n                current_text = \"\"\\n                current_page = int(block[\"Page\"])\\n            if \"Text\" in block:\\n                current_text += block[\"Text\"] + \" \"\\n\\n        yield Document(\\n            page_content=current_text,\\n            metadata={\"source\": blob.source, \"page\": current_page},\\n        )',\n",
       "   'd': 'Iterates over the Blob pages and returns an Iterator with a Document\\nfor each page, like the other parsers If multi-page document, blob.path\\nhas to be set to the S3 URI and for single page docs the blob.data is taken',\n",
       "   'l': True,\n",
       "   'g': ['(int) number: number of elements to add to the list.\\n    (bool) le: if True, randomize_nb_elements will return a number\\n        between 10 and 100.\\n    (bool) ge: if True, randomize_nb_elements will return a number\\n        between 100 and 140.\\n    return: int: number of elements to add to the list.',\n",
       "    '(int, bool, bool) -> int',\n",
       "    '(int) number: number of elements to randomize.\\n    (bool) le: lower bound for the random number.\\n    (bool) ge: upper bound for the random number.\\n\\n    Returns a random number between 10 and 100.',\n",
       "    '(int) Randomize the number of elements in the list.\\n    \\n    :param number: (int) The maximum number of elements to generate.\\n    :param le: (bool) Whether to generate a number less than the maximum.\\n    :param ge: (bool) Whether to generate a number greater than the maximum.\\n    :return: (int) The number of elements to generate.']},\n",
       "  {'c': 'def load_agent(\\n    path: Union[str, Path], **kwargs: Any\\n) -> Union[BaseSingleActionAgent, BaseMultiActionAgent]:\\n    valid_suffixes = {\"json\", \"yaml\"}\\n    if hub_result := try_load_from_hub(\\n        path, _load_agent_from_file, \"agents\", valid_suffixes\\n    ):\\n        return hub_result\\n    else:\\n        return _load_agent_from_file(path, **kwargs)',\n",
       "   'd': 'Create a class from a string template.\\n\\nArgs:\\n    template: a template.\\n    template_format: format of the template.\\n    **kwargs: keyword arguments to pass to the constructor.\\n\\nReturns:\\n    A new instance of this class.',\n",
       "   'l': False,\n",
       "   'g': [':param xml: \\n    :return: \\n    :rtype:',\n",
       "    '',\n",
       "    'Converts an XML node to a dictionary.\\n    :param xml: The XML node to convert.\\n    :return: The converted dictionary.',\n",
       "    '将xml转换成dict\\n    :param xml: xml\\n    :return: dict']},\n",
       "  {'c': '    def plan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[AgentAction, AgentFinish]:\\n        inputs = {**kwargs, **{\"intermediate_steps\": intermediate_steps}}\\n        output = self.runnable.invoke(inputs, config={\"callbacks\": callbacks})\\n        return output',\n",
       "   'd': 'Define retry mechanism.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def OutputType(self) -> Any:\\n    func = getattr(self, \"func\", None) or getattr(self, \"afunc\")\\n    try:\\n        sig = inspect.signature(func)\\n        return (\\n            sig.return_annotation\\n            if sig.return_annotation != inspect.Signature.empty\\n            else Any\\n        )\\n    except ValueError:\\n        return Any',\n",
       "   'd': 'Compute query embeddings using a HuggingFace instruct model.\\n\\nArgs:\\n    text: The text to embed.\\n\\nReturns:\\n    Embeddings for the text.',\n",
       "   'l': False,\n",
       "   'g': ['_sequential_loop():',\n",
       "    \"_sequential_loop():\\n    This method is used for the process 'sequential'.\\n    :return: The name of the agent that will be used to play the game.\\n    :rtype: str\",\n",
       "    '.',\n",
       "    '(This method is called by the main loop of the experiment.)']},\n",
       "  {'c': 'def validate_chains(cls, values: Dict) -> Dict:\\n    for chain in values[\"chains\"]:\\n        if len(chain.input_keys) != 1:\\n            raise ValueError(\\n                \"Chains used in SimplePipeline should all have one input, got \"\\n                f\"{chain} with {len(chain.input_keys)} inputs.\"\\n            )\\n        if len(chain.output_keys) != 1:\\n            raise ValueError(\\n                \"Chains used in SimplePipeline should all have one output, got \"\\n                f\"{chain} with {len(chain.output_keys)} outputs.\"\\n            )\\n    return values',\n",
       "   'd': 'Execute the query, return the results or an error message.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        collection_name: str = \"LangChainCollection\",\\n        connection_args: dict[str, Any] = {},\\n        consistency_level: str = \"Session\",\\n        index_params: Optional[dict] = None,\\n        search_params: Optional[dict] = None,\\n        drop_old: bool = False,\\n        **kwargs: Any,\\n    ) -> Zilliz:\\n        vector_db = cls(\\n            embedding_function=embedding,\\n            collection_name=collection_name,\\n            connection_args=connection_args,\\n            consistency_level=consistency_level,\\n            index_params=index_params,\\n            search_params=search_params,\\n            drop_old=drop_old,\\n            **kwargs,\\n        )\\n        vector_db.add_texts(texts=texts, metadatas=metadatas)\\n        return vector_db',\n",
       "   'd': 'Create a Zilliz collection, indexes it with HNSW, and insert data.\\n\\nArgs:\\n    texts (List[str]): Text data.\\n    embedding (Embeddings): Embedding function.\\n    metadatas (Optional[List[dict]]): Metadata for each text if it exists.\\n        Defaults to None.\\n    collection_name (str, optional): Collection name to use. Defaults to\\n        \"LangChainCollection\".\\n    connection_args (dict[str, Any], optional): Connection args to use. Defaults\\n        to DEFAULT_MILVUS_CONNECTION.\\n    consistency_level (str, optional): Which consistency level to use. Defaults\\n        to \"Session\".\\n    index_params (Optional[dict], optional): Which index_params to use.\\n        Defaults to None.\\n    search_params (Optional[dict], optional): Which search params to use.\\n        Defaults to None.\\n    drop_old (Optional[bool], optional): Whether to drop the collection with\\n        that name if it exists. Defaults to False.\\n\\nReturns:\\n    Zilliz: Zilliz Vector Store',\n",
       "   'l': True,\n",
       "   'g': ['Trains an autoencoder for the given problem and saves the model to the output directory.',\n",
       "    'Train autoencoder for a given number of steps.',\n",
       "    'Train autoencoder.\\n    :param problem_name:\\n    :param data_dir:\\n    :param output_dir:\\n    :param hparams:\\n    :param epoch:\\n    :return:\\n    :rtype: None',\n",
       "    'Trains an autoencoder on the given data set.\\n     Args:\\n       problem_name: The name of the problem to train the autoencoder on.\\n       data_dir: The directory where the data is stored.\\n       output_dir: The directory where the model is stored.\\n       hparams: The hyperparameters to use for training.\\n       epoch: The current epoch number.\\n     Returns:\\n       The number of steps completed during training.']},\n",
       "  {'c': '    def on_chain_end(self, outputs: Union[Dict[str, Any], Any], **kwargs: Any) -> None:\\n        handle_event(\\n            self.handlers,\\n            \"on_chain_end\",\\n            \"ignore_chain\",\\n            outputs,\\n            run_id=self.run_id,\\n            parent_run_id=self.parent_run_id,\\n            tags=self.tags,\\n            **kwargs,\\n        )',\n",
       "   'd': 'Run when chain ends running.\\n\\nArgs:\\n    outputs (Union[Dict[str, Any], Any]): The outputs of the chain.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def __init__(\\n    self,\\n    file_path: str,\\n    password: Optional[Union[str, bytes]] = None,\\n    headers: Optional[Dict] = None,\\n) -> None:\\n    try:\\n        import pypdf\\n    except ImportError:\\n        raise ImportError(\\n            \"pypdf package not found, please install it with \" \"`pip install pypdf`\"\\n        )\\n    self.parser = PyPDFParser(password=password)\\n    super().__init__(file_path, headers=headers)',\n",
       "   'd': 'Validate input variables.\\n\\nIf input_variables is not set, it will be set to the union of\\nall input variables in the messages.\\n\\nArgs:\\n    values: values to validate.\\n\\nReturns:\\n    Validated values.',\n",
       "   'l': False,\n",
       "   'g': ['_config_changed:\\n    True if config has changed since last save.\\n    False otherwise.\\n\\n    :param previous: The previous instance of the same model.\\n    :type previous: :class:`Model`\\n    :return: True if config has changed since last save.\\n    :rtype: bool\\n    :raises: :class:`AttributeError` if config is not a dictionary.\\n    :raises: :class:`TypeError` if config is not a dictionary.\\n    :raises: :class:`AttributeError` if config is not a dictionary.\\n    :raises: :class:`TypeError` if config is not a dictionary.\\n    :raises: :class:`AttributeError` if config is not a dictionary.\\n    :raises: :class:`TypeError` if config is not a dictionary.\\n    :raises: :class:`AttributeError` if config is not a dictionary.\\n    :raises: :class:`TypeError` if config is not a dictionary.\\n    :raises: :class:`AttributeError` if config is not a dictionary.\\n    :raises: :class:`TypeError` if config is not a dictionary.\\n    :raises: :class:`AttributeError` if config is not',\n",
       "    '_config_changed: bool',\n",
       "    'Save the model to the database.\\n    :param self:\\n    :param args:\\n    :param kwargs:\\n    :return:\\n    :rtype: None\\n    :raises:\\n    :',\n",
       "    '_config_changed = False\\n  def save(self, *args, **kwargs):']},\n",
       "  {'c': '    def lazy_load(self) -> Iterator[chat_loaders.ChatSession]:\\n        import sqlite3\\n\\n        try:\\n            conn = sqlite3.connect(self.db_path)\\n        except sqlite3.OperationalError as e:\\n            raise ValueError(\\n                f\"Could not open iMessage DB file {self.db_path}.\\\\n\"\\n                \"Make sure your terminal emulator has disk access to this file.\\\\n\"\\n                \"   You can either copy the DB file to an accessible location\"\\n                \" or grant full disk access for your terminal emulator.\"\\n                \"  You can grant full disk access for your terminal emulator\"\\n                \" in System Settings > Security and Privacy > Full Disk Access.\"\\n            ) from e\\n        cursor = conn.cursor()\\n\\n\\n        cursor.execute(\"SELECT ROWID FROM chat\")\\n        chat_ids = [row[0] for row in cursor.fetchall()]\\n\\n        for chat_id in chat_ids:\\n            yield self._load_single_chat_session(cursor, chat_id)\\n\\n        conn.close()',\n",
       "   'd': 'Lazy load the chat sessions from the iMessage chat.db\\nand yield them in the required format.\\n\\nYields:\\n    ChatSession: Loaded chat session.',\n",
       "   'l': True,\n",
       "   'g': ['.from_hex(color_hex) -> Color:\\n\\n    Return a Color object from a hex color string.\\n\\n    :param color_hex: A hex color string.',\n",
       "    '_from_hex(cls, color_hex: str) -> Color:\\n\\n    Returns a color object from a hex color string.\\n\\n    Parameters\\n    ----------\\n    color_hex : str\\n        The hex color string.\\n\\n    Returns\\n    -------\\n    Color\\n        The color object.',\n",
       "    '(str) -> Color:\\n\\n    Return a Color instance from a color hex string.\\n\\n    :param color_hex: A color hex string.',\n",
       "    '(cls, color_hex: str) -> Color:']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        cls._try_init_vertexai(values)\\n        tuned_model_name = values.get(\"tuned_model_name\")\\n        model_name = values[\"model_name\"]\\n        try:\\n            if not is_codey_model(model_name):\\n                from vertexai.preview.language_models import TextGenerationModel\\n\\n                if tuned_model_name:\\n                    values[\"client\"] = TextGenerationModel.get_tuned_model(\\n                        tuned_model_name\\n                    )\\n                else:\\n                    values[\"client\"] = TextGenerationModel.from_pretrained(model_name)\\n            else:\\n                from vertexai.preview.language_models import CodeGenerationModel\\n\\n                if tuned_model_name:\\n                    values[\"client\"] = CodeGenerationModel.get_tuned_model(\\n                        tuned_model_name\\n                    )\\n                else:\\n                    values[\"client\"] = CodeGenerationModel.from_pretrained(model_name)\\n        except ImportError:\\n            raise_vertex_import_error()\\n        return values',\n",
       "   'd': 'Validate that the python package exists in environment.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def lazy_load(self) -> Iterator[chat_loaders.ChatSession]:\\n        try:\\n            conn = sqlite3.connect(self.db_path)\\n        except sqlite3.OperationalError as e:\\n            raise ValueError(\\n                f\"Could not open iMessage DB file {self.db_path}.\\\\n\"\\n                \"Make sure your terminal emulator has disk access to this file.\\\\n\"\\n                \"   You can either copy the DB file to an accessible location\"\\n                \" or grant full disk access for your terminal emulator.\"\\n                \"  You can grant full disk access for your terminal emulator\"\\n                \" in System Settings > Security and Privacy > Full Disk Access.\"\\n            ) from e\\n        cursor = conn.cursor()\\n\\n\\n        cursor.execute(\"SELECT ROWID FROM chat\")\\n        chat_ids = [row[0] for row in cursor.fetchall()]\\n\\n        for chat_id in chat_ids:\\n            yield self._load_single_chat_session(cursor, chat_id)\\n\\n        conn.close()',\n",
       "   'd': 'Lazy load the chat sessions from the iMessage chat.db\\nand yield them in the required format.\\n\\nYields:\\n    ChatSession: Loaded chat session.',\n",
       "   'l': True,\n",
       "   'g': ['_get_cache_logic(self, cached_response: Any):',\n",
       "    '_get_cache_logic(self, cached_response: Any):',\n",
       "    '.get_cache_logic()',\n",
       "    '_get_cache_logic(self, cached_response: Any):']},\n",
       "  {'c': 'def _setup_evaluation(\\n    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,\\n    examples: Iterator[Example],\\n    evaluation: Optional[RunEvalConfig],\\n    data_type: DataType,\\n) -> Tuple[Optional[List[RunEvaluator]], Iterator[Example]]:\\n    if evaluation:\\n        first_example, examples = _first_example(examples)\\n        if isinstance(llm_or_chain_factory, BaseLanguageModel):\\n            run_inputs, run_outputs = None, None\\n            run_type = RunTypeEnum.llm\\n        else:\\n            run_type = RunTypeEnum.chain\\n            if data_type in (DataType.chat, DataType.llm):\\n                raise ValueError(\\n                    \"Cannot evaluate a chain on dataset with \"\\n                    f\"data_type={data_type.value}. \"\\n                    \"Please specify a dataset with the default \\'kv\\' data type.\"\\n                )\\n            chain = llm_or_chain_factory()\\n            run_inputs = chain.input_keys\\n            run_outputs = chain.output_keys\\n        run_evaluators = _load_run_evaluators(\\n            evaluation,\\n            run_type,\\n            data_type,\\n            list(first_example.outputs) if first_example.outputs else None,\\n            run_inputs,\\n            run_outputs,\\n        )\\n    else:\\n\\n        run_evaluators = None\\n    return run_evaluators, examples',\n",
       "   'd': 'Configure the evaluators to run on the results of the chain.',\n",
       "   'l': True,\n",
       "   'g': ['_repr_impl(self):',\n",
       "    '_left_tag_pattern:  _right_tag_pattern:  _left_tag_pattern:  _right_tag_pattern:',\n",
       "    '_repr_impl(self):',\n",
       "    '_']},\n",
       "  {'c': '    def __init__(self, **kwargs: Any):\\n        super().__init__(**kwargs)\\n        try:\\n            from InstructorEmbedding import INSTRUCTOR\\n\\n            self.client = INSTRUCTOR(\\n                self.model_name, cache_folder=self.cache_folder, **self.model_kwargs\\n            )\\n        except ImportError as e:\\n            raise ImportError(\"Dependencies for InstructorEmbedding not found.\") from e',\n",
       "   'd': 'Initialize the sentence_transformer.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def on_chain_end(\\n        self,\\n        outputs: Dict[str, Any],\\n        *,\\n        run_id: UUID,\\n        inputs: Optional[Dict[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        if not run_id:\\n            raise TracerException(\"No run_id provided for on_chain_end callback.\")\\n        chain_run = self.run_map.get(str(run_id))\\n        if chain_run is None:\\n            raise TracerException(f\"No chain Run found to be traced for {run_id}\")\\n\\n        chain_run.outputs = outputs\\n        chain_run.end_time = datetime.utcnow()\\n        chain_run.events.append({\"name\": \"end\", \"time\": chain_run.end_time})\\n        if inputs is not None:\\n            chain_run.inputs = inputs\\n        self._end_trace(chain_run)\\n        self._on_chain_end(chain_run)',\n",
       "   'd': 'Load file.',\n",
       "   'l': False,\n",
       "   'g': ['Delete a model.\\n  :return: The model name.\\n  :rtype: str',\n",
       "    'Delete a model.\\n\\n  :param model_name: The name of the model to delete.\\n  :type model_name: str',\n",
       "    'Delete a model.\\n  :return:  The response.\\n  :rtype:  str',\n",
       "    'Delete a model.\\n\\n  :param ModelName: The name of the model to delete.\\n  :type ModelName: str']},\n",
       "  {'c': 'def completion_with_retry(\\n    llm: VertexAI,\\n    *args: Any,\\n    run_manager: Optional[CallbackManagerForLLMRun] = None,\\n    **kwargs: Any,\\n) -> Any:\\n    retry_decorator = _create_retry_decorator(llm, run_manager=run_manager)\\n\\n    @retry_decorator\\n    def _completion_with_retry(*args: Any, **kwargs: Any) -> Any:\\n        return llm.client.predict(*args, **kwargs)\\n\\n    return _completion_with_retry(*args, **kwargs)',\n",
       "   'd': 'Use tenacity to retry the completion call.',\n",
       "   'l': True,\n",
       "   'g': ['() -> bool:', ':return:', '', '() -> bool:']},\n",
       "  {'c': '    def evaluate(\\n        self,\\n        examples: Sequence[dict],\\n        predictions: Sequence[dict],\\n        question_key: str = \"query\",\\n        answer_key: str = \"answer\",\\n        prediction_key: str = \"result\",\\n        *,\\n        callbacks: Callbacks = None,\\n    ) -> List[dict]:\\n        inputs = [\\n            {\\n                \"query\": example[question_key],\\n                \"answer\": example[answer_key],\\n                \"result\": predictions[i][prediction_key],\\n            }\\n            for i, example in enumerate(examples)\\n        ]\\n\\n        return self.apply(inputs, callbacks=callbacks)',\n",
       "   'd': 'Load file.',\n",
       "   'l': False,\n",
       "   'g': ['.', '', '.', '']},\n",
       "  {'c': 'def __init__(self, database_path: str = \".langchain.db\"):\\n    engine = create_engine(f\"sqlite:///{database_path}\")\\n    super().__init__(engine)',\n",
       "   'd': 'Raise error - saving not supported for Agent Executors.',\n",
       "   'l': False,\n",
       "   'g': ['Test a regular put.\\n\\n    :return:',\n",
       "    'Test a regular put.\\n\\n    :param bool partial: Whether to use a partial put.\\n    :return: Nothing.',\n",
       "    'Tests a put request.\\n\\n    :param partial:  True if the request should be partial.\\n\\n    :return:  The response.',\n",
       "    'Test a regular put.\\n\\n    :param partial: Whether to send a partial put.\\n    :type partial: bool\\n    :return: Nothing.\\n    :rtype: None\\n    :raises: :class:`~httpx.ConnectError` if the connection fails.\\n    :raises: :class:`~httpx.RequestError` if the request fails.\\n    :raises: :class:`~httpx.ResponseError` if the response fails.\\n    :raises: :class:`~httpx.ConnectError` if the connection fails.\\n    :raises: :class:`~httpx.RequestError` if the request fails.\\n    :raises: :class:`~httpx.ResponseError` if the response fails.\\n    :raises: :class:`~httpx.ConnectError` if the connection fails.\\n    :raises: :class:`~httpx.RequestError` if the request fails.\\n    :raises: :class:`~httpx.ResponseError` if the response fails.\\n    :raises: :class:`~httpx.ConnectError` if the connection fails.\\n    :raises: :class:`~httpx.RequestError` if the']},\n",
       "  {'c': '    def set(self, results, query):\\n        node = self.lookup[query]\\n        if node is not None:\\n\\n            node.results = results\\n            self.linked_list.move_to_front(node)\\n        else:\\n\\n            if self.size == self.MAX_SIZE:\\n\\n                self.lookup.pop(self.linked_list.tail.query, None)\\n                self.linked_list.remove_from_tail()\\n            else:\\n                self.size += 1\\n\\n            new_node = Node(results)\\n            self.linked_list.append_to_front(new_node)\\n            self.lookup[query] = new_node',\n",
       "   'd': 'Return Elasticsearch documents most similar to query.\\n\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 4.\\n    filter: Array of Elasticsearch filter clauses to apply to the query.\\n\\nReturns:\\n    List of Documents most similar to the query,\\n    in descending order of similarity.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    ':return:  :class:`msal.msal-python.MSAL.MSAL.MSAL.ClientApplication` object.\\n  :rtype:  :class:`msal.msal-python.MSAL.MSAL.MSAL.ClientApplication`\\n  :raises:  :class:`msal.msal-python.MSAL.MSAL.MSAL.ClientApplication` object.\\n  :raises:  :class:`msal.msal-python.MSAL.MSAL.MSAL.ClientApplication` object.\\n  :raises:  :class:`msal.msal-python.MSAL.MSAL.MSAL.ClientApplication` object.\\n  :raises:  :class:`msal.msal-python.MSAL.MSAL.MSAL.ClientApplication` object.\\n  :raises:  :class:`msal.msal-python.MSAL.MSAL.MSAL.ClientApplication` object.\\n  :raises:  :class:`msal.msal-python.MSAL.MSAL.MSAL.ClientApplication` object.\\n  :raises:  :class:`msal',\n",
       "    'Acquire a token for the Microsoft Graph API.\\n\\n  Returns:\\n    A dictionary containing the access token and refresh token.',\n",
       "    '']},\n",
       "  {'c': 'def get_pipeline() -> Any:\\n    model_id = \"facebook/bart-base\"\\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\\n    model = AutoModelForCausalLM.from_pretrained(model_id)\\n    return pipeline(\"feature-extraction\", model=model, tokenizer=tokenizer)',\n",
       "   'd': 'Get pipeline for testing.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'Add documents to Milvus.\\n\\n    :param self: The object instance.\\n    :param kb_file: The knowledge file to be added.\\n    :return: The status of the operation.',\n",
       "    '']},\n",
       "  {'c': '    def parse(self, text: str) -> T:\\n',\n",
       "   'd': 'Parse a single string model output into some structure.\\n\\nArgs:\\n    text: String output of a language model.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': True,\n",
       "   'g': ['This function is a wrapper around the scrape_dai_docs function.\\n    It is used to run the function on the local machine and then\\n    save the outputs to a json file.\\n\\n    :return:',\n",
       "    'This function is a helper function to scrape the entire dai-docs',\n",
       "    '',\n",
       "    'Scrape all the plain-text files from the dai-docs site and convert them to the\\n    train_cleaned.json file format.']},\n",
       "  {'c': '    def index(\\n        self,\\n        dims_length: Union[int, None],\\n        vector_query_field: str,\\n        similarity: Union[DistanceStrategy, None],\\n    ) -> Dict:\\n        if similarity is DistanceStrategy.COSINE:\\n            similarityAlgo = \"cosine\"\\n        elif similarity is DistanceStrategy.EUCLIDEAN_DISTANCE:\\n            similarityAlgo = \"l2_norm\"\\n        elif similarity is DistanceStrategy.DOT_PRODUCT:\\n            similarityAlgo = \"dot_product\"\\n        else:\\n            raise ValueError(f\"Similarity {similarity} not supported.\")\\n\\n        return {\\n            \"mappings\": {\\n                \"properties\": {\\n                    vector_query_field: {\\n                        \"type\": \"dense_vector\",\\n                        \"dims\": dims_length,\\n                        \"index\": True,\\n                        \"similarity\": similarityAlgo,\\n                    },\\n                }\\n            }\\n        }',\n",
       "   'd': 'Create the mapping for the Elasticsearch index.',\n",
       "   'l': True,\n",
       "   'g': ['_run_hook(self, hook, file_args)',\n",
       "    '_run_hook',\n",
       "    '_run_hook(self, hook, file_args):\\n    Run a hook.\\n\\n    Parameters:\\n        hook (dict): A hook dictionary.\\n        file_args (dict): A dictionary of arguments for the file.\\n\\n    Returns:\\n        A dictionary of the result of the hook.',\n",
       "    '_run_hook(self, hook, file_args):\\n    Run a hook.']},\n",
       "  {'c': 'def _type(self) -> str:\\n    return \"default\"',\n",
       "   'd': 'Call out to Clarfai\\'s PostModelOutputs endpoint.\\n\\nArgs:\\n    prompt: The prompt to pass into the model.\\n    stop: Optional list of stop words to use when generating.\\n\\nReturns:\\n    The string generated by the model.\\n\\nExample:\\n    .. code-block:: python\\n\\n        response = clarifai_llm(\"Tell me a joke.\")',\n",
       "   'l': False,\n",
       "   'g': ['.compute(bottomUpInput, enableLearn, computeInfOutput=None)',\n",
       "    '(see BaseFunction.compute)',\n",
       "    '(bottomUpInput, enableLearn, computeInfOutput) -> numpy.ndarray',\n",
       "    '.compute(bottomUpInput, enableLearn, computeInfOutput=None)']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        huggingface_api_key = get_from_dict_or_env(\\n            values, \"huggingface_api_key\", \"HUGGINGFACE_API_KEY\"\\n        )\\n        try:\\n            from petals import AutoDistributedModelForCausalLM\\n            from transformers import AutoTokenizer\\n\\n            model_name = values[\"model_name\"]\\n            values[\"tokenizer\"] = AutoTokenizer.from_pretrained(model_name)\\n            values[\"client\"] = AutoDistributedModelForCausalLM.from_pretrained(\\n                model_name\\n            )\\n            values[\"huggingface_api_key\"] = huggingface_api_key\\n\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import transformers or petals python package.\"\\n                \"Please install with `pip install -U transformers petals`.\"\\n            )\\n        return values',\n",
       "   'd': 'To a BaseMessage.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    List of BaseMessage.',\n",
       "   'l': False,\n",
       "   'g': ['Return a boolean mask of the selected rows.\\n\\n    Parameters\\n    ----------\\n    regex : str\\n        A regular expression to match the selected rows.\\n    columns : list\\n        A list of columns to match the selected rows.\\n    unselect : bool, optional\\n        If True, the selected rows will be unselected. By default, all selected rows are selected.\\n\\n    Returns\\n    -------\\n    pd.DataFrame\\n        A boolean mask of the selected rows.',\n",
       "    'Returns a boolean mask of the selected rows.\\n\\n    :param regex: The regex to match.\\n    :param columns: The columns to match.\\n    :param unselect: Whether to unselect the selected rows.\\n    :return: A boolean mask of the selected rows.\\n    :rtype: pandas.DataFrame',\n",
       "    '',\n",
       "    ':param regex:\\n    :param columns:\\n    :param unselect:\\n    :return:']},\n",
       "  {'c': '    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:\\n        _handle_event(\\n            self.handlers,\\n            \"on_chain_end\",\\n            \"ignore_chain\",\\n            outputs,\\n            run_id=self.run_id,\\n            parent_run_id=self.parent_run_id,\\n            tags=self.tags,\\n            **kwargs,\\n        )',\n",
       "   'd': 'Run when chain ends running.\\n\\nArgs:\\n    outputs (Dict[str, Any]): The outputs of the chain.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def on_chain_error(\\n    self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\\n) -> None:\\n    self.metrics[\"step\"] += 1\\n    self.metrics[\"errors\"] += 1',\n",
       "   'd': 'Run when chain errors.\\n\\nArgs:\\n    error (Exception or KeyboardInterrupt): The error.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Returns a tuple of the background choice and the background image.\\n\\n    :return: background_choice, background_image_path',\n",
       "    '',\n",
       "    ':return:']},\n",
       "  {'c': '    def load(self) -> List[Document]:\\n        from pdfminer.high_level import extract_text_to_fp\\n        from pdfminer.layout import LAParams\\n        from pdfminer.utils import open_filename\\n\\n        output_string = StringIO()\\n        with open_filename(self.file_path, \"rb\") as fp:\\n            extract_text_to_fp(\\n                fp,\\n                output_string,\\n                codec=\"\",\\n                laparams=LAParams(),\\n                output_type=\"html\",\\n            )\\n        metadata = {\"source\": self.file_path}\\n        return [Document(page_content=output_string.getvalue(), metadata=metadata)]',\n",
       "   'd': 'Clear the *whole* semantic cache.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Generator function that yields all the Python source files in the\\n  given paths.\\n\\n  :param paths: List of paths to directories or files to search.\\n  :type paths: list',\n",
       "    '',\n",
       "    'yields the source code of the given paths']},\n",
       "  {'c': '    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        text_key: str = \"text\",\\n        index_name: Optional[str] = None,\\n        client: Any = None,\\n        host: List[str] = [\"172.20.31.10:13000\"],\\n        user: str = \"root\",\\n        password: str = \"123123\",\\n        batch_size: int = 500,\\n        **kwargs: Any,\\n    ) -> Dingo:\\n        try:\\n            import dingodb\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import dingo python package. \"\\n                \"Please install it with `pip install dingodb`.\"\\n            )\\n\\n        if client is not None:\\n            dingo_client = client\\n        else:\\n            try:\\n\\n                dingo_client = dingodb.DingoDB(user, password, host)\\n            except ValueError as e:\\n                raise ValueError(f\"Dingo failed to connect: {e}\")\\n        if kwargs is not None and kwargs.get(\"self_id\") is True:\\n            if index_name not in dingo_client.get_index():\\n                dingo_client.create_index(index_name, 1024, auto_id=False)\\n        else:\\n            if index_name not in dingo_client.get_index():\\n                dingo_client.create_index(index_name, 1024)\\n\\n\\n\\n\\n        ids = ids or [str(uuid.uuid1().int)[:13] for _ in texts]\\n        metadatas_list = []\\n        texts = list(texts)\\n        embeds = embedding.embed_documents(texts)\\n        for i, text in enumerate(texts):\\n            metadata = metadatas[i] if metadatas else {}\\n            metadata[text_key] = text\\n            metadatas_list.append(metadata)\\n\\n\\n        for i in range(0, len(list(texts)), batch_size):\\n            j = i + batch_size\\n            add_res = dingo_client.vector_add(\\n                index_name, metadatas_list[i:j], embeds[i:j], ids[i:j]\\n            )\\n            if not add_res:\\n                raise Exception(\"vector add fail\")\\n        return cls(embedding, text_key, client=dingo_client, index_name=index_name)',\n",
       "   'd': 'Construct Dingo wrapper from raw documents.\\n\\n        This is a user friendly interface that:\\n            1. Embeds documents.\\n            2. Adds the documents to a provided Dingo index\\n\\n        This is intended to be a quick way to get started.\\n\\n        Example:\\n            .. code-block:: python\\n\\n                from langchain.vectorstores import Dingo\\n                from langchain.embeddings import OpenAIEmbeddings\\n                import dingodb\\nsss\\n                embeddings = OpenAIEmbeddings()\\n                dingo = Dingo.from_texts(\\n                    texts,\\n                    embeddings,\\n                    index_name=\"langchain-demo\"\\n                )',\n",
       "   'l': True,\n",
       "   'g': ['_init_',\n",
       "    '_init_',\n",
       "    '_init_\\n    :param init_data_by_df:\\n        type: pd.DataFrame\\n    :param dtype:\\n        type: str\\n    :param if_fq:\\n        type: str\\n    :return:\\n        type: None\\n    :raises:',\n",
       "    '_init_']},\n",
       "  {'c': 'def __init__(\\n    self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n) -> None:\\n    super().__init__(**kwargs)\\n    self._separator = separator\\n    self._is_separator_regex = is_separator_regex',\n",
       "   'd': 'Create an LLMChain that uses an OpenAI function to get a structured output.\\n\\nArgs:\\n    output_schema: Either a dictionary or pydantic.BaseModel class. If a dictionary\\n        is passed in, it\\'s assumed to already be a valid JsonSchema.\\n        For best results, pydantic.BaseModels should have docstrings describing what\\n        the schema represents and descriptions for the parameters.\\n    llm: Language model to use, assumed to support the OpenAI function-calling API.\\n    prompt: BasePromptTemplate to pass to the model.\\n    output_key: The key to use when returning the output in LLMChain.__call__.\\n    output_parser: BaseLLMOutputParser to use for parsing model outputs. By default\\n        will be inferred from the function types. If pydantic.BaseModels are passed\\n        in, then the OutputParser will try to parse outputs using those. Otherwise\\n        model outputs will simply be parsed as JSON.\\n\\nReturns:\\n    An LLMChain that will pass the given function to the model.\\n\\nExample:\\n    .. code-block:: python\\n\\n            from typing import Optional\\n\\n            from langchain.chains.openai_functions import create_structured_output_chain\\n            from langchain.chat_models import ChatOpenAI\\n            from langchain.prompts import ChatPromptTemplate\\n\\n            from langchain.pydantic_v1 import BaseModel, Field\\n\\n            class Dog(BaseModel):\\n                \"\"\"Identifying information about a dog.\"\"\"\\n\\n                name: str = Field(..., description=\"The dog\\'s name\")\\n                color: str = Field(..., description=\"The dog\\'s color\")\\n                fav_food: Optional[str] = Field(None, description=\"The dog\\'s favorite food\")\\n\\n            llm = ChatOpenAI(model=\"gpt-3.5-turbo-0613\", temperature=0)\\n            prompt = ChatPromptTemplate.from_messages(\\n                [\\n                    (\"system\", \"You are a world class algorithm for extracting information in structured formats.\"),\\n                    (\"human\", \"Use the given format to extract information from the following input: {input}\"),\\n                    (\"human\", \"Tip: Make sure to answer in the correct format\"),\\n                ]\\n            )\\n            chain = create_structured_output_chain(Dog, llm, prompt)\\n            chain.run(\"Harry was a chubby brown beagle who loved chicken\")\\n            # -> Dog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def similarity_search(\\n    self, query: str, k: int = 4, filter: Optional[dict] = None, **kwargs: Any\\n) -> List[Document]:\\n    results = self.knn_search(query=query, k=k, **kwargs)\\n    return [doc for doc, score in results]',\n",
       "   'd': 'Pass through to `knn_search`',\n",
       "   'l': True,\n",
       "   'g': ['_load_games(self, games_store_dir: Path):',\n",
       "    '(def_loaded_games_dir: Path, trainer: Trainer) -> None:',\n",
       "    'if self._loaded:\\n        return\\n    self._loaded = True\\n    self.games_store_dir = games_store_dir\\n    self.trainer = trainer',\n",
       "    '(self, games_store_dir: Path, trainer: Trainer):\\n    def load_games(self, games_store_dir: Path):\\n    def load_games(self, games_store_dir: Path):\\n    def load_games(self, games_store_dir: Path):\\n    def load_games(self, games_store_dir: Path):\\n    def load_games(self, games_store_dir: Path):\\n    def load_games(self, games_store_dir: Path):\\n    def load_games(self, games_store_dir: Path):\\n    def load_games(self, games_store_dir: Path):\\n    def load_games(self, games_store_dir: Path):\\n    def load_games(self, games_store_dir: Path):\\n    def load_games(self, games_store_dir: Path):\\n    def load_games(self, games_store_dir: Path):\\n    def load_games(self, games_store_dir: Path):\\n    def load_games(self, games_store_dir: Path):\\n    def']},\n",
       "  {'c': '    def semantic_hybrid_search_with_score(\\n        self, query: str, k: int = 4, filters: Optional[str] = None\\n    ) -> List[Tuple[Document, float]]:\\n        from azure.search.documents.models import Vector\\n\\n        results = self.client.search(\\n            search_text=query,\\n            vector=Vector(\\n                value=np.array(\\n                    self.embedding_function(query), dtype=np.float32\\n                ).tolist(),\\n                k=50,\\n                fields=FIELDS_CONTENT_VECTOR,\\n            ),\\n            select=[f\"{FIELDS_ID},{FIELDS_CONTENT},{FIELDS_METADATA}\"],\\n            filter=filters,\\n            query_type=\"semantic\",\\n            query_language=self.semantic_query_language,\\n            semantic_configuration_name=self.semantic_configuration_name,\\n            query_caption=\"extractive\",\\n            query_answer=\"extractive\",\\n            top=k,\\n        )\\n\\n        semantic_answers = results.get_answers()\\n        semantic_answers_dict = {}\\n        for semantic_answer in semantic_answers:\\n            semantic_answers_dict[semantic_answer.key] = {\\n                \"text\": semantic_answer.text,\\n                \"highlights\": semantic_answer.highlights,\\n            }\\n\\n        docs = [\\n            (\\n                Document(\\n                    page_content=result[\"content\"],\\n                    metadata={\\n                        **json.loads(result[\"metadata\"]),\\n                        **{\\n                            \"captions\": {\\n                                \"text\": result.get(\"@search.captions\", [{}])[0].text,\\n                                \"highlights\": result.get(\"@search.captions\", [{}])[\\n                                    0\\n                                ].highlights,\\n                            }\\n                            if result.get(\"@search.captions\")\\n                            else {},\\n                            \"answers\": semantic_answers_dict.get(\\n                                json.loads(result[\"metadata\"]).get(\"key\"), \"\"\\n                            ),\\n                        },\\n                    },\\n                ),\\n                float(result[\"@search.score\"]),\\n            )\\n            for result in results\\n        ]\\n        return docs',\n",
       "   'd': 'Return docs most similar to query.\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 4.\\nReturns:\\n    List of Documents most similar to the query.',\n",
       "   'l': False,\n",
       "   'g': ['Returns a string between two strings.\\n\\n    :param string: The string to search in.\\n    :param start: The start string.\\n    :param end: The end string.\\n    :return: The string between the start and end string.\\n    :rtype: str',\n",
       "    'Returns the content between two strings.\\n    :param string: The string to search in.\\n    :param start: The start string.\\n    :param end: The end string.\\n    :return: The content between the start and end strings.',\n",
       "    'Get the content between two string.\\n\\n    :param string: string to be processed\\n    :param start: start string\\n    :param end: end string\\n    :return: content string',\n",
       "    ':param string: string to search\\n    :param start: start of the substring\\n    :param end: end of the substring\\n    :return: the substring between start and end\\n    :rtype: str\\n    :Example:\\n    >>> str_between(\"abc1234567890\", \"1234567890\", \"1234567890\")\\n    \\'1234567890\\'\\n    :']},\n",
       "  {'c': '    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        try:\\n            from aleph_alpha_client import (\\n                Prompt,\\n                SemanticEmbeddingRequest,\\n                SemanticRepresentation,\\n            )\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import aleph_alpha_client python package. \"\\n                \"Please install it with `pip install aleph_alpha_client`.\"\\n            )\\n        document_embeddings = []\\n\\n        for text in texts:\\n            document_params = {\\n                \"prompt\": Prompt.from_text(text),\\n                \"representation\": SemanticRepresentation.Document,\\n                \"compress_to_size\": self.compress_to_size,\\n                \"normalize\": self.normalize,\\n                \"contextual_control_threshold\": self.contextual_control_threshold,\\n                \"control_log_additive\": self.control_log_additive,\\n            }\\n\\n            document_request = SemanticEmbeddingRequest(**document_params)\\n            document_response = self.client.semantic_embed(\\n                request=document_request, model=self.model\\n            )\\n\\n            document_embeddings.append(document_response.embedding)\\n\\n        return document_embeddings',\n",
       "   'd': 'Whether the evaluation requires a reference text.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '_defaultStack(layer = None, axolotl = False, groups = True, media = True, privacy = True, profiles = True)',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': 'def load(self) -> List[Document]:\\n    return list(self.lazy_load())',\n",
       "   'd': 'Eagerly load the content.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def post(self, url: str, data: Dict[str, Any], **kwargs: Any) -> str:\\n    return self.requests.post(url, data, **kwargs).text',\n",
       "   'd': 'POST to the URL and return the text.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n        for gen in return_val:\\n            if not isinstance(gen, Generation):\\n                raise ValueError(\\n                    \"Momento only supports caching of normal LLM generations, \"\\n                    f\"got {type(gen)}\"\\n                )\\n        key = self.__key(prompt, llm_string)\\n        value = _dump_generations_to_json(return_val)\\n        set_response = self.cache_client.set(self.cache_name, key, value, self.ttl)\\n        from momento.responses import CacheSet\\n\\n        if isinstance(set_response, CacheSet.Success):\\n            pass\\n        elif isinstance(set_response, CacheSet.Error):\\n            raise set_response.inner_exception\\n        else:\\n            raise Exception(f\"Unexpected response: {set_response}\")',\n",
       "   'd': 'Initialize the sentence_transformer.',\n",
       "   'l': False,\n",
       "   'g': ['_call__', '(im, label=None) -> (im, label)', '_call__', '_call__']},\n",
       "  {'c': '    def from_embeddings(\\n        cls,\\n        text_embeddings: List[Tuple[str, List[float]]],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        metric: str = DEFAULT_METRIC,\\n        trees: int = 100,\\n        n_jobs: int = -1,\\n        **kwargs: Any,\\n    ) -> Annoy:\\n        texts = [t[0] for t in text_embeddings]\\n        embeddings = [t[1] for t in text_embeddings]\\n\\n        return cls.__from(\\n            texts, embeddings, embedding, metadatas, metric, trees, n_jobs, **kwargs\\n        )',\n",
       "   'd': 'Construct Annoy wrapper from embeddings.\\n\\nArgs:\\n    text_embeddings: List of tuples of (text, embedding)\\n    embedding: Embedding function to use.\\n    metadatas: List of metadata dictionaries to associate with documents.\\n    metric: Metric to use for indexing. Defaults to \"angular\".\\n    trees: Number of trees to use for indexing. Defaults to 100.\\n    n_jobs: Number of jobs to use for indexing. Defaults to -1\\n\\nThis is a user friendly interface that:\\n    1. Creates an in memory docstore with provided embeddings\\n    2. Initializes the Annoy database\\n\\nThis is intended to be a quick way to get started.\\n\\nExample:\\n    .. code-block:: python\\n\\n        from langchain import Annoy\\n        from langchain.embeddings import OpenAIEmbeddings\\n        embeddings = OpenAIEmbeddings()\\n        text_embeddings = embeddings.embed_documents(texts)\\n        text_embedding_pairs = list(zip(texts, text_embeddings))\\n        db = Annoy.from_embeddings(text_embedding_pairs, embeddings)',\n",
       "   'l': True,\n",
       "   'g': ['_call(self, input_sequence, training=True, mask=None):',\n",
       "    '_call_function',\n",
       "    \"(input_sequence, training=True, mask=None) -> {'encoder_output': hidden}\",\n",
       "    '_call:\\n    :param input_sequence:\\n    :param training:\\n    :param mask:\\n    :return:\\n    :rtype: object']},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n    rows = self._search_rows(prompt, llm_string)\\n    if rows:\\n        return [loads(row[0]) for row in rows]\\n    return None',\n",
       "   'd': \"Call out to Aleph Alpha's asymmetric Document endpoint.\\n\\nArgs:\\n    texts: The list of texts to embed.\\n\\nReturns:\\n    List of embeddings, one for each text.\",\n",
       "   'l': False,\n",
       "   'g': ['Parse data from a file.\\n\\n  :param filename: the path of the file to be parsed.\\n\\n  :return: the data in the file.\\n\\n  :rtype: list',\n",
       "    '',\n",
       "    'Parse data from file.\\n\\n  :param filename: file name\\n  :return: data',\n",
       "    '']},\n",
       "  {'c': '    def _create_search_request(self, query: str) -> SearchRequest:\\n        from google.cloud.discoveryengine_v1beta import SearchRequest\\n\\n        query_expansion_spec = SearchRequest.QueryExpansionSpec(\\n            condition=self.query_expansion_condition,\\n        )\\n\\n        spell_correction_spec = SearchRequest.SpellCorrectionSpec(\\n            mode=self.spell_correction_mode\\n        )\\n\\n        if self.engine_data_type == 0:\\n            if self.get_extractive_answers:\\n                extractive_content_spec = (\\n                    SearchRequest.ContentSearchSpec.ExtractiveContentSpec(\\n                        max_extractive_answer_count=self.max_extractive_answer_count,\\n                    )\\n                )\\n            else:\\n                extractive_content_spec = (\\n                    SearchRequest.ContentSearchSpec.ExtractiveContentSpec(\\n                        max_extractive_segment_count=self.max_extractive_segment_count,\\n                    )\\n                )\\n            content_search_spec = SearchRequest.ContentSearchSpec(\\n                extractive_content_spec=extractive_content_spec\\n            )\\n        elif self.engine_data_type == 1:\\n            content_search_spec = None\\n        elif self.engine_data_type == 2:\\n            content_search_spec = SearchRequest.ContentSearchSpec(\\n                extractive_content_spec=SearchRequest.ContentSearchSpec.ExtractiveContentSpec(\\n                    max_extractive_answer_count=self.max_extractive_answer_count,\\n                )\\n            )\\n        else:\\n            raise NotImplementedError(\\n                \"Only data store type 0 (Unstructured), 1 (Structured),\"\\n                \"or 2 (Website with Advanced Indexing) are supported currently.\"\\n                + f\" Got {self.engine_data_type}\"\\n            )\\n\\n        return SearchRequest(\\n            query=query,\\n            filter=self.filter,\\n            serving_config=self._serving_config,\\n            page_size=self.max_documents,\\n            content_search_spec=content_search_spec,\\n            query_expansion_spec=query_expansion_spec,\\n            spell_correction_spec=spell_correction_spec,\\n        )',\n",
       "   'd': 'Prepares a SearchRequest object.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def from_chains(\\n        cls, llm: BaseLanguageModel, chains: List[ChainConfig], **kwargs: Any\\n    ) -> AgentExecutor:\\n        tools = [\\n            Tool(\\n                name=c.action_name,\\n                func=c.action,\\n                description=c.action_description,\\n            )\\n            for c in chains\\n        ]\\n        agent = ZeroShotAgent.from_llm_and_tools(llm, tools)\\n        return cls(agent=agent, tools=tools, **kwargs)',\n",
       "   'd': 'User friendly way to initialize the MRKL chain.\\n\\nThis is intended to be an easy way to get up and running with the\\nMRKL chain.\\n\\nArgs:\\n    llm: The LLM to use as the agent LLM.\\n    chains: The chains the MRKL system has access to.\\n    **kwargs: parameters to be passed to initialization.\\n\\nReturns:\\n    An initialized MRKL chain.\\n\\nExample:\\n    .. code-block:: python\\n\\n        from langchain import LLMMathChain, OpenAI, SerpAPIWrapper, MRKLChain\\n        from langchain.chains.mrkl.base import ChainConfig\\n        llm = OpenAI(temperature=0)\\n        search = SerpAPIWrapper()\\n        llm_math_chain = LLMMathChain(llm=llm)\\n        chains = [\\n            ChainConfig(\\n                action_name = \"Search\",\\n                action=search.search,\\n                action_description=\"useful for searching\"\\n            ),\\n            ChainConfig(\\n                action_name=\"Calculator\",\\n                action=llm_math_chain.run,\\n                action_description=\"useful for doing math\"\\n            )\\n        ]\\n        mrkl = MRKLChain.from_chains(llm, chains)',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def split_text(self, text: str) -> List[Document]:\\n        lines = text.split(\"\\\\n\")\\n\\n        lines_with_metadata: List[LineType] = []\\n\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n\\n\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            for sep, name in self.headers_to_split_on:\\n\\n                if stripped_line.startswith(sep) and (\\n\\n\\n                    len(stripped_line) == len(sep)\\n                    or stripped_line[len(sep)] == \" \"\\n                ):\\n\\n                    if name is not None:\\n\\n                        current_header_level = sep.count(\"\\n\\n\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n\\n\\n                            popped_header = header_stack.pop()\\n\\n\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])\\n\\n\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n\\n                        initial_metadata[name] = header[\"data\"]\\n\\n\\n\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n\\n\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
       "   'd': 'Initialize an instance of RedisCache.\\n\\nThis method initializes an object with Redis caching capabilities.\\nIt takes a `redis_` parameter, which should be an instance of a Redis\\nclient class, allowing the object to interact with a Redis\\nserver for caching purposes.\\n\\nParameters:\\n    redis_ (Any): An instance of a Redis client class\\n        (e.g., redis.Redis) used for caching.\\n        This allows the object to communicate with a\\n        Redis server for caching operations.\\n    ttl (int, optional): Time-to-live (TTL) for cached items in seconds.\\n        If provided, it sets the time duration for how long cached\\n        items will remain valid. If not provided, cached items will not\\n        have an automatic expiration.',\n",
       "   'l': False,\n",
       "   'g': [':return: tuple([input_example])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])\\n    :rtype: tuple([torch.Tensor])',\n",
       "    'Returns a tuple of input example.',\n",
       "    'Input example for a single image',\n",
       "    ':return:']},\n",
       "  {'c': 'def dict(self, **kwargs: Any) -> Dict:\\n    _dict = super().dict()\\n    del _dict[\"output_parser\"]\\n    return _dict',\n",
       "   'd': 'Return type of llm.',\n",
       "   'l': False,\n",
       "   'g': ['Save the document to the database.\\n\\n    :return: None.',\n",
       "    'Saves the document.\\n    :return: :class:`~django.db.models.Model` object.',\n",
       "    'Save the document to the database.\\n    :return:',\n",
       "    'Save the document to the database.']},\n",
       "  {'c': '    def index(\\n        self,\\n        dims_length: Union[int, None],\\n        vector_query_field: str,\\n        similarity: Union[DistanceStrategy, None],\\n    ) -> Dict:\\n        if similarity is DistanceStrategy.COSINE:\\n            similarityAlgo = \"cosine\"\\n        elif similarity is DistanceStrategy.EUCLIDEAN_DISTANCE:\\n            similarityAlgo = \"l2_norm\"\\n        elif similarity is DistanceStrategy.DOT_PRODUCT:\\n            similarityAlgo = \"dot_product\"\\n        else:\\n            raise ValueError(f\"Similarity {similarity} not supported.\")\\n\\n        return {\\n            \"mappings\": {\\n                \"properties\": {\\n                    vector_query_field: {\\n                        \"type\": \"dense_vector\",\\n                        \"dims\": dims_length,\\n                        \"index\": True,\\n                        \"similarity\": similarityAlgo,\\n                    },\\n                }\\n            }\\n        }',\n",
       "   'd': 'Create the mapping for the Elasticsearch index.',\n",
       "   'l': True,\n",
       "   'g': ['Check if data is not empty and not only whitespaces.\\n\\n  Args:\\n      data (str): data to check.\\n\\n  Returns:\\n      bool: True if data is not empty and not only whitespaces, False otherwise.',\n",
       "    'Check if the data is not empty and not only whitespace.\\n\\n    :param data: The data to check.\\n    :return: True if the data is not empty and not only whitespace, False otherwise.\\n    :rtype: bool',\n",
       "    ':param data: str\\n    :return: bool\\n    :rtype: bool\\n    :raises:',\n",
       "    'Check if the given data is not empty and not only whitespaces.\\n\\n  :param data: The data to check.\\n  :return: True if the data is not empty and not only whitespaces, False otherwise.\\n  :rtype: bool']},\n",
       "  {'c': '    def delete(\\n        self,\\n        ids: Optional[List[str]] = None,\\n        refresh_indices: Optional[bool] = True,\\n        **kwargs: Any,\\n    ) -> Optional[bool]:\\n        try:\\n            from elasticsearch.helpers import BulkIndexError, bulk\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import elasticsearch python package. \"\\n                \"Please install it with `pip install elasticsearch`.\"\\n            )\\n\\n        body = []\\n\\n        if ids is None:\\n            raise ValueError(\"ids must be provided.\")\\n\\n        for _id in ids:\\n            body.append({\"_op_type\": \"delete\", \"_index\": self.index_name, \"_id\": _id})\\n\\n        if len(body) > 0:\\n            try:\\n                bulk(self.client, body, refresh=refresh_indices, ignore_status=404)\\n                logger.debug(f\"Deleted {len(body)} texts from index\")\\n\\n                return True\\n            except BulkIndexError as e:\\n                logger.error(f\"Error deleting texts: {e}\")\\n                firstError = e.errors[0].get(\"index\", {}).get(\"error\", {})\\n                logger.error(f\"First error reason: {firstError.get(\\'reason\\')}\")\\n                raise e\\n\\n        else:\\n            logger.debug(\"No texts to delete from index\")\\n            return False',\n",
       "   'd': 'Delete documents from the Elasticsearch index.\\n\\nArgs:\\n    ids: List of ids of documents to delete.\\n    refresh_indices: Whether to refresh the index\\n                    after deleting documents. Defaults to True.',\n",
       "   'l': True,\n",
       "   'g': ['_load_prompt_cache(self, path):',\n",
       "    '_load_prompt_cache:\\n    load prompt cache from disk\\n    :param path:\\n    :return:',\n",
       "    '_load_prompt_cache(self, path):\\n    Load prompt cache from the given path.\\n\\n    Args:\\n        path (str): The path to the prompt cache.\\n\\n    Returns:\\n        dict: The prompt cache.',\n",
       "    '_load_prompt_cache(self, path):']},\n",
       "  {'c': 'def args(self) -> dict:\\n    return self.args_schema.schema()[\"properties\"]',\n",
       "   'd': 'Start a trace for a chain run.',\n",
       "   'l': False,\n",
       "   'g': ['', '', ':param cached_response:\\n    :return:', '']},\n",
       "  {'c': 'def add_documents(self, documents: List[Document], **kwargs: Any) -> List[str]:\\n    return self.vectorstore.add_documents(documents, **kwargs)',\n",
       "   'd': 'Create a new TextSplitter.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', 'Test seeking from current position.']},\n",
       "  {'c': 'def load_default_session() -> TracerSessionV1:\\n    return TracerSessionV1(\\n        id=TEST_SESSION_ID, name=\"default\", start_time=datetime.utcnow()\\n    )',\n",
       "   'd': 'Load a tracing session.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def dict(self, **kwargs: Any) -> Dict:\\n    _dict = super().dict()\\n    try:\\n        _type = self._agent_type\\n    except NotImplementedError:\\n        _type = None\\n    if isinstance(_type, AgentType):\\n        _dict[\"_type\"] = str(_type.value)\\n    elif _type is not None:\\n        _dict[\"_type\"] = _type\\n    return _dict',\n",
       "   'd': 'Return dictionary representation of agent.',\n",
       "   'l': True,\n",
       "   'g': ['_getSimulator() -> str\\n    Returns the name of the simulator.',\n",
       "    '_getSimulator() -> str\\n\\n    Returns the name of the simulator.',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': 'def _run(\\n    self,\\n    query: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> str:\\n    return self.llm_chain.predict(\\n        query=query, callbacks=run_manager.get_child() if run_manager else None\\n    )',\n",
       "   'd': 'Use the LLM to check the query.',\n",
       "   'l': True,\n",
       "   'g': ['.retry_report_exception()',\n",
       "    '.retry_report_exception()',\n",
       "    '(str)',\n",
       "    '_safe_report_exception(self):']},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n    generations = []\\n\\n    results = self.redis.hgetall(self._key(prompt, llm_string))\\n    if results:\\n        for _, text in results.items():\\n            generations.append(Generation(text=text))\\n    return generations if generations else None',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': ['(name, session, options, documentation=\"\", usage=\"\")',\n",
       "    '(name, session, options, documentation=\"\", usage=\"',\n",
       "    '(name, session, options, documentation=\"\", usage=\"\")):',\n",
       "    '(name, session, options, documentation=\"\", usage=\"']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        session: Optional[CassandraSession],\\n        keyspace: Optional[str],\\n        embedding: Embeddings,\\n        table_name: str = CASSANDRA_SEMANTIC_CACHE_DEFAULT_TABLE_NAME,\\n        distance_metric: str = CASSANDRA_SEMANTIC_CACHE_DEFAULT_DISTANCE_METRIC,\\n        score_threshold: float = CASSANDRA_SEMANTIC_CACHE_DEFAULT_SCORE_THRESHOLD,\\n        ttl_seconds: Optional[int] = CASSANDRA_SEMANTIC_CACHE_DEFAULT_TTL_SECONDS,\\n        skip_provisioning: bool = False,\\n    ):\\n        try:\\n            from cassio.table import MetadataVectorCassandraTable\\n        except (ImportError, ModuleNotFoundError):\\n            raise ValueError(\\n                \"Could not import cassio python package. \"\\n                \"Please install it with `pip install cassio`.\"\\n            )\\n        self.session = session\\n        self.keyspace = keyspace\\n        self.embedding = embedding\\n        self.table_name = table_name\\n        self.distance_metric = distance_metric\\n        self.score_threshold = score_threshold\\n        self.ttl_seconds = ttl_seconds\\n\\n\\n\\n\\n\\n\\n        @lru_cache(maxsize=CASSANDRA_SEMANTIC_CACHE_EMBEDDING_CACHE_SIZE)\\n        def _cache_embedding(text: str) -> List[float]:\\n            return self.embedding.embed_query(text=text)\\n\\n        self._get_embedding = _cache_embedding\\n        self.embedding_dimension = self._get_embedding_dimension()\\n\\n        self.table = MetadataVectorCassandraTable(\\n            session=self.session,\\n            keyspace=self.keyspace,\\n            table=self.table_name,\\n            primary_key_type=[\"TEXT\"],\\n            vector_dimension=self.embedding_dimension,\\n            ttl_seconds=self.ttl_seconds,\\n            metadata_indexing=(\"allow\", {\"_llm_string_hash\"}),\\n            skip_provisioning=skip_provisioning,\\n        )',\n",
       "   'd': 'Lazily parse the blob.',\n",
       "   'l': False,\n",
       "   'g': ['_get_data_format_members:',\n",
       "    '_get_data_format_members:',\n",
       "    '_get_data_format_members(cls, game_version: GameVersion) -> list[tuple[MemberAccess, str, StorageType, typing.Union[str, ReadMember]]]:',\n",
       "    '_get_data_format_members:']},\n",
       "  {'c': 'def parse(self, text: str) -> str:\\n    return text',\n",
       "   'd': 'Returns the input text with no changes.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def __init__(self, engine: Engine, cache_schema: Type[FullLLMCache] = FullLLMCache):\\n    self.engine = engine\\n    self.cache_schema = cache_schema\\n    self.cache_schema.metadata.create_all(self.engine)',\n",
       "   'd': 'Initialize by creating all tables.',\n",
       "   'l': True,\n",
       "   'g': ['Test that the pip install --no-index --find-links find_links\\n        works correctly with a case-mismatch upper-version.',\n",
       "    'Test that we can install a pip-compatible egg with a different\\n    version than the one we are currently using.',\n",
       "    'Test that we can install an upper-version-2.0-py2.7.egg-info',\n",
       "    'Test that the url_req function correctly handles case mismatch']},\n",
       "  {'c': '    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n        for gen in return_val:\\n            if not isinstance(gen, Generation):\\n                raise ValueError(\\n                    \"UpstashRedisCache supports caching of normal LLM generations, \"\\n                    f\"got {type(gen)}\"\\n                )\\n            if isinstance(gen, ChatGeneration):\\n                warnings.warn(\\n                    \"NOTE: Generation has not been cached. UpstashRedisCache does not\"\\n                    \" support caching ChatModel outputs.\"\\n                )\\n                return\\n\\n        key = self._key(prompt, llm_string)\\n\\n        mapping = {\\n            str(idx): generation.text for idx, generation in enumerate(return_val)\\n        }\\n        self.redis.hset(key=key, values=mapping)\\n\\n        if self.ttl is not None:\\n            self.redis.expire(key, self.ttl)',\n",
       "   'd': 'Update cache based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': [\"(Re)start the jail.  This will re-load the jail's database,\\n    and re-load any actions that were already in the\\n    database.\\n\\n    :return:  None\",\n",
       "    '(override)',\n",
       "    \"(Re)start the jail.  This function is called when a new jail is\\n    created or when a jail is re-started.  This function will\\n    check the jail's database for any bans and add them to the\\n    jail's queue.  This function will also check the jail's\\n    database for any bans that have been removed and add them\\n    to the jail's queue.  This function will also check the jail's\\n    database for any bans that have been added and add them to\\n    the jail's queue.  This function will also check the jail's\\n    database for any bans that have been removed and add them to\\n    the jail's queue.  This function will also check the jail's\\n    database for any bans that have been added and add them to\\n    the jail's queue.  This function will also check the jail's\\n    database for any bans that have been removed and add them to\\n    the jail's queue.  This function will also check the jail's\\n    database for any bans that have been added and add them to\\n    the jail's queue.  This function will also check the jail\",\n",
       "    '(function)']},\n",
       "  {'c': 'def __init__(\\n    self, file_path: str, password: Optional[Union[str, bytes]] = None\\n) -> None:\\n    try:\\n        import pypdf\\n    except ImportError:\\n        raise ImportError(\\n            \"pypdf package not found, please install it with \" \"`pip install pypdf`\"\\n        )\\n    self.parser = PyPDFParser(password=password)\\n    super().__init__(file_path)',\n",
       "   'd': 'Initialize the OpenAI object.',\n",
       "   'l': False,\n",
       "   'g': [':param sentence:\\n    :return:',\n",
       "    '',\n",
       "    '_normalized_numbers(self, sentence):',\n",
       "    '']},\n",
       "  {'c': 'def __init__(self) -> None:\\n    try:\\n        import pypdfium2\\n    except ImportError:\\n        raise ImportError(\\n            \"pypdfium2 package not found, please install it with\"\\n            \" `pip install pypdfium2`\"\\n        )',\n",
       "   'd': 'Initialize the parser.',\n",
       "   'l': True,\n",
       "   'g': [':param model: str or Path to a model or a path to a model file.\\n  :param task: str or None. If not None, use a task to predict on the model.\\n  :param verbose: bool. If True, print verbose output.',\n",
       "    '',\n",
       "    \":param model: Path to model file or model file name\\n  :param task: 'classify' or 'recognize'\\n  :param verbose: True or False\",\n",
       "    ':param model: yolov8n.pt or yolov8n.pt.yaml or yolov8n.pt.yaml.torchscript\\n  :param task: detect, segment, classify, or (optional) a custom']},\n",
       "  {'c': '    def from_template(\\n        cls: Type[MessagePromptTemplateT],\\n        template: str,\\n        template_format: str = \"f-string\",\\n        **kwargs: Any,\\n    ) -> MessagePromptTemplateT:\\n        prompt = PromptTemplate.from_template(template, template_format=template_format)\\n        return cls(prompt=prompt, **kwargs)',\n",
       "   'd': 'Use the tool.',\n",
       "   'l': False,\n",
       "   'g': ['Return 405 with the allowed methods.\\n    :param allowed_methods: list of allowed methods.',\n",
       "    '(str)',\n",
       "    ':param allowed_methods: list of strings\\n      :return: function',\n",
       "    'Creates a method not allowed handler.\\n\\n    :param allowed_methods: a list of allowed methods.\\n\\n    :return: a function.']},\n",
       "  {'c': '    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[PromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> QAEvalChain:\\n        prompt = prompt or PROMPT\\n        expected_input_vars = {\"query\", \"answer\", \"result\"}\\n        if expected_input_vars != set(prompt.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt.input_variables}\"\\n            )\\n        return cls(llm=llm, prompt=prompt, **kwargs)',\n",
       "   'd': 'Return the output parser type for serialization.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'Validates the given exchange.\\n\\n  :param value: The exchange to validate.\\n\\n  :return: The error message if the exchange is invalid, or None.',\n",
       "    'Validates the exchange.\\n\\n  :param value: The exchange to be validated.\\n\\n  :return: The error message if the exchange is invalid, otherwise None.']},\n",
       "  {'c': '    def parse(self, text: str) -> T:\\n',\n",
       "   'd': 'Parse a single string model output into some structure.\\n\\nArgs:\\n    text: String output of a language model.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': True,\n",
       "   'g': [':param enckey:',\n",
       "    ':param enckey:',\n",
       "    ':param enckey:',\n",
       "    ':param enckey:']},\n",
       "  {'c': '    def _call(\\n        self,\\n        prompt: str,\\n        stop: Optional[List[str]] = None,\\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\\n        **kwargs: Any,\\n    ) -> str:\\n        try:\\n            from pipeline import PipelineCloud\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import pipeline-ai python package. \"\\n                \"Please install it with `pip install pipeline-ai`.\"\\n            )\\n        client = PipelineCloud(token=self.pipeline_api_key)\\n        params = self.pipeline_kwargs or {}\\n        params = {**params, **kwargs}\\n\\n        run = client.run_pipeline(self.pipeline_key, [prompt, params])\\n        try:\\n            text = run.result_preview[0][0]\\n        except AttributeError:\\n            raise AttributeError(\\n                f\"A pipeline run should have a `result_preview` attribute.\"\\n                f\"Run was: {run}\"\\n            )\\n        if stop is not None:\\n\\n\\n            text = enforce_stop_tokens(text, stop)\\n        return text',\n",
       "   'd': 'Call to Pipeline Cloud endpoint.',\n",
       "   'l': True,\n",
       "   'g': ['Returns a string representation of the widget.\\n\\n    :return: string\\n    :rtype: str\\n    :see: :func:`~django_plotly_',\n",
       "    '',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': 'def similarity_search_with_score(\\n    self, query: str, k: int = 4, filter: Optional[dict] = None, **kwargs: Any\\n) -> List[Tuple[Document, float]]:\\n    embedding = self.embedding.embed_query(query)\\n    script_query = _default_script_query(embedding, filter)\\n    response = self.client_search(\\n        self.client, self.index_name, script_query, size=k\\n    )\\n    hits = [hit for hit in response[\"hits\"][\"hits\"]]\\n    docs_and_scores = [\\n        (\\n            Document(\\n                page_content=hit[\"_source\"][\"text\"],\\n                metadata=hit[\"_source\"][\"metadata\"],\\n            ),\\n            hit[\"_score\"],\\n        )\\n        for hit in hits\\n    ]\\n    return docs_and_scores',\n",
       "   'd': 'Row-wise cosine similarity with optional top-k and score threshold filtering.\\n\\nArgs:\\n    X: Matrix.\\n    Y: Matrix, same width as X.\\n    top_k: Max number of results to return.\\n    score_threshold: Minimum cosine similarity of results.\\n\\nReturns:\\n    Tuple of two lists. First contains two-tuples of indices (X_idx, Y_idx),\\n        second contains corresponding cosine similarities.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    ':param cls:\\n    :param status:\\n    :return:',\n",
       "    '',\n",
       "    'Return message by status']},\n",
       "  {'c': '    def _on_run_create(self, run: Run) -> None:\\n        if run.parent_run_id is None:\\n            self.send_stream.send_nowait(\\n                RunLogPatch(\\n                    {\\n                        \"op\": \"replace\",\\n                        \"path\": \"\",\\n                        \"value\": RunState(\\n                            id=str(run.id),\\n                            streamed_output=[],\\n                            final_output=None,\\n                            logs={},\\n                        ),\\n                    }\\n                )\\n            )\\n\\n        if not self.include_run(run):\\n            return\\n\\n\\n        with self.lock:\\n            self._counter_map_by_name[run.name] += 1\\n            count = self._counter_map_by_name[run.name]\\n            self._key_map_by_run_id[run.id] = (\\n                run.name if count == 1 else f\"{run.name}:{count}\"\\n            )\\n\\n\\n        self.send_stream.send_nowait(\\n            RunLogPatch(\\n                {\\n                    \"op\": \"add\",\\n                    \"path\": f\"/logs/{self._key_map_by_run_id[run.id]}\",\\n                    \"value\": LogEntry(\\n                        id=str(run.id),\\n                        name=run.name,\\n                        type=run.run_type,\\n                        tags=run.tags or [],\\n                        metadata=(run.extra or {}).get(\"metadata\", {}),\\n                        start_time=run.start_time.isoformat(timespec=\"milliseconds\"),\\n                        streamed_output_str=[],\\n                        final_output=None,\\n                        end_time=None,\\n                    ),\\n                }\\n            )\\n        )',\n",
       "   'd': 'Initialize with web page and whether to load all paths.\\n\\nArgs:\\n    web_page: The web page to load or the starting point from where\\n        relative paths are discovered.\\n    load_all_paths: If set to True, all relative paths in the navbar\\n        are loaded instead of only `web_page`.\\n    base_url: If `load_all_paths` is True, the relative paths are\\n        appended to this base url. Defaults to `web_page`.\\n    content_selector: The CSS selector for the content to load.\\n        Defaults to \"main\".\\n    continue_on_failure: whether to continue loading the sitemap if an error\\n        occurs loading a url, emitting a warning instead of raising an\\n        exception. Setting this to True makes the loader more robust, but also\\n        may result in missing data. Default: False',\n",
       "   'l': False,\n",
       "   'g': ['_get_heap_boundaries(self, addr=None)',\n",
       "    '_get_heap_boundaries()',\n",
       "    '_get_heap_boundaries(self, addr=None):',\n",
       "    '_get_heap_boundaries()']},\n",
       "  {'c': '    def parse_result(self, result: List[Generation]) -> T:\\n',\n",
       "   'd': 'Return response when agent has been stopped due to max iterations.',\n",
       "   'l': False,\n",
       "   'g': ['(self, default=None, label=None, **kwargs):',\n",
       "    '(default: Optional[float] = None, label: Optional[str] = None, **kwargs) -> None:',\n",
       "    '(self, default: Optional[float] = None, label: Optional[str] = None, **kwargs):',\n",
       "    '']},\n",
       "  {'c': '    def clear(self, **kwargs: Any) -> None:\\n        from gptcache import Cache\\n\\n        for gptcache_instance in self.gptcache_dict.values():\\n            gptcache_instance = cast(Cache, gptcache_instance)\\n            gptcache_instance.flush()\\n\\n        self.gptcache_dict.clear()',\n",
       "   'd': 'Lazy load records from FeatureLayer.',\n",
       "   'l': False,\n",
       "   'g': ['_read_addr_range(start, end, addr_space=None)',\n",
       "    '(addr_space=None)',\n",
       "    '(start, end) -> list of',\n",
       "    '(start, end) is inclusive.']},\n",
       "  {'c': '    def get_child(self, tag: Optional[str] = None) -> AsyncCallbackManager:\\n        manager = AsyncCallbackManager(handlers=[], parent_run_id=self.run_id)\\n        manager.set_handlers(self.inheritable_handlers)\\n        manager.add_tags(self.inheritable_tags)\\n        manager.add_metadata(self.inheritable_metadata)\\n        if tag is not None:\\n            manager.add_tags([tag], False)\\n        return manager',\n",
       "   'd': 'Update an attribute of a specified task',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n    rows = self._search_rows(prompt, llm_string)\\n    if rows:\\n        return [loads(row[0]) for row in rows]\\n    return None',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': [':param X: np.array of shape (n_samples, n_features)\\n    :return: np.array of shape (k, n_features)',\n",
       "    '',\n",
       "    'Initialize centroids as random points from the data',\n",
       "    '']},\n",
       "  {'c': '    def OutputType(self) -> Type[Output]:\\n        for cls in self.__class__.__orig_bases__:\\n            type_args = get_args(cls)\\n            if type_args and len(type_args) == 2:\\n                return type_args[1]\\n\\n        raise TypeError(\\n            f\"Runnable {self.__class__.__name__} doesn\\'t have an inferable OutputType. \"\\n            \"Override the OutputType property to specify the output type.\"\\n        )',\n",
       "   'd': 'The type of output this runnable produces specified as a type annotation.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def __init__(self, **kwargs: Any) -> None:\\n    separators = self.get_separators_for_language(Language.LATEX)\\n    super().__init__(separators=separators, **kwargs)',\n",
       "   'd': 'Initialize a LatexTextSplitter.',\n",
       "   'l': True,\n",
       "   'g': ['_wrap_forward_with_set_fields',\n",
       "    '_wrap_forward_with_set_fields(predictor, default_args):\\n\\n    :param predictor: the predictor to wrap\\n    :param default_args: the default arguments to set',\n",
       "    '.forward(self, **kwargs):\\n    :param kwargs:\\n    :return:',\n",
       "    '_wrap_forward_with_set_fields']},\n",
       "  {'c': '    def __init__(self, **kwargs: Any):\\n        super().__init__(**kwargs)\\n        try:\\n            from InstructorEmbedding import INSTRUCTOR\\n\\n            self.client = INSTRUCTOR(\\n                self.model_name, cache_folder=self.cache_folder, **self.model_kwargs\\n            )\\n        except ImportError as e:\\n            raise ImportError(\"Dependencies for InstructorEmbedding not found.\") from e',\n",
       "   'd': 'Initialize the sentence_transformer.',\n",
       "   'l': True,\n",
       "   'g': ['Return all the children of an element, optionally filtered by tag.\\n\\n  :param element: The element to get children from.\\n  :param tag: The tag to filter by.\\n  :returns: The children of the element.',\n",
       "    '',\n",
       "    'Recursive function to get all children of an element.\\n\\n    :param element: Element to get children of.\\n    :param tag: Optional tag to filter children by.\\n    :return: List of children.',\n",
       "    '(element, tag, results) -> [results]']},\n",
       "  {'c': 'def _call_with_config(\\n    self,\\n    func: Union[\\n        Callable[[Input], Output],\\n        Callable[[Input, CallbackManagerForChainRun], Output],\\n        Callable[[Input, CallbackManagerForChainRun, RunnableConfig], Output],\\n    ],\\n    input: Input,\\n    config: Optional[RunnableConfig],\\n    run_type: Optional[str] = None,\\n) -> Output:\\n    config = ensure_config(config)\\n    callback_manager = get_callback_manager_for_config(config)\\n    run_manager = callback_manager.on_chain_start(\\n        dumpd(self),\\n        input,\\n        run_type=run_type,\\n    )\\n    try:\\n        if accepts_run_manager_and_config(func):\\n            output = func(\\n                input,\\n                run_manager=run_manager,\\n                config=config,\\n            )\\n        elif accepts_run_manager(func):\\n            output = func(input, run_manager=run_manager)\\n        else:\\n            output = func(input)\\n    except Exception as e:\\n        run_manager.on_chain_error(e)\\n        raise\\n    else:\\n        run_manager.on_chain_end(dumpd(output))\\n        return output',\n",
       "   'd': 'Initialize by passing in Redis instance.',\n",
       "   'l': False,\n",
       "   'g': ['Create a input widget.\\n    :return: input widget.',\n",
       "    '.createWidget() method is called when a new widget is created.\\n    :param self: the InputWidget instance.\\n    :return: a new InputWidget instance.',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': 'def stream(\\n    self, input: Input, config: Optional[RunnableConfig] = None\\n) -> Iterator[Output]:\\n    yield self.invoke(input, config)',\n",
       "   'd': 'Default implementation of stream, which calls invoke.\\nSubclasses should override this method if they support streaming output.',\n",
       "   'l': True,\n",
       "   'g': [':param df_lists: list of (df, prob) tuples',\n",
       "    ':param df_lists: list of (df, prob) tuples\\n    :return:',\n",
       "    \":param df_lists: [('df_name', df_name), ('df_name', df_name), ... ]\",\n",
       "    ':param df_lists: list of (data_frame, probability)\\n    :type df_lists: list of (pd.DataFrame, float)\\n    :return:']},\n",
       "  {'c': 'def test_vectara_add_documents() -> None:\\n    docsearch: Vectara = Vectara()\\n\\n\\n    texts1 = [\"grounded generation\", \"retrieval augmented generation\", \"data privacy\"]\\n    md = [{\"abbr\": get_abbr(t)} for t in texts1]\\n    doc_id1 = docsearch.add_texts(\\n        texts1,\\n        metadatas=md,\\n        doc_metadata={\"test_num\": \"1\"},\\n    )\\n\\n\\n    texts2 = [\"large language model\", \"information retrieval\", \"question answering\"]\\n    doc_id2 = docsearch.add_documents(\\n        [Document(page_content=t, metadata={\"abbr\": get_abbr(t)}) for t in texts2],\\n        doc_metadata={\"test_num\": \"2\"},\\n    )\\n    doc_ids = doc_id1 + doc_id2\\n\\n\\n    output1 = docsearch.similarity_search(\\n        \"large language model\",\\n        k=2,\\n        n_sentence_context=0,\\n    )\\n    assert len(output1) == 2\\n    assert output1[0].page_content == \"large language model\"\\n    assert output1[0].metadata[\"abbr\"] == \"llm\"\\n    assert output1[1].page_content == \"information retrieval\"\\n    assert output1[1].metadata[\"abbr\"] == \"ir\"\\n\\n\\n\\n    output2 = docsearch.similarity_search(\\n        \"large language model\",\\n        k=1,\\n        n_sentence_context=0,\\n        filter=\"doc.test_num = 1\",\\n    )\\n    assert len(output2) == 1\\n    assert output2[0].page_content == \"retrieval augmented generation\"\\n    assert output2[0].metadata[\"abbr\"] == \"rag\"\\n\\n    for doc_id in doc_ids:\\n        docsearch._delete_doc(doc_id)',\n",
       "   'd': 'Test end to end construction and search.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def dict(self, **kwargs: Any) -> Dict:\\n    output_parser_dict = super().dict(**kwargs)\\n    output_parser_dict[\"_type\"] = self._type\\n    return output_parser_dict',\n",
       "   'd': 'Return dictionary representation of output parser.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def __init__(self) -> None:\\n    self._cache: Dict[Tuple[str, str], RETURN_VAL_TYPE] = {}',\n",
       "   'd': 'Initialize with empty cache.',\n",
       "   'l': True,\n",
       "   'g': ['Register a resource body class with the given resource type.\\n\\n    :param cls: The class to register.\\n    :param resource_type: The resource type to register the class with.\\n    :return: The registered class.',\n",
       "    ':param cls: The class of the resource body.\\n  :return: The class of the resource body.\\n  :rtype: :class:`~openstack_dashboard.api.v2_0.types.ResourceBody`\\n  :raises: :class:`~openstack_dashboard.api.v2_0.exceptions.ResourceNotSupported`',\n",
       "    ':param cls:\\n      :param resource_body_cls:\\n  :return:\\n  :rtype: object\\n  :raises:',\n",
       "    'Register a new resource type.\\n\\n  Args:\\n    cls (class): The class to be registered.\\n\\n  Returns:\\n    class: The registered class.\\n\\n  Raises:\\n    AssertionError: If the resource type is already registered.']},\n",
       "  {'c': 'def return_stopped_response(\\n    self,\\n    early_stopping_method: str,\\n    intermediate_steps: List[Tuple[AgentAction, str]],\\n    **kwargs: Any,\\n) -> AgentFinish:\\n    if early_stopping_method == \"force\":\\n\\n        return AgentFinish(\\n            {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\\n        )\\n    elif early_stopping_method == \"generate\":\\n\\n        thoughts = \"\"\\n        for action, observation in intermediate_steps:\\n            thoughts += action.log\\n            thoughts += (\\n                f\"\\\\n{self.observation_prefix}{observation}\\\\n{self.llm_prefix}\"\\n            )\\n\\n        thoughts += (\\n            \"\\\\n\\\\nI now need to return a final answer based on the previous steps:\"\\n        )\\n        new_inputs = {\"agent_scratchpad\": thoughts, \"stop\": self._stop}\\n        full_inputs = {**kwargs, **new_inputs}\\n        full_output = self.llm_chain.predict(**full_inputs)\\n\\n        parsed_output = self.output_parser.parse(full_output)\\n        if isinstance(parsed_output, AgentFinish):\\n\\n            return parsed_output\\n        else:\\n\\n\\n            return AgentFinish({\"output\": full_output}, full_output)\\n    else:\\n        raise ValueError(\\n            \"early_stopping_method should be one of `force` or `generate`, \"\\n            f\"got {early_stopping_method}\"\\n        )',\n",
       "   'd': 'Return response when agent has been stopped due to max iterations.',\n",
       "   'l': True,\n",
       "   'g': ['_write(self, docs, metadatas):',\n",
       "    '_write(self, docs, metadatas):',\n",
       "    '.write()',\n",
       "    '(str) Write the indexable document to the store.\\n\\n    :returns: the store path.']},\n",
       "  {'c': 'def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n    for gen in return_val:\\n        if not isinstance(gen, Generation):\\n            raise ValueError(\\n                \"RedisSemanticCache only supports caching of \"\\n                f\"normal LLM generations, got {type(gen)}\"\\n            )\\n        if isinstance(gen, ChatGeneration):\\n            warnings.warn(\\n                \"NOTE: Generation has not been cached. RedisSentimentCache does not\"\\n                \" support caching ChatModel outputs.\"\\n            )\\n            return\\n    llm_cache = self._get_llm_cache(llm_string)\\n    _dump_generations_to_json([g for g in return_val])\\n    metadata = {\\n        \"llm_string\": llm_string,\\n        \"prompt\": prompt,\\n        \"return_val\": _dump_generations_to_json([g for g in return_val]),\\n    }\\n    llm_cache.add_texts(texts=[prompt], metadatas=[metadata])',\n",
       "   'd': 'Update cache based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': [':param value: The value to check.\\n    :type value: str',\n",
       "    'Check that the value is a valid transaction hash.\\n\\n  Args:\\n      value: The value to check.\\n\\n  Raises:\\n      RPCError: If the value is not a valid transaction hash.',\n",
       "    'Check if the value is a valid transaction hash.\\n\\n  :param value: The value to check.\\n\\n  :raises: `RPCError` if the value is not a valid transaction hash.',\n",
       "    'Check if a string is a valid transaction hash.\\n\\n  :param value: The string to check.\\n  :raises: RPCError if the string is not a valid transaction hash.']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        openai_api_key = get_from_dict_or_env(\\n            values, \"openai_api_key\", \"OPENAI_API_KEY\"\\n        )\\n        openai_api_base = get_from_dict_or_env(\\n            values,\\n            \"openai_api_base\",\\n            \"OPENAI_API_BASE\",\\n            default=\"\",\\n        )\\n        openai_proxy = get_from_dict_or_env(\\n            values,\\n            \"openai_proxy\",\\n            \"OPENAI_PROXY\",\\n            default=\"\",\\n        )\\n        openai_organization = get_from_dict_or_env(\\n            values, \"openai_organization\", \"OPENAI_ORGANIZATION\", default=\"\"\\n        )\\n        try:\\n            import openai\\n\\n            openai.api_key = openai_api_key\\n            if openai_api_base:\\n                openai.api_base = openai_api_base\\n            if openai_organization:\\n                openai.organization = openai_organization\\n            if openai_proxy:\\n                openai.proxy = {\"http\": openai_proxy, \"https\": openai_proxy}\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import openai python package. \"\\n                \"Please install it with `pip install openai`.\"\\n            )\\n        try:\\n            values[\"client\"] = openai.ChatCompletion\\n        except AttributeError:\\n            raise ValueError(\\n                \"`openai` has no `ChatCompletion` attribute, this is likely \"\\n                \"due to an old version of the openai package. Try upgrading it \"\\n                \"with `pip install --upgrade openai`.\"\\n            )\\n        warnings.warn(\\n            \"You are trying to use a chat model. This way of initializing it is \"\\n            \"no longer supported. Instead, please use: \"\\n            \"`from langchain.chat_models import ChatOpenAI`\"\\n        )\\n        return values',\n",
       "   'd': 'Validate that api key and python package exists in environment.',\n",
       "   'l': True,\n",
       "   'g': ['- get api url by stage name\\n- :param stage_name: stage name\\n- :return: api url\\n- :rtype: str\\n- :raises: 404 - stage name is not found\\n- :raises: 500 - get api url failed\\n- :raises: 502 - get api url failed\\n- :raises: 503 - get api url failed\\n- :raises: 504 - get api url failed\\n- :raises: 505 - get api url failed\\n- :raises: 506 - get api url failed\\n- :raises: 507 - get api url failed\\n- :raises: 508 - get api url failed\\n- :raises: 509 - get api url failed\\n- :raises: 510 - get api url failed\\n- :raises: 511 - get api url failed\\n- :raises: 512 - get api url failed\\n- :raises: 513 - get api url failed\\n- :raises: 514 - get api url failed\\n- :raises: 515 - get api url failed',\n",
       "    '- Gets the API url for the stage name.\\n\\n    - :param stage_name: The stage name to get the API url for.\\n\\n    - :return: The API url for the stage name.',\n",
       "    '_get_api_url',\n",
       "    '_get_api_url()']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        web_path: Union[str, List[str]],\\n        header_template: Optional[dict] = None,\\n        verify_ssl: Optional[bool] = True,\\n        proxies: Optional[dict] = None,\\n        requests_per_second: int = 2,\\n        requests_kwargs: Optional[Dict[str, Any]] = None,\\n        raise_for_status: bool = False,\\n    ):\\n        if isinstance(web_path, str):\\n            self.web_paths = [web_path]\\n        elif isinstance(web_path, List):\\n            self.web_paths = web_path\\n\\n        headers = header_template or default_header_template\\n        if not headers.get(\"User-Agent\"):\\n            try:\\n                from fake_useragent import UserAgent\\n\\n                headers[\"User-Agent\"] = UserAgent().random\\n            except ImportError:\\n                logger.info(\\n                    \"fake_useragent not found, using default user agent.\"\\n                    \"To get a realistic header for requests, \"\\n                    \"`pip install fake_useragent`.\"\\n                )\\n\\n        self.session = requests.Session()\\n        self.session.headers = dict(headers)\\n        self.session.verify = verify_ssl\\n\\n        if proxies:\\n            self.session.proxies.update(proxies)\\n\\n        self.requests_per_second = requests_per_second\\n        self.requests_kwargs = requests_kwargs or {}\\n        self.raise_for_status = raise_for_status',\n",
       "   'd': 'Initialize with a webpage path.',\n",
       "   'l': True,\n",
       "   'g': ['_make_sine_merge(self, x):',\n",
       "    '_sin_gen(self, x):',\n",
       "    '_forward(self, x):',\n",
       "    '_get_sine_merge(self, x):']},\n",
       "  {'c': '    def _evaluate_in_project(self, run: Run, evaluator: RunEvaluator) -> None:\\n        try:\\n            if self.project_name is None:\\n                self.client.evaluate_run(run, evaluator)\\n            with tracing_v2_enabled(\\n                project_name=self.project_name, tags=[\"eval\"], client=self.client\\n            ):\\n                self.client.evaluate_run(run, evaluator)\\n        except Exception as e:\\n            logger.error(\\n                f\"Error evaluating run {run.id} with \"\\n                f\"{evaluator.__class__.__name__}: {e}\",\\n                exc_info=True,\\n            )\\n            raise e',\n",
       "   'd': 'Evaluate the run in the project.\\n\\nParameters\\n----------\\nrun : Run\\n    The run to be evaluated.\\nevaluator : RunEvaluator\\n    The evaluator to use for evaluating the run.',\n",
       "   'l': True,\n",
       "   'g': ['_set_ignore_users(self, users: list = []):\\n\\n    _set_ignore_users(self, users)\\n\\n    :param users: list of user_ids to ignore',\n",
       "    '(ignore_users)',\n",
       "    '(Optional) Set users to ignore when sending messages.\\n        \\n    :param users: List of users to ignore.',\n",
       "    '(optional)']},\n",
       "  {'c': '    def similarity_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        embedding = self.embedding.embed_query(text=query)\\n        return self.similarity_search_by_vector(\\n            embedding=embedding,\\n            k=k,\\n        )',\n",
       "   'd': 'Validate that the python package exists in the environment.',\n",
       "   'l': False,\n",
       "   'g': ['', '', ':param args:\\n    :param kwargs:\\n    :return:', '']},\n",
       "  {'c': '    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Any = None,\\n        metadatas: Optional[List[dict]] = None,\\n        index_name: str = \"\",\\n        url: str = \"http://localhost:8882\",\\n        api_key: str = \"\",\\n        add_documents_settings: Optional[Dict[str, Any]] = None,\\n        searchable_attributes: Optional[List[str]] = None,\\n        page_content_builder: Optional[Callable[[Dict[str, str]], str]] = None,\\n        index_settings: Optional[Dict[str, Any]] = None,\\n        verbose: bool = True,\\n        **kwargs: Any,\\n    ) -> Marqo:\\n        try:\\n            import marqo\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import marqo python package. \"\\n                \"Please install it with `pip install marqo`.\"\\n            )\\n\\n        if not index_name:\\n            index_name = str(uuid.uuid4())\\n\\n        client = marqo.Client(url=url, api_key=api_key)\\n\\n        try:\\n            client.create_index(index_name, settings_dict=index_settings or {})\\n            if verbose:\\n                print(f\"Created {index_name} successfully.\")\\n        except Exception:\\n            if verbose:\\n                print(f\"Index {index_name} exists.\")\\n\\n        instance: Marqo = cls(\\n            client,\\n            index_name,\\n            searchable_attributes=searchable_attributes,\\n            add_documents_settings=add_documents_settings or {},\\n            page_content_builder=page_content_builder,\\n        )\\n        instance.add_texts(texts, metadatas)\\n        return instance',\n",
       "   'd': 'Return Marqo initialized from texts. Note that Marqo does not need\\nembeddings, we retain the parameter to adhere to the Liskov\\nsubstitution principle.\\n\\nThis is a quick way to get started with marqo - simply provide your texts and\\nmetadatas and this will create an instance of the data store and index the\\nprovided data.\\n\\nTo know the ids of your documents with this approach you will need to include\\nthem in under the key \"_id\" in your metadatas for each text\\n\\nExample:\\n.. code-block:: python\\n\\n        from langchain.vectorstores import Marqo\\n\\n        datastore = Marqo(texts=[\\'text\\'], index_name=\\'my-first-index\\',\\n        url=\\'http://localhost:8882\\')\\n\\nArgs:\\n    texts (List[str]): A list of texts to index into marqo upon creation.\\n    embedding (Any, optional): Embeddings (not required). Defaults to None.\\n    index_name (str, optional): The name of the index to use, if none is\\n    provided then one will be created with a UUID. Defaults to None.\\n    url (str, optional): The URL for Marqo. Defaults to \"http://localhost:8882\".\\n    api_key (str, optional): The API key for Marqo. Defaults to \"\".\\n    metadatas (Optional[List[dict]], optional): A list of metadatas, to\\n    accompany the texts. Defaults to None.\\n    this is only used when a new index is being created. Defaults to \"cpu\". Can\\n    be \"cpu\" or \"cuda\".\\n    add_documents_settings (Optional[Dict[str, Any]], optional): Settings\\n    for adding documents, see\\n    https://docs.marqo.ai/0.0.16/API-Reference/documents/#query-parameters.\\n    Defaults to {}.\\n    index_settings (Optional[Dict[str, Any]], optional): Index settings if\\n    the index doesn\\'t exist, see\\n    https://docs.marqo.ai/0.0.16/API-Reference/indexes/#index-defaults-object.\\n    Defaults to {}.\\n\\nReturns:\\n    Marqo: An instance of the Marqo vector store',\n",
       "   'l': True,\n",
       "   'g': ['', '', ':param result: result of the query', '']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special',\n",
       "   'd': 'Create a new TextSplitter.',\n",
       "   'l': True,\n",
       "   'g': ['_from_keypair(cls, keypair)\\n\\n    :param cls:\\n    :param keypair:\\n    :return:',\n",
       "    '_from_keypair(cls, keypair) ->',\n",
       "    '',\n",
       "    '_from_keypair(cls, keypair):\\n\\n    :param cls: The class to create.\\n    :param keypair: The keypair to use to create the new instance.\\n    :return: The new instance.\\n    :rtype: :class:`cls`\\n    :raises: :class:`ValueError` if the keypair is invalid.\\n    :raises: :class:`TypeError` if the keypair is not a tuple.\\n    :raises: :class:`TypeError` if the keypair is not a tuple of two\\n        strings.\\n    :raises: :class:`TypeError` if the keypair is not a tuple of two\\n        strings.']},\n",
       "  {'c': 'def create_openai_fn_chain(\\n    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\\n    llm: BaseLanguageModel,\\n    prompt: BasePromptTemplate,\\n    *,\\n    output_key: str = \"function\",\\n    output_parser: Optional[BaseLLMOutputParser] = None,\\n    **kwargs: Any,\\n) -> LLMChain:\\n    if not functions:\\n        raise ValueError(\"Need to pass in at least one function. Received zero.\")\\n    openai_functions = [convert_to_openai_function(f) for f in functions]\\n    fn_names = [oai_fn[\"name\"] for oai_fn in openai_functions]\\n    output_parser = output_parser or _get_openai_output_parser(functions, fn_names)\\n    llm_kwargs: Dict[str, Any] = {\\n        \"functions\": openai_functions,\\n    }\\n    if len(openai_functions) == 1:\\n        llm_kwargs[\"function_call\"] = {\"name\": openai_functions[0][\"name\"]}\\n    llm_chain = LLMChain(\\n        llm=llm,\\n        prompt=prompt,\\n        output_parser=output_parser,\\n        llm_kwargs=llm_kwargs,\\n        output_key=output_key,\\n        **kwargs,\\n    )\\n    return llm_chain',\n",
       "   'd': 'Eagerly load the content.',\n",
       "   'l': False,\n",
       "   'g': ['_add_count_data: Add count data to the data store.\\n\\n    Args:\\n        counts (dict): A dictionary of counts for the current page.\\n\\n    Returns:\\n        None: This method is abstract and must be implemented by a subclass.\\n\\n    Raises:\\n        NotImplementedError: If this method is not implemented by a subclass, it will raise an error.',\n",
       "    '_add_count_data: add counts to the data list.\\n\\n    :param counts: list of counts.\\n    :type counts: list of int.\\n    :return: None.\\n    :rtype: None.',\n",
       "    '_add_count_data(self, counts)',\n",
       "    '_add_count_data\\n    :param counts:\\n    :return:']},\n",
       "  {'c': 'def _call_with_config(\\n    self,\\n    func: Callable[[Any], Output],\\n    input: Any,\\n    config: Optional[RunnableConfig],\\n    run_type: Optional[str] = None,\\n) -> Output:\\n    config = ensure_config(config)\\n    callback_manager = get_callback_manager_for_config(config)\\n    run_manager = callback_manager.on_chain_start(\\n        dumpd(self),\\n        input if isinstance(input, dict) else {\"input\": input},\\n        run_type=run_type,\\n    )\\n    try:\\n        output = func(input)\\n    except Exception as e:\\n        run_manager.on_chain_error(e)\\n        raise\\n    else:\\n        output_for_tracer = dumpd(output)\\n        run_manager.on_chain_end(\\n            output_for_tracer\\n            if isinstance(output_for_tracer, dict)\\n            else {\"output\": output_for_tracer}\\n        )\\n        return output',\n",
       "   'd': 'Return Vectara documents most similar to query, along with scores.\\n\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 5.\\n    lambda_val: lexical match parameter for hybrid search.\\n    filter: Dictionary of argument(s) to filter on metadata. For example a\\n        filter can be \"doc.rating > 3.0 and part.lang = \\'deu\\'\"} see\\n        https://docs.vectara.com/docs/search-apis/sql/filter-overview\\n        for more details.\\n    n_sentence_context: number of sentences before/after the matching segment\\n        to add\\n\\nReturns:\\n    List of Documents most similar to the query and score for each.',\n",
       "   'l': False,\n",
       "   'g': ['_chunk_key_offset(self, key):',\n",
       "    '.chunk_key_offset(self, key)\\n\\n    Returns the offset of the given key in the chunk.\\n\\n    :param key: The key to get the offset of.\\n    :type key: str',\n",
       "    '_chunk_key_offset(self, key):\\n\\n    Return the offset of the key in the chunk.\\n\\n    :param key: The key to get the offset of.\\n    :type key: str',\n",
       "    '_chunk_key_offset() - returns the offset of a given chunk key.\\n\\n    :param key: the chunk key to find the offset of.\\n    :type key: str']},\n",
       "  {'c': '    def embed_query(self, text: str) -> List[float]:\\n        return self.embed_documents([text])[0]',\n",
       "   'd': \"Call out to Cohere's embedding endpoint.\\n\\nArgs:\\n    text: The text to embed.\\n\\nReturns:\\n    Embeddings for the text.\",\n",
       "   'l': True,\n",
       "   'g': ['(self, element):',\n",
       "    '(self, element):',\n",
       "    '(self, element):\\n\\n    def _last_id(self):\\n\\n    def _read(self):\\n\\n    def _write(self, data):\\n\\n    def _delete(self, id):\\n\\n    def _delete_all(self):\\n\\n    def _get_all(self):\\n\\n    def _get(self, id):\\n\\n    def _update(self, id, element):\\n\\n    def _insert(self, element):\\n\\n    def _update_all(self, data):\\n\\n    def _delete_all(self):\\n\\n    def _get_all(self):\\n\\n    def _get(self, id):\\n\\n    def _delete(self, id):\\n\\n    def _insert(self, element):\\n\\n    def _update(self, id, element):\\n\\n    def _update_all(self, data):\\n\\n    def _delete_all(self):\\n\\n    def _get_all(self):\\n\\n    def _get(self, id):',\n",
       "    '_insert(self, element):\\n    Inserts an element into the database.\\n\\n    :param element: The element to insert.']},\n",
       "  {'c': '    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[Dict[Any, Any]]] = None,\\n        **kwargs: Any,\\n    ) -> ElasticKnnSearch:\\n        index_name = kwargs.get(\"index_name\", str(uuid.uuid4()))\\n        es_connection = kwargs.get(\"es_connection\")\\n        es_cloud_id = kwargs.get(\"es_cloud_id\")\\n        es_user = kwargs.get(\"es_user\")\\n        es_password = kwargs.get(\"es_password\")\\n        vector_query_field = kwargs.get(\"vector_query_field\", \"vector\")\\n        query_field = kwargs.get(\"query_field\", \"text\")\\n        model_id = kwargs.get(\"model_id\")\\n        dims = kwargs.get(\"dims\")\\n\\n        if dims is None:\\n            raise ValueError(\"ElasticKnnSearch requires \\'dims\\' parameter\")\\n\\n        optional_args = {}\\n\\n        if vector_query_field is not None:\\n            optional_args[\"vector_query_field\"] = vector_query_field\\n\\n        if query_field is not None:\\n            optional_args[\"query_field\"] = query_field\\n\\n        knnvectorsearch = cls(\\n            index_name=index_name,\\n            embedding=embedding,\\n            es_connection=es_connection,\\n            es_cloud_id=es_cloud_id,\\n            es_user=es_user,\\n            es_password=es_password,\\n            **optional_args,\\n        )\\n\\n        knnvectorsearch.add_texts(texts, model_id=model_id, dims=dims, **optional_args)\\n\\n        return knnvectorsearch',\n",
       "   'd': 'Create a new ElasticKnnSearch instance and add a list of texts to the\\n    Elasticsearch index.\\n\\nArgs:\\n    texts (List[str]): The texts to add to the index.\\n    embedding (Embeddings): The embedding model to use for transforming the\\n        texts into vectors.\\n    metadatas (List[Dict[Any, Any]], optional): A list of metadata dictionaries\\n        to associate with the texts.\\n    **kwargs: Arbitrary keyword arguments.\\n\\nReturns:\\n    A new ElasticKnnSearch instance.',\n",
       "   'l': True,\n",
       "   'g': ['(PUT) /api/v1/spaces/1/default-space/',\n",
       "    '(PUT) /api/v1/spaces/{space_id}',\n",
       "    '(PUT /api/v1/space/1) - get space data and members (default space)\\n    :return: 200 OK\\n    :rtype: dict',\n",
       "    '(PUT) /api/v1/spaces/:id/default-space']},\n",
       "  {'c': 'def test_prompt_jinja2_wrong_input_variables() -> None:\\n    template = \"This is a {{ foo }} test.\"\\n    input_variables = [\"bar\"]\\n    with pytest.warns(UserWarning):\\n        PromptTemplate(\\n            input_variables=input_variables,\\n            template=template,\\n            template_format=\"jinja2\",\\n            validate_template=True,\\n        )\\n    assert PromptTemplate(\\n        input_variables=input_variables, template=template, template_format=\"jinja2\"\\n    ).input_variables == [\"foo\"]',\n",
       "   'd': 'Test warn deprecated.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '() -> bool:',\n",
       "    'Returns True if the request was successful.',\n",
       "    'Return True if the request was successful.  This is defined as a\\n  successful request in the following sense:\\n  - The request was successful.  This is defined as a successful\\n    request in the following sense:\\n    - The request was successful.  This is defined as a successful\\n      request in the following sense:\\n      - The request was successful.  This is defined as a successful\\n        request in the following sense:\\n        - The request was successful.  This is defined as a successful\\n          request in the following sense:\\n          - The request was successful.  This is defined as a successful\\n            request in the following sense:\\n            - The request was successful.  This is defined as a successful\\n              request in the following sense:\\n              - The request was successful.  This is defined as a successful\\n                request in the following sense:']},\n",
       "  {'c': 'def run(\\n    self,\\n    tool_input: Union[str, Dict],\\n    verbose: Optional[bool] = None,\\n    start_color: Optional[str] = \"green\",\\n    color: Optional[str] = \"green\",\\n    callbacks: Callbacks = None,\\n    *,\\n    tags: Optional[List[str]] = None,\\n    metadata: Optional[Dict[str, Any]] = None,\\n    run_id: Optional[UUID] = None,\\n    **kwargs: Any,\\n) -> Any:\\n    parsed_input = self._parse_input(tool_input)\\n    if not self.verbose and verbose is not None:\\n        verbose_ = verbose\\n    else:\\n        verbose_ = self.verbose\\n    callback_manager = CallbackManager.configure(\\n        callbacks,\\n        self.callbacks,\\n        verbose_,\\n        tags,\\n        self.tags,\\n        metadata,\\n        self.metadata,\\n    )\\n\\n    new_arg_supported = signature(self._run).parameters.get(\"run_manager\")\\n    run_manager = callback_manager.on_tool_start(\\n        {\"name\": self.name, \"description\": self.description},\\n        tool_input if isinstance(tool_input, str) else str(tool_input),\\n        color=start_color,\\n        run_id=run_id,\\n        **kwargs,\\n    )\\n    try:\\n        tool_args, tool_kwargs = self._to_args_and_kwargs(parsed_input)\\n        observation = (\\n            self._run(*tool_args, run_manager=run_manager, **tool_kwargs)\\n            if new_arg_supported\\n            else self._run(*tool_args, **tool_kwargs)\\n        )\\n    except ToolException as e:\\n        if not self.handle_tool_error:\\n            run_manager.on_tool_error(e)\\n            raise e\\n        elif isinstance(self.handle_tool_error, bool):\\n            if e.args:\\n                observation = e.args[0]\\n            else:\\n                observation = \"Tool execution error\"\\n        elif isinstance(self.handle_tool_error, str):\\n            observation = self.handle_tool_error\\n        elif callable(self.handle_tool_error):\\n            observation = self.handle_tool_error(e)\\n        else:\\n            raise ValueError(\\n                f\"Got unexpected type of `handle_tool_error`. Expected bool, str \"\\n                f\"or callable. Received: {self.handle_tool_error}\"\\n            )\\n        run_manager.on_tool_end(\\n            str(observation), color=\"red\", name=self.name, **kwargs\\n        )\\n        return observation\\n    except (Exception, KeyboardInterrupt) as e:\\n        run_manager.on_tool_error(e)\\n        raise e\\n    else:\\n        run_manager.on_tool_end(\\n            str(observation), color=color, name=self.name, **kwargs\\n        )\\n        return observation',\n",
       "   'd': 'Run the tool.',\n",
       "   'l': True,\n",
       "   'g': ['(cache_root=None) -> None:\\n\\n    :param cache_root: (optional) the root directory of the cache',\n",
       "    '(cache_root=None) -> None:\\n\\n    :param cache_root: The root directory to use for the cache.\\n        If None, the cache will not be used.\\n        Default: None.\\n        Type: str or None.\\n        Default: None.\\n        Type: str or None.',\n",
       "    '(cache_root=None) -> None:\\n\\n    :param cache_root: The directory to use for the cache. If None, then the cache is not used.',\n",
       "    '_init_']},\n",
       "  {'c': '    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n        for gen in return_val:\\n            if not isinstance(gen, Generation):\\n                raise ValueError(\\n                    \"GPTCache only supports caching of normal LLM generations, \"\\n                    f\"got {type(gen)}\"\\n                )\\n        from gptcache.adapter.api import put\\n\\n        _gptcache = self._get_gptcache(llm_string)\\n        handled_data = json.dumps([generation.dict() for generation in return_val])\\n        put(prompt, handled_data, cache_obj=_gptcache)\\n        return None',\n",
       "   'd': 'Load a tracing session.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def check_dependencies(cls, values: Dict) -> Dict:\\n        try:\\n            from nltk.translate.bleu_score import (\\n                SmoothingFunction,\\n                sentence_bleu,\\n            )\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Not all the correct dependencies for this ExampleSelect exist.\"\\n                \"Please install nltk with `pip install nltk`.\"\\n            ) from e\\n\\n        return values',\n",
       "   'd': 'Load a list of URLs using Playwright.',\n",
       "   'l': False,\n",
       "   'g': ['Initialize a new `PytorchArray` wrapper.\\n\\n  Args:\\n    env: The environment to wrap.',\n",
       "    'Initialize a new `PyTorch-Gym-Wrapper` with the given `Gym-Env`.\\n\\n  Args:\\n    env (Gym-Env): The `Gym-Env` to be wrapped',\n",
       "    'Args:\\n        env: (gym.Env)',\n",
       "    'Initialize a new `PyTorchGymWrapper` object.\\n\\n  Args:\\n      env (gym.Env):\\n      skip (int, optional): The number of timesteps to skip before taking a new action.\\n          Defaults to 4.\\n\\n  Returns:\\n      PyTorchGymWrapper: A new instance of `PyTorchGymWrapper`.\\n\\n  Example:\\n      >>> import torch_geometric.gym\\n      >>> import torch_geometric.gym.torch_geometric_gym\\n      >>> import torch_geometric.datasets\\n      >>> import torch_geometric.transforms\\n      >>> import torch_geometric.data\\n      >>> import torch_geometric.loader\\n      >>> import torch_geometric.nn\\n      >>> import torch_geometric.utils\\n      >>> import torch_geometric.datasets\\n      >>> import torch_geometric.transforms\\n      >>> import torch_geometric.data\\n      >>> import torch_geometric.loader\\n      >>> import torch_geometric.nn\\n      >>> import torch_geometric.utils\\n      >>> import torch_geometric.datasets\\n      >>> import torch_']},\n",
       "  {'c': '    def get_token_ids(self, text: str) -> List[int]:\\n        if sys.version_info[1] < 8:\\n            return super().get_token_ids(text)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate get_num_tokens. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        enc = tiktoken.encoding_for_model(self.model_name)\\n        return enc.encode(\\n            text,\\n            allowed_special=self.allowed_special,\\n            disallowed_special=self.disallowed_special,\\n        )',\n",
       "   'd': 'Get the token IDs using the tiktoken package.',\n",
       "   'l': True,\n",
       "   'g': ['(str, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th',\n",
       "    '(float, float)',\n",
       "    '(',\n",
       "    '(float, float) -> (th.Tensor, th.Tensor)']},\n",
       "  {'c': 'def _type(self) -> str:\\n    return \"default\"',\n",
       "   'd': 'Return the output parser type for serialization.',\n",
       "   'l': True,\n",
       "   'g': ['Generates a sequence of betas based on a given alpha_bar function.\\n\\n    Args:\\n        num_diffusion_timesteps (int): Number of timesteps in the diffusion process.\\n        alpha_bar (function): Function that takes a time t and returns the alpha_t parameter.\\n\\n    Returns:\\n        np.array: Sequence of betas.',\n",
       "    'Generates a sequence of betas used by the forward diffusion q_t(x_t + x_t + 1/beta_t).\\n\\n    Args:\\n        num_diffusion_timesteps (int): The number of diffusion timesteps.\\n        alpha_bar (callable): The alpha_bar function.\\n        max_beta (float, optional): The maximum beta value.\\n\\n    Returns:\\n        np.array: The sequence of betas.',\n",
       "    '',\n",
       "    ':param num_diffusion_timesteps: \\n    :param alpha_bar: \\n    :param max_beta: \\n    :return:']},\n",
       "  {'c': 'def dict(self, **kwargs: Any) -> Dict:\\n    _dict = super().dict()\\n    del _dict[\"output_parser\"]\\n    return _dict',\n",
       "   'd': 'Return dictionary representation of agent.',\n",
       "   'l': True,\n",
       "   'g': ['_get_config_ref() -> str:\\n    Return the configuration reference for the class.',\n",
       "    '_get_config_ref:\\n    :return: The config ref of the current class.',\n",
       "    '_get_config_ref() -> str:\\n    _get_config_ref() -> str:',\n",
       "    '_get_config_ref: Returns the config reference for this class.\\n    :rtype: str\\n    :return: The config reference for this class.']},\n",
       "  {'c': 'def lazy_import_playwright_browsers() -> Tuple[Type[AsyncBrowser], Type[SyncBrowser]]:\\n    try:\\n        from playwright.async_api import Browser as AsyncBrowser\\n        from playwright.sync_api import Browser as SyncBrowser\\n    except ImportError:\\n        raise ImportError(\\n            \"The \\'playwright\\' package is required to use the playwright tools.\"\\n            \" Please install it with \\'pip install playwright\\'.\"\\n        )\\n    return AsyncBrowser, SyncBrowser',\n",
       "   'd': 'Split incoming text and return chunks.',\n",
       "   'l': False,\n",
       "   'g': ['Initializes the display_once set and the dirty flag.  This is\\n    called once at the start of the program.  The program will\\n    be invalidated if any of the includes or excludes are\\n    changed.  This is used to make sure the display is\\n    updated only when the display_once set is changed.  This\\n    is also used to make sure the display is updated\\n    only when the dirty flag is set.  This is used to make\\n    sure the display is updated only when the display is\\n    dirty.  This is used to make sure the display is\\n    updated only when the display is dirty.  This is used to\\n    make sure the display is updated only when the display is\\n    dirty.  This is used to make sure the display is updated\\n    only when the display is dirty.  This is used to make sure\\n    the display is updated only when the display is dirty.  This is used to make sure the display is updated only when the display is dirty.',\n",
       "    ':param exclude: list of regular expressions to exclude\\n       :param include: list of regular expressions to include\\n       :param invalid: include invalid signatures in the output',\n",
       "    'Initializes the parser.\\n\\n     :param exclude: a list of regular expressions to exclude from the parser.\\n     :param include: a list of regular expressions to include in the parser.\\n     :param invalid: if True, the parser will include invalid characters.\\n     :return: None.',\n",
       "    'Initialize the class.\\n\\n    :param exclude: List of strings to exclude from the signature.\\n    :param include: List of strings to include in the signature.\\n    :param invalid: Whether or not to display invalid signatures.']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[Dict[Any, Any]]] = None,\\n        model_id: Optional[str] = None,\\n        refresh_indices: bool = False,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        if not self.client.indices.exists(index=self.index_name):\\n            dims = kwargs.get(\"dims\")\\n\\n            if dims is None:\\n                raise ValueError(\"ElasticKnnSearch requires \\'dims\\' parameter\")\\n\\n            similarity = kwargs.get(\"similarity\")\\n            optional_args = {}\\n\\n            if similarity is not None:\\n                optional_args[\"similarity\"] = similarity\\n\\n            mapping = self._default_knn_mapping(dims=dims, **optional_args)\\n            self.create_knn_index(mapping)\\n\\n        embeddings = self.embedding.embed_documents(list(texts))\\n\\n\\n        body: List[Mapping[str, Any]] = []\\n        for text, vector in zip(texts, embeddings):\\n            body.extend(\\n                [\\n                    {\"index\": {\"_index\": self.index_name}},\\n                    {\"text\": text, \"vector\": vector},\\n                ]\\n            )\\n\\n        responses = self.client.bulk(operations=body)\\n\\n        ids = [\\n            item[\"index\"][\"_id\"]\\n            for item in responses[\"items\"]\\n            if item[\"index\"][\"result\"] == \"created\"\\n        ]\\n\\n        if refresh_indices:\\n            self.client.indices.refresh(index=self.index_name)\\n\\n        return ids',\n",
       "   'd': 'Return the input keys.\\n\\nReturns:\\n    List of input keys.',\n",
       "   'l': False,\n",
       "   'g': ['_fit(self, X: np.ndarray, Y: np.ndarray) -> None:',\n",
       "    '_fit_predict()',\n",
       "    '_fit_inverse_model()',\n",
       "    '_fit_predict_model(self, X: np.ndarray, Y: np.ndarray) -> None:']},\n",
       "  {'c': 'def test_octoai_endpoint_call_error() -> None:\\n    llm = OctoAIEndpoint(\\n        endpoint_url=\"https://mpt-7b-demo-kk0powt97tmb.octoai.cloud/generate\",\\n        model_kwargs={\"max_new_tokens\": -1},\\n    )\\n    with pytest.raises(ValueError):\\n        llm(\"Which state is Los Angeles in?\")',\n",
       "   'd': 'Run the LLM on the given prompts.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        text_key: str = \"text\",\\n        index_name: Optional[str] = None,\\n        client: Any = None,\\n        host: List[str] = [\"172.20.31.10:13000\"],\\n        user: str = \"root\",\\n        password: str = \"123123\",\\n        batch_size: int = 500,\\n        **kwargs: Any,\\n    ) -> Dingo:\\n        try:\\n            import dingodb\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import dingo python package. \"\\n                \"Please install it with `pip install dingodb`.\"\\n            )\\n\\n        if client is not None:\\n            dingo_client = client\\n        else:\\n            try:\\n\\n                dingo_client = dingodb.DingoDB(user, password, host)\\n            except ValueError as e:\\n                raise ValueError(f\"Dingo failed to connect: {e}\")\\n        if kwargs is not None and kwargs.get(\"self_id\") is True:\\n            if index_name not in dingo_client.get_index():\\n                dingo_client.create_index(index_name, 1024, auto_id=False)\\n        else:\\n            if index_name not in dingo_client.get_index():\\n                dingo_client.create_index(index_name, 1024)\\n\\n\\n\\n\\n        ids = ids or [str(uuid.uuid1().int)[:13] for _ in texts]\\n        metadatas_list = []\\n        texts = list(texts)\\n        embeds = embedding.embed_documents(texts)\\n        for i, text in enumerate(texts):\\n            metadata = metadatas[i] if metadatas else {}\\n            metadata[text_key] = text\\n            metadatas_list.append(metadata)\\n\\n\\n        for i in range(0, len(list(texts)), batch_size):\\n            j = i + batch_size\\n            add_res = dingo_client.vector_add(\\n                index_name, metadatas_list[i:j], embeds[i:j], ids[i:j]\\n            )\\n            if not add_res:\\n                raise Exception(\"vector add fail\")\\n        return cls(embedding, text_key, client=dingo_client, index_name=index_name)',\n",
       "   'd': 'Run the tool.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def format_messages(self, **kwargs: Any) -> List[BaseMessage]:\\n        return [self.format(**kwargs)]',\n",
       "   'd': 'Initialize with Chroma client.',\n",
       "   'l': False,\n",
       "   'g': ['_get_item(self, entry, handle_value = 0):',\n",
       "    '_get_item() - Returns an Object from a low_value pointer.',\n",
       "    '_get_item',\n",
       "    'Return the item at the given entry.\\n\\n:param entry: _ENTRY_HEADER\\n:return: _OBJECT_HEADER\\n:rtype: _OBJECT_HEADER\\n:raises: _ERROR_CODE_INVALID_ENTRY_POINTER, _ERROR_CODE_INVALID_HANDLE_VALUE, _ERROR_CODE_INVALID_OBJECT_POINTER, _ERROR_CODE_INVALID_OBJECT_TYPE, _ERROR_CODE_INVALID_OBJECT_HANDLE_TYPE, _ERROR_CODE_INVALID_OBJECT_HANDLE_']},\n",
       "  {'c': '    def __init__(self, **data: Any) -> None:\\n        try:\\n            from google.cloud.discoveryengine_v1beta import SearchServiceClient\\n        except ImportError as exc:\\n            raise ImportError(\\n                \"google.cloud.discoveryengine is not installed.\"\\n                \"Please install it with pip install google-cloud-discoveryengine\"\\n            ) from exc\\n        try:\\n            from google.api_core.client_options import ClientOptions\\n        except ImportError as exc:\\n            raise ImportError(\\n                \"google.api_core.client_options is not installed.\"\\n                \"Please install it with pip install google-api-core\"\\n            ) from exc\\n\\n        super().__init__(**data)\\n\\n\\n\\n        api_endpoint = (\\n            \"discoveryengine.googleapis.com\"\\n            if self.location_id == \"global\"\\n            else f\"{self.location_id}-discoveryengine.googleapis.com\"\\n        )\\n\\n        self._client = SearchServiceClient(\\n            credentials=self.credentials,\\n            client_options=ClientOptions(api_endpoint=api_endpoint),\\n        )\\n\\n        self._serving_config = self._client.serving_config_path(\\n            project=self.project_id,\\n            location=self.location_id,\\n            data_store=self.data_store_id,\\n            serving_config=self.serving_config_id,\\n        )',\n",
       "   'd': 'Initializes private fields.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def _run(\\n    self,\\n    query: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> Union[List[Dict], str]:\\n    try:\\n        return self.api_wrapper.results(\\n            query,\\n            self.max_results,\\n        )\\n    except Exception as e:\\n        return repr(e)',\n",
       "   'd': 'Use the tool.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def similarity_search_with_score(\\n        self,\\n        query: str,\\n        k: int = 5,\\n        lambda_val: float = 0.025,\\n        filter: Optional[str] = None,\\n        n_sentence_context: int = 2,\\n        **kwargs: Any,\\n    ) -> List[Tuple[Document, float]]:\\n        data = json.dumps(\\n            {\\n                \"query\": [\\n                    {\\n                        \"query\": query,\\n                        \"start\": 0,\\n                        \"num_results\": k,\\n                        \"context_config\": {\\n                            \"sentences_before\": n_sentence_context,\\n                            \"sentences_after\": n_sentence_context,\\n                        },\\n                        \"corpus_key\": [\\n                            {\\n                                \"customer_id\": self._vectara_customer_id,\\n                                \"corpus_id\": self._vectara_corpus_id,\\n                                \"metadataFilter\": filter,\\n                                \"lexical_interpolation_config\": {\"lambda\": lambda_val},\\n                            }\\n                        ],\\n                    }\\n                ]\\n            }\\n        )\\n\\n        response = self._session.post(\\n            headers=self._get_post_headers(),\\n            url=\"https://api.vectara.io/v1/query\",\\n            data=data,\\n            timeout=self.vectara_api_timeout,\\n        )\\n\\n        if response.status_code != 200:\\n            logger.error(\\n                \"Query failed %s\",\\n                f\"(code {response.status_code}, reason {response.reason}, details \"\\n                f\"{response.text})\",\\n            )\\n            return []\\n\\n        result = response.json()\\n\\n        responses = result[\"responseSet\"][0][\"response\"]\\n        vectara_default_metadata = [\"lang\", \"len\", \"offset\"]\\n        docs = [\\n            (\\n                Document(\\n                    page_content=x[\"text\"],\\n                    metadata={\\n                        m[\"name\"]: m[\"value\"]\\n                        for m in x[\"metadata\"]\\n                        if m[\"name\"] not in vectara_default_metadata\\n                    },\\n                ),\\n                x[\"score\"],\\n            )\\n            for x in responses\\n        ]\\n        return docs',\n",
       "   'd': 'Return Vectara documents most similar to query, along with scores.\\n\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 5.\\n    lambda_val: lexical match parameter for hybrid search.\\n    filter: Dictionary of argument(s) to filter on metadata. For example a\\n        filter can be \"doc.rating > 3.0 and part.lang = \\'deu\\'\"} see\\n        https://docs.vectara.com/docs/search-apis/sql/filter-overview\\n        for more details.\\n    n_sentence_context: number of sentences before/after the matching segment\\n        to add, defaults to 2\\n\\nReturns:\\n    List of Documents most similar to the query and score for each.',\n",
       "   'l': True,\n",
       "   'g': ['.driver(self, driver):',\n",
       "    '(optional) The driver to use for this browser.  The driver must be\\n        a valid `webdriver.Remote` object.  If not specified,\\n        a new driver will be created using the `self.browser`\\n        and `self.browser_options` settings.  The driver will be\\n        reused for all requests to this browser.  The driver\\n        will be automatically closed after the browser is\\n        closed.  See the `browser` and `browser_options`\\n        settings for more information.\\n        :type driver: `webdriver.Remote` or None',\n",
       "    '.  Sets the driver to use to interact with the page.\\n\\n    :param driver: the driver to use to interact with the page.\\n\\n    :raises ValueError: if the driver is `None`',\n",
       "    \"(Optional)  A web browser driver to use for the tests.\\n        This is used to control the browser and to capture the\\n        browser's logs.\\n        Default: None.\\n        Type: :class:`~selenium.webdriver.WebDriver` or `None`\"]},\n",
       "  {'c': 'def return_values(self) -> List[str]:\\n    return []',\n",
       "   'd': 'Update cache based on prompt and llm_string.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def requires_input(self) -> bool:\\n    return True',\n",
       "   'd': 'Validate that api key and python package exists in environment',\n",
       "   'l': False,\n",
       "   'g': ['(self, source, results, device=\"cuda\") -> None:',\n",
       "    '(self, source, results, device=\"cuda\") -> None:',\n",
       "    '(self, source, results, device=\"cuda\") -> None:',\n",
       "    '(source, results, device=\"cuda\") -> None:']},\n",
       "  {'c': '    def input_keys(self) -> List[str]:\\n        return list(set(self.llm_chain.input_keys) - {\"intermediate_steps\"})',\n",
       "   'd': 'Return the input keys.\\n\\nReturns:\\n    List of input keys.',\n",
       "   'l': True,\n",
       "   'g': ['-',\n",
       "    '.set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit()\\n        .set_exit',\n",
       "    '',\n",
       "    '-']},\n",
       "  {'c': 'def serialize_chat_messages(self, messages: List[Dict]) -> str:\\n    if isinstance(messages, list) and messages:\\n        if isinstance(messages[0], dict):\\n            chat_messages = _get_messages_from_run_dict(messages)\\n        elif isinstance(messages[0], list):\\n\\n            chat_messages = _get_messages_from_run_dict(messages[0])\\n        else:\\n            raise ValueError(f\"Could not extract messages to evaluate {messages}\")\\n        return get_buffer_string(chat_messages)\\n    raise ValueError(f\"Could not extract messages to evaluate {messages}\")',\n",
       "   'd': 'Extract the input messages from the run.',\n",
       "   'l': True,\n",
       "   'g': ['.query(regex):\\n\\n    return QueryRegex(self._key, regex)\\n\\ndef query(regex):\\n\\n    return QueryRegex(self._key, regex)\\n\\ndef matches(regex):\\n\\n    return QueryRegex(self._key, regex)\\n\\ndef query(regex):\\n\\n    return QueryRegex(self._key, regex)\\n\\ndef matches(regex):\\n\\n    return QueryRegex(self._key, regex)\\n\\ndef query(regex):\\n\\n    return QueryRegex(self._key, regex)\\n\\ndef matches(regex):\\n\\n    return QueryRegex(self._key, regex)\\n\\ndef query(regex):\\n\\n    return QueryRegex(self._key, regex)\\n\\ndef matches(regex):\\n\\n    return QueryRegex(self._key, regex)\\n\\ndef query(regex):\\n\\n    return QueryRegex(self._key, regex)\\n\\ndef matches(regex):\\n\\n    return QueryRegex(self._key, regex)\\n\\ndef query(regex):\\n\\n    return QueryRegex(self._key, regex)\\n\\ndef matches(regex):\\n\\n    return QueryRegex(self._key, regex)\\n\\ndef query(regex):\\n\\n    return QueryRegex(self._key, regex)\\n\\ndef matches(regex',\n",
       "    '.match(self, regex):',\n",
       "    '',\n",
       "    '.matches(regex):\\n\\n    Returns a QueryRegex object that matches the given regex.\\n\\n    :param regex: a string or a compiled regex object.\\n    :return: a QueryRegex object that matches the given regex.\\n    :rtype: QueryRegex\\n    :raises: ValueError if the regex is not a string or a compiled regex object.\\n    :raises: ValueError if the regex is not a valid regex.\\n    :raises: ValueError if the regex is not a valid regex.\\n    :raises: ValueError if the regex is not a valid regex.\\n    :raises: ValueError if the regex is not a valid regex.\\n    :raises: ValueError if the regex is not a valid regex.\\n    :raises: ValueError if the regex is not a valid regex.\\n    :raises: ValueError if the regex is not a valid regex.\\n    :raises: ValueError if the regex is not a valid regex.\\n    :raises: ValueError if the regex is not a valid regex.\\n    :raises: ValueError if the regex is not a valid regex.\\n    :raises: ValueError if the regex is not a valid regex.\\n    :raises: ValueError if the regex is not a valid regex.\\n    :']},\n",
       "  {'c': '    def on_chain_start(\\n        self,\\n        serialized: Dict[str, Any],\\n        inputs: Dict[str, Any],\\n        run_id: Optional[UUID] = None,\\n        **kwargs: Any,\\n    ) -> CallbackManagerForChainRun:\\n        if run_id is None:\\n            run_id = uuid4()\\n\\n        _handle_event(\\n            self.handlers,\\n            \"on_chain_start\",\\n            \"ignore_chain\",\\n            serialized,\\n            inputs,\\n            run_id=run_id,\\n            parent_run_id=self.parent_run_id,\\n            tags=self.tags,\\n            metadata=self.metadata,\\n            **kwargs,\\n        )\\n\\n        return CallbackManagerForChainRun(\\n            run_id=run_id,\\n            handlers=self.handlers,\\n            inheritable_handlers=self.inheritable_handlers,\\n            parent_run_id=self.parent_run_id,\\n            tags=self.tags,\\n            inheritable_tags=self.inheritable_tags,\\n            metadata=self.metadata,\\n            inheritable_metadata=self.inheritable_metadata,\\n        )',\n",
       "   'd': 'Run when chain starts running.\\n\\nArgs:\\n    serialized (Dict[str, Any]): The serialized chain.\\n    inputs (Dict[str, Any]): The inputs to the chain.\\n    run_id (UUID, optional): The ID of the run. Defaults to None.\\n\\nReturns:\\n    CallbackManagerForChainRun: The callback manager for the chain run.',\n",
       "   'l': True,\n",
       "   'g': ['Remove a tensor from the tensor registry.',\n",
       "    'Delete a tensor.',\n",
       "    '() -> str:',\n",
       "    '']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        init_func: Union[\\n            Callable[[Any, str], None], Callable[[Any], None], None\\n        ] = None,\\n    ):\\n        try:\\n            import gptcache\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import gptcache python package. \"\\n                \"Please install it with `pip install gptcache`.\"\\n            )\\n\\n        self.init_gptcache_func: Union[\\n            Callable[[Any, str], None], Callable[[Any], None], None\\n        ] = init_func\\n        self.gptcache_dict: Dict[str, Any] = {}',\n",
       "   'd': 'Initialize by passing in init function (default: `None`).\\n\\nArgs:\\n    init_func (Optional[Callable[[Any], None]]): init `GPTCache` function\\n    (default: `None`)\\n\\nExample:\\n.. code-block:: python\\n\\n    # Initialize GPTCache with a custom init function\\n    import gptcache\\n    from gptcache.processor.pre import get_prompt\\n    from gptcache.manager.factory import get_data_manager\\n\\n    # Avoid multiple caches using the same file,\\n    causing different llm model caches to affect each other\\n\\n    def init_gptcache(cache_obj: gptcache.Cache, llm str):\\n        cache_obj.init(\\n            pre_embedding_func=get_prompt,\\n            data_manager=manager_factory(\\n                manager=\"map\",\\n                data_dir=f\"map_cache_{llm}\"\\n            ),\\n        )\\n\\n    langchain.llm_cache = GPTCache(init_gptcache)',\n",
       "   'l': True,\n",
       "   'g': ['_parse_result(self, d):\\n    return d[\"std\"]',\n",
       "    '_parse_result(self, d):\\n    return d[\"std\"]',\n",
       "    '_parse_result(self, d):\\n    return d[\"std\"]',\n",
       "    '_parse_result(self, d):\\n    return d[\"std\"]']},\n",
       "  {'c': 'def _batch_with_config(\\n    self,\\n    func: Union[\\n        Callable[[List[Input]], List[Union[Exception, Output]]],\\n        Callable[\\n            [List[Input], List[CallbackManagerForChainRun]],\\n            List[Union[Exception, Output]],\\n        ],\\n        Callable[\\n            [List[Input], List[CallbackManagerForChainRun], List[RunnableConfig]],\\n            List[Union[Exception, Output]],\\n        ],\\n    ],\\n    input: List[Input],\\n    config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,\\n    *,\\n    return_exceptions: bool = False,\\n    run_type: Optional[str] = None,\\n) -> List[Output]:\\n    configs = get_config_list(config, len(input))\\n    callback_managers = [get_callback_manager_for_config(c) for c in configs]\\n    run_managers = [\\n        callback_manager.on_chain_start(\\n            dumpd(self),\\n            input,\\n            run_type=run_type,\\n            name=config.get(\"run_name\"),\\n        )\\n        for callback_manager, input, config in zip(\\n            callback_managers, input, configs\\n        )\\n    ]\\n    try:\\n        if accepts_run_manager_and_config(func):\\n            output = func(\\n                input,\\n                run_manager=run_managers,\\n                config=configs,\\n            )\\n        elif accepts_run_manager(func):\\n            output = func(input, run_manager=run_managers)\\n        else:\\n            output = func(input)\\n    except Exception as e:\\n        for run_manager in run_managers:\\n            run_manager.on_chain_error(e)\\n        if return_exceptions:\\n            return cast(List[Output], [e for _ in input])\\n        else:\\n            raise\\n    else:\\n        first_exception: Optional[Exception] = None\\n        for run_manager, out in zip(run_managers, output):\\n            if isinstance(out, Exception):\\n                first_exception = first_exception or out\\n                run_manager.on_chain_error(out)\\n            else:\\n                run_manager.on_chain_end(dumpd(out))\\n        if return_exceptions or first_exception is None:\\n            return cast(List[Output], output)\\n        else:\\n            raise first_exception',\n",
       "   'd': 'Initialize the loader.\\n\\nArgs:\\n    file_path: A file, url or s3 path for input file\\n    textract_features: Features to be used for extraction, each feature\\n                       should be passed as a str that conforms to the enum\\n                       `Textract_Features`, see `amazon-textract-caller` pkg\\n    client: boto3 textract client (Optional)\\n    credentials_profile_name: AWS profile name, if not default (Optional)\\n    region_name: AWS region, eg us-east-1 (Optional)\\n    endpoint_url: endpoint url for the textract service (Optional)',\n",
       "   'l': False,\n",
       "   'g': [':param text:\\n    :param pattern:\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :return:\\n    :rtype: list\\n    :',\n",
       "    '解析 level 对应的内容\\n    :param text:\\n    :param pattern:\\n    :return:\\n    :rtype: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type: list\\n    :type:',\n",
       "    '',\n",
       "    ':param text:\\n      :param pattern:\\n      :return:']},\n",
       "  {'c': '    def from_run_and_data_type(\\n        cls,\\n        evaluator: StringEvaluator,\\n        run_type: str,\\n        data_type: DataType,\\n        input_key: Optional[str] = None,\\n        prediction_key: Optional[str] = None,\\n        reference_key: Optional[str] = None,\\n        tags: Optional[List[str]] = None,\\n    ) -> StringRunEvaluatorChain:\\n        if run_type == \"llm\":\\n            run_mapper: StringRunMapper = LLMStringRunMapper()\\n        elif run_type == \"chain\":\\n            run_mapper = ChainStringRunMapper(\\n                input_key=input_key, prediction_key=prediction_key\\n            )\\n        else:\\n            raise ValueError(\\n                f\"Unsupported run type {run_type}. Expected one of \\'llm\\' or \\'chain\\'.\"\\n            )\\n\\n\\n        if reference_key is not None or data_type in (DataType.llm, DataType.chat):\\n            example_mapper = StringExampleMapper(reference_key=reference_key)\\n        elif evaluator.requires_reference:\\n            raise ValueError(\\n                f\"Evaluator {evaluator.evaluation_name} requires a reference\"\\n                \" example from the dataset. Please specify the reference key from\"\\n                \" amongst the dataset outputs keys.\"\\n            )\\n        else:\\n            example_mapper = None\\n        return cls(\\n            name=evaluator.evaluation_name,\\n            run_mapper=run_mapper,\\n            example_mapper=example_mapper,\\n            string_evaluator=evaluator,\\n            tags=tags,\\n        )',\n",
       "   'd': 'Create a StringRunEvaluatorChain from an evaluator and the run and dataset types.\\n\\nThis method provides an easy way to instantiate a StringRunEvaluatorChain, by\\ntaking an evaluator and information about the type of run and the data.\\nThe method supports LLM and chain runs.\\n\\nArgs:\\n    evaluator (StringEvaluator): The string evaluator to use.\\n    run_type (str): The type of run being evaluated.\\n        Supported types are LLM and Chain.\\n    data_type (DataType): The type of dataset used in the run.\\n    input_key (str, optional): The key used to map the input from the run.\\n    prediction_key (str, optional): The key used to map the prediction from the run.\\n    reference_key (str, optional): The key used to map the reference from the dataset.\\n    tags (List[str], optional): List of tags to attach to the evaluation chain.\\n\\nReturns:\\n    StringRunEvaluatorChain: The instantiated evaluation chain.\\n\\nRaises:\\n    ValueError: If the run type is not supported, or if the evaluator requires a\\n        reference from the dataset but the reference key is not provided.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Clear all jobs from this job.\\n    :param tag: (str) the tag to clear',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': 'def step(\\n    self, inputs: dict, callbacks: Callbacks = None, **kwargs: Any\\n) -> StepResponse:\\n',\n",
       "   'd': 'Take step.',\n",
       "   'l': True,\n",
       "   'g': ['(checkpoint, directory, device, max_to_keep=10)',\n",
       "    'Initialize a new `ModelCheckpoint` instance.\\n\\n    Args:\\n        checkpoint (str): The directory to save the model after each epoch.\\n        directory (str): The directory to save the model after each epoch.\\n        device (str): The device to use for training.\\n        max_to_keep (int): The maximum number of checkpoints to keep.',\n",
       "    '(checkpoint, directory, device, max_to_keep=10)',\n",
       "    '(checkpoint, directory, device, max_to_keep=10)']},\n",
       "  {'c': 'def load(self) -> List[Document]:\\n    try:\\n        import boto3\\n    except ImportError:\\n        raise ImportError(\\n            \"Could not import boto3 python package. \"\\n            \"Please install it with `pip install boto3`.\"\\n        )\\n    s3 = boto3.resource(\\n        \"s3\",\\n        region_name=self.region_name,\\n        api_version=self.api_version,\\n        use_ssl=self.use_ssl,\\n        verify=self.verify,\\n        endpoint_url=self.endpoint_url,\\n        aws_access_key_id=self.aws_access_key_id,\\n        aws_secret_access_key=self.aws_secret_access_key,\\n        aws_session_token=self.aws_session_token,\\n        boto_config=self.boto_config,\\n    )\\n    bucket = s3.Bucket(self.bucket)\\n    docs = []\\n    for obj in bucket.objects.filter(Prefix=self.prefix):\\n        loader = S3FileLoader(\\n            self.bucket,\\n            obj.key,\\n            region_name=self.region_name,\\n            api_version=self.api_version,\\n            use_ssl=self.use_ssl,\\n            verify=self.verify,\\n            endpoint_url=self.endpoint_url,\\n            aws_access_key_id=self.aws_access_key_id,\\n            aws_secret_access_key=self.aws_secret_access_key,\\n            aws_session_token=self.aws_session_token,\\n            boto_config=self.boto_config,\\n        )\\n        docs.extend(loader.load())\\n    return docs',\n",
       "   'd': 'Configure the callback manager.\\n\\nArgs:\\n    inheritable_callbacks (Optional[Callbacks], optional): The inheritable\\n        callbacks. Defaults to None.\\n    local_callbacks (Optional[Callbacks], optional): The local callbacks.\\n        Defaults to None.\\n    verbose (bool, optional): Whether to enable verbose mode. Defaults to False.\\n    inheritable_tags (Optional[List[str]], optional): The inheritable tags.\\n        Defaults to None.\\n    local_tags (Optional[List[str]], optional): The local tags.\\n        Defaults to None.\\n    inheritable_metadata (Optional[Dict[str, Any]], optional): The inheritable\\n        metadata. Defaults to None.\\n    local_metadata (Optional[Dict[str, Any]], optional): The local metadata.\\n        Defaults to None.\\n\\nReturns:\\n    CallbackManager: The configured callback manager.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def index(\\n        self,\\n        dims_length: Union[int, None],\\n        vector_query_field: str,\\n        similarity: Union[DistanceStrategy, None],\\n    ) -> Dict:\\n        return {\\n            \"mappings\": {\\n                \"properties\": {\\n                    vector_query_field: {\\n                        \"type\": \"dense_vector\",\\n                        \"dims\": dims_length,\\n                        \"index\": False,\\n                    },\\n                }\\n            }\\n        }',\n",
       "   'd': 'Create the mapping for the Elasticsearch index.',\n",
       "   'l': True,\n",
       "   'g': ['Remove a desktop shortcut.\\n\\n  :param self:  The :class:`~app.views.desktop_shortcuts.DesktopShortcutsView` object.\\n  :param _args:  The arguments passed to the method.\\n  :return:  None.',\n",
       "    'Remove the desktop shortcut for the game.\\n\\n  :param self: The :class:`Game` instance.\\n  :param _args: The arguments.',\n",
       "    'Remove the desktop shortcut for the game.\\n    :param self: The current GameView.\\n    :param _args: The arguments passed to the method.\\n    :return: None.',\n",
       "    \"Remove the launcher shortcut from the user's desktop.\\n\\n    :param self: The :class:`~xdg.Desktop` instance.\\n    :param _args: The arguments passed to the shortcut.\"]},\n",
       "  {'c': 'def _llm_type(self) -> str:\\n    return \"fireworks-chat\"',\n",
       "   'd': 'Return type of llm.',\n",
       "   'l': True,\n",
       "   'g': ['(dLdy, X) -> (dX, dW, dB)',\n",
       "    '(dLdy, X) -> (dX, dW, dB)',\n",
       "    '_bwd(self, dLdy, X):\\n    Backward pass',\n",
       "    '(dLdy, X) -> (dX, dW, dB)']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        init_func: Union[\\n            Callable[[Any, str], None], Callable[[Any], None], None\\n        ] = None,\\n    ):\\n        try:\\n            import gptcache\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import gptcache python package. \"\\n                \"Please install it with `pip install gptcache`.\"\\n            )\\n\\n        self.init_gptcache_func: Union[\\n            Callable[[Any, str], None], Callable[[Any], None], None\\n        ] = init_func\\n        self.gptcache_dict: Dict[str, Any] = {}',\n",
       "   'd': 'Initialize with file path.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', ':param value:\\n    :param newdim:\\n    :return:']},\n",
       "  {'c': '    def similarity_search_with_score(\\n        self,\\n        query: str,\\n        k: int = 5,\\n        lambda_val: float = 0.025,\\n        filter: Optional[str] = None,\\n        n_sentence_context: int = 2,\\n        **kwargs: Any,\\n    ) -> List[Tuple[Document, float]]:\\n        data = json.dumps(\\n            {\\n                \"query\": [\\n                    {\\n                        \"query\": query,\\n                        \"start\": 0,\\n                        \"num_results\": k,\\n                        \"context_config\": {\\n                            \"sentences_before\": n_sentence_context,\\n                            \"sentences_after\": n_sentence_context,\\n                        },\\n                        \"corpus_key\": [\\n                            {\\n                                \"customer_id\": self._vectara_customer_id,\\n                                \"corpus_id\": self._vectara_corpus_id,\\n                                \"metadataFilter\": filter,\\n                                \"lexical_interpolation_config\": {\"lambda\": lambda_val},\\n                            }\\n                        ],\\n                    }\\n                ]\\n            }\\n        )\\n\\n        response = self._session.post(\\n            headers=self._get_post_headers(),\\n            url=\"https://api.vectara.io/v1/query\",\\n            data=data,\\n            timeout=self.vectara_api_timeout,\\n        )\\n\\n        if response.status_code != 200:\\n            logger.error(\\n                \"Query failed %s\",\\n                f\"(code {response.status_code}, reason {response.reason}, details \"\\n                f\"{response.text})\",\\n            )\\n            return []\\n\\n        result = response.json()\\n\\n        responses = result[\"responseSet\"][0][\"response\"]\\n        vectara_default_metadata = [\"lang\", \"len\", \"offset\"]\\n        docs = [\\n            (\\n                Document(\\n                    page_content=x[\"text\"],\\n                    metadata={\\n                        m[\"name\"]: m[\"value\"]\\n                        for m in x[\"metadata\"]\\n                        if m[\"name\"] not in vectara_default_metadata\\n                    },\\n                ),\\n                x[\"score\"],\\n            )\\n            for x in responses\\n        ]\\n        return docs',\n",
       "   'd': 'Return Vectara documents most similar to query, along with scores.\\n\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 5.\\n    lambda_val: lexical match parameter for hybrid search.\\n    filter: Dictionary of argument(s) to filter on metadata. For example a\\n        filter can be \"doc.rating > 3.0 and part.lang = \\'deu\\'\"} see\\n        https://docs.vectara.com/docs/search-apis/sql/filter-overview\\n        for more details.\\n    n_sentence_context: number of sentences before/after the matching segment\\n        to add, defaults to 2\\n\\nReturns:\\n    List of Documents most similar to the query and score for each.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    ':return: :class:`str`',\n",
       "    ':return: \\n    :rtype: str\\n    :param self: \\n    :type self:']},\n",
       "  {'c': 'def _type(self) -> str:\\n    return \"default\"',\n",
       "   'd': 'Return the output parser type for serialization.',\n",
       "   'l': True,\n",
       "   'g': [':param timeout: a float or a tuple (seconds, milliseconds)\\n  :return: a :class:`~urllib3.Timeout` instance\\n  :raises: :class:`~urllib3.exceptions.ProtocolError` if the timeout is not valid\\n  :raises: :class:`~urllib3.exceptions.LocationParseError` if the timeout is not valid\\n  :raises: :class:`~urllib3.exceptions.LocationParseError` if the timeout is not valid\\n  :raises: :class:`~urllib3.exceptions.LocationParseError` if the timeout is not valid\\n  :raises: :class:`~urllib3.exceptions.LocationParseError` if the timeout is not valid\\n  :raises: :class:`~urllib3.exceptions.LocationParseError` if the timeout is not valid\\n  :raises: :class:`~urllib3.exceptions.LocationParseError` if the timeout is not valid\\n  :raises: :class:`~urllib3.exceptions.LocationParseError` if the timeout is not valid\\n  :raises: :class:`~urllib3.exceptions.LocationParseError` if the timeout is not valid\\n  :raises: :class:`~urllib',\n",
       "    ':param timeout: a 2-tuple of (seconds, microseconds) or None',\n",
       "    ':param timeout:\\n    :return:\\n    :rtype: :class:`~requests.Timeout` or :class:`~requests.models.Timeout`\\n    :raises: :class:`~requests.exceptions.Timeout`\\n    :raises: :class:`~requests.models.Timeout`\\n    :raises: :class:`~requests.exceptions.ConnectionError`\\n    :raises: :class:`~requests.models.ConnectionError`\\n    :raises: :class:`~requests.exceptions.ReadTimeout`\\n    :raises: :class:`~requests.models.ReadTimeout`\\n    :raises: :class:`~requests.exceptions.ConnectTimeout`\\n    :raises: :class:`~requests.models.ConnectTimeout`\\n    :raises: :class:`~requests.exceptions.ReadTimeout`\\n    :raises: :class:`~requests.models.ReadTimeout`\\n    :raises: :class:`~requests.exceptions.ConnectionError`\\n    :raises: :class:`~requests.models.ConnectionError`\\n    :raises: :class:`~requests.exceptions.ConnectTimeout`\\n    :raises: :class:`~requests.models.ConnectTimeout`',\n",
       "    '']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        embeddings = None\\n        if self.embedding_func is not None:\\n            embeddings = self.embedding_func.embed_documents(list(texts))\\n        if embeddings is None:\\n            raise ValueError(\"embeddings is None\")\\n        if self.flag:\\n            dbs_list = self.vearch.list_dbs()\\n            if self.using_db_name not in dbs_list:\\n                create_db_code = self.vearch.create_db(self.using_db_name)\\n                if not create_db_code:\\n                    raise ValueError(\"create db failed!!!\")\\n            space_list = self.vearch.list_spaces(self.using_db_name)\\n            if self.using_table_name not in space_list:\\n                create_space_code = self._create_space(len(embeddings[0]))\\n                if not create_space_code:\\n                    raise ValueError(\"create space failed!!!\")\\n            docid = []\\n            if embeddings is not None and metadatas is not None:\\n                for text, metadata, embed in zip(texts, metadatas, embeddings):\\n                    profiles: dict[str, Any] = {}\\n                    profiles[\"text\"] = text\\n                    profiles[\"metadata\"] = metadata[\"source\"]\\n                    embed_np = np.array(embed)\\n                    profiles[\"text_embedding\"] = {\\n                        \"feature\": (embed_np / np.linalg.norm(embed_np)).tolist()\\n                    }\\n                    insert_res = self.vearch.insert_one(\\n                        self.using_db_name, self.using_table_name, profiles\\n                    )\\n                    if insert_res[\"status\"] == 200:\\n                        docid.append(insert_res[\"_id\"])\\n                        continue\\n                    else:\\n                        retry_insert = self.vearch.insert_one(\\n                            self.using_db_name, self.using_table_name, profiles\\n                        )\\n                        docid.append(retry_insert[\"_id\"])\\n                        continue\\n        else:\\n            table_path = os.path.join(\\n                self.using_metapath, self.using_table_name + \".schema\"\\n            )\\n            if not os.path.exists(table_path):\\n                dim = len(embeddings[0])\\n                response_code = self._create_table(dim)\\n                if response_code:\\n                    raise ValueError(\"create table failed!!!\")\\n            if embeddings is not None and metadatas is not None:\\n                doc_items = []\\n                for text, metadata, embed in zip(texts, metadatas, embeddings):\\n                    profiles_v: dict[str, Any] = {}\\n                    profiles_v[\"text\"] = text\\n                    profiles_v[\"metadata\"] = metadata[\"source\"]\\n                    embed_np = np.array(embed)\\n                    profiles_v[\"text_embedding\"] = embed_np / np.linalg.norm(embed_np)\\n                    doc_items.append(profiles_v)\\n\\n                docid = self.vearch.add(doc_items)\\n                t_time = 0\\n                while len(docid) != len(embeddings):\\n                    time.sleep(0.5)\\n                    if t_time > 6:\\n                        break\\n                    t_time += 1\\n                self.vearch.dump()\\n        return docid',\n",
       "   'd': 'Returns:\\n    List of ids from adding the texts into the vectorstore.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'Handle the mid-button click on a tab.\\n\\n    :param event: the mouse event.\\n    :return: nothing.',\n",
       "    'Handle the mouse press event.\\n\\n  :param event: The mouse press event.\\n\\n  :return: None.\\n\\n  :rtype: None.\\n\\n  :see: :class:`QTabWidget`\\n\\n  :see: :class:`QTabWidget.mousePressEvent`\\n\\n  :see: :class:`QTabWidget.close_tab(int)`']},\n",
       "  {'c': '    def evaluate(\\n        self,\\n        examples: Sequence[dict],\\n        predictions: Sequence[dict],\\n        question_key: str = \"query\",\\n        answer_key: str = \"answer\",\\n        prediction_key: str = \"result\",\\n        *,\\n        callbacks: Callbacks = None,\\n    ) -> List[dict]:\\n        inputs = [\\n            {\\n                \"query\": example[question_key],\\n                \"answer\": example[answer_key],\\n                \"result\": predictions[i][prediction_key],\\n            }\\n            for i, example in enumerate(examples)\\n        ]\\n\\n        return self.apply(inputs, callbacks=callbacks)',\n",
       "   'd': 'Evaluate question answering examples and predictions.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def _get_elements(self) -> List:\\n        from unstructured.partition.auto import partition\\n\\n        try:\\n            import boto3\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import `boto3` python package. \"\\n                \"Please install it with `pip install boto3`.\"\\n            )\\n        s3 = boto3.client(\"s3\")\\n        with tempfile.TemporaryDirectory() as temp_dir:\\n            file_path = f\"{temp_dir}/{self.key}\"\\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n            s3.download_file(self.bucket, self.key, file_path)\\n            return partition(filename=file_path)',\n",
       "   'd': 'Get elements.',\n",
       "   'l': True,\n",
       "   'g': ['Parse a Microsoft Word file.\\n\\n  Parameters\\n  ----------\\n  file : Path\\n      Path to the file to be parsed.\\n\\n  errors : str, optional\\n      Error handling method.\\n\\n  Returns\\n  -------\\n  str\\n      The text content of the file.',\n",
       "    'Parse a Microsoft Word file.\\n\\n  :param file: The file to parse.\\n  :param errors: How to handle errors when parsing the file.\\n  :return: The text of the file.',\n",
       "    'Parse a Microsoft Word file.\\n\\n  :param file: The file to parse.\\n  :param errors: How to handle errors when parsing the file.\\n  :return: The text of the file.\\n  :raises: ValueError if the file is not a Microsoft Word file.\\n  :raises: ValueError if the file is not a Microsoft Word file.\\n  :raises: ValueError if the file is not a Microsoft Word file.\\n  :raises: ValueError if the file is not a Microsoft Word file.\\n  :raises: ValueError if the file is not a Microsoft Word file.\\n  :raises: ValueError if the file is not a Microsoft Word file.\\n  :raises: ValueError if the file is not a Microsoft Word file.\\n  :raises: ValueError if the file is not a Microsoft Word file.\\n  :raises: ValueError if the file is not a Microsoft Word file.\\n  :raises: ValueError if the file is not a Microsoft Word file.\\n  :raises: ValueError if the file is not a Microsoft Word file.\\n  :raises: ValueError if the file is not a Microsoft Word file.\\n  :raises: ValueError if the file is not a Microsoft Word file.\\n  :raises',\n",
       "    'Parse a Microsoft Word file.\\n\\n  Parameters\\n  ----------\\n  file : Path\\n    Path to the file to be parsed.\\n  errors : str, optional\\n    Error handling method. One of \"ignore\", \"raise\", \"log\".\\n\\n  Returns\\n  -------\\n  str\\n    The text of the file.\\n\\n  Raises\\n  ------\\n  ValueError\\n    If the file is not a Microsoft Word file.\\n    If the error handling method is invalid.']},\n",
       "  {'c': '    def input_keys(self) -> List[str]:\\n',\n",
       "   'd': 'Return the input keys.\\n\\n:meta private:',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    ':param random: Random number generator\\n    :param cells: List of cells\\n    :param connections: List of connections\\n    :return: The least used cell in the list of cells.',\n",
       "    '']},\n",
       "  {'c': 'def search_api(query: str) -> str:\\n    assert isinstance(query, str)\\n    return f\"API result - {query}\"',\n",
       "   'd': 'Run when chain errors.\\n\\nArgs:\\n    error (Exception or KeyboardInterrupt): The error.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def load(self) -> List[Document]:\\n    return list(self.lazy_load())',\n",
       "   'd': 'Return type of llm.',\n",
       "   'l': False,\n",
       "   'g': ['Check that the function returns a path object.', '', '', '']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\\n        return DEFAULT_FORMATTER_MAPPING[self.template_format](self.template, **kwargs)',\n",
       "   'd': 'Add a list of texts to the Elasticsearch index.\\n\\nArgs:\\n    texts (Iterable[str]): The texts to add to the index.\\n    metadatas (List[Dict[Any, Any]], optional): A list of metadata dictionaries\\n        to associate with the texts.\\n    model_id (str, optional): The ID of the model to use for transforming the\\n        texts into vectors.\\n    refresh_indices (bool, optional): Whether to refresh the Elasticsearch\\n        indices after adding the texts.\\n    **kwargs: Arbitrary keyword arguments.\\n\\nReturns:\\n    A list of IDs for the added texts.',\n",
       "   'l': False,\n",
       "   'g': ['Patch the environment variables.\\n\\n    :param kwargs: The environment variables to patch.',\n",
       "    'Patch the environment with the given arguments.\\n\\n    :param kwargs: The arguments to patch the environment with.',\n",
       "    'Patch the environment with the given keyword arguments.\\n\\n    :param kwargs: The keyword arguments to patch the environment with.',\n",
       "    'Patch environment variables.\\n\\n  :param kwargs:\\n  :return:\\n  :rtype: :class:`Generator`\\n  :raises: :class:`ValueError`']},\n",
       "  {'c': '    def parse(self, text: str) -> T:\\n',\n",
       "   'd': 'Parse a single string model output into some structure.\\n\\nArgs:\\n    text: String output of a language model.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Saves an event to the default cache.\\n\\n    :param cache_key: The cache key to use.\\n    :param data: The event data to save.\\n    :param kwargs: The additional arguments to pass to the event manager.\\n    :return: The event data.',\n",
       "    'Save the event to the database.\\n\\n  :param cache_key: (optional) The cache key to use for the event.\\n  :param data: (optional) The event data.\\n  :param kwargs: (optional) The extra arguments to use for the event.\\n\\n  :return: The saved event.',\n",
       "    'Save an event to the database.\\n\\n    :param cache_key: The cache key to use for the event.\\n    :param data: The event data.\\n    :param kwargs: Additional arguments.']},\n",
       "  {'c': 'def _call(\\n    self,\\n    messages: List[BaseMessage],\\n    stop: Optional[List[str]] = None,\\n    run_manager: Optional[CallbackManagerForLLMRun] = None,\\n    **kwargs: Any,\\n) -> str:\\n    response = self.responses[self.i]\\n    if self.i < len(self.responses) - 1:\\n        self.i += 1\\n    else:\\n        self.i = 0\\n    return response',\n",
       "   'd': 'Create a new TextSplitter.',\n",
       "   'l': False,\n",
       "   'g': ['(n) = n * (n - 1) * (n - 2) * ... * 1',\n",
       "    '(n) = n * (n-1) * (n-2) * ... * 1',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': '    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n        for gen in return_val:\\n            if not isinstance(gen, Generation):\\n                raise ValueError(\\n                    \"Momento only supports caching of normal LLM generations, \"\\n                    f\"got {type(gen)}\"\\n                )\\n        key = self.__key(prompt, llm_string)\\n        value = _dump_generations_to_json(return_val)\\n        set_response = self.cache_client.set(self.cache_name, key, value, self.ttl)\\n        from momento.responses import CacheSet\\n\\n        if isinstance(set_response, CacheSet.Success):\\n            pass\\n        elif isinstance(set_response, CacheSet.Error):\\n            raise set_response.inner_exception\\n        else:\\n            raise Exception(f\"Unexpected response: {set_response}\")',\n",
       "   'd': 'Store llm generations in cache.\\n\\nArgs:\\n    prompt (str): The prompt run through the language model.\\n    llm_string (str): The language model string.\\n    return_val (RETURN_VAL_TYPE): A list of language model generations.\\n\\nRaises:\\n    SdkException: Momento service or network error\\n    Exception: Unexpected response',\n",
       "   'l': True,\n",
       "   'g': [':param input_tensor:\\n    :param output_tensor:\\n    :param output_tensor_grad:\\n    :return:',\n",
       "    ':param input_tensor:\\n    :param output_tensor:\\n    :param output_tensor_grad:\\n    :return:',\n",
       "    ':param input_tensor:\\n    :param output_tensor:\\n    :param output_tensor_grad:\\n    :return:\\n    :rtype: Optional[torch.Tensor]\\n    :raises: ValueError\\n    :raises: TypeError\\n    :raises: NotImplementedError\\n    :raises: Exception\\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises: \\n    :raises:',\n",
       "    ':param input_tensor:\\n:param output_tensor:\\n:param output_tensor_grad:\\n:return:']},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n    item = self.kv_cache.get(\\n        llm_string=_hash(llm_string),\\n        prompt=_hash(prompt),\\n    )\\n    if item:\\n        return _load_generations_from_json(item[\"body_blob\"])\\n    else:\\n        return None',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': ['.scatter(self, ax, data, center)',\n",
       "    '_draw_points(self, ax, data, center)',\n",
       "    '_draw_points(self, ax, data, center)',\n",
       "    '.scatter(self, ax, data, center)']},\n",
       "  {'c': '    def from_template(cls, template: str, **kwargs: Any) -> ChatPromptTemplate:\\n        prompt_template = PromptTemplate.from_template(template, **kwargs)\\n        message = HumanMessagePromptTemplate(prompt=prompt_template)\\n        return cls.from_messages([message])',\n",
       "   'd': 'Get the default parameters for calling OpenAI API.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def _identifying_params(self) -> Mapping[str, Any]:\\n    return {\\n        \"anyscale_service_url\": self.anyscale_service_url,\\n        \"anyscale_service_route\": self.anyscale_service_route,\\n    }',\n",
       "   'd': 'Get the identifying parameters.',\n",
       "   'l': True,\n",
       "   'g': ['_get_bad_permissions_response_code_403(self):',\n",
       "    '_get_bad_permissions',\n",
       "    '_summary',\n",
       "    '']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        refresh_indices: bool = True,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        try:\\n            from elasticsearch.exceptions import NotFoundError\\n            from elasticsearch.helpers import bulk\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import elasticsearch python package. \"\\n                \"Please install it with `pip install elasticsearch`.\"\\n            )\\n        requests = []\\n        ids = ids or [str(uuid.uuid4()) for _ in texts]\\n        embeddings = self.embedding.embed_documents(list(texts))\\n        dim = len(embeddings[0])\\n        mapping = _default_text_mapping(dim)\\n\\n\\n        try:\\n            self.client.indices.get(index=self.index_name)\\n        except NotFoundError:\\n\\n\\n            self.create_index(self.client, self.index_name, mapping)\\n\\n        for i, text in enumerate(texts):\\n            metadata = metadatas[i] if metadatas else {}\\n            request = {\\n                \"_op_type\": \"index\",\\n                \"_index\": self.index_name,\\n                \"vector\": embeddings[i],\\n                \"text\": text,\\n                \"metadata\": metadata,\\n                \"_id\": ids[i],\\n            }\\n            requests.append(request)\\n        bulk(self.client, requests)\\n\\n        if refresh_indices:\\n            self.client.indices.refresh(index=self.index_name)\\n        return ids',\n",
       "   'd': 'Call function that may optionally accept a run_manager and/or config.',\n",
       "   'l': False,\n",
       "   'g': ['_logcdf:\\n    Log-cdf of a gamma distribution.\\n    Parameters\\n    ----------\\n    value : float\\n        The value of the function.\\n    h : float\\n        The shape parameter of the gamma distribution.\\n    z : float\\n        The scale parameter of the gamma distribution.\\n    Returns\\n    -------\\n    float\\n        The log-cdf of the gamma distribution.\\n    Notes\\n    -----\\n    The log-cdf of a gamma distribution is defined as the following expression:\\n    .. math::\\n        \\\\log(x) = \\\\log(h) + \\\\log(z) + \\\\log(x - 1) - \\\\log(x) + \\\\log(x + 1) - \\\\log(x + 1) - \\\\log(x - 1)\\n    where :math:`x` is the value of the function.',\n",
       "    '_logcdf(value, h, z)\\n\\n    Log-cdf of a Polya-Gamma distribution.\\n\\n    Parameters\\n    ----------\\n    value : float\\n        Value of the function.\\n    h : float\\n        Shape parameter of the distribution.\\n    z : float\\n        Scale parameter of the distribution.\\n\\n    Returns\\n    -------\\n    float\\n        Log-cdf of the function.',\n",
       "    '_logcdf(value, h, z)',\n",
       "    '_logcdf:']},\n",
       "  {'c': '    def knn_search(\\n        self,\\n        query: Optional[str] = None,\\n        k: Optional[int] = 10,\\n        query_vector: Optional[List[float]] = None,\\n        model_id: Optional[str] = None,\\n        size: Optional[int] = 10,\\n        source: Optional[bool] = True,\\n        fields: Optional[\\n            Union[List[Mapping[str, Any]], Tuple[Mapping[str, Any], ...], None]\\n        ] = None,\\n        page_content: Optional[str] = \"text\",\\n    ) -> List[Tuple[Document, float]]:\\n        if not source and (\\n            fields is None or not any(page_content in field for field in fields)\\n        ):\\n            raise ValueError(\"If source=False `page_content` field must be in `fields`\")\\n\\n        knn_query_body = self._default_knn_query(\\n            query_vector=query_vector, query=query, model_id=model_id, k=k\\n        )\\n\\n\\n        response = self.client.search(\\n            index=self.index_name,\\n            knn=knn_query_body,\\n            size=size,\\n            source=source,\\n            fields=fields,\\n        )\\n\\n        hits = [hit for hit in response[\"hits\"][\"hits\"]]\\n        docs_and_scores = [\\n            (\\n                Document(\\n                    page_content=hit[\"_source\"][page_content]\\n                    if source\\n                    else hit[\"fields\"][page_content][0],\\n                    metadata=hit[\"fields\"] if fields else {},\\n                ),\\n                hit[\"_score\"],\\n            )\\n            for hit in hits\\n        ]\\n\\n        return docs_and_scores',\n",
       "   'd': 'Will always return list of memory variables.\\n:meta private:',\n",
       "   'l': False,\n",
       "   'g': ['get all documents from the database\\n\\n  :param file_name: the file name to search for\\n  :param metadata: the metadata to search for\\n  :return: the documents from the database\\n  :rtype: List[Document]\\n  :raises: Exception if the database is not connected\\n  :raises: Exception if the database is not connected\\n  :raises: Exception if the database is not connected\\n  :raises: Exception if the database is not connected\\n  :raises: Exception if the database is not connected\\n  :raises: Exception if the database is not connected\\n  :raises: Exception if the database is not connected\\n  :raises: Exception if the database is not connected\\n  :raises: Exception if the database is not connected\\n  :raises: Exception if the database is not connected\\n  :raises: Exception if the database is not connected\\n  :raises: Exception if the database is not connected\\n  :raises: Exception if the database is not connected\\n  :raises: Exception if the database is not connected\\n  :raises: Exception if the database is not connected\\n  :raises: Exception if the database is not connected\\n  :raises: Exception if the database is not connected',\n",
       "    'Get documents from the database.\\n  :param file_name: the file name of the document.\\n  :param metadata: the metadata of the document.\\n  :return: the documents.',\n",
       "    'list_docs',\n",
       "    'Return a list of documents from the specified file name and metadata.\\n  :param file_name: The name of the file to search.\\n  :param metadata: The metadata to search for.\\n  :return: A list of documents.\\n  :rtype: List[Document]\\n  :raises: :class:`~kb_client.exceptions.NotFoundException` if the file name is not found.\\n  :raises: :class:`~kb_client.exceptions.InvalidInputException` if the metadata is invalid.\\n  :raises: :class:`~kb_client.exceptions.UnauthorizedException` if the user is not authorized to access the file.\\n  :raises: :class:`~kb_client.exceptions.InternalException` if an error occurs while retrieving the documents from the database.\\n  :raises: :class:`~kb_client.exceptions.InvalidInputException` if the metadata is invalid.\\n  :raises: :class:`~kb_client.exceptions.InvalidInputException` if the file name is not found.\\n  :raises: :class:`~kb_client.exceptions.UnauthorizedException` if the user is not authorized to access the file.\\n  :raises']},\n",
       "  {'c': '    def load(self) -> List[Document]:\\n        docs = []\\n        with open(self.file_path, newline=\"\", encoding=self.encoding) as csvfile:\\n            csv_reader = csv.DictReader(csvfile, **self.csv_args)\\n            for i, row in enumerate(csv_reader):\\n                content = \"\\\\n\".join(f\"{k.strip()}: {v.strip()}\" for k, v in row.items())\\n                try:\\n                    source = (\\n                        row[self.source_column]\\n                        if self.source_column is not None\\n                        else self.file_path\\n                    )\\n                except KeyError:\\n                    raise ValueError(\\n                        f\"Source column \\'{self.source_column}\\' not found in CSV file.\"\\n                    )\\n                metadata = {\"source\": source, \"row\": i}\\n                doc = Document(page_content=content, metadata=metadata)\\n                docs.append(doc)\\n\\n        return docs',\n",
       "   'd': 'For Painless Scripting Search, this is the default query.',\n",
       "   'l': False,\n",
       "   'g': ['_init_',\n",
       "    '_init_',\n",
       "    '(w0) :\\n    :param w0: (float) the width of the input signal',\n",
       "    '_init_']},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n    return self._cache.get((prompt, llm_string), None)',\n",
       "   'd': 'Return the input keys.\\n\\n:meta private:',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    ':returns: string representation of the object\\n  :rtype: str\\n  :raises: :class:`~pdfplumber.pdf.PdfInfo`\\n  :raises: :class:`~pdfplumber.pdf.PdfPage`\\n  :raises: :class:`~pdfplumber.pdf.PdfPage`\\n  :raises: :class:`~pdfplumber.pdf.PdfPage`\\n  :raises: :class:`~pdfplumber.pdf.PdfPage`\\n  :raises: :class:`~pdfplumber.pdf.PdfPage`\\n  :raises: :class:`~pdfplumber.pdf.PdfPage`\\n  :raises: :class:`~pdfplumber.pdf.PdfPage`\\n  :raises: :class:`~pdfplumber.pdf.PdfPage`\\n  :raises: :class:`~pdfplumber.pdf.PdfPage`\\n  :raises: :class:`~pdfplumber.pdf.PdfPage`\\n  :raises: :class:`~pdfplumber.pdf.PdfPage`\\n  :raises: :class:`~pdfplumber.pdf.PdfPage`\\n  :raises: :class',\n",
       "    '']},\n",
       "  {'c': 'def from_function(\\n    cls,\\n    func: Callable,\\n    name: str,\\n    description: str,\\n    return_direct: bool = False,\\n    args_schema: Optional[Type[BaseModel]] = None,\\n    **kwargs: Any,\\n) -> Tool:\\n    return cls(\\n        name=name,\\n        func=func,\\n        description=description,\\n        return_direct=return_direct,\\n        args_schema=args_schema,\\n        **kwargs,\\n    )',\n",
       "   'd': 'Get default mime-type based parser.',\n",
       "   'l': False,\n",
       "   'g': ['delete a game by slug', '', '', 'Delete a game from the database.']},\n",
       "  {'c': 'def split_text(self, text: str) -> List[str]:\\n',\n",
       "   'd': 'Return the output parser type for serialization.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Unzip a file from a zip file.\\n\\n  :param zip_ref: the zip file reference.\\n  :param file_entry: the file entry to unzip.\\n  :param target_dir: the target directory to extract the file to.',\n",
       "    'Extract a single file from a zip archive.\\n\\n  :param zip_ref: The zip file reference.\\n  :param file_entry: The file entry in the zip archive.\\n  :param target_dir: The target directory.\\n  :return:',\n",
       "    'Unzip a file from a zip file.\\n\\n  :param zip_ref: The zip file reference.\\n  :param file_entry: The file entry from the zip file.\\n  :param target_dir: The target directory to extract the file to.']},\n",
       "  {'c': '    def _parse(self, content: str, docs: List[Document]) -> None:\\n        data = self._jq_schema.input(json.loads(content))\\n\\n\\n\\n\\n        if self._content_key is not None:\\n            self._validate_content_key(data)\\n        if self._metadata_func is not None:\\n            self._validate_metadata_func(data)\\n\\n        for i, sample in enumerate(data, len(docs) + 1):\\n            text = self._get_text(sample=sample)\\n            metadata = self._get_metadata(\\n                sample=sample, source=str(self.file_path), seq_num=i\\n            )\\n            docs.append(Document(page_content=text, metadata=metadata))',\n",
       "   'd': 'Convert given content to documents.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    ':param img: np.ndarray\\n  :param factor: int\\n  :return: np.ndarray',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': '    def similarity_search_with_score(\\n        self, query: str, k: int = 4, filter: Optional[dict] = None\\n    ) -> List[Tuple[Document, float]]:\\n        embedding = self.embedding.embed_query(query)\\n        conn = self.connection_pool.connect()\\n        result = []\\n        where_clause: str = \"\"\\n        where_clause_values: List[Any] = []\\n        if filter:\\n            where_clause = \"WHERE \"\\n            arguments = []\\n\\n            def build_where_clause(\\n                where_clause_values: List[Any],\\n                sub_filter: dict,\\n                prefix_args: List[str] = [],\\n            ) -> None:\\n                for key in sub_filter.keys():\\n                    if isinstance(sub_filter[key], dict):\\n                        build_where_clause(\\n                            where_clause_values, sub_filter[key], prefix_args + [key]\\n                        )\\n                    else:\\n                        arguments.append(\\n                            \"JSON_EXTRACT_JSON({}, {}) = %s\".format(\\n                                self.metadata_field,\\n                                \", \".join([\"%s\"] * (len(prefix_args) + 1)),\\n                            )\\n                        )\\n                        where_clause_values += prefix_args + [key]\\n                        where_clause_values.append(json.dumps(sub_filter[key]))\\n\\n            build_where_clause(where_clause_values, filter)\\n            where_clause += \" AND \".join(arguments)\\n\\n        try:\\n            cur = conn.cursor()\\n            try:\\n                cur.execute(\\n                    \"\"\"SELECT {}, {}, {}({}, JSON_ARRAY_PACK(%s)) as __score\\n                    FROM {} {} ORDER BY __score {} LIMIT %s\"\"\".format(\\n                        self.content_field,\\n                        self.metadata_field,\\n                        self.distance_strategy,\\n                        self.vector_field,\\n                        self.table_name,\\n                        where_clause,\\n                        ORDERING_DIRECTIVE[self.distance_strategy],\\n                    ),\\n                    (\"[{}]\".format(\",\".join(map(str, embedding))),)\\n                    + tuple(where_clause_values)\\n                    + (k,),\\n                )\\n\\n                for row in cur.fetchall():\\n                    doc = Document(page_content=row[0], metadata=row[1])\\n                    result.append((doc, float(row[2])))\\n            finally:\\n                cur.close()\\n        finally:\\n            conn.close()\\n        return result',\n",
       "   'd': 'Return docs most similar to query. Uses cosine similarity.\\n\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 4.\\n    filter: A dictionary of metadata fields and values to filter by.\\n            Defaults to None.\\n\\nReturns:\\n    List of Documents most similar to the query and score for each',\n",
       "   'l': True,\n",
       "   'g': ['_get_code_package()',\n",
       "    '_get_code_package()',\n",
       "    '_code(self):',\n",
       "    '_get_code_package_and_return_code_object(self):\\n    def get_code_object(self):\\n    def get_code_object_from_code_package(self):\\n    def get_code_object_from_code_package_and_return_code_object(self):\\n    def get_code_object_from_code_package_and_return_code_object_from_code_package(self):\\n    def get_code_object_from_code_package_and_return_code_object_from_code_package_and_return_code_object(self):\\n    def get_code_object_from_code_package_and_return_code_object_from_code_package_and_return_code_object_from_code_package(self):\\n    def get_code_object_from_code_package_and_return_code_object_from_code_package_and_return_code_object_from_code_package_and_return_code_object_from_code_package(self):\\n    def get_code_object_from_code']},\n",
       "  {'c': 'def _type(self) -> str:\\n    raise NotImplementedError(\\n        f\"_type property is not implemented in class {self.__class__.__name__}.\"\\n        \" This is required for serialization.\"\\n    )',\n",
       "   'd': 'Compute doc embeddings using a HuggingFace instruct model.\\n\\nArgs:\\n    texts: The list of texts to embed.\\n\\nReturns:\\n    List of embeddings, one for each text.',\n",
       "   'l': False,\n",
       "   'g': ['Convert a string to a text_type.\\n\\n  >>> safe_str(\"foo\")\\n  \\'foo\\'\\n  >>> safe_str(123)\\n  \\'123\\'\\n  >>> safe_str(1.23)\\n  \\'1.23\\'\\n  >>> safe_str(1 + 1j)\\n  \\'1 + 1j\\'\\n  >>> safe_str(None)\\n  \\'None\\'',\n",
       "    ':param v:\\n    :return:\\n    :rtype: text_type',\n",
       "    'Convert unicode to str, but not to utf-8.\\n\\n  >>> safe_str(u\"foo\")\\n  u\"foo\"\\n  >>> safe_str(u\"foo\".encode(\"utf-8\", \"replace\"))\\n  u\"foo\"\\n  >>> safe_str(u\"foo\".encode(\"ascii\", \"replace\"))\\n  u\"foo\"\\n  >>> safe_str(u\"foo\".encode(\"ascii\", \"ignore\"))\\n  u\"foo\"\\n  >>> safe_str(u\"foo\".encode(\"ascii\", \"strict\"))\\n  u\"foo\"\\n  >>> safe_str(u\"foo\".encode(\"ascii\", \"xmlcharrefreplace\"))\\n  u\"foo\"\\n  >>> safe_str(u\"foo\".encode(\"ascii\", \"xmlcharrefreplace\"))\\n  u\"foo\"\\n  >>> safe_str(u\"foo\".encode(\"ascii\", \"xmlcharrefreplace\"))\\n  u\"foo\"\\n  >>> safe_str(u\"foo\".encode(\"ascii\", \"xmlcharrefreplace\"))\\n  u\"foo\"\\n  >>> safe_str(u\"foo\".encode(\"ascii\", \"xmlcharrefreplace',\n",
       "    'Convert a value to a string.']},\n",
       "  {'c': 'def on_tool_error(\\n    self,\\n    error: Union[Exception, KeyboardInterrupt],\\n    *,\\n    run_id: UUID,\\n    parent_run_id: Optional[UUID] = None,\\n    **kwargs: Any,\\n) -> Any:\\n',\n",
       "   'd': 'Run when tool errors.',\n",
       "   'l': True,\n",
       "   'g': ['_init_',\n",
       "    '(result=None, verbose=False, print_all=False)',\n",
       "    '_init_',\n",
       "    '(str, bool, bool) ->']},\n",
       "  {'c': 'def dict(self, **kwargs: Any) -> Dict:\\n    _dict = super().dict()\\n    del _dict[\"output_parser\"]\\n    return _dict',\n",
       "   'd': 'Return dictionary representation of agent.',\n",
       "   'l': True,\n",
       "   'g': ['Returns the first element found by the given xpath expression\\n    with the given key and value.\\n    :param node: the element to search in\\n    :param xpath: the xpath expression\\n    :param key: the key of the attribute\\n    :param val: the value of the attribute\\n    :return: the first element found by the given xpath expression\\n    with the given key and value.',\n",
       "    ':param node:\\n      :param xpath:\\n      :param key:\\n      :param val:\\n      :return:',\n",
       "    ':param node:\\n      :param xpath:\\n      :param key:\\n      :param val:',\n",
       "    'Returns the element that matches the given xpath and attribute key.\\n\\n  :param node: The DOM node to search.\\n  :param xpath: The xpath to search.\\n  :param key: The attribute key to search.\\n  :param val: The attribute value to search.\\n  :return: The element that matches the given xpath and attribute key.']},\n",
       "  {'c': 'def on_retriever_start(\\n    self,\\n    serialized: Dict[str, Any],\\n    query: str,\\n    *,\\n    run_id: UUID,\\n    parent_run_id: Optional[UUID] = None,\\n    tags: Optional[List[str]] = None,\\n    metadata: Optional[Dict[str, Any]] = None,\\n    **kwargs: Any,\\n) -> None:\\n    parent_run_id_ = str(parent_run_id) if parent_run_id else None\\n    execution_order = self._get_execution_order(parent_run_id_)\\n    start_time = datetime.utcnow()\\n    if metadata:\\n        kwargs.update({\"metadata\": metadata})\\n    retrieval_run = Run(\\n        id=run_id,\\n        name=\"Retriever\",\\n        parent_run_id=parent_run_id,\\n        serialized=serialized,\\n        inputs={\"query\": query},\\n        extra=kwargs,\\n        events=[{\"name\": \"start\", \"time\": start_time}],\\n        start_time=start_time,\\n        execution_order=execution_order,\\n        child_execution_order=execution_order,\\n        tags=tags,\\n        child_runs=[],\\n        run_type=\"retriever\",\\n    )\\n    self._start_trace(retrieval_run)\\n    self._on_retriever_start(retrieval_run)',\n",
       "   'd': 'Run when Retriever starts running.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    \"'CreateInheritableAnnotation'\\n    :param self: :class:`qlast.qlast`\\n    :param kids: :class:`qlast.qlast`\\n    :return: :class:`qlast.qlast`\\n    :rtype: :class:`qlast.qlast`\",\n",
       "    'CreateInheritableAnnotation',\n",
       "    \"'CreateInheritableAnnotation' : :class:`qlast.CreateAnnotation`\\n    :param qlast.CreateAnnotation qlast.CreateAnnotation r : :class:`qlast.CreateAnnotation`\\n    :param qlast.CreateAnnotation qlast.CreateAnnotation qlast.CreateAnnotation qlast.CreateAnnotation qlast.CreateAnnotation\"]},\n",
       "  {'c': 'def search_api(query: str) -> str:\\n    assert isinstance(query, str)\\n    return f\"API result - {query}\"',\n",
       "   'd': 'Run when chain errors.',\n",
       "   'l': False,\n",
       "   'g': [':returns: a list of extensions that can be used in the `extensions`\\n    parameter of `__init__`',\n",
       "    ':returns: A list of extensions that are currently watched by the\\n    :class:`~pyglet.window.Window` object.',\n",
       "    ':returns: A list of extensions that this file can be watched with.\\n    :rtype: list of str',\n",
       "    ':return: list of extensions to be watched.']},\n",
       "  {'c': '    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Any = None,\\n        metadatas: Optional[List[dict]] = None,\\n        index_name: str = \"\",\\n        url: str = \"http://localhost:8882\",\\n        api_key: str = \"\",\\n        add_documents_settings: Optional[Dict[str, Any]] = None,\\n        searchable_attributes: Optional[List[str]] = None,\\n        page_content_builder: Optional[Callable[[Dict[str, str]], str]] = None,\\n        index_settings: Optional[Dict[str, Any]] = None,\\n        verbose: bool = True,\\n        **kwargs: Any,\\n    ) -> Marqo:\\n        try:\\n            import marqo\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import marqo python package. \"\\n                \"Please install it with `pip install marqo`.\"\\n            )\\n\\n        if not index_name:\\n            index_name = str(uuid.uuid4())\\n\\n        client = marqo.Client(url=url, api_key=api_key)\\n\\n        try:\\n            client.create_index(index_name, settings_dict=index_settings or {})\\n            if verbose:\\n                print(f\"Created {index_name} successfully.\")\\n        except Exception:\\n            if verbose:\\n                print(f\"Index {index_name} exists.\")\\n\\n        instance: Marqo = cls(\\n            client,\\n            index_name,\\n            searchable_attributes=searchable_attributes,\\n            add_documents_settings=add_documents_settings or {},\\n            page_content_builder=page_content_builder,\\n        )\\n        instance.add_texts(texts, metadatas)\\n        return instance',\n",
       "   'd': 'Return Marqo initialized from texts. Note that Marqo does not need\\nembeddings, we retain the parameter to adhere to the Liskov\\nsubstitution principle.\\n\\nThis is a quick way to get started with marqo - simply provide your texts and\\nmetadatas and this will create an instance of the data store and index the\\nprovided data.\\n\\nTo know the ids of your documents with this approach you will need to include\\nthem in under the key \"_id\" in your metadatas for each text\\n\\nExample:\\n.. code-block:: python\\n\\n        from langchain.vectorstores import Marqo\\n\\n        datastore = Marqo(texts=[\\'text\\'], index_name=\\'my-first-index\\',\\n        url=\\'http://localhost:8882\\')\\n\\nArgs:\\n    texts (List[str]): A list of texts to index into marqo upon creation.\\n    embedding (Any, optional): Embeddings (not required). Defaults to None.\\n    index_name (str, optional): The name of the index to use, if none is\\n    provided then one will be created with a UUID. Defaults to None.\\n    url (str, optional): The URL for Marqo. Defaults to \"http://localhost:8882\".\\n    api_key (str, optional): The API key for Marqo. Defaults to \"\".\\n    metadatas (Optional[List[dict]], optional): A list of metadatas, to\\n    accompany the texts. Defaults to None.\\n    this is only used when a new index is being created. Defaults to \"cpu\". Can\\n    be \"cpu\" or \"cuda\".\\n    add_documents_settings (Optional[Dict[str, Any]], optional): Settings\\n    for adding documents, see\\n    https://docs.marqo.ai/0.0.16/API-Reference/documents/#query-parameters.\\n    Defaults to {}.\\n    index_settings (Optional[Dict[str, Any]], optional): Index settings if\\n    the index doesn\\'t exist, see\\n    https://docs.marqo.ai/0.0.16/API-Reference/indexes/#index-defaults-object.\\n    Defaults to {}.\\n\\nReturns:\\n    Marqo: An instance of the Marqo vector store',\n",
       "   'l': True,\n",
       "   'g': ['Returns True if the toolbar should be shown.',\n",
       "    '',\n",
       "    'Return True if the toolbar should be shown.',\n",
       "    'Show toolbar in debug mode.\\n\\n    :param request: request object.\\n\\n    :return: True if toolbar should be shown, False otherwise.']},\n",
       "  {'c': '    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        document_embeddings = []\\n\\n        for text in texts:\\n            document_embeddings.append(self._embed(text))\\n        return document_embeddings',\n",
       "   'd': \"Call out to Aleph Alpha's Document endpoint.\\n\\nArgs:\\n    texts: The list of texts to embed.\\n\\nReturns:\\n    List of embeddings, one for each text.\",\n",
       "   'l': True,\n",
       "   'g': ['Returns a function that sorts the input according to the group index.\\n\\n  Parameters\\n  ----------\\n  group_index : array_like\\n    The group indices.\\n  ngroups : int\\n    The number of groups.\\n\\n  Returns\\n  -------\\n  function\\n    A function that sorts the input according to the group index.',\n",
       "    'Returns a function that sorts groups according to the group index.\\n\\n  :param group_index: A list of integers.\\n  :param ngroups: The number of groups.\\n  :return: A function that sorts groups according to the group index.',\n",
       "    'Sort groups by the number of samples in each group.\\n\\n  Parameters\\n  ----------\\n  group_index : array_like\\n    Index array of group elements.\\n  ngroups : int\\n    Number of groups.\\n\\n  Returns\\n  -------\\n  group_index : array_like\\n    Sorted group indices.',\n",
       "    'Returns a function to sort the groups by their index.\\n\\n    Parameters\\n    ----------\\n    group_index : array_like\\n        The index of each group.\\n    ngroups : int\\n        The number of groups.\\n\\n    Returns\\n    -------\\n    function : int -> int\\n        The function to sort the groups by their index.']},\n",
       "  {'c': 'def test_deeplakewith_persistence() -> None:\\n    dataset_path = \"./tests/persist_dir\"\\n    if deeplake.exists(dataset_path):\\n        deeplake.delete(dataset_path)\\n\\n    texts = [\"foo\", \"bar\", \"baz\"]\\n    docsearch = DeepLake.from_texts(\\n        dataset_path=dataset_path,\\n        texts=texts,\\n        embedding=FakeEmbeddings(),\\n    )\\n\\n    output = docsearch.similarity_search(\"foo\", k=1)\\n    assert output == [Document(page_content=\"foo\")]\\n\\n\\n    docsearch = DeepLake(\\n        dataset_path=dataset_path,\\n        embedding_function=FakeEmbeddings(),\\n    )\\n    output = docsearch.similarity_search(\"foo\", k=1)\\n\\n\\n    docsearch.delete_dataset()',\n",
       "   'd': 'Test error is raised when input variables are not provided.',\n",
       "   'l': False,\n",
       "   'g': ['', ':param event:', 'Handle new object file.', '']},\n",
       "  {'c': 'def test_deduplication(\\n    record_manager: SQLRecordManager, vector_store: VectorStore\\n) -> None:\\n    docs = [\\n        Document(\\n            page_content=\"This is a test document.\",\\n            metadata={\"source\": \"1\"},\\n        ),\\n        Document(\\n            page_content=\"This is a test document.\",\\n            metadata={\"source\": \"1\"},\\n        ),\\n    ]\\n\\n\\n    assert index(docs, record_manager, vector_store, cleanup=\"full\") == {\\n        \"num_added\": 1,\\n        \"num_deleted\": 0,\\n        \"num_skipped\": 0,\\n        \"num_updated\": 0,\\n    }',\n",
       "   'd': 'Append the message to the record in db',\n",
       "   'l': False,\n",
       "   'g': ['(str) The title of the song.',\n",
       "    '(str)',\n",
       "    '.player_config_args[\"title\"]',\n",
       "    '.player_config_args[\"title\"]']},\n",
       "  {'c': '    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n        for gen in return_val:\\n            if not isinstance(gen, Generation):\\n                raise ValueError(\\n                    \"Momento only supports caching of normal LLM generations, \"\\n                    f\"got {type(gen)}\"\\n                )\\n        key = self.__key(prompt, llm_string)\\n        value = _dump_generations_to_json(return_val)\\n        set_response = self.cache_client.set(self.cache_name, key, value, self.ttl)\\n        from momento.responses import CacheSet\\n\\n        if isinstance(set_response, CacheSet.Success):\\n            pass\\n        elif isinstance(set_response, CacheSet.Error):\\n            raise set_response.inner_exception\\n        else:\\n            raise Exception(f\"Unexpected response: {set_response}\")',\n",
       "   'd': 'Store llm generations in cache.\\n\\nArgs:\\n    prompt (str): The prompt run through the language model.\\n    llm_string (str): The language model string.\\n    return_val (RETURN_VAL_TYPE): A list of language model generations.\\n\\nRaises:\\n    SdkException: Momento service or network error\\n    Exception: Unexpected response',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def from_function(\\n        cls,\\n        func: Callable,\\n        name: Optional[str] = None,\\n        description: Optional[str] = None,\\n        return_direct: bool = False,\\n        args_schema: Optional[Type[BaseModel]] = None,\\n        infer_schema: bool = True,\\n        **kwargs: Any,\\n    ) -> StructuredTool:\\n        name = name or func.__name__\\n        description = description or func.__doc__\\n        assert (\\n            description is not None\\n        ), \"Function must have a docstring if description not provided.\"\\n\\n\\n\\n        description = f\"{name}{signature(func)} - {description.strip()}\"\\n        _args_schema = args_schema\\n        if _args_schema is None and infer_schema:\\n            _args_schema = create_schema_from_function(f\"{name}Schema\", func)\\n        return cls(\\n            name=name,\\n            func=func,\\n            args_schema=_args_schema,\\n            description=description,\\n            return_direct=return_direct,\\n            **kwargs,\\n        )',\n",
       "   'd': 'Create tool from a given function.\\n\\nA classmethod that helps to create a tool from a function.\\n\\nArgs:\\n    func: The function from which to create a tool\\n    name: The name of the tool. Defaults to the function name\\n    description: The description of the tool. Defaults to the function docstring\\n    return_direct: Whether to return the result directly or as a callback\\n    args_schema: The schema of the tool\\'s input arguments\\n    infer_schema: Whether to infer the schema from the function\\'s signature\\n    **kwargs: Additional arguments to pass to the tool\\n\\nReturns:\\n    The tool\\n\\nExamples:\\n    ... code-block:: python\\n        def add(a: int, b: int) -> int:\\n            \"\"\"Add two numbers\"\"\"\\n            return a + b\\n        tool = StructuredTool.from_function(add)\\n        tool.run(1, 2) # 3',\n",
       "   'l': True,\n",
       "   'g': [':param rank_results: if True, sort the results by cross_score,\\n  :param hits: list of hit results\\n  :return: sorted hits list',\n",
       "    'Sort the results by the score of the hits.\\n  :param hits: The list of hits.\\n  :param rank_results: Whether to rank the results.\\n  :return: The sorted hits.\\n  :rtype: List[dict]\\n  :raises: :class:`~hits.exceptions.InvalidHitException`',\n",
       "    'Sort the hits by score and cross_score.\\n  :param rank_results: whether to rank the results\\n  :param hits: the hits list\\n  :return: the hits list\\n  :rtype: List[dict]\\n  :raises: Exception',\n",
       "    'Sort the hits by score.\\n  :param rank_results: Whether to rank the results.\\n  :param hits: The list of hits.\\n  :return: The list of hits.\\n  :rtype: List[dict]\\n  :raises: ValueError if the rank_results is not True.\\n  :raises: ValueError if the hits is not a list of hits.\\n  :raises: ValueError if the hits is not a list of hits.\\n  :raises: ValueError if the hits is not a list of hits.\\n  :raises: ValueError if the hits is not a list of hits.\\n  :raises: ValueError if the hits is not a list of hits.\\n  :raises: ValueError if the hits is not a list of hits.\\n  :raises: ValueError if the hits is not a list of hits.\\n  :raises: ValueError if the hits is not a list of hits.\\n  :raises: ValueError if the hits is not a list of hits.\\n  :raises: ValueError if the hits is not a list of hits.\\n  :raises: ValueError if the hits is not a list of hits.\\n  :raises: ValueError if the hits is not a list of']},\n",
       "  {'c': 'def load_prompt_from_config(config: dict) -> BasePromptTemplate:\\n    if \"_type\" not in config:\\n        logger.warning(\"No `_type` key found, defaulting to `prompt`.\")\\n    config_type = config.pop(\"_type\", \"prompt\")\\n\\n    if config_type not in type_to_loader_dict:\\n        raise ValueError(f\"Loading {config_type} prompt not supported\")\\n\\n    prompt_loader = type_to_loader_dict[config_type]\\n\\n\\n\\n    return prompt_loader(config)',\n",
       "   'd': 'Load prompt from Config Dict.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    ':param attr: The attribute to import.',\n",
       "    '',\n",
       "    \"Import a symbol from the app's module.\\n\\n  :param attr: The symbol to import.\"]},\n",
       "  {'c': 'def _run(\\n    self,\\n    query: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> str:\\n    return self.llm_chain.predict(\\n        query=query, callbacks=run_manager.get_child() if run_manager else None\\n    )',\n",
       "   'd': 'Use the LLM to check the query.',\n",
       "   'l': True,\n",
       "   'g': ['_predict(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:',\n",
       "    '(observation: th.Tensor, deterministic: bool = False) -> th.Tensor:',\n",
       "    '.get_actions(deterministic=deterministic)',\n",
       "    '_predict(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        bulk_size: int = 500,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        embeddings = self.embedding_function.embed_documents(list(texts))\\n        _validate_embeddings_and_bulk_size(len(embeddings), bulk_size)\\n        index_name = _get_kwargs_value(kwargs, \"index_name\", self.index_name)\\n        text_field = _get_kwargs_value(kwargs, \"text_field\", \"text\")\\n        dim = len(embeddings[0])\\n        engine = _get_kwargs_value(kwargs, \"engine\", \"nmslib\")\\n        space_type = _get_kwargs_value(kwargs, \"space_type\", \"l2\")\\n        ef_search = _get_kwargs_value(kwargs, \"ef_search\", 512)\\n        ef_construction = _get_kwargs_value(kwargs, \"ef_construction\", 512)\\n        m = _get_kwargs_value(kwargs, \"m\", 16)\\n        vector_field = _get_kwargs_value(kwargs, \"vector_field\", \"vector_field\")\\n        max_chunk_bytes = _get_kwargs_value(kwargs, \"max_chunk_bytes\", 1 * 1024 * 1024)\\n\\n        _validate_aoss_with_engines(self.is_aoss, engine)\\n\\n        mapping = _default_text_mapping(\\n            dim, engine, space_type, ef_search, ef_construction, m, vector_field\\n        )\\n\\n        return _bulk_ingest_embeddings(\\n            self.client,\\n            index_name,\\n            embeddings,\\n            texts,\\n            metadatas=metadatas,\\n            ids=ids,\\n            vector_field=vector_field,\\n            text_field=text_field,\\n            mapping=mapping,\\n            max_chunk_bytes=max_chunk_bytes,\\n            is_aoss=self.is_aoss,\\n        )',\n",
       "   'd': 'Run more texts through the embeddings and add to the vectorstore.\\n\\nArgs:\\n    texts: Iterable of strings to add to the vectorstore.\\n    metadatas: Optional list of metadatas associated with the texts.\\n    ids: Optional list of ids to associate with the texts.\\n    bulk_size: Bulk API request count; Default: 500\\n\\nReturns:\\n    List of ids from adding the texts into the vectorstore.\\n\\nOptional Args:\\n    vector_field: Document field embeddings are stored in. Defaults to\\n    \"vector_field\".\\n\\n    text_field: Document field the text of the document is stored in. Defaults\\n    to \"text\".',\n",
       "   'l': True,\n",
       "   'g': ['Convert a set, list, or tuple to a list, or convert a string to a string.\\n\\n  :param item: the item to preconvert.\\n\\n  :return: the preconverted item.',\n",
       "    'Convert a set, list, or tuple to a list.\\n    :param item: the item to convert\\n    :type item: set|list|tuple|str',\n",
       "    'Preconvert a value to a list, set or path.\\n\\n  :param item:  the item to convert\\n  :return:     the converted item.',\n",
       "    '']},\n",
       "  {'c': 'def on_retriever_error(\\n    self,\\n    error: BaseException,\\n    **kwargs: Any,\\n) -> None:\\n    _handle_event(\\n        self.handlers,\\n        \"on_retriever_error\",\\n        \"ignore_retriever\",\\n        error,\\n        run_id=self.run_id,\\n        parent_run_id=self.parent_run_id,\\n        tags=self.tags,\\n        **kwargs,\\n    )',\n",
       "   'd': 'Run when retriever errors.',\n",
       "   'l': True,\n",
       "   'g': ['_ = lambda x: x',\n",
       "    '_logger.debug(\"call\")\\n        self.cfg = cfg\\n        self.init_params()\\n        self.init_model()\\n        self.init_dataloader()\\n        self.init_logger()\\n        self.init_evaluator()\\n        self.init_logger()\\n        self.init_optimizer()\\n        self.init_scheduler()\\n        self.init_train_dataloader()\\n        self.init_val_dataloader()\\n        self.init_test_dataloader()\\n        self.init_loss_function()\\n        self.init_metric()\\n        self.init_lr_scheduler()\\n        self.init_optimizer()\\n        self.init_loss_function()\\n        self.init_metric()\\n        self.init_lr_scheduler()\\n        self.init_optimizer()\\n        self.init_loss_function()\\n        self.init_metric()\\n        self.init_lr_scheduler()\\n        self.init_optimizer()\\n        self.init_loss_function()\\n        self.init_metric()\\n        self.init_lr_scheduler()\\n        self.init_optimizer()\\n        self.',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': 'def search_api(query: str) -> str:\\n    return \"API result\"',\n",
       "   'd': 'Search the API for the query.',\n",
       "   'l': True,\n",
       "   'g': [':return: dict of environment variables.',\n",
       "    ':return: dict of environment variables to be set in the process.\\n  :param prefer_system_libs: if True, will prefer system_libs to wine_path.\\n  :param wine_path: if not None, will prefer wine_path to system_libs.\\n  :return: dict of environment variables to be set in the process.',\n",
       "    '',\n",
       "    \"Returns a dict of environment variables to use.\\n\\n    :param prefer_system_libs: If True, prefer to use the system's\\n        libraries over the wine-specific ones.\\n    :param wine_path: The path to the wine.exe binary.\\n\\n    :return: A dict of environment variables to use.\"]},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: List[str],\\n        metadatas: Optional[List[dict]] = None,\\n        doc_metadata: Optional[dict] = None,\\n    ) -> None:\\n        self.vectorstore.add_texts(texts, metadatas, doc_metadata or {})',\n",
       "   'd': 'Return type of llm.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def __init__(self, file_path: str):\\n        try:\\n            from pdfminer.high_level import extract_text_to_fp\\n        except ImportError:\\n            raise ImportError(\\n                \"`pdfminer` package not found, please install it with \"\\n                \"`pip install pdfminer.six`\"\\n            )\\n\\n        super().__init__(file_path)',\n",
       "   'd': 'Initialize with a file path.',\n",
       "   'l': True,\n",
       "   'g': [':param img1_path: str or list of str, image path or list of image path\\n    :param img2_path: str or list of str, image path or list of image path\\n    :return: list of image path or list of list of image path, boolean',\n",
       "    ':param img1_path: \\n    :param img2_path:',\n",
       "    ':param img1_path: list or str\\n    :param img2_path: str\\n    :return: list, bool\\n    :return: list of image paths, boolean for bulk process or not\\n    :return: list of image paths, list of image paths for bulk process or not\\n    :return: list of image paths, list of image paths for bulk process or not\\n    :return: list of image paths, list of image paths for bulk process or not\\n    :return: list of image paths, list of image paths for bulk process or not\\n    :return: list of image paths, list of image paths for bulk process or not\\n    :return: list of image paths, list of image paths for bulk process or not\\n    :return: list of image paths, list of image paths for bulk process or not\\n    :return: list of image paths, list of image paths for bulk process or not\\n    :return: list of image paths, list of image paths for bulk process or not\\n    :return: list of image paths, list of image paths for bulk process or not\\n    :return: list of image paths, list of image paths for bulk process or not',\n",
       "    ':param img1_path:']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        bulk_size: int = 500,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        embeddings = self.embedding_function.embed_documents(list(texts))\\n        _validate_embeddings_and_bulk_size(len(embeddings), bulk_size)\\n        text_field = _get_kwargs_value(kwargs, \"text_field\", \"text\")\\n        dim = len(embeddings[0])\\n        engine = _get_kwargs_value(kwargs, \"engine\", \"nmslib\")\\n        space_type = _get_kwargs_value(kwargs, \"space_type\", \"l2\")\\n        ef_search = _get_kwargs_value(kwargs, \"ef_search\", 512)\\n        ef_construction = _get_kwargs_value(kwargs, \"ef_construction\", 512)\\n        m = _get_kwargs_value(kwargs, \"m\", 16)\\n        vector_field = _get_kwargs_value(kwargs, \"vector_field\", \"vector_field\")\\n        max_chunk_bytes = _get_kwargs_value(kwargs, \"max_chunk_bytes\", 1 * 1024 * 1024)\\n\\n        _validate_aoss_with_engines(self.is_aoss, engine)\\n\\n        mapping = _default_text_mapping(\\n            dim, engine, space_type, ef_search, ef_construction, m, vector_field\\n        )\\n\\n        return _bulk_ingest_embeddings(\\n            self.client,\\n            self.index_name,\\n            embeddings,\\n            texts,\\n            metadatas=metadatas,\\n            ids=ids,\\n            vector_field=vector_field,\\n            text_field=text_field,\\n            mapping=mapping,\\n            max_chunk_bytes=max_chunk_bytes,\\n            is_aoss=self.is_aoss,\\n        )',\n",
       "   'd': 'Run more texts through the embeddings and add to the vectorstore.\\n\\nArgs:\\n    texts: Iterable of strings to add to the vectorstore.\\n    metadatas: Optional list of metadatas associated with the texts.\\n    ids: Optional list of ids to associate with the texts.\\n    bulk_size: Bulk API request count; Default: 500\\n\\nReturns:\\n    List of ids from adding the texts into the vectorstore.\\n\\nOptional Args:\\n    vector_field: Document field embeddings are stored in. Defaults to\\n    \"vector_field\".\\n\\n    text_field: Document field the text of the document is stored in. Defaults\\n    to \"text\".',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def index(\\n        self,\\n        dims_length: Union[int, None],\\n        vector_query_field: str,\\n        similarity: Union[DistanceStrategy, None],\\n    ) -> Dict:\\n',\n",
       "   'd': 'Executes when the index is created.\\n\\nArgs:\\n    dims_length: Numeric length of the embedding vectors,\\n                or None if not using vector-based query.\\n    vector_query_field: The field containing the vector\\n                        representations in the index.\\n    similarity: The similarity strategy to use,\\n                or None if not using one.\\n\\nReturns:\\n    Dict: The Elasticsearch settings and mappings for the strategy.',\n",
       "   'l': True,\n",
       "   'g': ['(int, int, int, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str',\n",
       "    '_init__',\n",
       "    '(int, int, int, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str',\n",
       "    '(int, int, int, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[Dict[Any, Any]]] = None,\\n        model_id: Optional[str] = None,\\n        refresh_indices: bool = False,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        if not self.client.indices.exists(index=self.index_name):\\n            dims = kwargs.get(\"dims\")\\n\\n            if dims is None:\\n                raise ValueError(\"ElasticKnnSearch requires \\'dims\\' parameter\")\\n\\n            similarity = kwargs.get(\"similarity\")\\n            optional_args = {}\\n\\n            if similarity is not None:\\n                optional_args[\"similarity\"] = similarity\\n\\n            mapping = self._default_knn_mapping(dims=dims, **optional_args)\\n            self.create_knn_index(mapping)\\n\\n        embeddings = self.embedding.embed_documents(list(texts))\\n\\n\\n        body: List[Mapping[str, Any]] = []\\n        for text, vector in zip(texts, embeddings):\\n            body.extend(\\n                [\\n                    {\"index\": {\"_index\": self.index_name}},\\n                    {\"text\": text, \"vector\": vector},\\n                ]\\n            )\\n\\n        responses = self.client.bulk(operations=body)\\n\\n        ids = [\\n            item[\"index\"][\"_id\"]\\n            for item in responses[\"items\"]\\n            if item[\"index\"][\"result\"] == \"created\"\\n        ]\\n\\n        if refresh_indices:\\n            self.client.indices.refresh(index=self.index_name)\\n\\n        return ids',\n",
       "   'd': 'Add a list of texts to the Elasticsearch index.\\n\\nArgs:\\n    texts (Iterable[str]): The texts to add to the index.\\n    metadatas (List[Dict[Any, Any]], optional): A list of metadata dictionaries\\n        to associate with the texts.\\n    model_id (str, optional): The ID of the model to use for transforming the\\n        texts into vectors.\\n    refresh_indices (bool, optional): Whether to refresh the Elasticsearch\\n        indices after adding the texts.\\n    **kwargs: Arbitrary keyword arguments.\\n\\nReturns:\\n    A list of IDs for the added texts.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'Returns a list of tokens and the length of the string, as well as a\\n  list of token ranges.',\n",
       "    '']},\n",
       "  {'c': '    def input_keys(self) -> List[str]:\\n',\n",
       "   'd': 'Create k-shot example selector using example list and embeddings.\\n\\nReshuffles examples dynamically based on query similarity.\\n\\nArgs:\\n    examples: List of examples to use in the prompt.\\n    embeddings: An iniialized embedding API interface, e.g. OpenAIEmbeddings().\\n    vectorstore_cls: A vector store DB interface class, e.g. FAISS.\\n    k: Number of examples to select\\n    input_keys: If provided, the search is based on the input variables\\n        instead of all variables.\\n    vectorstore_cls_kwargs: optional kwargs containing url for vector store\\n\\nReturns:\\n    The ExampleSelector instantiated, backed by a vector store.',\n",
       "   'l': False,\n",
       "   'g': [':param payload:\\n        :return:',\n",
       "    ':param payload: 1 - windows/shell_reverse_tcp, 2 - windows/meterpreter/reverse_tcp, 3 - windows/vncinject/reverse_tcp, 4 - windows/x64/shell_reverse_tcp, 5 - windows/x64/meterpreter/reverse_tcp, 6 - windows/x64/shell_bind_tcp, 7 - windows/meterpreter/reverse_https\\n    :return: 1 - windows/shell_reverse_tcp, 2 - windows/meterpreter/reverse_tcp, 3 - windows/vncinject/reverse_tcp, 4 - windows/x64/shell_reverse_tcp, 5 - windows/x64/meterpreter/reverse_tcp, 6 - windows/x64/shell_bind_tcp, 7 - windows/meterpreter/reverse_https\\n    :return: 1 - windows/shell_reverse_tcp, 2 - windows/meterpreter/reverse_tcp, 3 - windows/vncinject/reverse_tcp, 4 - windows/x64/shell_reverse_tcp, 5 - windows',\n",
       "    ':param payload:\\n        :type payload: str\\n        :return:\\n        :rtype: str',\n",
       "    '1: \"windows/shell_reverse_tcp\"\\n      2: \"windows/meterpreter/reverse_tcp\"\\n      3: \"windows/vncinject/reverse_tcp\"\\n      4: \"windows/x64/shell_reverse_tcp\"\\n      5: \"windows/x64/meterpreter/reverse_tcp\"\\n      6: \"windows/x64/shell_bind_tcp\"\\n      7: \"windows/meterpreter/reverse_https\"\\n      8: \"windows/x64/shell_reverse_tcp\"\\n      9: \"windows/x64/meterpreter/reverse_tcp\"\\n      10: \"windows/x64/shell_bind_tcp\"\\n      11: \"windows/meterpreter/reverse_https\"\\n      12: \"windows/x64/shell_reverse_tcp\"\\n      13: \"windows/x64/meterpreter/reverse_tcp\"\\n      14: \"windows/x64/shell_bind_tcp\"\\n      15: \"windows/meterpreter/reverse_https\"\\n      16: \"windows/x64']},\n",
       "  {'c': 'def plan(self, inputs: dict, callbacks: Callbacks = None, **kwargs: Any) -> Plan:\\n    inputs[\"tools\"] = [\\n        f\"{tool.name}: {tool.description}\" for tool in inputs[\"hf_tools\"]\\n    ]\\n    llm_response = self.llm_chain.run(**inputs, stop=self.stop, callbacks=callbacks)\\n    return self.output_parser.parse(llm_response, inputs[\"hf_tools\"])',\n",
       "   'd': 'Compute the score for a prediction and reference.\\n\\nArgs:\\n    inputs (Dict[str, Any]): The input data.\\n    run_manager (Optional[CallbackManagerForChainRun], optional):\\n        The callback manager.\\n\\nReturns:\\n    Dict[str, Any]: The computed score.',\n",
       "   'l': False,\n",
       "   'g': [\".search_path = '$' if self.search_path is None else self.search_path.strip() + '/'\",\n",
       "    '_call__',\n",
       "    '_call__',\n",
       "    '_call__']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        messages = self.format_messages(**kwargs)\\n        return get_buffer_string(messages)',\n",
       "   'd': 'Lazily parse the blob.',\n",
       "   'l': False,\n",
       "   'g': ['', '', \"'Align the object on the border of the frame.'\", '']},\n",
       "  {'c': '    def __init__(self, **kwargs: Any):\\n        super().__init__(**kwargs)\\n        try:\\n            import sentence_transformers\\n\\n        except ImportError as exc:\\n            raise ImportError(\\n                \"Could not import sentence_transformers python package. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            ) from exc\\n\\n        self.client = sentence_transformers.SentenceTransformer(\\n            self.model_name, cache_folder=self.cache_folder, **self.model_kwargs\\n        )',\n",
       "   'd': 'Initialize the sentence_transformer.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def similarity_search(\\n        self, query: str, k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        docs_with_scores = self.similarity_search_with_score(query, k, **kwargs)\\n        return [doc[0] for doc in docs_with_scores]',\n",
       "   'd': 'Test the APIOperation class.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    ':param logins: dict of logins to usernames\\n      :param ins: input stream\\n      :param out: output stream',\n",
       "    '(logins, ins=sys.stdin, out=None) ->']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        huggingface_api_key = get_from_dict_or_env(\\n            values, \"huggingface_api_key\", \"HUGGINGFACE_API_KEY\"\\n        )\\n        try:\\n            from petals import DistributedBloomForCausalLM\\n            from transformers import BloomTokenizerFast\\n\\n            model_name = values[\"model_name\"]\\n            values[\"tokenizer\"] = BloomTokenizerFast.from_pretrained(model_name)\\n            values[\"client\"] = DistributedBloomForCausalLM.from_pretrained(model_name)\\n            values[\"huggingface_api_key\"] = huggingface_api_key\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers or petals python package.\"\\n                \"Please install with `pip install -U transformers petals`.\"\\n            )\\n        return values',\n",
       "   'd': 'Validate that api key and python package exists in environment.',\n",
       "   'l': True,\n",
       "   'g': [\":param kernel_type: 'primal' or 'dual'\\n  :param dim: dimension of the input\\n  :param lamb: coefficient of the l2-norm of the input\\n  :param gamma: coefficient of the l2-norm of the projection\",\n",
       "    'Parameters\\n  ----------\\n  kernel_type : str\\n      primal or primal_plus',\n",
       "    '',\n",
       "    ':param kernel_type: primal or polynomial\\n    :param dim: dimension of the input space\\n    :param lamb: regularization parameter\\n    :param gamma: coefficient of the polynomial kernel']},\n",
       "  {'c': 'def create_extraction_chain_pydantic(\\n    pydantic_schema: Any,\\n    llm: BaseLanguageModel,\\n    prompt: Optional[BasePromptTemplate] = None,\\n    verbose: bool = False,\\n) -> Chain:\\n    class PydanticSchema(BaseModel):\\n        info: List[pydantic_schema]\\n\\n    openai_schema = pydantic_schema.schema()\\n    openai_schema = _resolve_schema_references(\\n        openai_schema, openai_schema.get(\"definitions\", {})\\n    )\\n\\n    function = _get_extraction_function(openai_schema)\\n    extraction_prompt = prompt or ChatPromptTemplate.from_template(_EXTRACTION_TEMPLATE)\\n    output_parser = PydanticAttrOutputFunctionsParser(\\n        pydantic_schema=PydanticSchema, attr_name=\"info\"\\n    )\\n    llm_kwargs = get_llm_kwargs(function)\\n    chain = LLMChain(\\n        llm=llm,\\n        prompt=extraction_prompt,\\n        llm_kwargs=llm_kwargs,\\n        output_parser=output_parser,\\n        verbose=verbose,\\n    )\\n    return chain',\n",
       "   'd': 'Get docs to run questioning over.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Test that the ratio is in the correct range.',\n",
       "    'Test that the ratio must be between 0 and 1.0.',\n",
       "    'Test that bad ratios raise an error.']},\n",
       "  {'c': '    def on_tool_error(\\n        self,\\n        error: BaseException,\\n        *,\\n        run_id: UUID,\\n        **kwargs: Any,\\n    ) -> None:\\n        if not run_id:\\n            raise TracerException(\"No run_id provided for on_tool_error callback.\")\\n        tool_run = self.run_map.get(str(run_id))\\n        if tool_run is None or tool_run.run_type != \"tool\":\\n            raise TracerException(f\"No tool Run found to be traced for {run_id}\")\\n\\n        tool_run.error = repr(error)\\n        tool_run.end_time = datetime.utcnow()\\n        tool_run.events.append({\"name\": \"error\", \"time\": tool_run.end_time})\\n        self._end_trace(tool_run)\\n        self._on_tool_error(tool_run)',\n",
       "   'd': 'Get the token IDs using the tiktoken package.',\n",
       "   'l': False,\n",
       "   'g': [':param source:\\n    :param visited:\\n    :return:',\n",
       "    ':param source: \\n    :param visited: \\n    :return: \\n    :rtype:',\n",
       "    ':param source: int\\n    :param visited: list\\n    :return:',\n",
       "    '']},\n",
       "  {'c': '    def load(self) -> List[Document]:\\n        account = self._auth()\\n        storage = account.storage()\\n        drive = storage.get_drive(self.drive_id)\\n        docs: List[Document] = []\\n        if not drive:\\n            raise ValueError(f\"There isn\\'t a drive with id {self.drive_id}.\")\\n        if self.folder_path:\\n            folder = self._get_folder_from_path(drive=drive)\\n            docs.extend(self._load_from_folder(folder=folder))\\n        elif self.object_ids:\\n            docs.extend(self._load_from_object_ids(drive=drive))\\n        return docs',\n",
       "   'd': 'Loads all supported document files from the specified OneDrive drive\\nand return a list of Document objects.\\n\\nReturns:\\n    List[Document]: A list of Document objects\\n    representing the loaded documents.\\n\\nRaises:\\n    ValueError: If the specified drive ID\\n    does not correspond to a drive in the OneDrive storage.',\n",
       "   'l': True,\n",
       "   'g': ['(str) event\\n(list) params\\n(dict) kwparams\\n(function) handler\\n\\nReturn the result of the event handler, or None if no handler is found.',\n",
       "    \"_event(self, event, *params, **kwparams)\\n\\n    Return a function that will be called when the event is\\n    triggered.  The function will be called with the event\\n    and any other parameters that are passed to the function.\\n    The function should return None or a value that will be\\n    returned as the return value of the event handler.  The\\n    function will be called in the event handler's thread.\\n\\n    :param event: The event to be handled.\\n    :param params: The parameters to be passed to the function.\\n    :param kwparams: The keyword parameters to be passed to the function.\\n    :return: The function that will be called when the event is\\n    triggered.  The function will be called with the event and any\\n    other parameters that are passed to the function.  The function\\n    will be called in the event handler's thread.  The function will\\n    return the value that will be returned as the return value of the\\n    function.  The function will be called with the event and any\\n    other parameters that are passed to the function.  The function\\n    will be called in the event handler's thread.\",\n",
       "    '(str)',\n",
       "    '(str)']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        cls._try_init_vertexai(values)\\n        tuned_model_name = values.get(\"tuned_model_name\")\\n        model_name = values[\"model_name\"]\\n        try:\\n            if not is_codey_model(model_name):\\n                from vertexai.preview.language_models import TextGenerationModel\\n\\n                if tuned_model_name:\\n                    values[\"client\"] = TextGenerationModel.get_tuned_model(\\n                        tuned_model_name\\n                    )\\n                else:\\n                    values[\"client\"] = TextGenerationModel.from_pretrained(model_name)\\n            else:\\n                from vertexai.preview.language_models import CodeGenerationModel\\n\\n                if tuned_model_name:\\n                    values[\"client\"] = CodeGenerationModel.get_tuned_model(\\n                        tuned_model_name\\n                    )\\n                else:\\n                    values[\"client\"] = CodeGenerationModel.from_pretrained(model_name)\\n        except ImportError:\\n            raise_vertex_import_error()\\n        return values',\n",
       "   'd': 'Get docs to run questioning over.',\n",
       "   'l': False,\n",
       "   'g': ['Extract the best result from a list of choices.\\n\\n    Parameters\\n    ----------\\n    query : str\\n        The query to be used to extract the best result.\\n    choices : list\\n        A list of choices to be used to extract the best result.\\n    processor : callable, optional\\n        A function that processes the extracted result.\\n    scorer : callable, optional\\n        A function that scores the extracted result.\\n    score_cutoff : int, optional\\n        The minimum score that a result must have to be returned.\\n\\n    Returns\\n    -------\\n    result : tuple\\n        The best result.\\n\\n    Notes\\n    -----\\n    This function is a wrapper for `extract` function.',\n",
       "    ':param query:\\n  :param choices:\\n  :param processor:\\n  :param scorer:\\n  :param score_cutoff:\\n  :return:',\n",
       "    '',\n",
       "    ':param query:\\n      :param choices:\\n      :param processor:\\n      :param scorer:\\n      :param score_cutoff:\\n  :return:\\n  :rtype:']},\n",
       "  {'c': '    def __init__(self, **kwargs: Any):\\n        super().__init__(**kwargs)\\n        try:\\n            import sentence_transformers\\n\\n        except ImportError as exc:\\n            raise ImportError(\\n                \"Could not import sentence_transformers python package. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            ) from exc\\n\\n        self.client = sentence_transformers.SentenceTransformer(\\n            self.model_name, cache_folder=self.cache_folder, **self.model_kwargs\\n        )',\n",
       "   'd': 'Use the LLM to check the query.',\n",
       "   'l': False,\n",
       "   'g': ['_bwd(self, dLdy, X):',\n",
       "    '(dLdy, X) -> (dX, dW, dB)',\n",
       "    '(dLdy, X) -> (dX, dW, dB)',\n",
       "    '(dLdy, X) -> (dX, dW, dB)']},\n",
       "  {'c': 'def type(self) -> str:\\n',\n",
       "   'd': 'Validate that api key and python package exists in environment.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def parse_result(self, result: List[Generation]) -> T:\\n        return self.parse(result[0].text)',\n",
       "   'd': 'Parse a list of candidate model Generations into a specific format.\\n\\nThe return value is parsed from only the first Generation in the result, which\\n    is assumed to be the highest-likelihood Generation.\\n\\nArgs:\\n    result: A list of Generations to be parsed. The Generations are assumed\\n        to be different candidate outputs for a single model input.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': True,\n",
       "   'g': ['Save the current instance.\\n\\n  :param commit: If True, the instance will be saved to the database.\\n  :type commit: bool',\n",
       "    '',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': '    def update(\\n        self,\\n        keys: Sequence[str],\\n        *,\\n        group_ids: Optional[Sequence[Optional[str]]] = None,\\n        time_at_least: Optional[float] = None,\\n    ) -> None:\\n        if group_ids is None:\\n            group_ids = [None] * len(keys)\\n\\n        if len(keys) != len(group_ids):\\n            raise ValueError(\\n                f\"Number of keys ({len(keys)}) does not match number of \"\\n                f\"group_ids ({len(group_ids)})\"\\n            )\\n\\n\\n\\n\\n\\n\\n\\n\\n        update_time = self.get_time()\\n\\n        if time_at_least and update_time < time_at_least:\\n\\n            raise AssertionError(f\"Time sync issue: {update_time} < {time_at_least}\")\\n\\n        records_to_upsert = [\\n            {\\n                \"key\": key,\\n                \"namespace\": self.namespace,\\n                \"updated_at\": update_time,\\n                \"group_id\": group_id,\\n            }\\n            for key, group_id in zip(keys, group_ids)\\n        ]\\n\\n        with self._make_session() as session:\\n            if self.dialect == \"sqlite\":\\n                from sqlalchemy.dialects.sqlite import insert\\n\\n\\n\\n                insert_stmt = insert(UpsertionRecord).values(records_to_upsert)\\n                stmt = insert_stmt.on_conflict_do_update(\\n                    [UpsertionRecord.key, UpsertionRecord.namespace],\\n                    set_=dict(\\n\\n                        updated_at=insert_stmt.excluded.updated_at,\\n                        group_id=insert_stmt.excluded.group_id,\\n                    ),\\n                )\\n            elif self.dialect == \"postgresql\":\\n                from sqlalchemy.dialects.postgresql import insert\\n\\n\\n\\n                insert_stmt = insert(UpsertionRecord).values(records_to_upsert)\\n                stmt = insert_stmt.on_conflict_do_update(\\n                    \"uix_key_namespace\",\\n                    set_=dict(\\n\\n                        updated_at=insert_stmt.excluded.updated_at,\\n                        group_id=insert_stmt.excluded.group_id,\\n                    ),\\n                )\\n            else:\\n                raise NotImplementedError(f\"Unsupported dialect {self.dialect}\")\\n\\n            session.execute(stmt)\\n            session.commit()',\n",
       "   'd': 'Upsert records into the SQLite database.',\n",
       "   'l': True,\n",
       "   'g': ['(optional)', '', '', '']},\n",
       "  {'c': '    def lazy_load(self) -> Iterator[Document]:\\n        try:\\n            from qcloud_cos import CosS3Client\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import cos-python-sdk-v5 python package. \"\\n                \"Please install it with `pip install cos-python-sdk-v5`.\"\\n            )\\n\\n\\n        client = CosS3Client(self.conf)\\n        with tempfile.TemporaryDirectory() as temp_dir:\\n            file_path = f\"{temp_dir}/{self.bucket}/{self.key}\"\\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n\\n            client.download_file(\\n                Bucket=self.bucket, Key=self.key, DestFilePath=file_path\\n            )\\n            loader = UnstructuredFileLoader(file_path)\\n\\n            return iter(loader.load())',\n",
       "   'd': 'Load QA Eval Chain from LLM.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def type(self) -> str:\\n    return \"function\"',\n",
       "   'd': 'Type of the message, used for serialization.',\n",
       "   'l': True,\n",
       "   'g': ['(bool)',\n",
       "    '_go(self):',\n",
       "    '_go(self):',\n",
       "    '(self):\\n    def __call__(self):\\n        return self.go()']},\n",
       "  {'c': 'def requires_reference(self) -> bool:\\n    return True',\n",
       "   'd': 'Whether the evaluation requires a reference text.',\n",
       "   'l': True,\n",
       "   'g': ['_mayRaiseExceptionAbs',\n",
       "    '_mayRaiseExceptionAbs',\n",
       "    '_mayRaiseExceptionAbs(self, exception_type):\\n    :param exception_type:\\n    :return:',\n",
       "    '.']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: List[str],\\n        metadatas: Optional[List[dict]] = None,\\n        doc_metadata: Optional[dict] = {},\\n    ) -> None:\\n        self.vectorstore.add_texts(texts, metadatas, doc_metadata)',\n",
       "   'd': 'Add text to the Vectara vectorstore.\\n\\nArgs:\\n    texts (List[str]): The text\\n    metadatas (List[dict]): Metadata dicts, must line up with existing store',\n",
       "   'l': True,\n",
       "   'g': ['_make_save_file_name(self, store_type: str) -> str:\\n\\n    :param store_type: str\\n    :return: str\\n    :rtype: str\\n    :raises:',\n",
       "    \"_make_save_file_name(store_type: str) -> str:\\n\\n    :param store_type: 'data' or 'error'\\n    :return:\",\n",
       "    '_make_save_file_name(self, store_type: str) -> str:',\n",
       "    '_make_save_file_name(self, store_type: str) -> str:\\n\\n    _make_save_file_name(self, store_type) -> str:\\n\\n    _make_save_file_name(self, store_type, crawler_type_var.get()) -> str:\\n\\n    _make_save_file_name(self, store_type, crawler_type_var.get(), language_var.get()) -> str:\\n\\n    _make_save_file_name(self, store_type, crawler_type_var.get(), language_var.get(),\\n                         crawler_lang_var.get()) -> str:\\n\\n    _make_save_file_name(self, store_type, crawler_type_var.get(), language_var.get(),\\n                         crawler_lang_var.get(), country_var.get()) -> str:\\n\\n    _make_save_file_name(self, store_type, crawler_type_var.get(), language_var.get(),\\n                         crawler_lang_var.get(), country_var.get(),\\n                         crawler_country_var.get()) -> str:']},\n",
       "  {'c': 'def _split_sources(self, answer: str) -> Tuple[str, str]:\\n    if re.search(r\"SOURCES?[:\\\\s]\", answer, re.IGNORECASE):\\n        answer, sources = re.split(\\n            r\"SOURCES?[:]|QUESTION:\\\\s\", answer, flags=re.IGNORECASE\\n        )[:2]\\n        sources = re.split(r\"\\\\n\", sources)[0].strip()\\n    else:\\n        sources = \"\"\\n    return answer, sources',\n",
       "   'd': 'Split sources from answer.',\n",
       "   'l': True,\n",
       "   'g': ['Returns an iterator over the list elements.', '', '', '']},\n",
       "  {'c': '    def from_documents(\\n        cls,\\n        documents: List[Document],\\n        embedding: Optional[Embeddings] = None,\\n        **kwargs: Any,\\n    ) -> \"ElasticsearchStore\":\\n        elasticsearchStore = ElasticsearchStore._create_cls_from_kwargs(\\n            embedding=embedding, **kwargs\\n        )\\n\\n        elasticsearchStore.add_documents(documents)\\n\\n        return elasticsearchStore',\n",
       "   'd': 'Lookup llm generations in cache by prompt and associated model and settings.\\n\\nArgs:\\n    prompt (str): The prompt run through the language model.\\n    llm_string (str): The language model version and settings.\\n\\nRaises:\\n    SdkException: Momento service or network error\\n\\nReturns:\\n    Optional[RETURN_VAL_TYPE]: A list of language model generations.',\n",
       "   'l': False,\n",
       "   'g': ['(\\n    router: Optional[\"APIRouter\"] = None,\\n    router_prefix: str = \"/api/v1/awel/trigger\",\\n) -> None:',\n",
       "    '(router: Optional[\"APIRouter\"] = None,\\n        router_prefix: str = \"/api/v1/awel/trigger\",) -> None:',\n",
       "    '_init_',\n",
       "    '(router: Optional[\"APIRouter\"] = None, router_prefix: str = \"/api/v1/awel/trigger\") -> None:']},\n",
       "  {'c': 'def _chain_type(self) -> str:\\n    return \"vector_db_qa\"',\n",
       "   'd': 'Return the chain type.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def configure(\\n        cls,\\n        inheritable_callbacks: Callbacks = None,\\n        local_callbacks: Callbacks = None,\\n        verbose: bool = False,\\n        inheritable_tags: Optional[List[str]] = None,\\n        local_tags: Optional[List[str]] = None,\\n        inheritable_metadata: Optional[Dict[str, Any]] = None,\\n        local_metadata: Optional[Dict[str, Any]] = None,\\n    ) -> AsyncCallbackManager:\\n        return _configure(\\n            cls,\\n            inheritable_callbacks,\\n            local_callbacks,\\n            verbose,\\n            inheritable_tags,\\n            local_tags,\\n            inheritable_metadata,\\n            local_metadata,\\n        )',\n",
       "   'd': 'Configure the async callback manager.\\n\\nArgs:\\n    inheritable_callbacks (Optional[Callbacks], optional): The inheritable\\n        callbacks. Defaults to None.\\n    local_callbacks (Optional[Callbacks], optional): The local callbacks.\\n        Defaults to None.\\n    verbose (bool, optional): Whether to enable verbose mode. Defaults to False.\\n    inheritable_tags (Optional[List[str]], optional): The inheritable tags.\\n        Defaults to None.\\n    local_tags (Optional[List[str]], optional): The local tags.\\n        Defaults to None.\\n    inheritable_metadata (Optional[Dict[str, Any]], optional): The inheritable\\n        metadata. Defaults to None.\\n    local_metadata (Optional[Dict[str, Any]], optional): The local metadata.\\n        Defaults to None.\\n\\nReturns:\\n    AsyncCallbackManager: The configured async callback manager.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    ':param s: string to xor\\n    :param k: key to xor with\\n    :return: string after xoring with key\\n    :type s: str\\n    :type k: str\\n    :rtype: str\\n    :Example:\\n        >>> s = \"hello\"\\n        >>> k = \"world\"\\n        >>> s.xorstring(k)\\n        \\'hllw\\'\\n        >>> s = \"hello\"\\n        >>> k = \"world\"\\n        >>> k.xorstring(s)\\n        \\'hllw\\'\\n        >>> s = \"hello\"\\n        >>> k = \"world\"\\n        >>> k.xorstring(s)\\n        \\'hllw\\'\\n        >>> s = \"hello\"\\n        >>> k = \"world\"\\n        >>> k.xorstring(s)\\n        \\'hllw\\'\\n        >>> s = \"hello\"\\n        >>> k = \"world\"\\n        >>> k.xorstring(s)\\n        \\'hllw\\'\\n        >>> s = \"hello\"\\n        >>> k = \"world\"\\n        >>> k.xorstring(s)\\n        \\'hllw\\'',\n",
       "    '',\n",
       "    ':param s: string to be encrypted\\n    :param k: key to be used for encryption\\n    :return: string after encryption']},\n",
       "  {'c': 'def get_pipeline() -> Any:\\n    model_id = \"facebook/bart-base\"\\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\\n    model = AutoModelForCausalLM.from_pretrained(model_id)\\n    return pipeline(\"feature-extraction\", model=model, tokenizer=tokenizer)',\n",
       "   'd': 'Create a chat prompt template from a template string.\\n\\nCreates a chat template consisting of a single message assumed to be from\\nthe human.\\n\\nArgs:\\n    template: template string\\n    **kwargs: keyword arguments to pass to the constructor.\\n\\nReturns:\\n    A new instance of this class.',\n",
       "   'l': False,\n",
       "   'g': ['_get_opsi_fleet(self):',\n",
       "    '_get(self):',\n",
       "    '_get(self):',\n",
       "    '_get(self):']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\\n        embedding_function: Optional[Embeddings] = None,\\n        persist_directory: Optional[str] = None,\\n        client_settings: Optional[chromadb.config.Settings] = None,\\n        collection_metadata: Optional[Dict] = None,\\n        client: Optional[chromadb.Client] = None,\\n        relevance_score_fn: Optional[Callable[[float], float]] = None,\\n    ) -> None:\\n        try:\\n            import chromadb\\n            import chromadb.config\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import chromadb python package. \"\\n                \"Please install it with `pip install chromadb`.\"\\n            )\\n\\n        if client is not None:\\n            self._client_settings = client_settings\\n            self._client = client\\n            self._persist_directory = persist_directory\\n        else:\\n            if client_settings:\\n                _client_settings = client_settings\\n            elif persist_directory:\\n\\n                major, minor, _ = chromadb.__version__.split(\".\")\\n                if int(major) == 0 and int(minor) < 4:\\n                    _client_settings = chromadb.config.Settings(\\n                        chroma_db_impl=\"duckdb+parquet\",\\n                    )\\n                else:\\n                    _client_settings = chromadb.config.Settings(is_persistent=True)\\n                _client_settings.persist_directory = persist_directory\\n            else:\\n                _client_settings = chromadb.config.Settings()\\n            self._client_settings = _client_settings\\n            self._client = chromadb.Client(_client_settings)\\n            self._persist_directory = (\\n                _client_settings.persist_directory or persist_directory\\n            )\\n\\n        self._embedding_function = embedding_function\\n        self._collection = self._client.get_or_create_collection(\\n            name=collection_name,\\n            embedding_function=self._embedding_function.embed_documents\\n            if self._embedding_function is not None\\n            else None,\\n            metadata=collection_metadata,\\n        )\\n        self.override_relevance_score_fn = relevance_score_fn',\n",
       "   'd': 'Lazily load documents.',\n",
       "   'l': False,\n",
       "   'g': ['_init_', '_init_', '_init_', '_init_']},\n",
       "  {'c': 'def _get_builtin_translator(vectorstore: VectorStore) -> Visitor:\\n    vectorstore_cls = vectorstore.__class__\\n    BUILTIN_TRANSLATORS: Dict[Type[VectorStore], Type[Visitor]] = {\\n        Pinecone: PineconeTranslator,\\n        Chroma: ChromaTranslator,\\n        DashVector: DashvectorTranslator,\\n        Weaviate: WeaviateTranslator,\\n        Qdrant: QdrantTranslator,\\n        MyScale: MyScaleTranslator,\\n        DeepLake: DeepLakeTranslator,\\n        ElasticsearchStore: ElasticsearchTranslator,\\n        Milvus: MilvusTranslator,\\n    }\\n    if vectorstore_cls not in BUILTIN_TRANSLATORS:\\n        raise ValueError(\\n            f\"Self query retriever with Vector Store type {vectorstore_cls}\"\\n            f\" not supported.\"\\n        )\\n    if isinstance(vectorstore, Qdrant):\\n        return QdrantTranslator(metadata_key=vectorstore.metadata_payload_key)\\n    elif isinstance(vectorstore, MyScale):\\n        return MyScaleTranslator(metadata_key=vectorstore.metadata_column)\\n    return BUILTIN_TRANSLATORS[vectorstore_cls]()',\n",
       "   'd': 'Get the translator class corresponding to the vector store class.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Writes data to the stream.\\n\\n    :param data: data to write to the stream.\\n\\n    :return: None.',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace',\n",
       "   'd': 'Process a list of pages into a list of documents.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        web_path: Union[str, List[str]],\\n        header_template: Optional[dict] = None,\\n        verify_ssl: Optional[bool] = True,\\n        proxies: Optional[dict] = None,\\n        requests_per_second: int = 2,\\n        requests_kwargs: Dict[str, Any] = {},\\n        raise_for_status: bool = False,\\n    ):\\n        if isinstance(web_path, str):\\n            self.web_paths = [web_path]\\n        elif isinstance(web_path, List):\\n            self.web_paths = web_path\\n\\n        headers = header_template or default_header_template\\n        if not headers.get(\"User-Agent\"):\\n            try:\\n                from fake_useragent import UserAgent\\n\\n                headers[\"User-Agent\"] = UserAgent().random\\n            except ImportError:\\n                logger.info(\\n                    \"fake_useragent not found, using default user agent.\"\\n                    \"To get a realistic header for requests, \"\\n                    \"`pip install fake_useragent`.\"\\n                )\\n\\n        self.session = requests.Session()\\n        self.session.headers = dict(headers)\\n        self.session.verify = verify_ssl\\n\\n        if proxies:\\n            self.session.proxies.update(proxies)\\n\\n        self.requests_per_second = requests_per_second\\n        self.requests_kwargs = requests_kwargs\\n        self.raise_for_status = raise_for_status',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': False,\n",
       "   'g': ['This function is used to ask a yes or no question.\\n    :param question: The question to ask.',\n",
       "    \"Prompt user for a Yes/No answer.\\n    :param question: The question to ask the user.\\n    :return: True if the user answered 'yes' or 'y', False if 'no' or 'n'.\",\n",
       "    \"Prompts user for a yes or no answer.\\n    :param question: The question to ask the user.\\n    :return: True if the user answered 'yes' or 'y', False if the user answered 'no' or 'n'.\\n    :rtype: bool\",\n",
       "    \":param question: A string that is the question to be asked.\\n  :returns: True if the user answers 'yes' or 'y' otherwise False.\"]},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        messages = self.format_messages(**kwargs)\\n        return get_buffer_string(messages)',\n",
       "   'd': 'Format the prompt with inputs generating a string.\\n\\nUse this method to generate a string representation of a prompt consisting\\nof chat messages.\\n\\nUseful for feeding into a string based completion language model or debugging.\\n\\nArgs:\\n    **kwargs: keyword arguments to use for formatting.\\n\\nReturns:\\n    A string representation of the prompt',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '(parent_state: BaseState | None = None, init_substates: bool = True, **kwargs) -> None:',\n",
       "    '',\n",
       "    '(parent_state: BaseState | None = None, init_substates: bool = True, **kwargs):']},\n",
       "  {'c': '    def _anonymize(self, text: str, language: Optional[str] = None) -> str:\\n        if language is None:\\n            language = self.supported_languages[0]\\n\\n        if language not in self.supported_languages:\\n            raise ValueError(\\n                f\"Language \\'{language}\\' is not supported. \"\\n                f\"Supported languages are: {self.supported_languages}. \"\\n                \"Change your language configuration file to add more languages.\"\\n            )\\n\\n        analyzer_results = self._analyzer.analyze(\\n            text,\\n            entities=self.analyzed_fields,\\n            language=language,\\n        )\\n\\n        filtered_analyzer_results = (\\n            self._anonymizer._remove_conflicts_and_get_text_manipulation_data(\\n                analyzer_results\\n            )\\n        )\\n\\n        anonymizer_results = self._anonymizer.anonymize(\\n            text,\\n            analyzer_results=analyzer_results,\\n            operators=self.operators,\\n        )\\n\\n        self._update_deanonymizer_mapping(\\n            text, filtered_analyzer_results, anonymizer_results\\n        )\\n\\n        return anonymizer_results.text',\n",
       "   'd': \"Test intervention chain correctly transforms\\nthe LLM's text completion into a setting-like object.\",\n",
       "   'l': False,\n",
       "   'g': ['', '', '_mul__', '_mul__']},\n",
       "  {'c': '    def max_marginal_relevance_search_by_vector(\\n        self,\\n        embedding: List[float],\\n        k: int = 4,\\n        fetch_k: int = 20,\\n        lambda_mult: float = 0.5,\\n        search_params: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        results = self._client.vector_search(\\n            self._index_name, [embedding], search_params, k\\n        )\\n\\n        mmr_selected = maximal_marginal_relevance(\\n            np.array([embedding], dtype=np.float32),\\n            [item[\"floatValues\"] for item in results[0][\"vectorWithDistances\"]],\\n            k=k,\\n            lambda_mult=lambda_mult,\\n        )\\n        selected = [\\n            results[0][\"vectorWithDistances\"][i][\"metaData\"] for i in mmr_selected\\n        ]\\n        return [\\n            Document(page_content=metadata.pop((self._text_key)), metadata=metadata)\\n            for metadata in selected\\n        ]',\n",
       "   'd': 'Return docs selected using the maximal marginal relevance.\\n\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\n\\nArgs:\\n    embedding: Embedding to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 4.\\n    fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n    lambda_mult: Number between 0 and 1 that determines the degree\\n                of diversity among the results with 0 corresponding\\n                to maximum diversity and 1 to minimum diversity.\\n                Defaults to 0.5.\\nReturns:\\n    List of Documents selected by maximal marginal relevance.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '获取微博内容\\n  :param info: 微博信息\\n  :param is_original: 是否是原创微博\\n  :return: 微博内容',\n",
       "    '获取微博内容\\n  :param info: 微博信息\\n  :param is_original: 是否是原创微博\\n  :return: 微博内容',\n",
       "    '获取微博内容\\n\\n  :param info: 微博信息\\n  :param is_original: 是否是原创\\n  :return: 微博内容']},\n",
       "  {'c': 'def _generate(\\n    self,\\n    prompts: List[str],\\n    stop: Optional[List[str]] = None,\\n    run_manager: Optional[CallbackManagerForLLMRun] = None,\\n    **kwargs: Any,\\n) -> LLMResult:\\n',\n",
       "   'd': 'Run the LLM on the given prompts.',\n",
       "   'l': True,\n",
       "   'g': ['Save model to the output_dir.\\n\\n  Args:\\n      trainer (transformers.Trainer): The trainer.\\n      output_dir (str): The output directory.',\n",
       "    'Save the model to the output directory.\\n\\n  Args:\\n      trainer: The trainer object.\\n      output_dir: The output directory.',\n",
       "    'Save model to output_dir.\\n\\n  Args:\\n      trainer (transformers.Trainer): The trainer.\\n      output_dir (str): The output directory.',\n",
       "    '']},\n",
       "  {'c': 'def cosine_similarity_top_k(\\n    X: Matrix,\\n    Y: Matrix,\\n    top_k: Optional[int] = 5,\\n    score_threshold: Optional[float] = None,\\n) -> Tuple[List[Tuple[int, int]], List[float]]:\\n    if len(X) == 0 or len(Y) == 0:\\n        return [], []\\n    score_array = cosine_similarity(X, Y)\\n    sorted_idxs = score_array.flatten().argsort()[::-1]\\n    top_k = top_k or len(sorted_idxs)\\n    top_idxs = sorted_idxs[:top_k]\\n    score_threshold = score_threshold or -1.0\\n    top_idxs = top_idxs[score_array.flatten()[top_idxs] > score_threshold]\\n    ret_idxs = [(x // score_array.shape[1], x % score_array.shape[1]) for x in top_idxs]\\n    scores = score_array.flatten()[top_idxs].tolist()\\n    return ret_idxs, scores',\n",
       "   'd': 'Row-wise cosine similarity with optional top-k and score threshold filtering.\\n\\nArgs:\\n    X: Matrix.\\n    Y: Matrix, same width as X.\\n    top_k: Max number of results to return.\\n    score_threshold: Minimum cosine similarity of results.\\n\\nReturns:\\n    Tuple of two lists. First contains two-tuples of indices (X_idx, Y_idx),\\n        second contains corresponding cosine similarities.',\n",
       "   'l': True,\n",
       "   'g': ['_next__\\n    :return:',\n",
       "    '',\n",
       "    'Return the next value.\\n  :return: None, the current image, None, the current text.\\n  :rtype: tuple(int, np.ndarray, int, str)\\n  :raises: StopIteration if there are no more values.\\n  :raises: TypeError if the current value is not a numpy array.\\n  :raises: TypeError if the current text is not a string.\\n  :raises: ValueError if the current value is not a numpy array.\\n  :raises: ValueError if the current text is not a string.\\n  :raises: ValueError if the current value is not a numpy array.\\n  :raises: ValueError if the current text is not a string.\\n  :raises: ValueError if the current value is not a numpy array.\\n  :raises: ValueError if the current text is not a string.\\n  :raises: ValueError if the current value is not a numpy array.\\n  :raises: ValueError if the current text is not a string.\\n  :raises: ValueError if the current value is not a numpy array.\\n  :raises: ValueError if the current text is not a string.\\n  :raises: ValueError if the current value is not a numpy array.',\n",
       "    '']},\n",
       "  {'c': '    def from_function(\\n        cls,\\n        func: Callable,\\n        name: Optional[str] = None,\\n        description: Optional[str] = None,\\n        return_direct: bool = False,\\n        args_schema: Optional[Type[BaseModel]] = None,\\n        infer_schema: bool = True,\\n        **kwargs: Any,\\n    ) -> StructuredTool:\\n        name = name or func.__name__\\n        description = description or func.__doc__\\n        assert (\\n            description is not None\\n        ), \"Function must have a docstring if description not provided.\"\\n\\n\\n\\n        description = f\"{name}{signature(func)} - {description.strip()}\"\\n        _args_schema = args_schema\\n        if _args_schema is None and infer_schema:\\n            _args_schema = create_schema_from_function(f\"{name}Schema\", func)\\n        return cls(\\n            name=name,\\n            func=func,\\n            args_schema=_args_schema,\\n            description=description,\\n            return_direct=return_direct,\\n            **kwargs,\\n        )',\n",
       "   'd': 'Create tool from a given function.\\n\\nA classmethod that helps to create a tool from a function.\\n\\nArgs:\\n    func: The function from which to create a tool\\n    name: The name of the tool. Defaults to the function name\\n    description: The description of the tool. Defaults to the function docstring\\n    return_direct: Whether to return the result directly or as a callback\\n    args_schema: The schema of the tool\\'s input arguments\\n    infer_schema: Whether to infer the schema from the function\\'s signature\\n    **kwargs: Additional arguments to pass to the tool\\n\\nReturns:\\n    The tool\\n\\nExamples:\\n    ... code-block:: python\\n        def add(a: int, b: int) -> int:\\n            \"\"\"Add two numbers\"\"\"\\n            return a + b\\n        tool = StructuredTool.from_function(add)\\n        tool.run(1, 2) # 3',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def index(\\n        self,\\n        dims_length: Union[int, None],\\n        vector_query_field: str,\\n        similarity: Union[DistanceStrategy, None],\\n    ) -> Dict:\\n        if similarity is DistanceStrategy.COSINE:\\n            similarityAlgo = \"cosine\"\\n        elif similarity is DistanceStrategy.EUCLIDEAN_DISTANCE:\\n            similarityAlgo = \"l2_norm\"\\n        elif similarity is DistanceStrategy.DOT_PRODUCT:\\n            similarityAlgo = \"dot_product\"\\n        else:\\n            raise ValueError(f\"Similarity {similarity} not supported.\")\\n\\n        return {\\n            \"mappings\": {\\n                \"properties\": {\\n                    vector_query_field: {\\n                        \"type\": \"dense_vector\",\\n                        \"dims\": dims_length,\\n                        \"index\": True,\\n                        \"similarity\": similarityAlgo,\\n                    },\\n                }\\n            }\\n        }',\n",
       "   'd': 'Formats response',\n",
       "   'l': False,\n",
       "   'g': ['Initialize the model with random points.\\n\\n    :param init_points: number of random points to initialize the model with.',\n",
       "    'Initialize the data space with random points.\\n\\n    :param init_points: number of random points to initialize with',\n",
       "    'Initialize the model with random points.\\n\\n    :param init_points: number of random points to initialize the model with.',\n",
       "    'Initialize the model with random points.\\n\\n    :param init_points: number of random points to initialize the model with.\\n    :type init_points: int\\n    :return: None\\n    :rtype: None\\n    :raises: None\\n    :']},\n",
       "  {'c': '    def get_task_attribute(self, query: str) -> Dict:\\n        task = self.get_task(query, fault_tolerant=True)\\n        params, error = load_query(query, fault_tolerant=True)\\n        if not isinstance(params, dict):\\n            return {\"Error\": error}\\n\\n        if params[\"attribute_name\"] not in task:\\n            return {\\n                \"Error\": f\"\"\"attribute_name = {params[\\'attribute_name\\']} was not\\nfound in task keys {task.keys()}. Please call again with one of the key names.\"\"\"\\n            }\\n\\n        return {params[\"attribute_name\"]: task[params[\"attribute_name\"]]}',\n",
       "   'd': 'Pass through to `knn_search`',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Check if events are no longer supported by dash.',\n",
       "    'Check if events are being used in a component.',\n",
       "    'Check if the events are no longer supported by dash.']},\n",
       "  {'c': '    def clear(self, **kwargs: Any) -> None:\\n        from gptcache import Cache\\n\\n        for gptcache_instance in self.gptcache_dict.values():\\n            gptcache_instance = cast(Cache, gptcache_instance)\\n            gptcache_instance.flush()\\n\\n        self.gptcache_dict.clear()',\n",
       "   'd': 'Clear cache.',\n",
       "   'l': True,\n",
       "   'g': ['_with_additional_config',\n",
       "    '(str, dict or None) -> PresetDefinition or None',\n",
       "    '(optional)  :param environment_dict:  The environment dictionary to use for this preset.\\n    :type environment_dict:  dict\\n    :returns:  The same instance of this class, with the environment dictionary merged into the existing\\n    environment dictionary.  If the environment dictionary is None, the entire environment dictionary is\\n    removed from the entire environment dictionary.  If the environment dictionary is not None, the\\n    entire environment dictionary is replaced with the new environment dictionary.  If the environment\\n    dictionary is None, the entire environment dictionary is removed from the entire environment\\n    dictionary.  If the environment dictionary is not None, the entire environment dictionary is replaced\\n    with the new environment dictionary.  If the environment dictionary is None, the entire environment\\n    dictionary is removed from the entire environment dictionary.',\n",
       "    '_with_additional_config']},\n",
       "  {'c': 'def validate_environment(cls, values: Dict) -> Dict:\\n    values[\"fireworks_api_key\"] = get_from_dict_or_env(\\n        values, \"fireworks_api_key\", \"FIREWORKS_API_KEY\"\\n    )\\n    return values',\n",
       "   'd': 'Initialize the PairwiseStringEvalChain from an LLM.\\n\\nArgs:\\n    llm (BaseLanguageModel): The LLM to use.\\n    prompt (PromptTemplate, optional): The prompt to use.\\n    **kwargs (Any): Additional keyword arguments.\\n\\nReturns:\\n    PairwiseStringEvalChain: The initialized PairwiseStringEvalChain.\\n\\nRaises:\\n    ValueError: If the input variables are not as expected.',\n",
       "   'l': False,\n",
       "   'g': ['_extract_code(self, response: str, separator: str = \"```\") -> str:',\n",
       "    '_extract_code(self, response: str, separator: str = \"```\") -> str:',\n",
       "    '_extract_code(self, response: str, separator: str = \"```\") -> str:',\n",
       "    '_extract_code(self, response: str, separator: str = \"```\") -> str:']},\n",
       "  {'c': 'def map(self, run: Run) -> Dict[str, str]:\\n    if run.run_type != \"llm\":\\n        raise ValueError(\"LLM RunMapper only supports LLM runs.\")\\n    elif not run.outputs:\\n        if run.error:\\n            raise ValueError(\\n                f\"Cannot evaluate errored LLM run {run.id}: {run.error}\"\\n            )\\n        else:\\n            raise ValueError(\\n                f\"Run {run.id} has no outputs. Cannot evaluate this run.\"\\n            )\\n    else:\\n        try:\\n            inputs = self.serialize_inputs(run.inputs)\\n        except Exception as e:\\n            raise ValueError(\\n                f\"Could not parse LM input from run inputs {run.inputs}\"\\n            ) from e\\n        try:\\n            output_ = self.serialize_outputs(run.outputs)\\n        except Exception as e:\\n            raise ValueError(\\n                f\"Could not parse LM prediction from run outputs {run.outputs}\"\\n            ) from e\\n        return {\"input\": inputs, \"prediction\": output_}',\n",
       "   'd': 'Maps the Run to a dictionary.',\n",
       "   'l': True,\n",
       "   'g': ['_units: dict[int, str]\\n_abilities: dict[int, Ability]\\n_general_abilities: set[int]\\n\\n:param data: data.py instance\\n:type data: data.py',\n",
       "    '.init(self, data)\\n\\n    :param data: :class:`~pylon.data.Data`\\n\\n    :rtype: :class:`~pylon.data.PylonData`\\n    :raises: :class:`~pylon.data.DataError`\\n    :raises: :class:`~pylon.data.DataError`\\n    :raises: :class:`~pylon.data.DataError`\\n    :raises: :class:`~pylon.data.DataError`\\n    :raises: :class:`~pylon.data.DataError`\\n    :raises: :class:`~pylon.data.DataError`\\n    :raises: :class:`~pylon.data.DataError`\\n    :raises: :class:`~pylon.data.DataError`\\n    :raises: :class:`~pylon.data.DataError`\\n    :raises: :class:`~pylon.data.DataError`\\n    :raises: :class:`~pylon.data.DataError`\\n    :raises: :class:`~pylon.data.DataError`\\n    :raises: :class:`~py',\n",
       "    '.',\n",
       "    '_units: dict[int, str]\\n_abilities: dict[int, Ability]\\n_general_abilities: set[int]\\n\\n    :param data: Data from the database.']},\n",
       "  {'c': 'def _load_agent_from_file(\\n    file: Union[str, Path], **kwargs: Any\\n) -> Union[BaseSingleActionAgent, BaseMultiActionAgent]:\\n    if isinstance(file, str):\\n        file_path = Path(file)\\n    else:\\n        file_path = file\\n\\n    if file_path.suffix == \".json\":\\n        with open(file_path) as f:\\n            config = json.load(f)\\n    elif file_path.suffix == \".yaml\":\\n        with open(file_path, \"r\") as f:\\n            config = yaml.safe_load(f)\\n    else:\\n        raise ValueError(\"File type must be json or yaml\")\\n\\n    return load_agent_from_config(config, **kwargs)',\n",
       "   'd': 'Format the prompt template.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    Formatted message.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def load(\\n        self,\\n        space_key: Optional[str] = None,\\n        page_ids: Optional[List[str]] = None,\\n        label: Optional[str] = None,\\n        cql: Optional[str] = None,\\n        include_restricted_content: bool = False,\\n        include_archived_content: bool = False,\\n        include_attachments: bool = False,\\n        include_comments: bool = False,\\n        content_format: ContentFormat = ContentFormat.STORAGE,\\n        limit: Optional[int] = 50,\\n        max_pages: Optional[int] = 1000,\\n        ocr_languages: Optional[str] = None,\\n    ) -> List[Document]:\\n        if not space_key and not page_ids and not label and not cql:\\n            raise ValueError(\\n                \"Must specify at least one among `space_key`, `page_ids`, \"\\n                \"`label`, `cql` parameters.\"\\n            )\\n\\n        docs = []\\n\\n        if space_key:\\n            pages = self.paginate_request(\\n                self.confluence.get_all_pages_from_space,\\n                space=space_key,\\n                limit=limit,\\n                max_pages=max_pages,\\n                status=\"any\" if include_archived_content else \"current\",\\n                expand=content_format.value,\\n            )\\n            docs += self.process_pages(\\n                pages,\\n                include_restricted_content,\\n                include_attachments,\\n                include_comments,\\n                content_format,\\n                ocr_languages,\\n            )\\n\\n        if label:\\n            pages = self.paginate_request(\\n                self.confluence.get_all_pages_by_label,\\n                label=label,\\n                limit=limit,\\n                max_pages=max_pages,\\n            )\\n            ids_by_label = [page[\"id\"] for page in pages]\\n            if page_ids:\\n                page_ids = list(set(page_ids + ids_by_label))\\n            else:\\n                page_ids = list(set(ids_by_label))\\n\\n        if cql:\\n            pages = self.paginate_request(\\n                self._search_content_by_cql,\\n                cql=cql,\\n                limit=limit,\\n                max_pages=max_pages,\\n                include_archived_spaces=include_archived_content,\\n                expand=content_format.value,\\n            )\\n            docs += self.process_pages(\\n                pages,\\n                include_restricted_content,\\n                include_attachments,\\n                include_comments,\\n                content_format,\\n                ocr_languages,\\n            )\\n\\n        if page_ids:\\n            for page_id in page_ids:\\n                get_page = retry(\\n                    reraise=True,\\n                    stop=stop_after_attempt(\\n                        self.number_of_retries\\n                    ),\\n                    wait=wait_exponential(\\n                        multiplier=1,\\n                        min=self.min_retry_seconds,\\n                        max=self.max_retry_seconds,\\n                    ),\\n                    before_sleep=before_sleep_log(logger, logging.WARNING),\\n                )(self.confluence.get_page_by_id)\\n                page = get_page(page_id=page_id, expand=content_format.value)\\n                if not include_restricted_content and not self.is_public_page(page):\\n                    continue\\n                doc = self.process_page(\\n                    page,\\n                    include_attachments,\\n                    include_comments,\\n                    content_format,\\n                    ocr_languages,\\n                )\\n                docs.append(doc)\\n\\n        return docs',\n",
       "   'd': 'Initialize with necessary components.',\n",
       "   'l': False,\n",
       "   'g': ['_get_job_batch',\n",
       "    '(x)',\n",
       "    '(self):',\n",
       "    '_get_job_batch_list_from_arguments(self):']},\n",
       "  {'c': '    def input_keys(self) -> List[str]:\\n        return [self.input_key]',\n",
       "   'd': 'Expect input key.\\n\\n:meta private:',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def _load_run_evaluators(\\n    config: smith_eval.RunEvalConfig,\\n    run_type: str,\\n    data_type: DataType,\\n    example_outputs: Optional[List[str]],\\n    run_inputs: Optional[List[str]],\\n    run_outputs: Optional[List[str]],\\n) -> List[RunEvaluator]:\\n    run_evaluators = []\\n    input_key, prediction_key, reference_key = None, None, None\\n    if (\\n        config.evaluators\\n        or any([isinstance(e, EvaluatorType) for e in config.evaluators])\\n        or (\\n            config.custom_evaluators\\n            and any([isinstance(e, StringEvaluator) for e in config.custom_evaluators])\\n        )\\n    ):\\n        input_key, prediction_key, reference_key = _get_keys(\\n            config, run_inputs, run_outputs, example_outputs\\n        )\\n    for eval_config in config.evaluators:\\n        run_evaluator = _construct_run_evaluator(\\n            eval_config,\\n            config.eval_llm,\\n            run_type,\\n            data_type,\\n            example_outputs,\\n            reference_key,\\n            input_key,\\n            prediction_key,\\n        )\\n        run_evaluators.append(run_evaluator)\\n    custom_evaluators = config.custom_evaluators or []\\n    for custom_evaluator in custom_evaluators:\\n        if isinstance(custom_evaluator, RunEvaluator):\\n            run_evaluators.append(custom_evaluator)\\n        elif isinstance(custom_evaluator, StringEvaluator):\\n            run_evaluators.append(\\n                smith_eval.StringRunEvaluatorChain.from_run_and_data_type(\\n                    custom_evaluator,\\n                    run_type,\\n                    data_type,\\n                    input_key=input_key,\\n                    prediction_key=prediction_key,\\n                    reference_key=reference_key,\\n                )\\n            )\\n        else:\\n            raise ValueError(\\n                f\"Unsupported custom evaluator: {custom_evaluator}.\"\\n                f\" Expected RunEvaluator or StringEvaluator.\"\\n            )\\n\\n    return run_evaluators',\n",
       "   'd': 'Load run evaluators from a configuration.\\n\\nArgs:\\n    config: Configuration for the run evaluators.\\n\\nReturns:\\n    A list of run evaluators.',\n",
       "   'l': True,\n",
       "   'g': ['(source: str, results: List[str], device: str = \"cuda\") -> None:\\n\\n    (',\n",
       "    '(source: str, results: str, device: str = \"cuda\") -> None:\\n\\n    (',\n",
       "    '(source, results, device=\"cuda\") -> None:',\n",
       "    '_load_model']},\n",
       "  {'c': '    def refresh_schema(self) -> None:\\n        db_schema = self.query(SCHEMA_QUERY)[0].get(\"schema\")\\n        assert db_schema is not None\\n        self.schema = db_schema',\n",
       "   'd': 'Refreshes the Memgraph graph schema information.',\n",
       "   'l': True,\n",
       "   'g': ['', '', ':param message:\\n  :param ostream:\\n  :return:', '']},\n",
       "  {'c': '    def lazy_load(self) -> Iterator[Document]:\\n        try:\\n            from qcloud_cos import CosS3Client\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import cos-python-sdk-v5 python package. \"\\n                \"Please install it with `pip install cos-python-sdk-v5`.\"\\n            )\\n\\n\\n        client = CosS3Client(self.conf)\\n        with tempfile.TemporaryDirectory() as temp_dir:\\n            file_path = f\"{temp_dir}/{self.bucket}/{self.key}\"\\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n\\n            client.download_file(\\n                Bucket=self.bucket, Key=self.key, DestFilePath=file_path\\n            )\\n            loader = UnstructuredFileLoader(file_path)\\n\\n            return iter(loader.load())',\n",
       "   'd': 'PUT the URL and return the text.',\n",
       "   'l': False,\n",
       "   'g': [':param str var_name: name of the variable to be set',\n",
       "    ':param str var_name: name of the variable to be set\\n    :param str file_name: name of the file containing the variable to be set\\n    :return:',\n",
       "    ':param var_name: str\\n    :param file_name: str\\n    :return:',\n",
       "    ':param str var_name: Name of the variable to set.\\n    :param str file_name: Name of the file containing the variable to set.\\n    :return: None.']},\n",
       "  {'c': 'def parse(self, text: str) -> List[str]:\\n',\n",
       "   'd': 'Parse the output of an LLM call.',\n",
       "   'l': True,\n",
       "   'g': ['Convert a string to a continuation align style.\\n\\n  :param s: The string to convert.\\n\\n  :returns: The converted style.',\n",
       "    'Convert a string to a continuation align style.\\n\\n  :param s: The string to convert.\\n\\n  :return: The converted style.',\n",
       "    '',\n",
       "    'Convert a string to a continuation align style.\\n\\n  :param s: the string to convert\\n  :returns: the converted string\\n  :raises ValueError: if the string is not a valid continuation align style\\n  :returns: the converted string\\n  :raises ValueError: if the string is not a valid continuation align style']},\n",
       "  {'c': '    def _run(\\n        self,\\n        *args: Any,\\n        **kwargs: Any,\\n    ) -> Any:\\n',\n",
       "   'd': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing,',\n",
       "   'l': True,\n",
       "   'g': ['Cancel a job.\\n\\n    :param job: The job to cancel.',\n",
       "    'Cancel a job.\\n\\n    :param job: The job to cancel.',\n",
       "    '',\n",
       "    ':param job: The job to cancel.\\n    :return: None.']},\n",
       "  {'c': 'def try_load_from_hub(\\n    path: Union[str, Path],\\n    loader: Callable[[str], T],\\n    valid_prefix: str,\\n    valid_suffixes: Set[str],\\n    **kwargs: Any,\\n) -> Optional[T]:\\n    if not isinstance(path, str) or not (match := HUB_PATH_RE.match(path)):\\n        return None\\n    ref, remote_path_str = match.groups()\\n    ref = ref[1:] if ref else DEFAULT_REF\\n    remote_path = Path(remote_path_str)\\n    if remote_path.parts[0] != valid_prefix:\\n        return None\\n    if remote_path.suffix[1:] not in valid_suffixes:\\n        raise ValueError(f\"Unsupported file type, must be one of {valid_suffixes}.\")\\n\\n\\n\\n\\n\\n\\n    full_url = urljoin(URL_BASE.format(ref=ref), PurePosixPath(remote_path).__str__())\\n\\n    r = requests.get(full_url, timeout=5)\\n    if r.status_code != 200:\\n        raise ValueError(f\"Could not find file at {full_url}\")\\n    with tempfile.TemporaryDirectory() as tmpdirname:\\n        file = Path(tmpdirname) / remote_path.name\\n        with open(file, \"wb\") as f:\\n            f.write(r.content)\\n        return loader(str(file), **kwargs)',\n",
       "   'd': 'Split markdown file\\nArgs:\\n    text: Markdown file',\n",
       "   'l': False,\n",
       "   'g': ['(torch.FloatTensor, InferenceParams) -> None:',\n",
       "    '_update_kv_cache(kv, inference_params, layer_idx) -> None:',\n",
       "    '_update_kv_cache(kv: torch.FloatTensor, inference_params: InferenceParams) -> None:',\n",
       "    '(torch.FloatTensor, InferenceParams) -> None:']},\n",
       "  {'c': '    def _run(\\n        self,\\n        query: str,\\n        run_manager: Optional[CallbackManagerForToolRun] = None,\\n    ) -> str:\\n        try:\\n            import yfinance\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import yfinance python package. \"\\n                \"Please install it with `pip install yfinance`.\"\\n            )\\n        company = yfinance.Ticker(query)\\n        try:\\n            if company.isin is None:\\n                return f\"Company ticker {query} not found.\"\\n        except (HTTPError, ReadTimeout, ConnectionError):\\n            return f\"Company ticker {query} not found.\"\\n\\n        links = []\\n        try:\\n            links = [n[\"link\"] for n in company.news if n[\"type\"] == \"STORY\"]\\n        except (HTTPError, ReadTimeout, ConnectionError):\\n            if not links:\\n                return f\"No news found for company that searched with {query} ticker.\"\\n        if not links:\\n            return f\"No news found for company that searched with {query} ticker.\"\\n        loader = WebBaseLoader(links)\\n        docs = loader.load()\\n        result = self._format_results(docs, query)\\n        if not result:\\n            return f\"No news found for company that searched with {query} ticker.\"\\n        return result',\n",
       "   'd': 'Use the Yahoo Finance News tool.',\n",
       "   'l': True,\n",
       "   'g': ['_call__',\n",
       "    '_call__',\n",
       "    '_call__',\n",
       "    '(r):\\n    :param Request r:\\n    :rtype: Request']},\n",
       "  {'c': 'def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\\n    if self.input_keys:\\n        input_variables = {key: input_variables[key] for key in self.input_keys}\\n    query = \" \".join(sorted_values(input_variables))\\n    example_docs = self.vectorstore.similarity_search(query, k=self.k)\\n\\n\\n    examples = [dict(e.metadata) for e in example_docs]\\n\\n    if self.example_keys:\\n        examples = [{k: eg[k] for k in self.example_keys} for eg in examples]\\n    return examples',\n",
       "   'd': 'Parse the output of an LLM call.',\n",
       "   'l': False,\n",
       "   'g': ['_forward',\n",
       "    'Args:\\n        pred (paddle.Tensor): [B, H, W]\\n        gt (paddle.Tensor): [B, H, W]\\n        mask (paddle.Tensor): [B, H, W]\\n        weights (paddle.Tensor, optional): [B, H, W]. Defaults to None.\\n    \\n    Returns:\\n        paddle.Tensor: [B, 1, 1]',\n",
       "    'Args:\\n        pred (paddle.Tensor): \\n        gt (paddle.Tensor): \\n        mask (paddle.Tensor): \\n        weights (paddle.Tensor, optional):',\n",
       "    'Parameters\\n    ----------\\n    pred: paddle.Tensor\\n        The prediction tensor.\\n    gt: paddle.Tensor\\n        The ground-truth tensor.\\n    mask: paddle.Tensor\\n        The mask tensor.\\n    weights: paddle.Tensor, optional\\n        The weights tensor.\\n    \\n    Returns\\n    -------\\n    loss: paddle.Tensor\\n        The loss tensor.']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        try:\\n            from bs4 import BeautifulSoup\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import bs4 python package. \"\\n                \"Please install it with `pip install bs4`.\"\\n            )\\n        return values',\n",
       "   'd': 'Upsert records into the SQLite database.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def validate_input_variables(cls, values: dict) -> dict:\\n        messages = values[\"messages\"]\\n        input_vars = set()\\n        input_types: Dict[str, Any] = values.get(\"input_types\", {})\\n        for message in messages:\\n            if isinstance(message, (BaseMessagePromptTemplate, BaseChatPromptTemplate)):\\n                input_vars.update(message.input_variables)\\n            if isinstance(message, MessagesPlaceholder):\\n                if message.variable_name not in input_types:\\n                    input_types[message.variable_name] = List[AnyMessage]\\n        if \"partial_variables\" in values:\\n            input_vars = input_vars - set(values[\"partial_variables\"])\\n        if \"input_variables\" in values:\\n            if input_vars != set(values[\"input_variables\"]):\\n                raise ValueError(\\n                    \"Got mismatched input_variables. \"\\n                    f\"Expected: {input_vars}. \"\\n                    f\"Got: {values[\\'input_variables\\']}\"\\n                )\\n        else:\\n            values[\"input_variables\"] = sorted(input_vars)\\n        values[\"input_types\"] = input_types\\n        return values',\n",
       "   'd': 'Format the chat template into a string.\\n\\nArgs:\\n    **kwargs: keyword arguments to use for filling in template variables\\n              in all the template messages in this chat template.\\n\\nReturns:\\n    formatted string',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Detect <Tab> key press.\\n\\n  :param event: :class:`~pyglet.window.event.WindowEvent`\\n  :return:',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n    stmt = (\\n        select(self.cache_schema.response)\\n        .where(self.cache_schema.prompt == prompt)\\n        .where(self.cache_schema.llm == llm_string)\\n        .order_by(self.cache_schema.idx)\\n    )\\n    with Session(self.engine) as session:\\n        rows = session.execute(stmt).fetchall()\\n        if rows:\\n            try:\\n                return [loads(row[0]) for row in rows]\\n            except Exception:\\n                logger.warning(\\n                    \"Retrieving a cache value that could not be deserialized \"\\n                    \"properly. This is likely due to the cache being in an \"\\n                    \"older format. Please recreate your cache to avoid this \"\\n                    \"error.\"\\n                )\\n\\n\\n                return [Generation(text=row[0]) for row in rows]\\n    return None',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def __gt__(self, other: int) -> \"RedisFilterExpression\":\\n        self._set_value(other, int, RedisFilterOperator.GT)\\n        return RedisFilterExpression(str(self))',\n",
       "   'd': 'Create a RedisNumeric greater than filter expression\\n\\nArgs:\\n    other (int): The value to filter on.\\n\\nExample:\\n    >>> from langchain.vectorstores.redis import RedisNum\\n    >>> filter = RedisNum(\"age\") > 18',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'Returns the number of pages in the document.\\n  \\n  @return  The number of pages in the document.\\n  @rtype   int\\n  @see     getNumPages\\n  @see     getNumPagesInDocument\\n  @see     getNumPagesInCollection\\n  @see     getNumPagesInDocumentAndCollection\\n  @see     getNumPagesInCollectionAndDocument\\n  @see     getNumPagesInCollectionAndDocumentAndPage',\n",
       "    '']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        github_repository = get_from_dict_or_env(\\n            values, \"github_repository\", \"GITHUB_REPOSITORY\"\\n        )\\n\\n        github_app_id = get_from_dict_or_env(values, \"github_app_id\", \"GITHUB_APP_ID\")\\n\\n        github_app_private_key = get_from_dict_or_env(\\n            values, \"github_app_private_key\", \"GITHUB_APP_PRIVATE_KEY\"\\n        )\\n\\n        github_branch = get_from_dict_or_env(\\n            values, \"github_branch\", \"GITHUB_BRANCH\", default=\"master\"\\n        )\\n        github_base_branch = get_from_dict_or_env(\\n            values, \"github_base_branch\", \"GITHUB_BASE_BRANCH\", default=\"master\"\\n        )\\n\\n        try:\\n            from github import Auth, GithubIntegration\\n\\n        except ImportError:\\n            raise ImportError(\\n                \"PyGithub is not installed. \"\\n                \"Please install it with `pip install PyGithub`\"\\n            )\\n\\n        with open(github_app_private_key, \"r\") as f:\\n            private_key = f.read()\\n\\n        auth = Auth.AppAuth(\\n            github_app_id,\\n            private_key,\\n        )\\n        gi = GithubIntegration(auth=auth)\\n        installation = gi.get_installations()[0]\\n\\n\\n        g = installation.get_github_for_installation()\\n\\n        values[\"github\"] = g\\n        values[\"github_repo_instance\"] = g.get_repo(github_repository)\\n        values[\"github_repository\"] = github_repository\\n        values[\"github_app_id\"] = github_app_id\\n        values[\"github_app_private_key\"] = github_app_private_key\\n        values[\"github_branch\"] = github_branch\\n        values[\"github_base_branch\"] = github_base_branch\\n\\n        return values',\n",
       "   'd': 'Validate that api key and python package exists in environment.',\n",
       "   'l': True,\n",
       "   'g': ['_add_column(self, name, data):\\n    Add a column to the DataFrame.',\n",
       "    '',\n",
       "    '_add_column(self, name, data):\\n\\n    :param name: str\\n    :param data: numpy.ndarray or numpy.recarray\\n    :return:',\n",
       "    '_add_column(self, name, data):\\n\\n    Add a column to the dataframe.\\n\\n    Parameters\\n    ----------\\n    name : str\\n        Name of the new column.\\n    data : numpy.ndarray\\n        Data to be added to the column.\\n\\n    Returns\\n    -------\\n    None.']},\n",
       "  {'c': '    def get_input_schema(\\n        self, config: Optional[RunnableConfig] = None\\n    ) -> Type[BaseModel]:\\n        func = getattr(self, \"func\", None) or getattr(self, \"afunc\")\\n\\n        if isinstance(func, itemgetter):\\n\\n\\n            items = str(func).replace(\"operator.itemgetter(\", \"\")[:-1].split(\", \")\\n            if all(\\n                item[0] == \"\\'\" and item[-1] == \"\\'\" and len(item) > 2 for item in items\\n            ):\\n\\n                return create_model(\\n                    \"RunnableLambdaInput\",\\n                    **{item[1:-1]: (Any, None) for item in items},\\n                )\\n            else:\\n                return create_model(\"RunnableLambdaInput\", __root__=(List[Any], None))\\n\\n        if self.InputType != Any:\\n            return super().get_input_schema(config)\\n\\n        if dict_keys := get_function_first_arg_dict_keys(func):\\n            return create_model(\\n                \"RunnableLambdaInput\",\\n                **{key: (Any, None) for key in dict_keys},\\n            )\\n\\n        return super().get_input_schema(config)',\n",
       "   'd': 'The pydantic schema for the input to this runnable.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def from_examples(\\n        cls,\\n        examples: List[dict],\\n        embeddings: Embeddings,\\n        vectorstore_cls: Type[VectorStore],\\n        k: int = 4,\\n        input_keys: Optional[List[str]] = None,\\n        **vectorstore_cls_kwargs: Any,\\n    ) -> SemanticSimilarityExampleSelector:\\n        if input_keys:\\n            string_examples = [\\n                \" \".join(sorted_values({k: eg[k] for k in input_keys}))\\n                for eg in examples\\n            ]\\n        else:\\n            string_examples = [\" \".join(sorted_values(eg)) for eg in examples]\\n        vectorstore = vectorstore_cls.from_texts(\\n            string_examples, embeddings, metadatas=examples, **vectorstore_cls_kwargs\\n        )\\n        return cls(vectorstore=vectorstore, k=k, input_keys=input_keys)',\n",
       "   'd': 'Returns the input text with no changes.',\n",
       "   'l': False,\n",
       "   'g': ['Create a Vision-Transformer (ViT-B/32-224-in21k) with a given number of\\n  layers, patch size, and number of heads.',\n",
       "    \"(pretrained=False, **kwargs) -> 'torch.nn.Module':\",\n",
       "    'Create a Vision-Transformer-based model from the `vit_base_patch32_224_in21k`\\n  pretrained model.\\n\\n  Args:\\n      pretrained (bool): If True, load a pre-trained model.\\n      **kwargs (optional keyword arguments): The keyword arguments of the\\n          `vit_base_patch32_224_in21k` model.\\n\\n  Returns:\\n      torch.nn.Module:',\n",
       "    'Create a `vit_base_patch32_224_in21k` model.']},\n",
       "  {'c': 'def _call(\\n    self,\\n    messages: List[BaseMessage],\\n    stop: Optional[List[str]] = None,\\n    run_manager: Optional[CallbackManagerForLLMRun] = None,\\n    **kwargs: Any,\\n) -> str:\\n    response = self.responses[self.i]\\n    if self.i < len(self.responses) - 1:\\n        self.i += 1\\n    else:\\n        self.i = 0\\n    return response',\n",
       "   'd': \"First try to lookup in queries, else return 'foo' or 'bar'.\",\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def _run(\\n    self, text: str, run_manager: Optional[CallbackManagerForToolRun] = None\\n) -> str:\\n    try:\\n        data = _parse_input(text)\\n        return self.requests_wrapper.patch(_clean_url(data[\"url\"]), data[\"data\"])\\n    except Exception as e:\\n        return repr(e)',\n",
       "   'd': 'Test end to end construction and search.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Returns the number of units in the output layer.',\n",
       "    'Returns the number of output units in the layer.',\n",
       "    '']},\n",
       "  {'c': 'def __new__(cls, **data: Any) -> Union[OpenAIChat, BaseOpenAI]:\\n    model_name = data.get(\"model_name\", \"\")\\n    if model_name.startswith(\"gpt-3.5-turbo\") or model_name.startswith(\"gpt-4\"):\\n        warnings.warn(\\n            \"You are trying to use a chat model. This way of initializing it is \"\\n            \"no longer supported. Instead, please use: \"\\n            \"`from langchain.chat_models import ChatOpenAI`\"\\n        )\\n        return OpenAIChat(**data)\\n    return super().__new__(cls)',\n",
       "   'd': 'Initialize the OpenAI object.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def similarity_search_with_score(\\n    self, query: str, k: int = 4, filter: Optional[dict] = None, **kwargs: Any\\n) -> List[Tuple[Document, float]]:\\n    embedding = self.embedding.embed_query(query)\\n    script_query = _default_script_query(embedding, filter)\\n    response = self.client_search(\\n        self.client, self.index_name, script_query, size=k\\n    )\\n    hits = [hit for hit in response[\"hits\"][\"hits\"]]\\n    docs_and_scores = [\\n        (\\n            Document(\\n                page_content=hit[\"_source\"][\"text\"],\\n                metadata=hit[\"_source\"][\"metadata\"],\\n            ),\\n            hit[\"_score\"],\\n        )\\n        for hit in hits\\n    ]\\n    return docs_and_scores',\n",
       "   'd': 'Return docs most similar to query.\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 4.\\nReturns:\\n    List of Documents most similar to the query.',\n",
       "   'l': True,\n",
       "   'g': ['_processCmd(self, cmd, aInfo):',\n",
       "    '_processCmd(self, aInfo, aAction=None):',\n",
       "    '_processCmd(self, aInfo, aAction=None):',\n",
       "    '_processCmd(self, aAction, aInfo):']},\n",
       "  {'c': '    def from_template(\\n        cls: Type[MessagePromptTemplateT],\\n        template: str,\\n        template_format: str = \"f-string\",\\n        **kwargs: Any,\\n    ) -> MessagePromptTemplateT:\\n        prompt = PromptTemplate.from_template(template, template_format=template_format)\\n        return cls(prompt=prompt, **kwargs)',\n",
       "   'd': 'Return type of llm.',\n",
       "   'l': False,\n",
       "   'g': ['_call__', '_call__', '_call__', '_call__']},\n",
       "  {'c': 'def InputType(self) -> Any:\\n    func = getattr(self, \"func\", None) or getattr(self, \"afunc\")\\n    try:\\n        params = inspect.signature(func).parameters\\n        first_param = next(iter(params.values()), None)\\n        if first_param and first_param.annotation != inspect.Parameter.empty:\\n            return first_param.annotation\\n        else:\\n            return Any\\n    except ValueError:\\n        return Any',\n",
       "   'd': \"Adds documents to the docstore and vectorstores.\\n\\nArgs:\\n    documents: List of documents to add\\n    ids: Optional list of ids for documents. If provided should be the same\\n        length as the list of documents. Can provided if parent documents\\n        are already in the document store and you don't want to re-add\\n        to the docstore. If not provided, random UUIDs will be used as\\n        ids.\\n    add_to_docstore: Boolean of whether to add documents to docstore.\\n        This can be false if and only if `ids` are provided. You may want\\n        to set this to False if the documents are already in the docstore\\n        and you don't want to re-add them.\",\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def index(\\n        self,\\n        dims_length: Union[int, None],\\n        vector_query_field: str,\\n        similarity: Union[DistanceStrategy, None],\\n    ) -> Dict:\\n',\n",
       "   'd': 'Executes when the index is created.\\n\\nArgs:\\n    dims_length: Numeric length of the embedding vectors,\\n                or None if not using vector-based query.\\n    vector_query_field: The field containing the vector\\n                        representations in the index.\\n    similarity: The similarity strategy to use,\\n                or None if not using one.\\n\\nReturns:\\n    Dict: The Elasticsearch settings and mappings for the strategy.',\n",
       "   'l': True,\n",
       "   'g': ['_from_node:\\n    :param index: int\\n    :param data: str\\n    :param modify_index: int\\n    :return: ClusterConfig\\n    :rtype: ClusterConfig\\n    :raises: ValueError\\n    :raises: TypeError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises: ImportError\\n    :raises',\n",
       "    '(int, str, int) -> ClusterConfig',\n",
       "    '_from_node:\\n    :param index: index of the node\\n    :param data: data of the node\\n    :param modify_index: index of the node to modify, or None if no modification is required\\n    :return: a ClusterConfig object\\n    :rtype: ClusterConfig\\n    :raises: ValueError if data is not a valid json object\\n    :raises: TypeError if data is not a valid json object\\n    :raises: ValueError if index is not a valid integer\\n    :raises: ValueError if modify_index is not a valid integer\\n    :raises: ValueError if modify_index is not None and not a valid integer\\n    :raises: ValueError if modify_index is not None and not a valid integer\\n    :raises: ValueError if modify_index is not None and not a valid integer\\n    :raises: ValueError if modify_index is not None and not a valid integer\\n    :raises: ValueError if modify_index is not None and not a valid integer\\n    :raises: ValueError if modify_index is not None and not a valid integer\\n    :raises: ValueError if modify_index is not None and not a valid integer\\n    :raises: ValueError if modify_index is',\n",
       "    '_from_node(index, data, modify_index=None)']},\n",
       "  {'c': '    def save(self, file_path: Union[Path, str]) -> None:\\n        if isinstance(file_path, str):\\n            save_path = Path(file_path)\\n        else:\\n            save_path = file_path\\n\\n        directory_path = save_path.parent\\n        directory_path.mkdir(parents=True, exist_ok=True)\\n\\n\\n        agent_dict = self.dict()\\n\\n        if save_path.suffix == \".json\":\\n            with open(file_path, \"w\") as f:\\n                json.dump(agent_dict, f, indent=4)\\n        elif save_path.suffix == \".yaml\":\\n            with open(file_path, \"w\") as f:\\n                yaml.dump(agent_dict, f, default_flow_style=False)\\n        else:\\n            raise ValueError(f\"{save_path} must be json or yaml\")',\n",
       "   'd': 'Get docs.',\n",
       "   'l': False,\n",
       "   'g': [\".is_partly_within_image(self, image):\\n\\n    '''\\n    Check if the given image is within the boundaries of the current\\n    image.\\n    :param image: Image to check\",\n",
       "    '.is_partly_within_image(image)\\n\\n    Returns True if the current image is partly within the image.\\n\\n    Parameters\\n    ----------\\n    image : Image\\n\\n    Returns\\n    -------\\n    bool',\n",
       "    '.is_partly_within_image(self, image)',\n",
       "    '.is_partly_within_image(self, image):\\n\\n    ..']},\n",
       "  {'c': 'def test_deprecated_property() -> None:\\n    with warnings.catch_warnings(record=True) as warning_list:\\n        warnings.simplefilter(\"always\")\\n\\n        obj = ClassWithDeprecatedMethods()\\n        assert obj.deprecated_property == \"This is a deprecated property.\"\\n\\n        assert len(warning_list) == 1\\n        warning = warning_list[0].message\\n\\n        assert str(warning) == (\\n            \"The function `deprecated_property` was deprecated in \"\\n            \"LangChain 2.0.0 and will be removed in 3.0.0\"\\n        )\\n        doc = ClassWithDeprecatedMethods.deprecated_property.__doc__\\n        assert isinstance(doc, str)\\n        assert doc.startswith(\"[*Deprecated*]  original doc\")',\n",
       "   'd': 'Test deprecated staticmethod.',\n",
       "   'l': True,\n",
       "   'g': ['_split_device_string splits a string into a device type and a device id.\\n    :param string: A string in the format \"device_type:device_id\".\\n    :return: A device.\\n    :rtype: Device.',\n",
       "    '.split_device_string(string: str) -> (DeviceType, str):',\n",
       "    '_split_device_string:\\n    :param string:\\n    :return:',\n",
       "    '.split(string: str) -> \"Device\":']},\n",
       "  {'c': '    def with_listeners(\\n        self,\\n        *,\\n        on_start: Optional[Listener] = None,\\n        on_end: Optional[Listener] = None,\\n        on_error: Optional[Listener] = None,\\n    ) -> Runnable[Input, Output]:\\n        from langchain.callbacks.tracers.root_listeners import RootListenersTracer\\n\\n        return self.__class__(\\n            bound=self.bound,\\n            kwargs=self.kwargs,\\n            config=self.config,\\n            config_factories=[\\n                lambda config: {\\n                    \"callbacks\": [\\n                        RootListenersTracer(\\n                            config=config,\\n                            on_start=on_start,\\n                            on_end=on_end,\\n                            on_error=on_error,\\n                        )\\n                    ],\\n                }\\n            ],\\n            custom_input_type=self.custom_input_type,\\n            custom_output_type=self.custom_output_type,\\n        )',\n",
       "   'd': 'Bind lifecycle listeners to a Runnable, returning a new Runnable.\\n\\non_start: Called before the runnable starts running, with the Run object.\\non_end: Called after the runnable finishes running, with the Run object.\\non_error: Called if the runnable throws an error, with the Run object.\\n\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '_get_session_info(self) -> (uid, frontaddr):',\n",
       "    ':return: (uid, frontaddr)',\n",
       "    ':return:']},\n",
       "  {'c': 'def _run(\\n    self,\\n    query: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> str:\\n    chain = RetrievalQAWithSourcesChain.from_chain_type(\\n        self.llm, retriever=self.vectorstore.as_retriever()\\n    )\\n    return json.dumps(\\n        chain(\\n            {chain.question_key: query},\\n            return_only_outputs=True,\\n            callbacks=run_manager.get_child() if run_manager else None,\\n        )\\n    )',\n",
       "   'd': 'Use the tool.',\n",
       "   'l': True,\n",
       "   'g': ['_call__', '_call__', '_call__', '_call__']},\n",
       "  {'c': 'def get_tools(self) -> List[BaseTool]:\\n    description = VectorStoreQATool.get_description(\\n        self.vectorstore_info.name, self.vectorstore_info.description\\n    )\\n    qa_tool = VectorStoreQATool(\\n        name=self.vectorstore_info.name,\\n        description=description,\\n        vectorstore=self.vectorstore_info.vectorstore,\\n        llm=self.llm,\\n    )\\n    description = VectorStoreQAWithSourcesTool.get_description(\\n        self.vectorstore_info.name, self.vectorstore_info.description\\n    )\\n    qa_with_sources_tool = VectorStoreQAWithSourcesTool(\\n        name=f\"{self.vectorstore_info.name}_with_sources\",\\n        description=description,\\n        vectorstore=self.vectorstore_info.vectorstore,\\n        llm=self.llm,\\n    )\\n    return [qa_tool, qa_with_sources_tool]',\n",
       "   'd': 'Return type of llm.',\n",
       "   'l': False,\n",
       "   'g': ['Creates a regular link.\\n\\n  :param kids: The kids of this node.\\n  :type kids: list of :class:`qlast.qlast.QlastNode`\\n  :return: The created link.\\n  :rtype: :class:`qlast.qlast.QlastLink`\\n  :raises ValueError: If the number of kids is not 4.\\n  :raises TypeError: If the type of the kids is not a list of\\n    :class:`qlast.qlast.QlastNode`.\\n  :raises ValueError: If the type of the kids is not a list of\\n    :class:`qlast.qlast.QlastLink`.\\n  :raises TypeError: If the type of the kids is not a list of\\n    :class:`qlast.qlast.QlastLink`.\\n  :raises ValueError: If the type of the kids is not a list of\\n    :class:`qlast.qlast.QlastLink`.\\n  :raises ValueError: If the type of the kids is not a list of\\n    :class:`qlast.qlast.QlastLink`.\\n  :raises ValueError: If the type of the kids is not',\n",
       "    '',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': '    def lazy_parse(self, blob: Blob) -> Iterator[Document]:\\n        import fitz\\n\\n        with blob.as_bytes_io() as file_path:\\n            doc = fitz.open(file_path)\\n\\n            yield from [\\n                Document(\\n                    page_content=page.get_text(**self.text_kwargs),\\n                    metadata=dict(\\n                        {\\n                            \"source\": blob.source,\\n                            \"file_path\": blob.source,\\n                            \"page\": page.number,\\n                            \"total_pages\": len(doc),\\n                        },\\n                        **{\\n                            k: doc.metadata[k]\\n                            for k in doc.metadata\\n                            if type(doc.metadata[k]) in [str, int]\\n                        },\\n                    ),\\n                )\\n                for page in doc\\n            ]',\n",
       "   'd': 'Lazily parse the blob.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '_predict_on_image(self, img):',\n",
       "    '(x) -> torch.Tensor\\n\\n    :param img: (torch.Tensor) image to predict on',\n",
       "    '.predict_on_image(self, img):\\n\\n    :param img: np.ndarray or torch.Tensor\\n    :return: np.ndarray or torch.Tensor\\n    :rtype: np.ndarray or torch.Tensor\\n    :raises: ValueError\\n    :raises: TypeError\\n    :raises: Exception\\n    :raises: ImportError\\n    :raises: ImportError: when `img` is not a np.ndarray or torch.Tensor\\n    :raises: ValueError: when `img` is not a np.ndarray or torch.Tensor\\n    :raises: TypeError: when `img` is not a np.ndarray or torch.Tensor\\n    :raises: Exception: when `img` is not a np.ndarray or torch.Tensor\\n    :raises: ImportError: when `img` is not a np.ndarray or torch.Tensor\\n    :raises: ImportError: when `img` is not a np.ndarray or torch.Tensor\\n    :raises: Exception: when `img` is not a np.ndarray or torch.Tensor\\n    :raises: ImportError: when `img` is not a np.ndarray or torch.Tensor\\n    :raises: ValueError: when `img` is not a np.']},\n",
       "  {'c': 'def return_stopped_response(\\n    self,\\n    early_stopping_method: str,\\n    intermediate_steps: List[Tuple[AgentAction, str]],\\n    **kwargs: Any,\\n) -> AgentFinish:\\n    if early_stopping_method == \"force\":\\n\\n        return AgentFinish({\"output\": \"Agent stopped due to max iterations.\"}, \"\")\\n    else:\\n        raise ValueError(\\n            f\"Got unsupported early_stopping_method `{early_stopping_method}`\"\\n        )',\n",
       "   'd': 'Return response when agent has been stopped due to max iterations.',\n",
       "   'l': True,\n",
       "   'g': ['_fit(self, X, y)',\n",
       "    '(x, y) -> self',\n",
       "    '(X, y) -> self',\n",
       "    '_fit(self, X, y):\\n    Return the fitted model.\\n\\n    Parameters\\n    ----------\\n    X : array_like, shape = [n_samples, n_features]\\n        Training data.\\n\\n    y : array_like, shape = [n_samples]\\n        Target values.\\n\\n    Returns\\n    -------\\n    self : object\\n        Returns the instance of the fitted model.\\n\\n    See also\\n    --------\\n    sklearn.linear_model.LinearRegression : Linear model.']},\n",
       "  {'c': 'def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n    all_required_field_names = get_pydantic_field_names(cls)\\n    extra = values.get(\"model_kwargs\", {})\\n    values[\"model_kwargs\"] = build_extra_kwargs(\\n        extra, values, all_required_field_names\\n    )\\n    return values',\n",
       "   'd': 'Build extra kwargs from additional params that were passed in.',\n",
       "   'l': True,\n",
       "   'g': ['Save the model checkpoint.\\n\\n    Args:\\n        iteration (int): The current iteration.\\n        model (torch.nn.Module): The model to save.\\n        args (argparse.Namespace): The arguments.\\n\\n    Returns:\\n        None: The model is saved to a file.',\n",
       "    'Save the model checkpoint.\\n\\n    Args:\\n        iteration (int): The current iteration.\\n        model (Model): The model to save.\\n        args (ArgumentParser): The command line arguments.\\n\\n    Returns:\\n        None: The model is saved in the checkpoint file.',\n",
       "    'Save the model checkpoint with the current random state.\\n\\n    :param iteration: The current iteration number.\\n    :param model: The model to save.\\n    :param args: The arguments.',\n",
       "    'Save the model checkpoint to the specified directory.\\n\\n    Args:\\n        iteration (int): The current iteration number.\\n        model (Model): The model to save.\\n        args (ArgumentParser): The command line arguments.\\n\\n    Returns:\\n        None: The model is saved to the specified directory.']},\n",
       "  {'c': 'def save(self, file_path: Union[Path, str]) -> None:\\n    raise ValueError(\\n        \"Saving not supported for agent executors. \"\\n        \"If you are trying to save the agent, please use the \"\\n        \"`.save_agent(...)`\"\\n    )',\n",
       "   'd': 'Raise error - saving not supported for Agent Executors.',\n",
       "   'l': True,\n",
       "   'g': ['Remove SQL from a question.\\n\\n  :param question: The question to remove SQL from.\\n  :type question: str',\n",
       "    '移除 SQL',\n",
       "    'Remove SQL from a question.\\n\\n  :param question: The question to remove SQL from.\\n  :type question: str',\n",
       "    'Removes the SQL from the question.\\n\\n  :param question: The question to remove the SQL from.\\n  :type question: str']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> BaseMessage:\\n        text = self.prompt.format(**kwargs)\\n        return AIMessage(content=text, additional_kwargs=self.additional_kwargs)',\n",
       "   'd': 'Get default mime-type based parser.',\n",
       "   'l': False,\n",
       "   'g': ['_failure_threshold:',\n",
       "    '- Sets the failure threshold for the given number of failed\\n    requests.\\n\\n    :param int failure_threshold: The number of failed requests to\\n        trigger the failure threshold.\\n        :type failure_threshold: int\\n        :return: self\\n        :rtype: FailureThreshold',\n",
       "    '',\n",
       "    ':param int failure_threshold: The number of failures before the server is\\n        considered failed.']},\n",
       "  {'c': 'def similarity_search_with_score(\\n    self, query: str, k: int = 10, **kwargs: Any\\n) -> List[Tuple[Document, float]]:\\n    return self.knn_search(query=query, k=k, **kwargs)',\n",
       "   'd': 'Pass through to `knn_search including score`',\n",
       "   'l': True,\n",
       "   'g': ['获取视频的id\\n    :param url:\\n    :return:\\n    :rtype: str',\n",
       "    ':param url: str\\n    :return: str',\n",
       "    '获取视频的id\\n    :param url:\\n    :return:\\n    :rtype: int',\n",
       "    '获取视频的id\\n    :param url:\\n    :return:\\n    :rtype: str']},\n",
       "  {'c': 'def map(self, run: Run) -> Dict[str, str]:\\n',\n",
       "   'd': 'Maps the Run to a dictionary.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def __init__(self) -> None:\\n    self.store: Dict[str, Document] = {}',\n",
       "   'd': 'Run when tool errors.',\n",
       "   'l': False,\n",
       "   'g': ['_call__', '_call__', '_call_ with sample_weight', '_call__']},\n",
       "  {'c': '    def execute(self, query: str, params: Optional[dict] = None, retry: int = 0) -> Any:\\n        from nebula3.Exception import IOErrorException, NoValidSessionException\\n        from nebula3.fbthrift.transport.TTransport import TTransportException\\n\\n        params = params or {}\\n        try:\\n            result = self.session_pool.execute_parameter(query, params)\\n            if not result.is_succeeded():\\n                logger.warning(\\n                    f\"Error executing query to NebulaGraph. \"\\n                    f\"Error: {result.error_msg()}\\\\n\"\\n                    f\"Query: {query} \\\\n\"\\n                )\\n            return result\\n\\n        except NoValidSessionException:\\n            logger.warning(\\n                f\"No valid session found in session pool. \"\\n                f\"Please consider increasing the session pool size. \"\\n                f\"Current size: {self.session_pool_size}\"\\n            )\\n            raise ValueError(\\n                f\"No valid session found in session pool. \"\\n                f\"Please consider increasing the session pool size. \"\\n                f\"Current size: {self.session_pool_size}\"\\n            )\\n\\n        except RuntimeError as e:\\n            if retry < RETRY_TIMES:\\n                retry += 1\\n                logger.warning(\\n                    f\"Error executing query to NebulaGraph. \"\\n                    f\"Retrying ({retry}/{RETRY_TIMES})...\\\\n\"\\n                    f\"query: {query} \\\\n\"\\n                    f\"Error: {e}\"\\n                )\\n                return self.execute(query, params, retry)\\n            else:\\n                raise ValueError(f\"Error executing query to NebulaGraph. Error: {e}\")\\n\\n        except (TTransportException, IOErrorException):\\n\\n            if retry < RETRY_TIMES:\\n                retry += 1\\n                logger.warning(\\n                    f\"Connection issue with NebulaGraph. \"\\n                    f\"Retrying ({retry}/{RETRY_TIMES})...\\\\n to recreate session pool\"\\n                )\\n                self.session_pool = self._get_session_pool()\\n                return self.execute(query, params, retry)',\n",
       "   'd': 'Add operators to the anonymizer\\n\\nArgs:\\n    operators: Operators to add to the anonymizer.',\n",
       "   'l': False,\n",
       "   'g': ['_get_item(self, entry, handle_value = 0):\\n    Get the object at the specified entry.\\n\\n    :param entry: The entry to get the object from.\\n    :param handle_value: The handle value to use.\\n    :return: The object at the specified entry.\\n    :raises: :class:`~obj.ObjectError` if the object is not found.\\n    :raises: :class:`~obj.ObjectError` if the object is not found.\\n    :raises: :class:`~obj.ObjectError` if the object is not found.\\n    :raises: :class:`~obj.ObjectError` if the object is not found.\\n    :raises: :class:`~obj.ObjectError` if the object is not found.\\n    :raises: :class:`~obj.ObjectError` if the object is not found.\\n    :raises: :class:`~obj.ObjectError` if the object is not found.\\n    :raises: :class:`~obj.ObjectError` if the object is not found.\\n    :raises: :class:`~obj.ObjectError` if the object is not found.',\n",
       "    '_get_item(self, entry, handle_value = 0):\\n\\n    Return a reference to an object or NoneObject if the entry is a\\n    NoneObject.\\n\\n    :param entry: an Entry object\\n    :param handle_value: the handle value to use if the entry is a NoneObject.\\n    :return: an object or NoneObject.',\n",
       "    '_get_item(self, entry, handle_value = 0):\\n    Return the item at the given entry.\\n\\n    :param entry: The entry to get the item from.\\n    :param handle_value: The handle value to get the item from.\\n    :return: The item at the given entry.\\n    :raises: :class:`~pyglet.error.PygletError` if the item is not found.\\n    :raises: :class:`~pyglet.error.PygletError` if the item is not found.\\n    :raises: :class:`~pyglet.error.PygletError` if the item is not found.\\n    :raises: :class:`~pyglet.error.PygletError` if the item is not found.\\n    :raises: :class:`~pyglet.error.PygletError` if the item is not found.\\n    :raises: :class:`~pyglet.error.PygletError` if the item is not found.\\n    :raises: :class:`~pyglet.error.PygletError` if the item is not',\n",
       "    '_get_item(self, entry, handle_value = 0):\\n    Return the object of the given entry.']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> BaseMessage:\\n        text = self.prompt.format(**kwargs)\\n        return AIMessage(content=text, additional_kwargs=self.additional_kwargs)',\n",
       "   'd': 'Format the prompt template.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    Formatted message.',\n",
       "   'l': True,\n",
       "   'g': [':param x: [batch_size, seq_len, dim]\\n    :param x_mask: [batch_size, seq_len]',\n",
       "    ':param x: (batch, 1, 1, 1, 1)\\n  :param x_mask: (batch, 1, 1, 1, 1)\\n  :param offset: (batch, 1, 1, 1, 1)\\n  :return:',\n",
       "    ':param x: (batch_size, seq_len, d_model)\\n    :param x_mask: (batch_size, 1, seq_len)\\n    :param offset: (int)',\n",
       "    ':param x: [Tensor] [batch_size, max_length, hidden_size]\\n  :param x_mask: [Tensor] [batch_size, max_length]']},\n",
       "  {'c': 'def _generate(\\n    self,\\n    prompts: List[str],\\n    stop: Optional[List[str]] = None,\\n    run_manager: Optional[CallbackManagerForLLMRun] = None,\\n    **kwargs: Any,\\n) -> LLMResult:\\n',\n",
       "   'd': 'Run the LLM on the given prompts.',\n",
       "   'l': True,\n",
       "   'g': ['_is_same_host(self, url):\\n\\n    Return True if the given url is the same host as this url.',\n",
       "    '_is_same_host',\n",
       "    '_is_same_host(self, url):',\n",
       "    '.']},\n",
       "  {'c': 'def args(self) -> dict:\\n    return self.args_schema.schema()[\"properties\"]',\n",
       "   'd': \"The tool's input arguments.\",\n",
       "   'l': True,\n",
       "   'g': ['Converts a raw string to a protobuf Feature.\\n\\n    :param raw: A string containing a raw protobuf Feature.\\n    :return: A protobuf Feature.\\n    :rtype: feature_pb2.Feature',\n",
       "    'Convert a raw feature to a protobuf feature.\\n\\n    :param raw: A raw feature.\\n    :type raw: bytes',\n",
       "    'Convert raw data to proto object.\\n\\n    :param raw: raw data\\n    :return: proto object\\n    :rtype: feature_pb2.Feature',\n",
       "    '']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace',\n",
       "   'd': 'Use the tool.',\n",
       "   'l': False,\n",
       "   'g': ['Gets the path for the given key from the environment or the default.\\n\\n    Args:\\n        key (str): The key to look up in the environment.\\n        default (Optional[Union[PathLike, str]]): The default path to use if the key is not found.\\n\\n    Returns:\\n        Optional[Path]: The path for the given key.\\n\\n    Example:\\n        >>> path = __get_path(\"PATH\", \"/usr/bin\")\\n        >>> print(path)  # prints \"/usr/bin\"\\n        >>> path = __get_path(\"PATH\", None)\\n        >>> print(path)  # prints \"/usr/bin\"\\n        >>> path = __get_path(\"PATH\", \"/usr/bin\")\\n        >>> print(path)  # prints \"/usr/bin\"\\n        >>> path = __get_path(\"PATH\", \"/usr/bin\")\\n        >>> print(path)  # prints \"/usr/bin\"\\n        >>> path = __get_path(\"PATH\", \"/usr/bin\")\\n        >>> print(path)  # prints \"/usr/bin\"\\n        >>> path = __get_path(\"PATH\", None)\\n        >>> print',\n",
       "    '',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': 'def __init__(\\n    self,\\n    pl_id_callback: Optional[Callable[..., Any]] = None,\\n    pl_tags: Optional[List[str]] = None,\\n) -> None:\\n    _lazy_import_promptlayer()\\n    self.pl_id_callback = pl_id_callback\\n    self.pl_tags = pl_tags or []\\n    self.runs: Dict[UUID, Dict[str, Any]] = {}',\n",
       "   'd': 'Initialize the PromptLayerCallbackHandler.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '_init_']},\n",
       "  {'c': 'def on_tool_error(\\n    self,\\n    error: Union[Exception, KeyboardInterrupt],\\n    *,\\n    run_id: UUID,\\n    parent_run_id: Optional[UUID] = None,\\n    **kwargs: Any,\\n) -> Any:\\n',\n",
       "   'd': 'Run when chain errors.\\n\\nArgs:\\n    error (Exception or KeyboardInterrupt): The error.',\n",
       "   'l': False,\n",
       "   'g': ['(self, f: mitmproxy.flow.Flow) -> None:\\n\\n    This method is called when a flow is added to the store.\\n\\n    :param f: The flow that is added to the store.\\n    :return: None.',\n",
       "    '_store = {}\\n_base_add = add',\n",
       "    'Add a flow to the store.\\n\\n  :param f: The flow to add.\\n  :type f: mitmproxy.flow.Flow\\n  :return: None.\\n  :rtype: None.',\n",
       "    '_store: dict[int, mitmproxy.flow.Flow]\\n_base_add: mitmproxy.flow.add_flow_internal\\n_focus: mitmproxy.']},\n",
       "  {'c': '    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        criteria: Optional[CRITERIA_TYPE] = None,\\n        *,\\n        prompt: Optional[BasePromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> CriteriaEvalChain:\\n        prompt_ = cls._resolve_prompt(prompt)\\n        if criteria == Criteria.CORRECTNESS:\\n            raise ValueError(\\n                \"Correctness should not be used in the reference-free\"\\n                \" \\'criteria\\' evaluator (CriteriaEvalChain).\"\\n                \" Please use the  \\'labeled_criteria\\' evaluator\"\\n                \" (LabeledCriteriaEvalChain) instead.\"\\n            )\\n        criteria_ = cls.resolve_criteria(criteria)\\n        criteria_str = \"\\\\n\".join(f\"{k}: {v}\" for k, v in criteria_.items())\\n        prompt_ = prompt_.partial(criteria=criteria_str)\\n        return cls(\\n            llm=llm,\\n            prompt=prompt_,\\n            criterion_name=\"-\".join(criteria_),\\n            **kwargs,\\n        )',\n",
       "   'd': 'Create a `CriteriaEvalChain` instance from an llm and criteria.\\n\\nParameters\\n----------\\nllm : BaseLanguageModel\\n    The language model to use for evaluation.\\ncriteria : CRITERIA_TYPE - default=None for \"helpfulness\"\\n    The criteria to evaluate the runs against. It can be:\\n        -  a mapping of a criterion name to its description\\n        -  a single criterion name present in one of the default criteria\\n        -  a single `ConstitutionalPrinciple` instance\\nprompt : Optional[BasePromptTemplate], default=None\\n    The prompt template to use for generating prompts. If not provided,\\n    a default prompt template will be used.\\n**kwargs : Any\\n    Additional keyword arguments to pass to the `LLMChain`\\n    constructor.\\n\\nReturns\\n-------\\nCriteriaEvalChain\\n    An instance of the `CriteriaEvalChain` class.\\n\\nExamples\\n--------\\n>>> from langchain.llms import OpenAI\\n>>> from langchain.evaluation.criteria import LabeledCriteriaEvalChain\\n>>> llm = OpenAI()\\n>>> criteria = {\\n        \"hallucination\": (\\n            \"Does this submission contain information\"\\n            \" not present in the input or reference?\"\\n        ),\\n    }\\n>>> chain = LabeledCriteriaEvalChain.from_llm(\\n        llm=llm,\\n        criteria=criteria,\\n    )',\n",
       "   'l': True,\n",
       "   'g': ['', '', '() ->', '']},\n",
       "  {'c': '    def __init__(self, file_path: str, *, headers: Optional[Dict] = None):\\n        try:\\n            from pdfminer.high_level import extract_text_to_fp\\n        except ImportError:\\n            raise ImportError(\\n                \"`pdfminer` package not found, please install it with \"\\n                \"`pip install pdfminer.six`\"\\n            )\\n\\n        super().__init__(file_path, headers=headers)',\n",
       "   'd': 'Initialize with a file path.',\n",
       "   'l': True,\n",
       "   'g': ['_send_attachments: send all the media in the list of attachments',\n",
       "    '',\n",
       "    '_send_attachments - Send a list of media to a chat_id.\\n    :param chat_id: Chat id to send the media.\\n    :param notify_type: Type of notification to send.\\n    :param attach: List of media to send.\\n    :return: True if all media were successfully sent, False otherwise.',\n",
       "    '_send_attachments(self, chat_id, notify_type, attach):\\n\\n    Send a list of attachments.\\n\\n    Parameters\\n    ----------\\n    chat_id : str\\n        The ID of the chat to send the attachments to.\\n    notify_type : str\\n        The type of notification to send.\\n    attach : list\\n        The list of attachments to send.\\n\\n    Returns\\n    -------\\n    bool\\n        True if any of the attachments could not be sent, False otherwise.']},\n",
       "  {'c': '    def parse(self, text: str) -> List[str]:\\n        pattern = r\"\\\\d+\\\\.\\\\s([^\\\\n]+)\"\\n\\n\\n        matches = re.findall(pattern, text)\\n        return matches',\n",
       "   'd': 'Format the prompt with the inputs.\\n\\nArgs:\\n    kwargs: Any arguments to be passed to the prompt template.\\n\\nReturns:\\n    A formatted string.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    prompt.format(variable1=\"foo\")',\n",
       "   'l': False,\n",
       "   'g': [':param path:\\n    :param commit:\\n    :return:',\n",
       "    '',\n",
       "    '',\n",
       "    ':param path: \\n:param commit: \\n:return: \\n:rtype:']},\n",
       "  {'c': 'def load(self) -> List[Document]:\\n    loader = UnstructuredPDFLoader(str(self.file_path))\\n    return loader.load()',\n",
       "   'd': 'Construct ElasticVectorSearch wrapper from raw documents.\\n\\nThis is a user-friendly interface that:\\n    1. Embeds documents.\\n    2. Creates a new index for the embeddings in the Elasticsearch instance.\\n    3. Adds the documents to the newly created Elasticsearch index.\\n\\nThis is intended to be a quick way to get started.\\n\\nExample:\\n    .. code-block:: python\\n\\n        from langchain import ElasticVectorSearch\\n        from langchain.embeddings import OpenAIEmbeddings\\n        embeddings = OpenAIEmbeddings()\\n        elastic_vector_search = ElasticVectorSearch.from_texts(\\n            texts,\\n            embeddings,\\n            elasticsearch_url=\"http://localhost:9200\"\\n        )',\n",
       "   'l': False,\n",
       "   'g': ['*',\n",
       "    \"Register the server's public key with the client.\\n    :param s: The client's :class:`~httpx.core.http_client.Http_client` session.\\n    :return: None.\",\n",
       "    'Register the public key of the server in the session.\\n\\n    :return: None.',\n",
       "    ':return:\\n        :rtype: :class:`dh.DHPublicNumbers`']},\n",
       "  {'c': '    def __init__(\\n        self, file_path: str, mode: str = \"single\", **unstructured_kwargs: Any\\n    ):\\n        validate_unstructured_version(min_unstructured_version=\"0.6.8\")\\n        super().__init__(file_path=file_path, mode=mode, **unstructured_kwargs)',\n",
       "   'd': 'Handle an error for a chain run.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    ':param string: string to be modified\\n    :param state: state of the process\\n    :return: modified string',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': '    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        prompt = inputs[self.input_key]\\n\\n        _intent = self.sparql_intent_chain.run({\"prompt\": prompt}, callbacks=callbacks)\\n        intent = _intent.strip()\\n\\n        if \"SELECT\" in intent and \"UPDATE\" not in intent:\\n            sparql_generation_chain = self.sparql_generation_select_chain\\n            intent = \"SELECT\"\\n        elif \"UPDATE\" in intent and \"SELECT\" not in intent:\\n            sparql_generation_chain = self.sparql_generation_update_chain\\n            intent = \"UPDATE\"\\n        else:\\n            raise ValueError(\\n                \"I am sorry, but this prompt seems to fit none of the currently \"\\n                \"supported SPARQL query types, i.e., SELECT and UPDATE.\"\\n            )\\n\\n        _run_manager.on_text(\"Identified intent:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(intent, color=\"green\", end=\"\\\\n\", verbose=self.verbose)\\n\\n        generated_sparql = sparql_generation_chain.run(\\n            {\"prompt\": prompt, \"schema\": self.graph.get_schema}, callbacks=callbacks\\n        )\\n\\n        _run_manager.on_text(\"Generated SPARQL:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(\\n            generated_sparql, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n\\n        if intent == \"SELECT\":\\n            context = self.graph.query(generated_sparql)\\n\\n            _run_manager.on_text(\"Full Context:\", end=\"\\\\n\", verbose=self.verbose)\\n            _run_manager.on_text(\\n                str(context), color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n            )\\n            result = self.qa_chain(\\n                {\"prompt\": prompt, \"context\": context},\\n                callbacks=callbacks,\\n            )\\n            res = result[self.qa_chain.output_key]\\n        elif intent == \"UPDATE\":\\n            self.graph.update(generated_sparql)\\n            res = \"Successfully inserted triples into the graph.\"\\n        else:\\n            raise ValueError(\"Unsupported SPARQL query type.\")\\n        return {self.output_key: res}',\n",
       "   'd': 'Generate SPARQL query, use it to retrieve a response from the gdb and answer\\nthe question.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def search_api(query: str) -> str:\\n    assert isinstance(query, str)\\n    return \"API result\"',\n",
       "   'd': 'Search the API for the query.',\n",
       "   'l': True,\n",
       "   'g': [':param embed_size: \\n    :type embed_size: int',\n",
       "    'Parameters\\n    ----------\\n    embed_size : int\\n        size of the final encoded vector.',\n",
       "    '',\n",
       "    ':param embed_size: \\n    :type embed_size: int\\n    :return: \\n    :rtype:']},\n",
       "  {'c': 'def __init__(self, engine: Engine, cache_schema: Type[FullLLMCache] = FullLLMCache):\\n    self.engine = engine\\n    self.cache_schema = cache_schema\\n    self.cache_schema.metadata.create_all(self.engine)',\n",
       "   'd': 'Initialize by creating all tables.',\n",
       "   'l': True,\n",
       "   'g': ['.cost(self, s_start, s_goal):',\n",
       "    '_cost(self, s_start, s_goal):',\n",
       "    '_cost(self, s_start, s_goal):',\n",
       "    '_cost(self, s_start, s_goal):\\n\\n    Return the euclidean distance between two states.\\n\\n    :param s_start: start state\\n    :param s_goal: goal state\\n    :return: cost']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        config: Mapping[str, Any],\\n        stream_name: str,\\n        record_handler: Optional[RecordHandler] = None,\\n        state: Optional[Any] = None,\\n    ) -> None:\\n        source_class = guard_import(\\n            \"source_stripe\", pip_name=\"airbyte-source-stripe\"\\n        ).SourceStripe\\n        super().__init__(\\n            config=config,\\n            source_class=source_class,\\n            stream_name=stream_name,\\n            record_handler=record_handler,\\n            state=state,\\n        )',\n",
       "   'd': 'Initializes the loader.\\n\\nArgs:\\n    config: The config to pass to the source connector.\\n    stream_name: The name of the stream to load.\\n    record_handler: A function that takes in a record and an optional id and\\n        returns a Document. If None, the record will be used as the document.\\n        Defaults to None.\\n    state: The state to pass to the source connector. Defaults to None.',\n",
       "   'l': True,\n",
       "   'g': ['.display_markdown_message(message)\\n\\n    Displays a message as a markdown message.\\n\\n    :param message: The message to display.\\n\\n    :return: None.',\n",
       "    'Display a message as a markdown formatted message.\\n\\n  :param message: The message to display.',\n",
       "    'Display a message in markdown format.\\n\\n  :param message: The message to display.',\n",
       "    'Display a message in a rich text format.\\n\\n    :param message: The message to display.']},\n",
       "  {'c': 'def return_stopped_response(\\n    self,\\n    early_stopping_method: str,\\n    intermediate_steps: List[Tuple[AgentAction, str]],\\n    **kwargs: Any,\\n) -> AgentFinish:\\n    if early_stopping_method == \"force\":\\n\\n        return AgentFinish(\\n            {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\\n        )\\n    elif early_stopping_method == \"generate\":\\n\\n        thoughts = \"\"\\n        for action, observation in intermediate_steps:\\n            thoughts += action.log\\n            thoughts += (\\n                f\"\\\\n{self.observation_prefix}{observation}\\\\n{self.llm_prefix}\"\\n            )\\n\\n        thoughts += (\\n            \"\\\\n\\\\nI now need to return a final answer based on the previous steps:\"\\n        )\\n        new_inputs = {\"agent_scratchpad\": thoughts, \"stop\": self._stop}\\n        full_inputs = {**kwargs, **new_inputs}\\n        full_output = self.llm_chain.predict(**full_inputs)\\n\\n        parsed_output = self.output_parser.parse(full_output)\\n        if isinstance(parsed_output, AgentFinish):\\n\\n            return parsed_output\\n        else:\\n\\n\\n            return AgentFinish({\"output\": full_output}, full_output)\\n    else:\\n        raise ValueError(\\n            \"early_stopping_method should be one of `force` or `generate`, \"\\n            f\"got {early_stopping_method}\"\\n        )',\n",
       "   'd': 'Clear cache.',\n",
       "   'l': False,\n",
       "   'g': ['Return  the  state  of  the  widget  as',\n",
       "    '',\n",
       "    '',\n",
       "    '(Callable)\\n    Returns the current state of the widget.\\n    :return: A dictionary of the current state.\\n    :rtype: dict']},\n",
       "  {'c': 'def _llm_type(self) -> str:\\n    return \"fireworks-chat\"',\n",
       "   'd': 'Validate that the python package exists in environment.',\n",
       "   'l': False,\n",
       "   'g': ['Build a data parallel model.\\n\\n  Args:\\n      model (nn.Module): The model to be built.\\n      device (str): The device to build the model on.\\n      dim (int): The dimension of the data parallel model.\\n      *args: Additional arguments to the DataParallel constructor.\\n      **kwargs: Additional keyword arguments to the DataParallel constructor.\\n\\n  Returns:\\n      nn.DataParallel: The built data parallel model.',\n",
       "    ':param model:\\n      :param device:\\n      :param dim:\\n      :param args:\\n      :param kwargs:\\n    :return:',\n",
       "    \"Build a DataParallel model.\\n\\n  Args:\\n      model (nn.Module): The model to be built.\\n      device (str): The device where the model will be put on.\\n          Options include 'cuda', 'mlu'.\\n      dim (int): The dimension of the data to be put on.\\n          Options include 0, 1, 2.\\n      *args: Additional arguments for the DataParallel model.\\n      **kwargs: Additional keyword arguments for the DataParallel model.\\n\\n  Returns:\\n      nn.DataParallel: A DataParallel model.\",\n",
       "    \"Build a DataParallel or MLUDataParallel.\\n\\n    Args:\\n        model (nn.Module): The model to be built.\\n        device (str): The device to be used.\\n        dim (int): The dimension to be used.\\n        *args: The arguments to be passed to the DataParallel or MLUDataParallel.\\n        **kwargs: The keyword arguments to be passed to the DataParallel or MLUDataParallel.\\n\\n    Returns:\\n        DataParallel or MLUDataParallel.\\n\\n    Example:\\n        >>> from torch import nn\\n        >>> model = nn.Sequential(...)\\n        >>> model = build_dp(model, 'cuda', 0)\\n        >>> model = build_dp(model, 'mlu', 0)\"]},\n",
       "  {'c': 'def split_text(self, text: str) -> List[str]:\\n    splits = self._tokenizer(text)\\n    return self._merge_splits(splits, self._separator)',\n",
       "   'd': 'Initialize with necessary components.',\n",
       "   'l': False,\n",
       "   'g': [':param addr: \\n    :param verbose:',\n",
       "    'Returns the unsortedbin at the given address.\\n\\n    :param addr:  The address to retrieve the unsortedbin at.\\n    :type addr:  int\\n\\n    :return:     The unsortedbin at the given address.\\n    :rtype:      int\\n\\n    :Example:\\n    >>> unsortedbin(0x0000000000000000000000000000000000000000)\\n    0x0000000000000000000000000000000000000000\\n    0x0000000000000000000000000000000000000000\\n    0x0000000000000000000000000000000000000000\\n    0x0000000',\n",
       "    'Gets unsortedbin from the current heap.\\n\\n    :param addr: Address to get the unsortedbin from.\\n    :param verbose: If to print the unsortedbin.\\n    :return: The unsortedbin.\\n    :type: bin',\n",
       "    ':param addr: Address to check\\n    :type addr: int\\n    :param verbose: Print formatted bins\\n    :type verbose: bool\\n    :return: None\\n    :rtype: None']},\n",
       "  {'c': 'def _run(\\n    self,\\n    *args: Any,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n    **kwargs: Any,\\n) -> Any:\\n    new_argument_supported = signature(self.func).parameters.get(\"callbacks\")\\n    return (\\n        self.func(\\n            *args,\\n            callbacks=run_manager.get_child() if run_manager else None,\\n            **kwargs,\\n        )\\n        if new_argument_supported\\n        else self.func(*args, **kwargs)\\n    )',\n",
       "   'd': 'Use the tool.',\n",
       "   'l': True,\n",
       "   'g': [':param x: input tensor\\n    :return: output tensor',\n",
       "    ':param x: \\n    :return:',\n",
       "    '',\n",
       "    ':param x: \\n    :return:']},\n",
       "  {'c': \"def check_if_link_is_working(link: str) -> Tuple[bool, str]:\\n    has_error = False\\n    error_message = ''\\n\\n    try:\\n        resp = requests.get(link + '/', timeout=25, headers={\\n            'User-Agent': fake_user_agent(),\\n            'host': get_host_from_link(link)\\n        })\\n\\n        code = resp.status_code\\n\\n        if code >= 400 and not has_cloudflare_protection(resp):\\n            has_error = True\\n            error_message = f'ERR:CLT: {code} : {link}'\\n\\n    except requests.exceptions.SSLError as error:\\n        has_error = True\\n        error_message = f'ERR:SSL: {error} : {link}'\\n\\n    except requests.exceptions.ConnectionError as error:\\n        has_error = True\\n        error_message = f'ERR:CNT: {error} : {link}'\\n\\n    except (TimeoutError, requests.exceptions.ConnectTimeout):\\n        has_error = True\\n        error_message = f'ERR:TMO: {link}'\\n\\n    except requests.exceptions.TooManyRedirects as error:\\n        has_error = True\\n        error_message = f'ERR:TMR: {error} : {link}'\\n\\n    except (Exception, requests.exceptions.RequestException) as error:\\n        has_error = True\\n        error_message = f'ERR:UKN: {error} : {link}'\\n\\n    return (has_error, error_message)\",\n",
       "   'd': 'Handle an error for a tool run.',\n",
       "   'l': False,\n",
       "   'g': ['_get_study_direction_from_study_id_',\n",
       "    '_get_study_direction(self, study_id: int) -> StudyDirection:\\n\\n_get_study_direction(self, study_id: int) -> StudyDirection:\\n\\n_get_study_direction(self, study_id: int) -> StudyDirection:\\n\\n_get_study_direction(self, study_id: int) -> StudyDirection:\\n\\n_get_study_direction(self, study_id: int) -> StudyDirection:\\n\\n_get_study_direction(self, study_id: int) -> StudyDirection:\\n\\n_get_study_direction(self, study_id: int) -> StudyDirection:\\n\\n_get_study_direction(self, study_id: int) -> StudyDirection:\\n\\n_get_study_direction(self, study_id: int) -> StudyDirection:\\n\\n_get_study_direction(self, study_id: int) -> StudyDirection:\\n\\n_get_study_direction(self, study_id: int) -> StudyDirection:\\n\\n_get_study_direction(self, study_id: int) -> StudyDirection:\\n\\n_get_study_direction(self, study_id: int) ->',\n",
       "    '_get_study_direction(self, study_id: int) -> StudyDirection:',\n",
       "    '_get_study_direction_from_study_id_is_not_implemented_by_implementation_of_this_method_error_']},\n",
       "  {'c': '    def embed_query(self, text: str) -> List[float]:\\n        return self._embed(text)',\n",
       "   'd': 'Return the output parser type for serialization.',\n",
       "   'l': False,\n",
       "   'g': ['', ':param key:', ':param key:\\n      :return:', '']},\n",
       "  {'c': '    def similarity_search(\\n        self, query: str, k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        docs_with_scores = self.similarity_search_with_score(query, k, **kwargs)\\n        return [doc[0] for doc in docs_with_scores]',\n",
       "   'd': 'Make tools out of functions, can be used with or without arguments.\\n\\nArgs:\\n    *args: The arguments to the tool.\\n    return_direct: Whether to return directly from the tool rather\\n        than continuing the agent loop.\\n    args_schema: optional argument schema for user to specify\\n    infer_schema: Whether to infer the schema of the arguments from\\n        the function\\'s signature. This also makes the resultant tool\\n        accept a dictionary input to its `run()` function.\\n\\nRequires:\\n    - Function must be of type (str) -> str\\n    - Function must have a docstring\\n\\nExamples:\\n    .. code-block:: python\\n\\n        @tool\\n        def search_api(query: str) -> str:\\n            # Searches the API for the query.\\n            return\\n\\n        @tool(\"search\", return_direct=True)\\n        def search_api(query: str) -> str:\\n            # Searches the API for the query.\\n            return',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def clear(self, **kwargs: Any) -> None:\\n    self._cache = {}',\n",
       "   'd': 'Lazily parse the blob.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', 'Returns the maximum sequence length in the model.']},\n",
       "  {'c': '    def get_token_ids(self, text: str) -> List[int]:\\n        if sys.version_info[1] < 8:\\n            return super().get_token_ids(text)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate get_num_tokens. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        enc = tiktoken.encoding_for_model(self.model_name)\\n        return enc.encode(\\n            text,\\n            allowed_special=self.allowed_special,\\n            disallowed_special=self.disallowed_special,\\n        )',\n",
       "   'd': 'Get the token IDs using the tiktoken package.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def __init__(\\n    self,\\n    opensearch_url: str,\\n    index_name: str,\\n    embedding_function: Embeddings,\\n    **kwargs: Any,\\n):\\n    self.embedding_function = embedding_function\\n    self.index_name = index_name\\n    http_auth = _get_kwargs_value(kwargs, \"http_auth\", None)\\n    self.is_aoss = _is_aoss_enabled(http_auth=http_auth)\\n    self.client = _get_opensearch_client(opensearch_url, **kwargs)',\n",
       "   'd': 'Initialize with necessary components.',\n",
       "   'l': True,\n",
       "   'g': ['获取年报增长数据\\n    :param year: 年\\n    :param quarter: 季度\\n    :return:',\n",
       "    '获取年报数据\\n    :param year:\\n    :param quarter:\\n    :return:',\n",
       "    '获取数据\\n    :param year: 年\\n    :param quarter: 季度\\n    :return: 股票数据',\n",
       "    '获取增长数据\\n    :param year: 年\\n    :param quarter: 季度\\n    :return: 增长数据\\n    :rtype: pd.DataFrame']},\n",
       "  {'c': 'def lazy_import_playwright_browsers() -> Tuple[Type[AsyncBrowser], Type[SyncBrowser]]:\\n    try:\\n        from playwright.async_api import Browser as AsyncBrowser\\n        from playwright.sync_api import Browser as SyncBrowser\\n    except ImportError:\\n        raise ImportError(\\n            \"The \\'playwright\\' package is required to use the playwright tools.\"\\n            \" Please install it with \\'pip install playwright\\'.\"\\n        )\\n    return AsyncBrowser, SyncBrowser',\n",
       "   'd': 'Initialize with a file path.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def _run(\\n    self,\\n    query: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> str:\\n    return self.llm_chain.predict(\\n        query=query,\\n        dialect=self.db.dialect,\\n        callbacks=run_manager.get_child() if run_manager else None,\\n    )',\n",
       "   'd': 'Use the LLM to check the query.',\n",
       "   'l': True,\n",
       "   'g': ['Test static proxy request.\\n\\n  :return:',\n",
       "    'Test static proxy requests.',\n",
       "    'Test static proxy requests.',\n",
       "    'Test static proxy request.\\n\\n  :return: None.']},\n",
       "  {'c': 'def tool(\\n    *args: Union[str, Callable],\\n    return_direct: bool = False,\\n    args_schema: Optional[Type[BaseModel]] = None,\\n    infer_schema: bool = True,\\n) -> Callable:\\n    def _make_with_name(tool_name: str) -> Callable:\\n        def _make_tool(dec_func: Callable) -> BaseTool:\\n            if inspect.iscoroutinefunction(dec_func):\\n                coroutine = dec_func\\n                func = None\\n            else:\\n                coroutine = None\\n                func = dec_func\\n\\n            if infer_schema or args_schema is not None:\\n                return StructuredTool.from_function(\\n                    func,\\n                    coroutine,\\n                    name=tool_name,\\n                    return_direct=return_direct,\\n                    args_schema=args_schema,\\n                    infer_schema=infer_schema,\\n                )\\n\\n\\n            if func.__doc__ is None:\\n                raise ValueError(\\n                    \"Function must have a docstring if \"\\n                    \"description not provided and infer_schema is False.\"\\n                )\\n            return Tool(\\n                name=tool_name,\\n                func=func,\\n                description=f\"{tool_name} tool\",\\n                return_direct=return_direct,\\n                coroutine=coroutine,\\n            )\\n\\n        return _make_tool\\n\\n    if len(args) == 1 and isinstance(args[0], str):\\n\\n\\n        return _make_with_name(args[0])\\n    elif len(args) == 1 and callable(args[0]):\\n\\n\\n        return _make_with_name(args[0].__name__)(args[0])\\n    elif len(args) == 0:\\n\\n\\n        def _partial(func: Callable[[str], str]) -> BaseTool:\\n            return _make_with_name(func.__name__)(func)\\n\\n        return _partial\\n    else:\\n        raise ValueError(\"Too many arguments for tool decorator\")',\n",
       "   'd': 'Test instantiating APIRequestBody from RequestBody with a schema.',\n",
       "   'l': False,\n",
       "   'g': ['This function is called by the server to set the model to use.\\n  :param model: The name of the model to use.',\n",
       "    '',\n",
       "    '',\n",
       "    'Set the model to use in the environment.\\n\\n    :param model: The name of the model to use.']},\n",
       "  {'c': '    def configure(\\n        cls,\\n        inheritable_callbacks: Callbacks = None,\\n        local_callbacks: Callbacks = None,\\n        verbose: bool = False,\\n        inheritable_tags: Optional[List[str]] = None,\\n        local_tags: Optional[List[str]] = None,\\n        inheritable_metadata: Optional[Dict[str, Any]] = None,\\n        local_metadata: Optional[Dict[str, Any]] = None,\\n    ) -> AsyncCallbackManager:\\n        return _configure(\\n            cls,\\n            inheritable_callbacks,\\n            local_callbacks,\\n            verbose,\\n            inheritable_tags,\\n            local_tags,\\n            inheritable_metadata,\\n            local_metadata,\\n        )',\n",
       "   'd': 'Configure the async callback manager.\\n\\nArgs:\\n    inheritable_callbacks (Optional[Callbacks], optional): The inheritable\\n        callbacks. Defaults to None.\\n    local_callbacks (Optional[Callbacks], optional): The local callbacks.\\n        Defaults to None.\\n    verbose (bool, optional): Whether to enable verbose mode. Defaults to False.\\n    inheritable_tags (Optional[List[str]], optional): The inheritable tags.\\n        Defaults to None.\\n    local_tags (Optional[List[str]], optional): The local tags.\\n        Defaults to None.\\n    inheritable_metadata (Optional[Dict[str, Any]], optional): The inheritable\\n        metadata. Defaults to None.\\n    local_metadata (Optional[Dict[str, Any]], optional): The local metadata.\\n        Defaults to None.\\n\\nReturns:\\n    AsyncCallbackManager: The configured async callback manager.',\n",
       "   'l': True,\n",
       "   'g': ['_get_value_trackers()',\n",
       "    '_get_value_trackers',\n",
       "    '_get_value_trackers',\n",
       "    '_get_value_trackers()']},\n",
       "  {'c': '    def input_variables(self) -> List[str]:\\n        return self.prompt.input_variables',\n",
       "   'd': 'Input variables for this prompt template.\\n\\nReturns:\\n    List of input variable names.',\n",
       "   'l': True,\n",
       "   'g': ['.test_cleanup() - test that the cleanup method works.',\n",
       "    '.',\n",
       "    '.test_cleanup() should remove all the jobs from the registry.',\n",
       "    '.']},\n",
       "  {'c': '    def generate(\\n        self,\\n        prompts: List[str],\\n        stop: Optional[List[str]] = None,\\n        callbacks: Optional[Union[Callbacks, List[Callbacks]]] = None,\\n        *,\\n        tags: Optional[Union[List[str], List[List[str]]]] = None,\\n        metadata: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\\n        **kwargs: Any,\\n    ) -> LLMResult:\\n        if not isinstance(prompts, list):\\n            raise ValueError(\\n                \"Argument \\'prompts\\' is expected to be of type List[str], received\"\\n                f\" argument of type {type(prompts)}.\"\\n            )\\n\\n        if (\\n            isinstance(callbacks, list)\\n            and callbacks\\n            and (\\n                isinstance(callbacks[0], (list, BaseCallbackManager))\\n                or callbacks[0] is None\\n            )\\n        ):\\n\\n            assert len(callbacks) == len(prompts)\\n            assert tags is None or (\\n                isinstance(tags, list) and len(tags) == len(prompts)\\n            )\\n            assert metadata is None or (\\n                isinstance(metadata, list) and len(metadata) == len(prompts)\\n            )\\n            callbacks = cast(List[Callbacks], callbacks)\\n            tags_list = cast(List[Optional[List[str]]], tags or ([None] * len(prompts)))\\n            metadata_list = cast(\\n                List[Optional[Dict[str, Any]]], metadata or ([{}] * len(prompts))\\n            )\\n            callback_managers = [\\n                CallbackManager.configure(\\n                    callback,\\n                    self.callbacks,\\n                    self.verbose,\\n                    tag,\\n                    self.tags,\\n                    meta,\\n                    self.metadata,\\n                )\\n                for callback, tag, meta in zip(callbacks, tags_list, metadata_list)\\n            ]\\n        else:\\n\\n            callback_managers = [\\n                CallbackManager.configure(\\n                    cast(Callbacks, callbacks),\\n                    self.callbacks,\\n                    self.verbose,\\n                    cast(List[str], tags),\\n                    self.tags,\\n                    cast(Dict[str, Any], metadata),\\n                    self.metadata,\\n                )\\n            ] * len(prompts)\\n\\n        params = self.dict()\\n        params[\"stop\"] = stop\\n        options = {\"stop\": stop}\\n        (\\n            existing_prompts,\\n            llm_string,\\n            missing_prompt_idxs,\\n            missing_prompts,\\n        ) = get_prompts(params, prompts)\\n        disregard_cache = self.cache is not None and not self.cache\\n        new_arg_supported = inspect.signature(self._generate).parameters.get(\\n            \"run_manager\"\\n        )\\n        if langchain.llm_cache is None or disregard_cache:\\n            if self.cache is not None and self.cache:\\n                raise ValueError(\\n                    \"Asked to cache, but no cache found at `langchain.cache`.\"\\n                )\\n            run_managers = [\\n                callback_manager.on_llm_start(\\n                    dumpd(self), [prompt], invocation_params=params, options=options\\n                )[0]\\n                for callback_manager, prompt in zip(callback_managers, prompts)\\n            ]\\n            output = self._generate_helper(\\n                prompts, stop, run_managers, bool(new_arg_supported), **kwargs\\n            )\\n            return output\\n        if len(missing_prompts) > 0:\\n            run_managers = [\\n                callback_managers[idx].on_llm_start(\\n                    dumpd(self),\\n                    [prompts[idx]],\\n                    invocation_params=params,\\n                    options=options,\\n                )[0]\\n                for idx in missing_prompt_idxs\\n            ]\\n            new_results = self._generate_helper(\\n                missing_prompts, stop, run_managers, bool(new_arg_supported), **kwargs\\n            )\\n            llm_output = update_cache(\\n                existing_prompts, llm_string, missing_prompt_idxs, new_results, prompts\\n            )\\n            run_info = (\\n                [RunInfo(run_id=run_manager.run_id) for run_manager in run_managers]\\n                if run_managers\\n                else None\\n            )\\n        else:\\n            llm_output = {}\\n            run_info = None\\n        generations = [existing_prompts[i] for i in range(len(prompts))]\\n        return LLMResult(generations=generations, llm_output=llm_output, run=run_info)',\n",
       "   'd': 'Clear the cache.\\n\\nRaises:\\n    SdkException: Momento service or network error',\n",
       "   'l': False,\n",
       "   'g': ['_top(self, body_output, _):\\n\\n    This function takes in a body_output and returns a prediction.\\n\\n    Args:\\n      body_output: A tensor of shape (batch_size, max_length, 1)\\n      _ : A placeholder for the number of timesteps in the body_output.\\n\\n    Returns:\\n      A tensor of shape (batch_size, 1)',\n",
       "    '_top(self, body_output, _):\\n\\n    :param body_output:\\n    :return:',\n",
       "    '(x) -> (y)',\n",
       "    '_top(self, body_output, _):']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        cohere_api_key = get_from_dict_or_env(\\n            values, \"cohere_api_key\", \"COHERE_API_KEY\"\\n        )\\n        try:\\n            import cohere\\n\\n            values[\"client\"] = cohere.Client(cohere_api_key)\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import cohere python package. \"\\n                \"Please install it with `pip install cohere`.\"\\n            )\\n        return values',\n",
       "   'd': 'Initialize with Marqo client.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    ':param bboxes:\\n  :param img_shape:\\n  :param scale_factor:\\n  :param flip:\\n  :return:\\n  :',\n",
       "    '',\n",
       "    ':param bboxes:\\n      :param img_shape:\\n      :param scale_factor:\\n      :param flip:\\n      :return:']},\n",
       "  {'c': 'def load_llm_from_config(config: dict) -> BaseLLM:\\n    if \"_type\" not in config:\\n        raise ValueError(\"Must specify an LLM Type in config\")\\n    config_type = config.pop(\"_type\")\\n\\n    if config_type not in type_to_cls_dict:\\n        raise ValueError(f\"Loading {config_type} LLM not supported\")\\n\\n    llm_cls = type_to_cls_dict[config_type]\\n    return llm_cls(**config)',\n",
       "   'd': 'Lazy load the chat sessions from the iMessage chat.db\\nand yield them in the required format.\\n\\nYields:\\n    ChatSession: Loaded chat session.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def _run(\\n    self,\\n    *args: Any,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n    **kwargs: Any,\\n) -> Any:\\n    new_argument_supported = signature(self.func).parameters.get(\"callbacks\")\\n    return (\\n        self.func(\\n            *args,\\n            callbacks=run_manager.get_child() if run_manager else None,\\n            **kwargs,\\n        )\\n        if new_argument_supported\\n        else self.func(*args, **kwargs)\\n    )',\n",
       "   'd': 'Use the tool.',\n",
       "   'l': True,\n",
       "   'g': ['_cell_to_text',\n",
       "    '_cell_to_text:\\n    :return: str',\n",
       "    '_cell_to_text(self):',\n",
       "    '_cell_to_text:\\n    :return: str']},\n",
       "  {'c': '    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[Dict[Any, Any]]] = None,\\n        **kwargs: Any,\\n    ) -> ElasticKnnSearch:\\n        index_name = kwargs.get(\"index_name\", str(uuid.uuid4()))\\n        es_connection = kwargs.get(\"es_connection\")\\n        es_cloud_id = kwargs.get(\"es_cloud_id\")\\n        es_user = kwargs.get(\"es_user\")\\n        es_password = kwargs.get(\"es_password\")\\n        vector_query_field = kwargs.get(\"vector_query_field\", \"vector\")\\n        query_field = kwargs.get(\"query_field\", \"text\")\\n        model_id = kwargs.get(\"model_id\")\\n        dims = kwargs.get(\"dims\")\\n\\n        if dims is None:\\n            raise ValueError(\"ElasticKnnSearch requires \\'dims\\' parameter\")\\n\\n        optional_args = {}\\n\\n        if vector_query_field is not None:\\n            optional_args[\"vector_query_field\"] = vector_query_field\\n\\n        if query_field is not None:\\n            optional_args[\"query_field\"] = query_field\\n\\n        knnvectorsearch = cls(\\n            index_name=index_name,\\n            embedding=embedding,\\n            es_connection=es_connection,\\n            es_cloud_id=es_cloud_id,\\n            es_user=es_user,\\n            es_password=es_password,\\n            **optional_args,\\n        )\\n\\n        knnvectorsearch.add_texts(texts, model_id=model_id, dims=dims, **optional_args)\\n\\n        return knnvectorsearch',\n",
       "   'd': 'Get a child callback manager.\\n\\nArgs:\\n    tag (str, optional): The tag for the child callback manager.\\n        Defaults to None.\\n\\nReturns:\\n    AsyncCallbackManager: The child callback manager.',\n",
       "   'l': False,\n",
       "   'g': ['(output_sequence: torch.Tensor, output_sequence_mask: torch.Tensor) -> torch.Tensor:',\n",
       "    '_get_reward(self, output_sequence: torch.Tensor, output_sequence_mask: torch.Tensor) -> torch.Tensor:',\n",
       "    '(output_sequence: torch.Tensor, output_sequence_mask: torch.Tensor) -> torch.Tensor:',\n",
       "    '(output_sequence: torch.Tensor, output_sequence_mask: torch.Tensor) -> torch.Tensor:']},\n",
       "  {'c': 'def from_llm(\\n    cls,\\n    llm: BaseLanguageModel,\\n    db: SQLDatabase,\\n    query_prompt: BasePromptTemplate = PROMPT,\\n    decider_prompt: BasePromptTemplate = DECIDER_PROMPT,\\n    **kwargs: Any,\\n) -> SQLDatabaseSequentialChain:\\n    sql_chain = SQLDatabaseChain.from_llm(llm, db, prompt=query_prompt, **kwargs)\\n    decider_chain = LLMChain(\\n        llm=llm, prompt=decider_prompt, output_key=\"table_names\"\\n    )\\n    return cls(sql_chain=sql_chain, decider_chain=decider_chain, **kwargs)',\n",
       "   'd': 'Load the necessary chains.',\n",
       "   'l': True,\n",
       "   'g': ['(text)', '', '(text)', '']},\n",
       "  {'c': 'def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n    for gen in return_val:\\n        if not isinstance(gen, Generation):\\n            raise ValueError(\\n                \"RedisSemanticCache only supports caching of \"\\n                f\"normal LLM generations, got {type(gen)}\"\\n            )\\n        if isinstance(gen, ChatGeneration):\\n            warnings.warn(\\n                \"NOTE: Generation has not been cached. RedisSentimentCache does not\"\\n                \" support caching ChatModel outputs.\"\\n            )\\n            return\\n    llm_cache = self._get_llm_cache(llm_string)\\n    _dump_generations_to_json([g for g in return_val])\\n    metadata = {\\n        \"llm_string\": llm_string,\\n        \"prompt\": prompt,\\n        \"return_val\": _dump_generations_to_json([g for g in return_val]),\\n    }\\n    llm_cache.add_texts(texts=[prompt], metadatas=[metadata])',\n",
       "   'd': 'Query NebulaGraph database.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Ask a question.\\n  :param args: The question.\\n  :param prompt_type: The type of the prompt.\\n  :return: The answer.\\n  :raises: ValueError if the prompt type is invalid.\\n  :see: https://python-prompt.readthedocs.io/en/latest/index.html#python-prompt-api-reference-functions-and-methods\\n  :see: https://python-prompt.readthedocs.io/en/latest/index.html#python-prompt-api-reference-functions-and-methods-1\\n  :see: https://python-prompt.readthedocs.io/en/latest/index.html#python-prompt-api-reference-functions-and-methods-2\\n  :see: https://python-prompt.readthedocs.io/en/latest/index.html#python-prompt-api-reference-functions-and-methods-3\\n  :see: https://python-prompt.readthedocs.io/en/latest/index.html#python-prompt-api-reference-functions-and-methods-4\\n  :see: https://python-',\n",
       "    '(str | int | None) *args, **kwargs: Any) -> str:',\n",
       "    'Ask a question.\\n\\n  Parameters\\n  ----------\\n  prompt_type : type[str] | type[int] | None\\n    The type of prompt to use. If None, then the type of prompt is\\n    determined by the type of the first argument. If the first\\n    argument is a string, then the type of prompt is string. If\\n    the first argument is an integer, then the type of prompt is\\n    int. If the first argument is not a string or integer, then\\n    an error is raised.']},\n",
       "  {'c': 'def _handle_event(\\n    handlers: List[BaseCallbackHandler],\\n    event_name: str,\\n    ignore_condition_name: Optional[str],\\n    *args: Any,\\n    **kwargs: Any,\\n) -> None:\\n    message_strings: Optional[List[str]] = None\\n    for handler in handlers:\\n        try:\\n            if ignore_condition_name is None or not getattr(\\n                handler, ignore_condition_name\\n            ):\\n                getattr(handler, event_name)(*args, **kwargs)\\n        except NotImplementedError as e:\\n            if event_name == \"on_chat_model_start\":\\n                if message_strings is None:\\n                    message_strings = [get_buffer_string(m) for m in args[1]]\\n                _handle_event(\\n                    [handler],\\n                    \"on_llm_start\",\\n                    \"ignore_llm\",\\n                    args[0],\\n                    message_strings,\\n                    *args[2:],\\n                    **kwargs,\\n                )\\n            else:\\n                logger.warning(\\n                    f\"NotImplementedError in {handler.__class__.__name__}.{event_name}\"\\n                    f\" callback: {e}\"\\n                )\\n        except Exception as e:\\n            logger.warning(\\n                f\"Error in {handler.__class__.__name__}.{event_name} callback: {e}\"\\n            )\\n            if handler.raise_error:\\n                raise e',\n",
       "   'd': 'Generic event handler for CallbackManager.',\n",
       "   'l': True,\n",
       "   'g': ['(name, value, minval=None, maxval=None, check_fn=None)',\n",
       "    '(self, name, value, minval=None, maxval=None, check_fn=None):',\n",
       "    '(name, value, minval=None, maxval=None, check_fn=None)',\n",
       "    '_init_']},\n",
       "  {'c': 'def test_query_chain(self) -> None:\\n    llm = OpenAI(temperature=0, max_tokens=512)\\n    query_chain = QueryChain.from_univariate_prompt(llm)\\n    narrative_question = \"How many pets will Marcia end up with? \"\\n    data = query_chain(narrative_question)[Constant.chain_data.value]\\n    self.assertEqual(type(data), QueryModel)',\n",
       "   'd': 'Test end to end construction and search.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def plan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[List[AgentAction], AgentFinish]:\\n',\n",
       "   'd': 'Given input, decided what to do.\\n\\nArgs:\\n    intermediate_steps: Steps the LLM has taken to date,\\n        along with the observations.\\n    callbacks: Callbacks to run.\\n    **kwargs: User inputs.\\n\\nReturns:\\n    Actions specifying what tool to use.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def similarity_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        pre_filter: Optional[dict] = None,\\n        post_filter_pipeline: Optional[List[Dict]] = None,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        docs_and_scores = self.similarity_search_with_score(\\n            query,\\n            k=k,\\n            pre_filter=pre_filter,\\n            post_filter_pipeline=post_filter_pipeline,\\n        )\\n        return [doc for doc, _ in docs_and_scores]',\n",
       "   'd': 'Return MongoDB documents most similar to query.\\n\\nUse the knnBeta Operator available in MongoDB Atlas Search\\nThis feature is in early access and available only for evaluation purposes, to\\nvalidate functionality, and to gather feedback from a small closed group of\\nearly access users. It is not recommended for production deployments as we may\\nintroduce breaking changes.\\nFor more: https://www.mongodb.com/docs/atlas/atlas-search/knn-beta\\n\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Optional Number of Documents to return. Defaults to 4.\\n    pre_filter: Optional Dictionary of argument(s) to prefilter on document\\n        fields.\\n    post_filter_pipeline: Optional Pipeline of MongoDB aggregation stages\\n        following the knnBeta search.\\n\\nReturns:\\n    List of Documents most similar to the query and score for each',\n",
       "   'l': True,\n",
       "   'g': [':return:',\n",
       "    '',\n",
       "    '',\n",
       "    ':return: a dictionary of hyperparameters for this layer.\\n  :rtype: dict\\n  :Example:\\n    >>> class Embedding(Layer):\\n    ...     def hyperparameters(self):\\n    ...         return {\\n    ...             \"layer\": \"Embedding\",\\n    ...             \"init\": self.init,\\n    ...             \"pool\": self.pool,\\n    ...             \"n_out\": self.n_out,\\n    ...             \"vocab_size\": self.vocab_size,\\n    ...             \"optimizer\": {\\n    ...                 \"cache\": self.optimizer.cache,\\n    ...                 \"hyperparameters\": self.optimizer.hyperparameters,\\n    ...             }\\n    ...         }\\n    ...     return self.hyperparameters()\\n    ...\\n    {\\'layer\\': \\'Embedding\\', \\'init\\': \\'random\\', \\'pool\\': \\'mean\\', \\'n_out\\': 100, \\'vocab_size\\': 1000, \\'optimizer\\': {\\'cache\\': \\'sgd\\', \\'hyperparameters\\': {\\'lr\\': 0.001, \\'momentum\\': 0.9}}}']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        doc_metadata: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        doc_hash = md5()\\n        for t in texts:\\n            doc_hash.update(t.encode())\\n        doc_id = doc_hash.hexdigest()\\n        if metadatas is None:\\n            metadatas = [{} for _ in texts]\\n        if doc_metadata:\\n            doc_metadata[\"source\"] = \"langchain\"\\n        else:\\n            doc_metadata = {\"source\": \"langchain\"}\\n        doc = {\\n            \"document_id\": doc_id,\\n            \"metadataJson\": json.dumps(doc_metadata),\\n            \"section\": [\\n                {\"text\": text, \"metadataJson\": json.dumps(md)}\\n                for text, md in zip(texts, metadatas)\\n            ],\\n        }\\n\\n        success_str = self._index_doc(doc)\\n        if success_str == \"E_ALREADY_EXISTS\":\\n            self._delete_doc(doc_id)\\n            self._index_doc(doc)\\n        elif success_str == \"E_NO_PERMISSIONS\":\\n            print(\\n                \"\"\"No permissions to add document to Vectara.\\n                Check your corpus ID, customer ID and API key\"\"\"\\n            )\\n        return [doc_id]',\n",
       "   'd': 'Run the LLM on the given prompt and input.',\n",
       "   'l': False,\n",
       "   'g': ['.init(self, label, interval=1, max_step=20)\\n    :param label: str',\n",
       "    '(label, interval=1, max_step=20)',\n",
       "    ':param label: str\\n:param interval: int\\n:param max_step: int\\n:return:',\n",
       "    '_init_\\n    :param label: label for the progress bar\\n    :param interval: interval for the progress bar\\n    :param max_step: max step for the progress bar\\n    :return:']},\n",
       "  {'c': 'def return_stopped_response(\\n    self,\\n    early_stopping_method: str,\\n    intermediate_steps: List[Tuple[AgentAction, str]],\\n    **kwargs: Any,\\n) -> AgentFinish:\\n    if early_stopping_method == \"force\":\\n\\n        return AgentFinish(\\n            {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\\n        )\\n    elif early_stopping_method == \"generate\":\\n\\n        thoughts = \"\"\\n        for action, observation in intermediate_steps:\\n            thoughts += action.log\\n            thoughts += (\\n                f\"\\\\n{self.observation_prefix}{observation}\\\\n{self.llm_prefix}\"\\n            )\\n\\n        thoughts += (\\n            \"\\\\n\\\\nI now need to return a final answer based on the previous steps:\"\\n        )\\n        new_inputs = {\"agent_scratchpad\": thoughts, \"stop\": self._stop}\\n        full_inputs = {**kwargs, **new_inputs}\\n        full_output = self.llm_chain.predict(**full_inputs)\\n\\n        parsed_output = self.output_parser.parse(full_output)\\n        if isinstance(parsed_output, AgentFinish):\\n\\n            return parsed_output\\n        else:\\n\\n\\n            return AgentFinish({\"output\": full_output}, full_output)\\n    else:\\n        raise ValueError(\\n            \"early_stopping_method should be one of `force` or `generate`, \"\\n            f\"got {early_stopping_method}\"\\n        )',\n",
       "   'd': 'Validate that either folder_id or document_ids is set, but not both.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'Update the latest price in all Boms.\\n  :return: None.',\n",
       "    'Update latest price in all Boms.']},\n",
       "  {'c': '    def __init__(self, **kwargs: Any):\\n        super().__init__(**kwargs)\\n        try:\\n            import sentence_transformers\\n\\n        except ImportError as exc:\\n            raise ImportError(\\n                \"Could not import sentence_transformers python package. \"\\n                \"Please install it with `pip install sentence_transformers`.\"\\n            ) from exc\\n\\n        self.client = sentence_transformers.SentenceTransformer(\\n            self.model_name, cache_folder=self.cache_folder, **self.model_kwargs\\n        )',\n",
       "   'd': 'Initialize the sentence_transformer.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> BaseMessage:\\n        text = self.prompt.format(**kwargs)\\n        return ChatMessage(\\n            content=text, role=self.role, additional_kwargs=self.additional_kwargs\\n        )',\n",
       "   'd': 'Whether this evaluator requires an input string.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '_UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo(self):\\n\\n    __UnameInfo',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': 'def on_retriever_start(\\n    self,\\n    serialized: Dict[str, Any],\\n    query: str,\\n    *,\\n    run_id: UUID,\\n    parent_run_id: Optional[UUID] = None,\\n    tags: Optional[List[str]] = None,\\n    metadata: Optional[Dict[str, Any]] = None,\\n    **kwargs: Any,\\n) -> None:\\n    parent_run_id_ = str(parent_run_id) if parent_run_id else None\\n    execution_order = self._get_execution_order(parent_run_id_)\\n    start_time = datetime.utcnow()\\n    if metadata:\\n        kwargs.update({\"metadata\": metadata})\\n    retrieval_run = Run(\\n        id=run_id,\\n        name=\"Retriever\",\\n        parent_run_id=parent_run_id,\\n        serialized=serialized,\\n        inputs={\"query\": query},\\n        extra=kwargs,\\n        events=[{\"name\": \"start\", \"time\": start_time}],\\n        start_time=start_time,\\n        execution_order=execution_order,\\n        child_execution_order=execution_order,\\n        tags=tags,\\n        child_runs=[],\\n        run_type=\"retriever\",\\n    )\\n    self._start_trace(retrieval_run)\\n    self._on_retriever_start(retrieval_run)',\n",
       "   'd': 'Run when chain ends running.\\n\\nArgs:\\n    outputs (Union[Dict[str, Any], Any]): The outputs of the chain.',\n",
       "   'l': False,\n",
       "   'g': ['This is a hack to make the type system happy.  The type system is\\n  not happy if we have a type that is a forward ref to a class\\n  that is not in the global namespace.  This function is a hack\\n  to make the type system happy.',\n",
       "    'Update forward references in the given namespace.\\n\\n  :param cls:  The class to update forward references for.\\n  :param localns:  The local namespace to update forward references for.\\n  :return:  None.',\n",
       "    'Update forward refs in the given namespace.\\n\\n  :param cls: The class to update forward refs for.\\n  :param localns: The namespace to update forward refs in.\\n  :return: None.\\n  :rtype: None.\\n  :raises: :class:`~typing.',\n",
       "    'Update forward references in the class definition.']},\n",
       "  {'c': '    def compute_metric(self, a: str, b: str) -> float:\\n        score = self.metric(a, b)\\n        if self.normalize_score and self.distance in (\\n            StringDistance.DAMERAU_LEVENSHTEIN,\\n            StringDistance.LEVENSHTEIN,\\n        ):\\n            score = score / max(len(a), len(b))\\n        return score',\n",
       "   'd': 'Compute the distance between two strings.\\n\\nArgs:\\n    a (str): The first string.\\n    b (str): The second string.\\n\\nReturns:\\n    float: The distance between the two strings.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)',\n",
       "   'd': 'The type of the input to this runnable.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Returns the URL of the current page.\\n\\n  :rtype: str\\n  :return: The URL of the current page.',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        messages = self.format_messages(**kwargs)\\n        return get_buffer_string(messages)',\n",
       "   'd': 'String buffer of memory.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Convert a time value to a datetime.time.',\n",
       "    '',\n",
       "    ':param value:\\n    :return:']},\n",
       "  {'c': '    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        vectors = np.array(\\n            self.embeddings.embed_documents([inputs[\"prediction\"], inputs[\"reference\"]])\\n        )\\n        score = self._compute_score(vectors)\\n        return {\"score\": score}',\n",
       "   'd': 'Get a child callback manager.\\n\\nArgs:\\n    tag (str, optional): The tag for the child callback manager.\\n        Defaults to None.\\n\\nReturns:\\n    CallbackManager: The child callback manager.',\n",
       "   'l': False,\n",
       "   'g': ['_stop(self, now=False):',\n",
       "    '_stop()\\n    Stop the coroutine.\\n    :param now: Stop the coroutine immediately if True.\\n    :type now: bool',\n",
       "    '_stop(self, now=False):',\n",
       "    '_stop(self, now=False):']},\n",
       "  {'c': 'def requires_reference(self) -> bool:\\n    return True',\n",
       "   'd': 'Combine two prompt templates.\\n\\nArgs:\\n    other: Another prompt template.\\n\\nReturns:\\n    Combined prompt template.',\n",
       "   'l': False,\n",
       "   'g': ['update budget\\n    :param budget_id:\\n    :param budget:\\n    :param Authorize:\\n    :return:',\n",
       "    'Update a budget.\\n\\n    Args:\\n        budget_id (int): The ID of the budget to update.\\n        budget (BudgetIn): The updated budget data.\\n    \\n    Returns:\\n        Budget: The updated budget.',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': 'def embed_query(self, text: str) -> List[float]:\\n    try:\\n        angle = float(text)\\n        return [math.cos(angle * math.pi), math.sin(angle * math.pi)]\\n    except ValueError:\\n\\n        return [0.0, 0.0]',\n",
       "   'd': 'Parse a single string model output into some structure.\\n\\nArgs:\\n    text: String output of a language model.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': False,\n",
       "   'g': ['_handle_starttag()',\n",
       "    '_handle_starttag -',\n",
       "    '_handle_starttag(self, tag, attrs):\\n    Handle the start tag of an HTML element.\\n\\n    :param tag: The tag name.\\n    :param attrs: The attributes of the tag.\\n    :return: None.',\n",
       "    '_handle_starttag:']},\n",
       "  {'c': 'def __init__(\\n    self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n) -> None:\\n    super().__init__(**kwargs)\\n    self._separator = separator\\n    self._is_separator_regex = is_separator_regex',\n",
       "   'd': \"Call out to Aleph Alpha's Document endpoint.\\n\\nArgs:\\n    texts: The list of texts to embed.\\n\\nReturns:\\n    List of embeddings, one for each text.\",\n",
       "   'l': False,\n",
       "   'g': ['(str fullname, str path=None, str target=None) -> Optional[ModuleSpec]',\n",
       "    '(path=None, target=None) -> ModuleSpec or None.',\n",
       "    '(fullname, path=None, target=None) -> ModuleSpec or None.',\n",
       "    '_find_spec(fullname, path=None, target=None) -> ModuleSpec or None.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        client: Any,\\n        embeddings: Embeddings,\\n        collection_name: str,\\n        text_key: str,\\n        embedding_key: str,\\n        workspace: str = \"commons\",\\n    ):\\n        try:\\n            from rockset import RocksetClient\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import rockset client python package. \"\\n                \"Please install it with `pip install rockset`.\"\\n            )\\n\\n        if not isinstance(client, RocksetClient):\\n            raise ValueError(\\n                f\"client should be an instance of rockset.RocksetClient, \"\\n                f\"got {type(client)}\"\\n            )\\n\\n        self._client = client\\n        self._collection_name = collection_name\\n        self._embeddings = embeddings\\n        self._text_key = text_key\\n        self._embedding_key = embedding_key\\n        self._workspace = workspace\\n\\n        try:\\n            self._client.set_application(\"langchain\")\\n        except AttributeError:\\n\\n            pass',\n",
       "   'd': 'Initialize with Rockset client.\\nArgs:\\n    client: Rockset client object\\n    collection: Rockset collection to insert docs / query\\n    embeddings: Langchain Embeddings object to use to generate\\n                embedding for given text.\\n    text_key: column in Rockset collection to use to store the text\\n    embedding_key: column in Rockset collection to use to store the embedding.\\n                   Note: We must apply `VECTOR_ENFORCE()` on this column via\\n                   Rockset ingest transformation.',\n",
       "   'l': True,\n",
       "   'g': ['爬楼梯\\n    :param n: 阶梯数\\n    :return: 爬到第n个阶梯的爬法',\n",
       "    ':param n: int\\n    :return: int\\n    :rtype: int\\n    :Example:\\n    >>> climb_stairs(1)\\n    1\\n    >>> climb_stairs(2)\\n    2\\n    >>> climb_stairs(3)\\n    3\\n    >>>',\n",
       "    '',\n",
       "    '爬楼梯，返回n阶���������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������']},\n",
       "  {'c': 'def _load_run_evaluators(\\n    config: RunEvalConfig,\\n    run_type: str,\\n    data_type: DataType,\\n    example_outputs: Optional[List[str]],\\n    run_inputs: Optional[List[str]],\\n    run_outputs: Optional[List[str]],\\n) -> List[RunEvaluator]:\\n    eval_llm = config.eval_llm or ChatOpenAI(model=\"gpt-4\", temperature=0.0)\\n    run_evaluators = []\\n    input_key = _determine_input_key(config, run_inputs)\\n    prediction_key = _determine_prediction_key(config, run_outputs)\\n    reference_key = _determine_reference_key(config, example_outputs)\\n    for eval_config in config.evaluators:\\n        run_evaluator = _construct_run_evaluator(\\n            eval_config,\\n            eval_llm,\\n            run_type,\\n            data_type,\\n            example_outputs,\\n            reference_key,\\n            input_key,\\n            prediction_key,\\n        )\\n        run_evaluators.append(run_evaluator)\\n    custom_evaluators = config.custom_evaluators or []\\n    for custom_evaluator in custom_evaluators:\\n        if isinstance(custom_evaluator, RunEvaluator):\\n            run_evaluators.append(custom_evaluator)\\n        elif isinstance(custom_evaluator, StringEvaluator):\\n            run_evaluators.append(\\n                StringRunEvaluatorChain.from_run_and_data_type(\\n                    custom_evaluator,\\n                    run_type,\\n                    data_type,\\n                    input_key=input_key,\\n                    prediction_key=prediction_key,\\n                    reference_key=reference_key,\\n                )\\n            )\\n        else:\\n            raise ValueError(\\n                f\"Unsupported custom evaluator: {custom_evaluator}.\"\\n                f\" Expected RunEvaluator or StringEvaluator.\"\\n            )\\n\\n    return run_evaluators',\n",
       "   'd': 'Load run evaluators from a configuration.\\n\\nArgs:\\n    config: Configuration for the run evaluators.\\n\\nReturns:\\n    A list of run evaluators.',\n",
       "   'l': True,\n",
       "   'g': [':type self: object\\n    :rtype: int',\n",
       "    ':return:  The maximum number of processes that can be used for the process pool.',\n",
       "    '',\n",
       "    ':type self: object\\n    :rtype: int']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        return self.format_prompt(**kwargs).to_string()',\n",
       "   'd': 'Evaluate question answering examples and predictions.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'Update status of the job.\\n\\n  :param ft: Future object.\\n  :return: None.',\n",
       "    '']},\n",
       "  {'c': '    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        vectors = np.array(\\n            self.embeddings.embed_documents([inputs[\"prediction\"], inputs[\"reference\"]])\\n        )\\n        score = self._compute_score(vectors)\\n        return {\"score\": score}',\n",
       "   'd': 'Get the default parameters for calling text generation inference API.',\n",
       "   'l': False,\n",
       "   'g': ['_',\n",
       "    '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _',\n",
       "    '_  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _  _',\n",
       "    '_summary_text\\n\\n    :param param: description']},\n",
       "  {'c': 'def tool(\\n    *args: Union[str, Callable, Runnable],\\n    return_direct: bool = False,\\n    args_schema: Optional[Type[BaseModel]] = None,\\n    infer_schema: bool = True,\\n) -> Callable:\\n    def _make_with_name(tool_name: str) -> Callable:\\n        def _make_tool(dec_func: Union[Callable, Runnable]) -> BaseTool:\\n            if isinstance(dec_func, Runnable):\\n                if dec_func.input_schema.schema().get(\"type\") != \"object\":\\n                    raise ValueError(\"Runnable must have an object schema.\")\\n\\n                async def ainvoke_wrapper(\\n                    callbacks: Optional[Callbacks] = None, **kwargs: Any\\n                ) -> Any:\\n                    return await dec_func.ainvoke(kwargs, {\"callbacks\": callbacks})\\n\\n                def invoke_wrapper(\\n                    callbacks: Optional[Callbacks] = None, **kwargs: Any\\n                ) -> Any:\\n                    return dec_func.invoke(kwargs, {\"callbacks\": callbacks})\\n\\n                coroutine = ainvoke_wrapper\\n                func = invoke_wrapper\\n                schema = dec_func.input_schema\\n                description = repr(dec_func)\\n            elif inspect.iscoroutinefunction(dec_func):\\n                coroutine = dec_func\\n                func = None\\n                schema = args_schema\\n                description = None\\n            else:\\n                coroutine = None\\n                func = dec_func\\n                schema = args_schema\\n                description = None\\n\\n            if infer_schema or args_schema is not None:\\n                return StructuredTool.from_function(\\n                    func,\\n                    coroutine,\\n                    name=tool_name,\\n                    description=description,\\n                    return_direct=return_direct,\\n                    args_schema=schema,\\n                    infer_schema=infer_schema,\\n                )\\n\\n\\n            if func.__doc__ is None:\\n                raise ValueError(\\n                    \"Function must have a docstring if \"\\n                    \"description not provided and infer_schema is False.\"\\n                )\\n            return Tool(\\n                name=tool_name,\\n                func=func,\\n                description=f\"{tool_name} tool\",\\n                return_direct=return_direct,\\n                coroutine=coroutine,\\n            )\\n\\n        return _make_tool\\n\\n    if len(args) == 2 and isinstance(args[0], str) and isinstance(args[1], Runnable):\\n        return _make_with_name(args[0])(args[1])\\n    elif len(args) == 1 and isinstance(args[0], str):\\n\\n\\n        return _make_with_name(args[0])\\n    elif len(args) == 1 and callable(args[0]):\\n\\n\\n        return _make_with_name(args[0].__name__)(args[0])\\n    elif len(args) == 0:\\n\\n\\n        def _partial(func: Callable[[str], str]) -> BaseTool:\\n            return _make_with_name(func.__name__)(func)\\n\\n        return _partial\\n    else:\\n        raise ValueError(\"Too many arguments for tool decorator\")',\n",
       "   'd': 'Make tools out of functions, can be used with or without arguments.\\n\\nArgs:\\n    *args: The arguments to the tool.\\n    return_direct: Whether to return directly from the tool rather\\n        than continuing the agent loop.\\n    args_schema: optional argument schema for user to specify\\n    infer_schema: Whether to infer the schema of the arguments from\\n        the function\\'s signature. This also makes the resultant tool\\n        accept a dictionary input to its `run()` function.\\n\\nRequires:\\n    - Function must be of type (str) -> str\\n    - Function must have a docstring\\n\\nExamples:\\n    .. code-block:: python\\n\\n        @tool\\n        def search_api(query: str) -> str:\\n            # Searches the API for the query.\\n            return\\n\\n        @tool(\"search\", return_direct=True)\\n        def search_api(query: str) -> str:\\n            # Searches the API for the query.\\n            return',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'Returns an array from a file.\\n    :param args:\\n        - filename\\n        - dtype\\n        - count\\n        - sep\\n        - -1 for autodetect',\n",
       "    ':param args:\\n    :param kwargs:\\n  :return: :class:`~numpy.ndarray`']},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n    llm_cache = self._get_llm_cache(llm_string)\\n    generations: List = []\\n\\n    results = llm_cache.similarity_search(\\n        query=prompt,\\n        k=1,\\n        distance_threshold=self.score_threshold,\\n    )\\n    if results:\\n        for document in results:\\n            generations.extend(\\n                _load_generations_from_json(document.metadata[\"return_val\"])\\n            )\\n    return generations if generations else None',\n",
       "   'd': 'Get docs.',\n",
       "   'l': False,\n",
       "   'g': [':param ciphertext_blob: The ciphertext_blob to deserialize.\\n    :type ciphertext_blob: str\\n    :return: The deserialized ciphertext_blob.\\n    :rtype: str',\n",
       "    '',\n",
       "    'Deserializes a `Ciphertext` from a `CiphertextBlob`.\\n\\n  :param ciphertext_blob: The `CiphertextBlob` to deserialize.\\n  :return: The deserialized `Ciphertext`.',\n",
       "    ':param ciphertext_blob: \\n    :return: \\n    :rtype:']},\n",
       "  {'c': 'def test_fireworks_streaming() -> None:\\n    llm = ChatFireworks()\\n\\n    for token in llm.stream(\"I\\'m Pickle Rick\"):\\n        assert isinstance(token.content, str)',\n",
       "   'd': 'Get the schema for a specific table.',\n",
       "   'l': False,\n",
       "   'g': ['_get_experiment(self, experiment_id):',\n",
       "    '_get_experiment(self, experiment_id) -> Experiment:',\n",
       "    '_call_endpoint(self, method_name, req_body):',\n",
       "    '_call_endpoint(self, method, req_body):\\n    :param experiment_id: str']},\n",
       "  {'c': 'def __init__(self, **kwargs: Any) -> None:\\n    separators = self.get_separators_for_language(Language.PYTHON)\\n    super().__init__(separators=separators, **kwargs)',\n",
       "   'd': 'Initialize a PythonCodeTextSplitter.',\n",
       "   'l': True,\n",
       "   'g': ['Return a unicode string representation of the object.\\n\\n  :return: A unicode string representation of the object.',\n",
       "    'Return a unicode string representation of the object.\\n  :return: A unicode string representation of the object.\\n  :rtype: str\\n  :raises: UnicodeError if unicode() is not implemented.',\n",
       "    'Returns a unicode string representation of the object.\\n\\n  :returns: The unicode string representation of the object.\\n  :',\n",
       "    ':return:']},\n",
       "  {'c': '    def test_intervention_chain(self) -> None:\\n        llm = OpenAI(temperature=0, max_tokens=512)\\n        story_conditions_chain = InterventionChain.from_univariate_prompt(llm)\\n        question = \"if cindy has ten pets\"\\n        data = story_conditions_chain(question)[Constant.chain_data.value]\\n        self.assertEqual(type(data), InterventionModel)',\n",
       "   'd': 'Initialize the OpenAI object.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '_summary_ or _summary_\\n\\n    Args:\\n        markdown (str): markdown text\\n\\n    Returns:\\n        list: list of dicts with name and value of each markdown section',\n",
       "    '',\n",
       "    '\\\\brief Extract markdown sections from a markdown string\\n\\n:param markdown: String with markdown sections\\n:return: List of dictionaries with name and value of the markdown section\\n:rtype: List[Dict[str, str]]\\n:raises:']},\n",
       "  {'c': 'def type(self) -> str:\\n    return \"human\"',\n",
       "   'd': 'Initialize with a file path.\\n\\nArgs:\\n    file_path: Either a local, S3 or web path to a PDF file.\\n    headers: Headers to use for GET request to download a file from a web path.',\n",
       "   'l': False,\n",
       "   'g': ['(self, logicalId, resource):',\n",
       "    '_set(self, logicalId, resource):\\n\\n    Sets the resource for a logicalId.\\n\\n    :param logicalId: The logicalId of the resource.\\n    :param resource: The resource to set.\\n    :return: The resource.\\n    :rtype: SamResource\\n    :raises: ValueError if the resource is not a SamResource.\\n    :raises: ValueError if the resource is not a dictionary.\\n    :raises: ValueError if the resource is not a dictionary.\\n    :raises: ValueError if the resource is not a dictionary.\\n    :raises: ValueError if the resource is not a dictionary.\\n    :raises: ValueError if the resource is not a dictionary.\\n    :raises: ValueError if the resource is not a dictionary.\\n    :raises: ValueError if the resource is not a dictionary.\\n    :raises: ValueError if the resource is not a dictionary.\\n    :raises: ValueError if the resource is not a dictionary.\\n    :raises: ValueError if the resource is not a dictionary.\\n    :raises: ValueError if the resource is not a dictionary.\\n    :raises: ValueError if the resource is not a dictionary.\\n    :raises: ValueError if the resource is not a dictionary',\n",
       "    '_set(self, logicalId, resource):\\n\\n    Set the value of a resource.',\n",
       "    '_set(self, logicalId, resource):\\n\\n    :param str logicalId:\\n    :param resource:\\n    :return:\\n    :rtype:']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> BaseMessage:\\n',\n",
       "   'd': 'Format the prompt template.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    Formatted message.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def _type(self) -> str:\\n    raise NotImplementedError(\\n        f\"_type property is not implemented in class {self.__class__.__name__}.\"\\n        \" This is required for serialization.\"\\n    )',\n",
       "   'd': 'Initialize the loader.\\n\\nArgs:\\n    file_path: A file, url or s3 path for input file\\n    textract_features: Features to be used for extraction, each feature\\n                       should be passed as a str that conforms to the enum\\n                       `Textract_Features`, see `amazon-textract-caller` pkg\\n    client: boto3 textract client (Optional)\\n    credentials_profile_name: AWS profile name, if not default (Optional)\\n    region_name: AWS region, eg us-east-1 (Optional)\\n    endpoint_url: endpoint url for the textract service (Optional)',\n",
       "   'l': False,\n",
       "   'g': [':param extra_headers: Extra headers to add to the request.\\n    :param extra_query: Extra query parameters to add to the request.\\n    :param extra_body: Extra request body to add to the request.\\n    :param timeout: The timeout to use for the request.\\n    :return: A list of models.',\n",
       "    ':param extra_headers:\\n        :param extra_query:\\n        :param extra_body:\\n        :param timeout:\\n    :return:\\n    :rtype: SyncPage[Model]',\n",
       "    '',\n",
       "    'Get a list of models.\\n\\n    :param extra_headers: Extra headers to include in the request.\\n    :param extra_query: Extra query parameters to include in the request.\\n    :param extra_body: Extra body to include in the request.\\n    :param timeout: The timeout to use for the request.\\n    :return: The list of models.']},\n",
       "  {'c': '    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        retriever: BaseRetriever,\\n        condense_question_prompt: BasePromptTemplate = CONDENSE_QUESTION_PROMPT,\\n        chain_type: str = \"stuff\",\\n        verbose: bool = False,\\n        condense_question_llm: Optional[BaseLanguageModel] = None,\\n        combine_docs_chain_kwargs: Optional[Dict] = None,\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> BaseConversationalRetrievalChain:\\n        combine_docs_chain_kwargs = combine_docs_chain_kwargs or {}\\n        doc_chain = load_qa_chain(\\n            llm,\\n            chain_type=chain_type,\\n            verbose=verbose,\\n            callbacks=callbacks,\\n            **combine_docs_chain_kwargs,\\n        )\\n\\n        _llm = condense_question_llm or llm\\n        condense_question_chain = LLMChain(\\n            llm=_llm,\\n            prompt=condense_question_prompt,\\n            verbose=verbose,\\n            callbacks=callbacks,\\n        )\\n        return cls(\\n            retriever=retriever,\\n            combine_docs_chain=doc_chain,\\n            question_generator=condense_question_chain,\\n            callbacks=callbacks,\\n            **kwargs,\\n        )',\n",
       "   'd': 'Validate that either folder_id or document_ids is set, but not both.',\n",
       "   'l': False,\n",
       "   'g': ['', '', \"'Use a named query'\", '']},\n",
       "  {'c': '    def configure(\\n        cls,\\n        inheritable_callbacks: Callbacks = None,\\n        local_callbacks: Callbacks = None,\\n        verbose: bool = False,\\n        inheritable_tags: Optional[List[str]] = None,\\n        local_tags: Optional[List[str]] = None,\\n        inheritable_metadata: Optional[Dict[str, Any]] = None,\\n        local_metadata: Optional[Dict[str, Any]] = None,\\n    ) -> CallbackManager:\\n        return _configure(\\n            cls,\\n            inheritable_callbacks,\\n            local_callbacks,\\n            verbose,\\n            inheritable_tags,\\n            local_tags,\\n            inheritable_metadata,\\n            local_metadata,\\n        )',\n",
       "   'd': \"Call out to Aleph Alpha's asymmetric Document endpoint.\\n\\nArgs:\\n    texts: The list of texts to embed.\\n\\nReturns:\\n    List of embeddings, one for each text.\",\n",
       "   'l': False,\n",
       "   'g': ['(Internal) Update the label map path in the configs.',\n",
       "    '(tensorflow_serving_predict_image_classifier_configs)',\n",
       "    '(tensorflow_hub.load_module_config)',\n",
       "    ':param configs:\\n  :param label_map_path:\\n  :return:']},\n",
       "  {'c': 'def search_api(query: str) -> str:\\n    return \"API result\"',\n",
       "   'd': 'Return the input keys.\\n\\nReturns:\\n    List of input keys.',\n",
       "   'l': False,\n",
       "   'g': ['Saves a query to the database.',\n",
       "    ':param client_id:\\n    :param query:\\n    :return:\\n    :rtype: str\\n    :rtype: str\\n    :type query: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :type client_id: str\\n    :',\n",
       "    '',\n",
       "    'Saves the query to the database.']},\n",
       "  {'c': '    def output_keys(self) -> List[str]:\\n        return self.output_variables',\n",
       "   'd': 'Return output key.\\n\\n:meta private:',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    \":param encoder_path: path to the encoder's weights\\n  :param decoder_path: path to the decoder's weights\\n  :return:\",\n",
       "    ':param encoder_path: path to the encoder state_dict\\n  :param decoder_path: path to the decoder state_dict\\n  :return:',\n",
       "    'Initialize the model.\\n\\n  Parameters\\n  ----------\\n  encoder_path : str, optional\\n    Path to the encoder model weights.\\n  decoder_path : str, optional\\n    Path to the decoder model weights.\\n\\n  Returns\\n  -------\\n  model : torch.nn.Module\\n    The model instance.']},\n",
       "  {'c': 'def dict(self, **kwargs: Any) -> Dict:\\n    output_parser_dict = super().dict(**kwargs)\\n    output_parser_dict[\"_type\"] = self._type\\n    return output_parser_dict',\n",
       "   'd': 'Retrieve documents relevant to a query.\\nArgs:\\n    query: string to find relevant documents for\\n    callbacks: Callback manager or list of callbacks\\n    tags: Optional list of tags associated with the retriever. Defaults to None\\n        These tags will be associated with each call to this retriever,\\n        and passed as arguments to the handlers defined in `callbacks`.\\n    metadata: Optional metadata associated with the retriever. Defaults to None\\n        This metadata will be associated with each call to this retriever,\\n        and passed as arguments to the handlers defined in `callbacks`.\\nReturns:\\n    List of relevant documents',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        texts = [t.replace(\"\\\\n\", \" \") for t in texts]\\n        embeddings = self.client.encode(texts, **self.encode_kwargs)\\n        return embeddings.tolist()',\n",
       "   'd': 'Compute doc embeddings using a HuggingFace transformer model.\\n\\nArgs:\\n    texts: The list of texts to embed.\\n\\nReturns:\\n    List of embeddings, one for each text.',\n",
       "   'l': True,\n",
       "   'g': ['```python',\n",
       "    ':return:',\n",
       "    'Disable the active adapter.\\n\\n  :return: The forward function of the base model.',\n",
       "    'Disable the adapter and all its layers.\\n\\n    :return: None.']},\n",
       "  {'c': 'def __init__(\\n    self, file_path: str, password: Optional[Union[str, bytes]] = None\\n) -> None:\\n    try:\\n        import pypdf\\n    except ImportError:\\n        raise ImportError(\\n            \"pypdf package not found, please install it with \" \"`pip install pypdf`\"\\n        )\\n    self.parser = PyPDFParser(password=password)\\n    super().__init__(file_path)',\n",
       "   'd': 'Initialize with a file path.',\n",
       "   'l': True,\n",
       "   'g': ['Convert a .wav file to a .silk file.\\n\\n    Args:\\n        pcm_path (str): The path to the .wav file.\\n        silk_path (str): The path to the .silk file.\\n\\n    Returns:\\n        float: The duration of the .wav file in milliseconds.',\n",
       "    'silk_path: str\\n    pcm_path: str',\n",
       "    'Convert a .wav file to silk.\\n\\n    :param pcm_path: The path to the .wav file.\\n    :param silk_path: The path to the silk file.\\n    :return: The duration of the silk file in milliseconds.',\n",
       "    'Convert a .wav file to a .silk file.\\n\\n    Parameters\\n    ----------\\n    pcm_path : str\\n        Path to a .wav file.\\n    silk_path : str\\n        Path to a .silk file.\\n\\n    Returns\\n    -------\\n    float\\n        The duration of the .silk file in milliseconds.']},\n",
       "  {'c': '    def resolve_criteria(\\n        cls,\\n        criteria: Optional[Union[CRITERIA_TYPE, str]],\\n    ) -> Dict[str, str]:\\n        return resolve_criteria(criteria)',\n",
       "   'd': 'Return response when agent has been stopped due to max iterations.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def _call(\\n        self,\\n        prompt: str,\\n        stop: Optional[List[str]] = None,\\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\\n        **kwargs: Any,\\n    ) -> str:\\n        model = self.client.get_model(self.model_uid)\\n\\n        generate_config: \"LlamaCppGenerateConfig\" = kwargs.get(\"generate_config\", {})\\n\\n        generate_config = {**self.model_kwargs, **generate_config}\\n\\n        if stop:\\n            generate_config[\"stop\"] = stop\\n\\n        if generate_config and generate_config.get(\"stream\"):\\n            combined_text_output = \"\"\\n            for token in self._stream_generate(\\n                model=model,\\n                prompt=prompt,\\n                run_manager=run_manager,\\n                generate_config=generate_config,\\n            ):\\n                combined_text_output += token\\n            return combined_text_output\\n\\n        else:\\n            completion = model.generate(prompt=prompt, generate_config=generate_config)\\n            return completion[\"choices\"][0][\"text\"]',\n",
       "   'd': 'Parse the output of an LLM call.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    ':return: Number of threads to be used.',\n",
       "    'Returns the number of threads to create.\\n\\n  :return: The number of threads to create.\\n  :rtype: int',\n",
       "    '']},\n",
       "  {'c': '    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        all_required_field_names = get_pydantic_field_names(cls)\\n        extra = values.get(\"model_kwargs\", {})\\n        for field_name in list(values):\\n            if field_name in extra:\\n                raise ValueError(f\"Found {field_name} supplied twice.\")\\n            if field_name not in all_required_field_names:\\n                warnings.warn(\\n                    f\"\"\"WARNING! {field_name} is not default parameter.\\n                    {field_name} was transferred to model_kwargs.\\n                    Please confirm that {field_name} is what you intended.\"\"\"\\n                )\\n                extra[field_name] = values.pop(field_name)\\n\\n        invalid_model_kwargs = all_required_field_names.intersection(extra.keys())\\n        if invalid_model_kwargs:\\n            raise ValueError(\\n                f\"Parameters {invalid_model_kwargs} should be specified explicitly. \"\\n                f\"Instead they were passed in as part of `model_kwargs` parameter.\"\\n            )\\n\\n        values[\"model_kwargs\"] = extra\\n        return values',\n",
       "   'd': 'Build extra kwargs from additional params that were passed in.',\n",
       "   'l': True,\n",
       "   'g': [':return: the request if it is a SAML request, None otherwise.\\n  :raises: :class:`~google.cloud.exceptions.NotImplementedError`\\n  :',\n",
       "    '',\n",
       "    '',\n",
       "    'Return the request if it is a SAML request, otherwise raise an error.']},\n",
       "  {'c': '    def parse_result(self, result: List[Generation]) -> T:\\n        return self.parse(result[0].text)',\n",
       "   'd': 'Parse a list of candidate model Generations into a specific format.\\n\\nThe return value is parsed from only the first Generation in the result, which\\n    is assumed to be the highest-likelihood Generation.\\n\\nArgs:\\n    result: A list of Generations to be parsed. The Generations are assumed\\n        to be different candidate outputs for a single model input.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Check if exploit can be run on given target.\\n\\n  :param target: target to execute exploit on',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': 'def generate_with_retry(llm: Tongyi, **kwargs: Any) -> Any:\\n    retry_decorator = _create_retry_decorator(llm)\\n\\n    @retry_decorator\\n    def _generate_with_retry(**_kwargs: Any) -> Any:\\n        resp = llm.client.call(**_kwargs)\\n        if resp.status_code == 200:\\n            return resp\\n        elif resp.status_code in [400, 401]:\\n            raise ValueError(\\n                f\"status_code: {resp.status_code} \\\\n \"\\n                f\"code: {resp.code} \\\\n message: {resp.message}\"\\n            )\\n        else:\\n            raise HTTPError(\\n                f\"HTTP error occurred: status_code: {resp.status_code} \\\\n \"\\n                f\"code: {resp.code} \\\\n message: {resp.message}\",\\n                response=resp,\\n            )\\n\\n    return _generate_with_retry(**kwargs)',\n",
       "   'd': 'Create a `LabeledCriteriaEvalChain` instance from an llm and criteria.\\n\\nParameters\\n----------\\nllm : BaseLanguageModel\\n    The language model to use for evaluation.\\ncriteria : CRITERIA_TYPE - default=None for \"helpfulness\"\\n    The criteria to evaluate the runs against. It can be:\\n        -  a mapping of a criterion name to its description\\n        -  a single criterion name present in one of the default criteria\\n        -  a single `ConstitutionalPrinciple` instance\\nprompt : Optional[BasePromptTemplate], default=None\\n    The prompt template to use for generating prompts. If not provided,\\n    a default prompt will be used.\\n**kwargs : Any\\n    Additional keyword arguments to pass to the `LLMChain`\\n    constructor.\\n\\nReturns\\n-------\\nLabeledCriteriaEvalChain\\n    An instance of the `LabeledCriteriaEvalChain` class.\\n\\nExamples\\n--------\\n>>> from langchain.llms import OpenAI\\n>>> from langchain.evaluation.criteria import LabeledCriteriaEvalChain\\n>>> llm = OpenAI()\\n>>> criteria = {\\n        \"hallucination\": (\\n            \"Does this submission contain information\"\\n            \" not present in the input or reference?\"\\n        ),\\n    }\\n>>> chain = LabeledCriteriaEvalChain.from_llm(\\n        llm=llm,\\n        criteria=criteria,\\n    )',\n",
       "   'l': False,\n",
       "   'g': [':param program: \\n    :param ckpt_name: \\n    :return: \\n    :rtype:',\n",
       "    'Save model checkpoint to the specified directory\\n    \\n    Args:\\n        ckpt_name (str): The name of the model checkpoint to be saved.\\n    \\n    Returns:\\n        str: The directory where the model checkpoint is saved.\\n    \\n    Note:\\n        This function is called by the ``train`` and ``test`` function in ``train_and_test.py`` and ``test.py`` respectively.',\n",
       "    'Save model checkpoint to a directory.\\n\\n    Args:\\n        program: the fluid.Program.\\n        ckpt_name: the name of the checkpoint.\\n\\n    Returns:\\n        the directory of the checkpoint.',\n",
       "    'Save model checkpoint to the directory\\n    \\n    Args:\\n        ckpt_name (str): model name\\n    \\n    Returns:\\n        str: model save path']},\n",
       "  {'c': '    def input_variables(self) -> List[str]:\\n',\n",
       "   'd': 'Input variables for this prompt template.\\n\\nReturns:\\n    List of input variables.',\n",
       "   'l': True,\n",
       "   'g': ['(str)',\n",
       "    '_get_attribute(self, attribute):',\n",
       "    '_get_attribute(self, attribute):\\n\\n    :param attribute: The attribute to describe.\\n    :return: The attribute value.\\n    :rtype: str',\n",
       "    '_get_attribute(self, attribute):\\n\\n    Returns the attribute of the resource.\\n\\n    :param attribute: The name of the attribute to get.\\n    :return: The attribute value.\\n    :rtype: str or unicode or int or float or bool or None.\\n    :raises: :class:`~botocore.exceptions.ClientError` if the attribute does not exist.']},\n",
       "  {'c': 'def run_on_dataset(\\n    client: Client,\\n    dataset_name: str,\\n    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,\\n    *,\\n    evaluation: Optional[RunEvalConfig] = None,\\n    concurrency_level: int = 5,\\n    project_name: Optional[str] = None,\\n    verbose: bool = False,\\n    tags: Optional[List[str]] = None,\\n    input_mapper: Optional[Callable[[Dict], Any]] = None,\\n    **kwargs: Any,\\n) -> Dict[str, Any]:\\n    if kwargs:\\n        warnings.warn(\\n            \"The following arguments are deprecated and \"\\n            \"will be removed in a future release: \"\\n            f\"{kwargs.keys()}.\",\\n            DeprecationWarning,\\n        )\\n    wrapped_model, project_name, dataset, examples = _prepare_eval_run(\\n        client, dataset_name, llm_or_chain_factory, project_name\\n    )\\n    if concurrency_level in (0, 1):\\n        results = _run_on_examples(\\n            client,\\n            examples,\\n            wrapped_model,\\n            project_name=project_name,\\n            verbose=verbose,\\n            tags=tags,\\n            evaluation=evaluation,\\n            input_mapper=input_mapper,\\n            data_type=dataset.data_type,\\n        )\\n    else:\\n\\n        coro = _arun_on_examples(\\n            client,\\n            examples,\\n            wrapped_model,\\n            concurrency_level=concurrency_level,\\n            project_name=project_name,\\n            verbose=verbose,\\n            tags=tags,\\n            evaluation=evaluation,\\n            input_mapper=input_mapper,\\n            data_type=dataset.data_type,\\n        )\\n        results = _handle_coroutine(coro)\\n    return TestResult(\\n        project_name=project_name,\\n        results=results,\\n    )',\n",
       "   'd': 'Run the Chain or language model on a dataset and store traces\\nto the specified project name.\\n\\nArgs:\\n    client: LangSmith client to use to access the dataset and to\\n        log feedback and run traces.\\n    dataset_name: Name of the dataset to run the chain on.\\n    llm_or_chain_factory: Language model or Chain constructor to run\\n        over the dataset. The Chain constructor is used to permit\\n        independent calls on each example without carrying over state.\\n    evaluation: Configuration for evaluators to run on the\\n        results of the chain\\n    concurrency_level: The number of async tasks to run concurrently.\\n    project_name: Name of the project to store the traces in.\\n        Defaults to {dataset_name}-{chain class name}-{datetime}.\\n    verbose: Whether to print progress.\\n    tags: Tags to add to each run in the project.\\n    input_mapper: A function to map to the inputs dictionary from an Example\\n        to the format expected by the model to be evaluated. This is useful if\\n        your model needs to deserialize more complex schema or if your dataset\\n        has inputs with keys that differ from what is expected by your chain\\n        or agent.\\n\\nReturns:\\n    A dictionary containing the run\\'s project name and the resulting model outputs.\\n\\n\\nFor the (usually faster) async version of this function, see :func:`arun_on_dataset`.\\n\\nExamples\\n--------\\n\\n.. code-block:: python\\n\\n    from langsmith import Client\\n    from langchain.chat_models import ChatOpenAI\\n    from langchain.chains import LLMChain\\n    from langchain.smith import RunEvalConfig, run_on_dataset\\n\\n    # Chains may have memory. Passing in a constructor function lets the\\n    # evaluation framework avoid cross-contamination between runs.\\n    def construct_chain():\\n        llm = ChatOpenAI(temperature=0)\\n        chain = LLMChain.from_string(\\n            llm,\\n            \"What\\'s the answer to {your_input_key}\"\\n        )\\n        return chain\\n\\n    # Load off-the-shelf evaluators via config or the EvaluatorType (string or enum)\\n    evaluation_config = RunEvalConfig(\\n        evaluators=[\\n            \"qa\",  # \"Correctness\" against a reference answer\\n            \"embedding_distance\",\\n            RunEvalConfig.Criteria(\"helpfulness\"),\\n            RunEvalConfig.Criteria({\\n                \"fifth-grader-score\": \"Do you have to be smarter than a fifth grader to answer this question?\"\\n            }),\\n        ]\\n    )\\n\\n    client = Client()\\n    run_on_dataset(\\n        client,\\n        \"<my_dataset_name>\",\\n        construct_chain,\\n        evaluation=evaluation_config,\\n    )\\n\\nYou can also create custom evaluators by subclassing the\\n:class:`StringEvaluator <langchain.evaluation.schema.StringEvaluator>`\\nor LangSmith\\'s `RunEvaluator` classes.\\n\\n.. code-block:: python\\n\\n    from typing import Optional\\n    from langchain.evaluation import StringEvaluator\\n\\n    class MyStringEvaluator(StringEvaluator):\\n\\n        @property\\n        def requires_input(self) -> bool:\\n            return False\\n\\n        @property\\n        def requires_reference(self) -> bool:\\n            return True\\n\\n        @property\\n        def evaluation_name(self) -> str:\\n            return \"exact_match\"\\n\\n        def _evaluate_strings(self, prediction, reference=None, input=None, **kwargs) -> dict:\\n            return {\"score\": prediction == reference}\\n\\n\\n    evaluation_config = RunEvalConfig(\\n        custom_evaluators = [MyStringEvaluator()],\\n    )\\n\\n    run_on_dataset(\\n        client,\\n        \"<my_dataset_name>\",\\n        construct_chain,\\n        evaluation=evaluation_config,\\n    )',\n",
       "   'l': True,\n",
       "   'g': ['Delete a conversation.\\n\\n  :param str convo_id: The ID of the conversation to delete.\\n  :return: None.',\n",
       "    'Delete a conversation.',\n",
       "    'Delete a conversation.\\n  :param convo_id: The ID of the conversation to delete.\\n  :type convo_id: str\\n  :return: None.',\n",
       "    'Delete a conversation.\\n\\n  Args:\\n      convo_id: The id of the conversation to delete.\\n\\n  Returns:\\n      None.']},\n",
       "  {'c': 'def similarity_search_with_score(\\n    self, query: str, k: int = 4, filter: Optional[dict] = None, **kwargs: Any\\n) -> List[Tuple[Document, float]]:\\n    embedding = self.embedding.embed_query(query)\\n    script_query = _default_script_query(embedding, filter)\\n    response = self.client_search(\\n        self.client, self.index_name, script_query, size=k\\n    )\\n    hits = [hit for hit in response[\"hits\"][\"hits\"]]\\n    docs_and_scores = [\\n        (\\n            Document(\\n                page_content=hit[\"_source\"][\"text\"],\\n                metadata=hit[\"_source\"][\"metadata\"],\\n            ),\\n            hit[\"_score\"],\\n        )\\n        for hit in hits\\n    ]\\n    return docs_and_scores',\n",
       "   'd': 'Return docs most similar to query.\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 4.\\nReturns:\\n    List of Documents most similar to the query.',\n",
       "   'l': True,\n",
       "   'g': ['Return a queryset of all the objects in the model.\\n\\n    :returns: A queryset of all the objects in the model.',\n",
       "    '',\n",
       "    'Return a queryset of all the objects that can be accessed by the\\n    user.  This includes the objects the user can see, the objects that\\n    the user can edit, and the objects that the user can create.\\n\\n    :returns: A queryset of all the objects that can be accessed by\\n      the user.  This includes the objects the user can see, the\\n      objects that the user can edit, and the objects that the\\n      user can create.\\n\\n    :rtype: ``django.db.models.query.QuerySet``',\n",
       "    '']},\n",
       "  {'c': 'def _generate(\\n    self,\\n    prompts: List[str],\\n    stop: Optional[List[str]] = None,\\n    run_manager: Optional[CallbackManagerForLLMRun] = None,\\n    **kwargs: Any,\\n) -> LLMResult:\\n',\n",
       "   'd': 'Run the LLM on the given prompts.',\n",
       "   'l': True,\n",
       "   'g': [':param path: the path to the directory to be created.',\n",
       "    '(str) ->',\n",
       "    ':param str path:',\n",
       "    ':param path: path to the model']},\n",
       "  {'c': 'def _run(\\n    self,\\n    query: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> str:\\n    return str(self.api_wrapper.results(query))',\n",
       "   'd': 'Create a new TextSplitter.',\n",
       "   'l': False,\n",
       "   'g': ['_init_', '_init__', '', '_init__']},\n",
       "  {'c': 'def _generate(\\n    self,\\n    prompts: List[str],\\n    stop: Optional[List[str]] = None,\\n    run_manager: Optional[CallbackManagerForLLMRun] = None,\\n    **kwargs: Any,\\n) -> LLMResult:\\n',\n",
       "   'd': 'Run the LLM on the given prompts.',\n",
       "   'l': True,\n",
       "   'g': ['Return a name for the staff group in the given location.\\n\\n    :param location: the location to get the name for.\\n\\n    :return: a name for the staff group.',\n",
       "    'Return the name of the staff group for a given location.\\n\\n    :param location: The location to get the staff group for.\\n    :type location: str',\n",
       "    'Returns the name of the group for the staff_course_location.\\n  :param location: The location of the staff_course_location.\\n  :return: The name of the group for the staff_course_location.\\n  :rtype: str\\n  :raises: ValueError if the legacy name does not exist.',\n",
       "    '']},\n",
       "  {'c': 'def format_response_payload(self, output: bytes) -> str:\\n',\n",
       "   'd': 'Helper method to transform an Iterator of Input values into an Iterator of\\nOutput values, with callbacks.\\nUse this to implement `stream()` or `transform()` in Runnable subclasses.',\n",
       "   'l': False,\n",
       "   'g': ['(async)', '_check_storage_needs_cleanup(self):', '', '']},\n",
       "  {'c': '    def _create_search_request(self, query: str) -> SearchRequest:\\n        from google.cloud.discoveryengine_v1beta import SearchRequest\\n\\n        query_expansion_spec = SearchRequest.QueryExpansionSpec(\\n            condition=self.query_expansion_condition,\\n        )\\n\\n        spell_correction_spec = SearchRequest.SpellCorrectionSpec(\\n            mode=self.spell_correction_mode\\n        )\\n\\n        if self.engine_data_type == 0:\\n            if self.get_extractive_answers:\\n                extractive_content_spec = (\\n                    SearchRequest.ContentSearchSpec.ExtractiveContentSpec(\\n                        max_extractive_answer_count=self.max_extractive_answer_count,\\n                    )\\n                )\\n            else:\\n                extractive_content_spec = (\\n                    SearchRequest.ContentSearchSpec.ExtractiveContentSpec(\\n                        max_extractive_segment_count=self.max_extractive_segment_count,\\n                    )\\n                )\\n            content_search_spec = SearchRequest.ContentSearchSpec(\\n                extractive_content_spec=extractive_content_spec\\n            )\\n        elif self.engine_data_type == 1:\\n            content_search_spec = None\\n        elif self.engine_data_type == 2:\\n            content_search_spec = SearchRequest.ContentSearchSpec(\\n                extractive_content_spec=SearchRequest.ContentSearchSpec.ExtractiveContentSpec(\\n                    max_extractive_answer_count=self.max_extractive_answer_count,\\n                )\\n            )\\n        else:\\n            raise NotImplementedError(\\n                \"Only data store type 0 (Unstructured), 1 (Structured),\"\\n                \"or 2 (Website with Advanced Indexing) are supported currently.\"\\n                + f\" Got {self.engine_data_type}\"\\n            )\\n\\n        return SearchRequest(\\n            query=query,\\n            filter=self.filter,\\n            serving_config=self._serving_config,\\n            page_size=self.max_documents,\\n            content_search_spec=content_search_spec,\\n            query_expansion_spec=query_expansion_spec,\\n            spell_correction_spec=spell_correction_spec,\\n        )',\n",
       "   'd': 'Add a list of texts to the Elasticsearch index.\\n\\nArgs:\\n    texts (Iterable[str]): The texts to add to the index.\\n    metadatas (List[Dict[Any, Any]], optional): A list of metadata dictionaries\\n        to associate with the texts.\\n    model_id (str, optional): The ID of the model to use for transforming the\\n        texts into vectors.\\n    refresh_indices (bool, optional): Whether to refresh the Elasticsearch\\n        indices after adding the texts.\\n    **kwargs: Arbitrary keyword arguments.\\n\\nReturns:\\n    A list of IDs for the added texts.',\n",
       "   'l': False,\n",
       "   'g': ['', '(xs) -> float', '(xs, **kwargs) -> float', '(xs) -> float']},\n",
       "  {'c': '    def __add__(self, other: Any) -> ChatPromptTemplate:\\n        if isinstance(other, ChatPromptTemplate):\\n            return ChatPromptTemplate(messages=self.messages + other.messages)\\n        elif isinstance(\\n            other, (BaseMessagePromptTemplate, BaseMessage, BaseChatPromptTemplate)\\n        ):\\n            return ChatPromptTemplate(messages=self.messages + [other])\\n        elif isinstance(other, (list, tuple)):\\n            _other = ChatPromptTemplate.from_messages(other)\\n            return ChatPromptTemplate(messages=self.messages + _other.messages)\\n        elif isinstance(other, str):\\n            prompt = HumanMessagePromptTemplate.from_template(other)\\n            return ChatPromptTemplate(messages=self.messages + [prompt])\\n        else:\\n            raise NotImplementedError(f\"Unsupported operand type for +: {type(other)}\")',\n",
       "   'd': 'Combine two prompt templates.\\n\\nArgs:\\n    other: Another prompt template.\\n\\nReturns:\\n    Combined prompt template.',\n",
       "   'l': True,\n",
       "   'g': ['Generates samples',\n",
       "    'Saves generated images',\n",
       "    'Generates saved images by feed',\n",
       "    'Saves a plot of generated images.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        path: Union[str, Path],\\n        encoding: Optional[str] = None,\\n        errors: Optional[str] = None,\\n        custom_html_tag: Optional[Tuple[str, dict]] = None,\\n        **kwargs: Optional[Any]\\n    ):\\n        try:\\n            from bs4 import BeautifulSoup\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import python packages. \"\\n                \"Please install it with `pip install beautifulsoup4`. \"\\n            )\\n\\n        try:\\n            _ = BeautifulSoup(\\n                \"<html><body>Parser builder library test.</body></html>\", **kwargs\\n            )\\n        except Exception as e:\\n            raise ValueError(\"Parsing kwargs do not appear valid\") from e\\n\\n        self.file_path = Path(path)\\n        self.encoding = encoding\\n        self.errors = errors\\n        self.custom_html_tag = custom_html_tag\\n        self.bs_kwargs = kwargs',\n",
       "   'd': 'Update cache based on prompt and llm_string.',\n",
       "   'l': False,\n",
       "   'g': ['_ParseAndUnwrap - parse and unwrap a code string to a Python tree.\\n\\n    :param code: the code to parse and unwrap.\\n    :param dumptree: whether to print the parsed tree to stderr.\\n    :return: the Python tree.',\n",
       "    '_ParseAndUnwrap(self, code, dumptree=False)',\n",
       "    '(str) -> (str, str, str)',\n",
       "    '_ParseAndUnwrap:']},\n",
       "  {'c': '    def run(self, query: str) -> str:\\n        res = self.lambda_client.invoke(\\n            FunctionName=self.function_name,\\n            InvocationType=\"RequestResponse\",\\n            Payload=json.dumps({\"body\": query}),\\n        )\\n\\n        try:\\n            payload_stream = res[\"Payload\"]\\n            payload_string = payload_stream.read().decode(\"utf-8\")\\n            answer = json.loads(payload_string)[\"body\"]\\n\\n        except StopIteration:\\n            return \"Failed to parse response from Lambda\"\\n\\n        if answer is None or answer == \"\":\\n\\n            return \"Request failed.\"\\n        else:\\n            return f\"Result: {answer}\"',\n",
       "   'd': 'Add documents to vectorstore.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    \":param endianness: '?', 'little' or 'big'\\n  :param sizes: list of 32-bit or 64-bit sizes\\n  :return: list of 32-bit or 64-bit floating-point dtypes.\",\n",
       "    '',\n",
       "    \":param endianness: 'b' or 'B' or 'h' or 'H' or 'i' or 'I' or 'l' or 'L' or 'q' or 'Q' or 'd' or 'D' or 'f' or 'F' or 'e' or 'E' or 'g' or 'G' or 'a' or 'A' or 's' or 'S' or 'p' or 'P' or 'c' or 'C' or 'n' or 'N' or 'x' or 'X' or 'u' or 'U' or 'y' or 'Y' or 'z' or 'Z' or 'f' or 'F' or 'e' or 'E' or 'g' or 'G' or 'a' or 'A' or 's' or 'S' or 'p' or 'P' or 'c' or 'C' or 'n' or 'N' or 'x' or 'X' or 'b' or 'B' or 'h' or 'H' or 'i' or 'I' or 'l' or 'L' or 'q\"]},\n",
       "  {'c': '    def plan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[AgentAction, AgentFinish]:\\n        full_inputs = self.get_full_inputs(intermediate_steps, **kwargs)\\n        full_output = self.llm_chain.predict(callbacks=callbacks, **full_inputs)\\n        return self.output_parser.parse(full_output)',\n",
       "   'd': 'Type of the Message, used for serialization.',\n",
       "   'l': False,\n",
       "   'g': ['(f, numpy.allclose)\\n    def test_optimizations_vm(self):',\n",
       "    '(x, A) -> y',\n",
       "    '(function, function, function, function, function, function,\\n     function, function, function, function, function, function,\\n     function, function, function, function, function, function,\\n     function, function, function, function, function, function,\\n     function, function, function, function, function, function,\\n     function, function, function, function, function, function,\\n     function, function, function, function, function, function,\\n     function, function, function, function, function, function,\\n     function, function, function, function, function, function,\\n     function, function, function, function, function, function,\\n     function, function, function, function, function, function,\\n     function, function, function, function, function, function,\\n     function, function, function, function, function, function,\\n     function, function, function, function, function, function,\\n     function, function, function, function, function, function,\\n     function, function, function, function, function, function,\\n     function, function, function, function, function, function,\\n     function, function, function, function, function, function,\\n     function, function',\n",
       "    '_test_optimizations_vm']},\n",
       "  {'c': '    def load(self) -> List[Document]:\\n        from pdfminer.high_level import extract_text_to_fp\\n        from pdfminer.layout import LAParams\\n        from pdfminer.utils import open_filename\\n\\n        output_string = StringIO()\\n        with open_filename(self.file_path, \"rb\") as fp:\\n            extract_text_to_fp(\\n                fp,\\n                output_string,\\n                codec=\"\",\\n                laparams=LAParams(),\\n                output_type=\"html\",\\n            )\\n        metadata = {\"source\": self.file_path}\\n        return [Document(page_content=output_string.getvalue(), metadata=metadata)]',\n",
       "   'd': 'Load file.',\n",
       "   'l': True,\n",
       "   'g': ['_can_add', '_can_add', '_can_add', '_can_add(self, data)']},\n",
       "  {'c': 'def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n    out_vectors = []\\n    for text in texts:\\n        if text not in self.known_texts:\\n            self.known_texts.append(text)\\n        vector = [float(1.0)] * (self.dimensionality - 1) + [\\n            float(self.known_texts.index(text))\\n        ]\\n        out_vectors.append(vector)\\n    return out_vectors',\n",
       "   'd': 'Return consistent embeddings for each text seen so far.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def _run_on_examples(\\n    client: Client,\\n    examples: Iterator[Example],\\n    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,\\n    *,\\n    evaluation: Optional[RunEvalConfig] = None,\\n    project_name: Optional[str] = None,\\n    verbose: bool = False,\\n    tags: Optional[List[str]] = None,\\n    input_mapper: Optional[Callable[[Dict], Any]] = None,\\n    data_type: DataType = DataType.kv,\\n) -> Dict[str, Any]:\\n    results: Dict[str, Any] = {}\\n    wrapped_model = _wrap_in_chain_factory(llm_or_chain_factory)\\n    project_name = _get_project_name(project_name, wrapped_model)\\n    tracer = LangChainTracer(\\n        project_name=project_name, client=client, use_threading=False\\n    )\\n    run_evaluators, examples = _setup_evaluation(\\n        wrapped_model, examples, evaluation, data_type\\n    )\\n    examples = _validate_example_inputs(examples, wrapped_model, input_mapper)\\n    evalution_handler = EvaluatorCallbackHandler(\\n        evaluators=run_evaluators or [],\\n        client=client,\\n    )\\n    callbacks: List[BaseCallbackHandler] = [tracer, evalution_handler]\\n    for i, example in enumerate(examples):\\n        result = _run_llm_or_chain(\\n            example,\\n            wrapped_model,\\n            tags=tags,\\n            callbacks=callbacks,\\n            input_mapper=input_mapper,\\n        )\\n        if verbose:\\n            print(f\"{i+1} processed\", flush=True, end=\"\\\\r\")\\n        results[str(example.id)] = result\\n    tracer.wait_for_futures()\\n    evalution_handler.wait_for_futures()\\n    return results',\n",
       "   'd': \"Run the Chain or language model on examples and store\\ntraces to the specified project name.\\n\\nArgs:\\n    client: LangSmith client to use to log feedback and runs.\\n    examples: Examples to run the model or chain over.\\n    llm_or_chain_factory: Language model or Chain constructor to run\\n        over the dataset. The Chain constructor is used to permit\\n        independent calls on each example without carrying over state.\\n    evaluation: Optional evaluation configuration to use when evaluating\\n    project_name: Name of the project to store the traces in.\\n        Defaults to {dataset_name}-{chain class name}-{datetime}.\\n    verbose: Whether to print progress.\\n    tags: Tags to add to each run in the project.\\n    input_mapper: A function to map to the inputs dictionary from an Example\\n        to the format expected by the model to be evaluated. This is useful if\\n        your model needs to deserialize more complex schema or if your dataset\\n        has inputs with keys that differ from what is expected by your chain\\n        or agent.\\n    data_type: The dataset's data type. This is used to determine determine\\n        how to deserialize the reference data and model compatibility.\\n\\nReturns:\\n    A dictionary mapping example ids to the model outputs.\",\n",
       "   'l': True,\n",
       "   'g': ['(self, prompt: str | list[str]) -> LLMEmbeddings:\\n    if isinstance(prompt, str):\\n        prompt = [prompt]\\n    return self.embeddings.run(prompt)',\n",
       "    '.embeddings.run(prompt)\\n    if isinstance(prompt, str):\\n        prompt = [prompt]\\n    return __self.embeddings.run(prompt)',\n",
       "    '_wrapped_embeddings_run(self, prompt: str | list[str]) -> LLMRunner:\\n    if isinstance(prompt, str):\\n        prompt = [prompt]\\n    return self.embeddings.run(prompt)\\n\\ndef run(self, prompt: str | list[str]) -> LLMRunner:\\n    if isinstance(prompt, str):\\n        prompt = [prompt]\\n    return self.embeddings.run(prompt)\\n\\ndef _wrapped_embeddings_run(self, prompt: str | list[str]) -> LLMRunner:\\n    if isinstance(prompt, str):\\n        prompt = [prompt]\\n    return self.embeddings.run(prompt)\\n\\ndef run(self, prompt: str | list[str]) -> LLMRunner:\\n    if isinstance(prompt, str):\\n        prompt = [prompt]\\n    return self.embeddings.run(prompt)\\n\\ndef _wrapped_embeddings_run(self, prompt: str | list[str]) -> LLMRunner:\\n    if isinstance(prompt, str):\\n        prompt = [prompt]\\n    return self.embeddings.run(prompt)\\n\\ndef run(self, prompt: str | list[str',\n",
       "    '_wrapped_embeddings_run(self, prompt: str | list[str]) -> LLMEmbeddings:\\n    if isinstance(prompt, str):\\n        prompt = [prompt]\\n    return self.embeddings.run(prompt)']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        session: Optional[CassandraSession],\\n        keyspace: Optional[str],\\n        embedding: Embeddings,\\n        table_name: str = CASSANDRA_SEMANTIC_CACHE_DEFAULT_TABLE_NAME,\\n        distance_metric: str = CASSANDRA_SEMANTIC_CACHE_DEFAULT_DISTANCE_METRIC,\\n        score_threshold: float = CASSANDRA_SEMANTIC_CACHE_DEFAULT_SCORE_THRESHOLD,\\n        ttl_seconds: Optional[int] = CASSANDRA_SEMANTIC_CACHE_DEFAULT_TTL_SECONDS,\\n        skip_provisioning: bool = False,\\n    ):\\n        try:\\n            from cassio.table import MetadataVectorCassandraTable\\n        except (ImportError, ModuleNotFoundError):\\n            raise ValueError(\\n                \"Could not import cassio python package. \"\\n                \"Please install it with `pip install cassio`.\"\\n            )\\n        self.session = session\\n        self.keyspace = keyspace\\n        self.embedding = embedding\\n        self.table_name = table_name\\n        self.distance_metric = distance_metric\\n        self.score_threshold = score_threshold\\n        self.ttl_seconds = ttl_seconds\\n\\n\\n\\n\\n\\n\\n        @lru_cache(maxsize=CASSANDRA_SEMANTIC_CACHE_EMBEDDING_CACHE_SIZE)\\n        def _cache_embedding(text: str) -> List[float]:\\n            return self.embedding.embed_query(text=text)\\n\\n        self._get_embedding = _cache_embedding\\n        self.embedding_dimension = self._get_embedding_dimension()\\n\\n        self.table = MetadataVectorCassandraTable(\\n            session=self.session,\\n            keyspace=self.keyspace,\\n            table=self.table_name,\\n            primary_key_type=[\"TEXT\"],\\n            vector_dimension=self.embedding_dimension,\\n            ttl_seconds=self.ttl_seconds,\\n            metadata_indexing=(\"allow\", {\"_llm_string_hash\"}),\\n            skip_provisioning=skip_provisioning,\\n        )',\n",
       "   'd': \"Initialize the cache with all relevant parameters.\\nArgs:\\n    session (cassandra.cluster.Session): an open Cassandra session\\n    keyspace (str): the keyspace to use for storing the cache\\n    embedding (Embedding): Embedding provider for semantic\\n        encoding and search.\\n    table_name (str): name of the Cassandra (vector) table\\n        to use as cache\\n    distance_metric (str, 'dot'): which measure to adopt for\\n        similarity searches\\n    score_threshold (optional float): numeric value to use as\\n        cutoff for the similarity searches\\n    ttl_seconds (optional int): time-to-live for cache entries\\n        (default: None, i.e. forever)\\nThe default score threshold is tuned to the default metric.\\nTune it carefully yourself if switching to another distance metric.\",\n",
       "   'l': True,\n",
       "   'g': ['_init_',\n",
       "    '_init_\\n:param K: int\\n:param device: str\\n:return:',\n",
       "    '_init_',\n",
       "    '_init_\\n    :param K:\\n    :param device:']},\n",
       "  {'c': '    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        return {\\n            \"score\": self.compute_metric(inputs[\"prediction\"], inputs[\"prediction_b\"])\\n        }',\n",
       "   'd': 'Load documents.',\n",
       "   'l': False,\n",
       "   'g': [':param name: name of the command\\n    :param linux_command: linux command\\n    :param windows_command: windows command',\n",
       "    '',\n",
       "    ':param name: name of the command\\n  :param linux_command: linux command\\n  :param windows_command: windows command\\n  :return:',\n",
       "    '']},\n",
       "  {'c': '    def embed_query(self, text: str) -> List[float]:\\n        instruction_pair = [self.query_instruction, text]\\n        embedding = self.client.encode([instruction_pair], **self.encode_kwargs)[0]\\n        return embedding.tolist()',\n",
       "   'd': 'Compute query embeddings using a HuggingFace instruct model.\\n\\nArgs:\\n    text: The text to embed.\\n\\nReturns:\\n    Embeddings for the text.',\n",
       "   'l': True,\n",
       "   'g': ['', 'Returns a dictionary of unarchived links, by folder.', '', '']},\n",
       "  {'c': 'def _run(\\n    self,\\n    query: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> str:\\n    return self.db.run_no_throw(query)',\n",
       "   'd': 'Execute the query, return the results or an error message.',\n",
       "   'l': True,\n",
       "   'g': [':param mean1:\\n    :param logvar1:\\n    :param mean2:\\n    :param logvar2:\\n    :return:\\n    :rtype: th.Tensor',\n",
       "    '',\n",
       "    'Calculates the KL-divergence between two normal distributions.',\n",
       "    'Calculates the KL-divergence between two normal distributions.\\n    \\n    Parameters\\n    ----------\\n    mean1 : float or th.Tensor\\n        Mean of the first normal distribution.\\n    logvar1 : float or th.Tensor\\n        Log-variance of the first normal distribution.\\n    mean2 : float or th.Tensor\\n        Mean of the second normal distribution.\\n    logvar2 : float or th.Tensor\\n        Log-variance of the second normal distribution.\\n    \\n    Returns\\n    -------\\n    kl : th.Tensor\\n        The KL-divergence between the two normal distributions.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n    ) -> None:\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index',\n",
       "   'd': 'Parse a single string model output into some structure.\\n\\nArgs:\\n    text: String output of a language model.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': False,\n",
       "   'g': ['_forward(self, tensor_input):',\n",
       "    '_call(self, tensor_input):\\n    return self.basic_module(tensor_input)',\n",
       "    '_call(self, tensor_input):',\n",
       "    '_call(self, tensor_input):\\n    return self.basic_module(tensor_input)']},\n",
       "  {'c': 'def refresh_schema(self) -> None:\\n    self.schema = (\\n        f\"Node properties: {self.query(node_properties_query)}\\\\n\"\\n        f\"Relationships properties: {self.query(rel_properties_query)}\\\\n\"\\n        f\"Relationships: {self.query(rel_query)}\\\\n\"\\n    )',\n",
       "   'd': 'Refreshes the schema of the FalkorDB database',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    ':param cls:\\n    :param mapping:\\n    :return:',\n",
       "    ':param cls:\\n  :param mapping:\\n  :return:']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        file_path: Union[str, List[str]] = \"\",\\n        mode: str = \"single\",\\n        url: str = \"https://api.unstructured.io/general/v0/general\",\\n        api_key: str = \"\",\\n        **unstructured_kwargs: Any,\\n    ):\\n        if isinstance(file_path, str):\\n            validate_unstructured_version(min_unstructured_version=\"0.6.2\")\\n        else:\\n            validate_unstructured_version(min_unstructured_version=\"0.6.3\")\\n\\n        self.url = url\\n        self.api_key = api_key\\n\\n        super().__init__(file_path=file_path, mode=mode, **unstructured_kwargs)',\n",
       "   'd': 'Initialize with file path.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '_wait_for_other_player()\\n  _timestep = self._underlying_env.step(self._actions)\\n  _actions = [None] * self._num_players\\n  _wait_for_other_player()\\n  _player_timestep(player)\\n  return self._player_timestep(player)',\n",
       "    '.  Return the next timestep.  The environment is assumed to be in\\n    the state of the current player at this timestep.  The\\n    environment will be in the state of the other player at the\\n    next timestep.  The environment will be in the state of the\\n    current player at the final timestep.  The environment will\\n    be in the state of the other player at the final timestep.',\n",
       "    '_wait_for_other_player()\\n\\n_timestep(player: int)\\n\\n_player_timestep(player: int)\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions[player] = action\\n\\n_actions']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        session: Optional[CassandraSession],\\n        keyspace: Optional[str],\\n        embedding: Embeddings,\\n        table_name: str = CASSANDRA_SEMANTIC_CACHE_DEFAULT_TABLE_NAME,\\n        distance_metric: str = CASSANDRA_SEMANTIC_CACHE_DEFAULT_DISTANCE_METRIC,\\n        score_threshold: float = CASSANDRA_SEMANTIC_CACHE_DEFAULT_SCORE_THRESHOLD,\\n        ttl_seconds: Optional[int] = CASSANDRA_SEMANTIC_CACHE_DEFAULT_TTL_SECONDS,\\n        skip_provisioning: bool = False,\\n    ):\\n        try:\\n            from cassio.table import MetadataVectorCassandraTable\\n        except (ImportError, ModuleNotFoundError):\\n            raise ValueError(\\n                \"Could not import cassio python package. \"\\n                \"Please install it with `pip install cassio`.\"\\n            )\\n        self.session = session\\n        self.keyspace = keyspace\\n        self.embedding = embedding\\n        self.table_name = table_name\\n        self.distance_metric = distance_metric\\n        self.score_threshold = score_threshold\\n        self.ttl_seconds = ttl_seconds\\n\\n\\n\\n\\n\\n\\n        @lru_cache(maxsize=CASSANDRA_SEMANTIC_CACHE_EMBEDDING_CACHE_SIZE)\\n        def _cache_embedding(text: str) -> List[float]:\\n            return self.embedding.embed_query(text=text)\\n\\n        self._get_embedding = _cache_embedding\\n        self.embedding_dimension = self._get_embedding_dimension()\\n\\n        self.table = MetadataVectorCassandraTable(\\n            session=self.session,\\n            keyspace=self.keyspace,\\n            table=self.table_name,\\n            primary_key_type=[\"TEXT\"],\\n            vector_dimension=self.embedding_dimension,\\n            ttl_seconds=self.ttl_seconds,\\n            metadata_indexing=(\"allow\", {\"_llm_string_hash\"}),\\n            skip_provisioning=skip_provisioning,\\n        )',\n",
       "   'd': \"Initialize the cache with all relevant parameters.\\nArgs:\\n    session (cassandra.cluster.Session): an open Cassandra session\\n    keyspace (str): the keyspace to use for storing the cache\\n    embedding (Embedding): Embedding provider for semantic\\n        encoding and search.\\n    table_name (str): name of the Cassandra (vector) table\\n        to use as cache\\n    distance_metric (str, 'dot'): which measure to adopt for\\n        similarity searches\\n    score_threshold (optional float): numeric value to use as\\n        cutoff for the similarity searches\\n    ttl_seconds (optional int): time-to-live for cache entries\\n        (default: None, i.e. forever)\\nThe default score threshold is tuned to the default metric.\\nTune it carefully yourself if switching to another distance metric.\",\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    ':param weights: \\n    :param name: \\n    :return:',\n",
       "    ':param weights:\\n    :param scale:\\n    :return:\\n    :rtype:\\n    :type scale:\\n    :type name:',\n",
       "    ':param weights:\\n    :param name:\\n    :return:\\n    :']},\n",
       "  {'c': 'def map(self, run: Run) -> Dict[str, str]:\\n    if run.run_type != \"llm\":\\n        raise ValueError(\"LLM RunMapper only supports LLM runs.\")\\n    elif not run.outputs:\\n        if run.error:\\n            raise ValueError(\\n                f\"Cannot evaluate errored LLM run {run.id}: {run.error}\"\\n            )\\n        else:\\n            raise ValueError(\\n                f\"Run {run.id} has no outputs. Cannot evaluate this run.\"\\n            )\\n    else:\\n        try:\\n            inputs = self.serialize_inputs(run.inputs)\\n        except Exception as e:\\n            raise ValueError(\\n                f\"Could not parse LM input from run inputs {run.inputs}\"\\n            ) from e\\n        try:\\n            output_ = self.serialize_outputs(run.outputs)\\n        except Exception as e:\\n            raise ValueError(\\n                f\"Could not parse LM prediction from run outputs {run.outputs}\"\\n            ) from e\\n        return {\"input\": inputs, \"prediction\": output_}',\n",
       "   'd': 'Construct FAISS wrapper from raw documents.\\n\\nThis is a user friendly interface that:\\n    1. Embeds documents.\\n    2. Creates an in memory docstore\\n    3. Initializes the FAISS database\\n\\nThis is intended to be a quick way to get started.\\n\\nExample:\\n    .. code-block:: python\\n\\n        from langchain.vectorstores import FAISS\\n        from langchain.embeddings import OpenAIEmbeddings\\n\\n        embeddings = OpenAIEmbeddings()\\n        faiss = FAISS.from_texts(texts, embeddings)',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'Check exploit.check_default() returns a value when the exploit is\\n  successful.\\n\\n  :param generic_target: A target to exploit.\\n  :type generic_target: target_types.Target\\n  :return: None.',\n",
       "    'Check if the exploit is working properly.\\n\\n  :param generic_target: The target to execute the exploit on.\\n\\n  :return: None.']},\n",
       "  {'c': '    def process_pages(\\n        self,\\n        pages: List[dict],\\n        include_restricted_content: bool,\\n        include_attachments: bool,\\n        include_comments: bool,\\n        content_format: ContentFormat,\\n        ocr_languages: Optional[str] = None,\\n        keep_markdown_format: Optional[bool] = False,\\n    ) -> List[Document]:\\n        docs = []\\n        for page in pages:\\n            if not include_restricted_content and not self.is_public_page(page):\\n                continue\\n            doc = self.process_page(\\n                page,\\n                include_attachments,\\n                include_comments,\\n                content_format,\\n                ocr_languages,\\n                keep_markdown_format,\\n            )\\n            docs.append(doc)\\n\\n        return docs',\n",
       "   'd': 'Process a list of pages into a list of documents.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def test_does_not_allow_extra_kwargs() -> None:\\n    template = \"This is a {foo} test.\"\\n    with pytest.raises(KeyError):\\n        formatter.format(template, foo=\"good\", bar=\"oops\")',\n",
       "   'd': 'Load documents.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def _run(\\n    self,\\n    *args: Any,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n    **kwargs: Any,\\n) -> Any:\\n    new_argument_supported = signature(self.func).parameters.get(\"callbacks\")\\n    return (\\n        self.func(\\n            *args,\\n            callbacks=run_manager.get_child() if run_manager else None,\\n            **kwargs,\\n        )\\n        if new_argument_supported\\n        else self.func(*args, **kwargs)\\n    )',\n",
       "   'd': 'Lazily parse the blob.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        openai_api_key = get_from_dict_or_env(\\n            values, \"openai_api_key\", \"OPENAI_API_KEY\"\\n        )\\n        openai_api_base = get_from_dict_or_env(\\n            values,\\n            \"openai_api_base\",\\n            \"OPENAI_API_BASE\",\\n            default=\"\",\\n        )\\n        openai_proxy = get_from_dict_or_env(\\n            values,\\n            \"openai_proxy\",\\n            \"OPENAI_PROXY\",\\n            default=\"\",\\n        )\\n        openai_organization = get_from_dict_or_env(\\n            values, \"openai_organization\", \"OPENAI_ORGANIZATION\", default=\"\"\\n        )\\n        try:\\n            import openai\\n\\n            openai.api_key = openai_api_key\\n            if openai_api_base:\\n                openai.api_base = openai_api_base\\n            if openai_organization:\\n                openai.organization = openai_organization\\n            if openai_proxy:\\n                openai.proxy = {\"http\": openai_proxy, \"https\": openai_proxy}\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import openai python package. \"\\n                \"Please install it with `pip install openai`.\"\\n            )\\n        try:\\n            values[\"client\"] = openai.ChatCompletion\\n        except AttributeError:\\n            raise ValueError(\\n                \"`openai` has no `ChatCompletion` attribute, this is likely \"\\n                \"due to an old version of the openai package. Try upgrading it \"\\n                \"with `pip install --upgrade openai`.\"\\n            )\\n        warnings.warn(\\n            \"You are trying to use a chat model. This way of initializing it is \"\\n            \"no longer supported. Instead, please use: \"\\n            \"`from langchain.chat_models import ChatOpenAI`\"\\n        )\\n        return values',\n",
       "   'd': 'Add a list of texts to the Elasticsearch index.\\n\\nArgs:\\n    texts (Iterable[str]): The texts to add to the index.\\n    metadatas (List[Dict[Any, Any]], optional): A list of metadata dictionaries\\n        to associate with the texts.\\n    model_id (str, optional): The ID of the model to use for transforming the\\n        texts into vectors.\\n    refresh_indices (bool, optional): Whether to refresh the Elasticsearch\\n        indices after adding the texts.\\n    **kwargs: Arbitrary keyword arguments.\\n\\nReturns:\\n    A list of IDs for the added texts.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        parser: BaseOutputParser[T],\\n        prompt: BasePromptTemplate = NAIVE_FIX_PROMPT,\\n    ) -> OutputFixingParser[T]:\\n        from langchain.chains.llm import LLMChain\\n\\n        chain = LLMChain(llm=llm, prompt=prompt)\\n        return cls(parser=parser, retry_chain=chain)',\n",
       "   'd': 'PATCH the URL and return the text.',\n",
       "   'l': False,\n",
       "   'g': ['(str) -> str',\n",
       "    '(im)',\n",
       "    '_getheaderAnim:\\n    :param im:\\n    :return:',\n",
       "    '_getheaderAnim : :param im: :return:']},\n",
       "  {'c': 'def __init__(\\n    self,\\n    file_path: Union[str, List[str]],\\n    mode: str = \"single\",\\n    **unstructured_kwargs: Any,\\n):\\n    self.file_path = file_path\\n    super().__init__(mode=mode, **unstructured_kwargs)',\n",
       "   'd': 'Select which examples to use based on semantic similarity.',\n",
       "   'l': False,\n",
       "   'g': ['\"pip-upgrade-packages\" option no existing file',\n",
       "    '',\n",
       "    '',\n",
       "    '\"small-fake-a==0.2\" and \"small-fake-b==0.3\" should be added to the\\n    requirements.txt file']},\n",
       "  {'c': '    def get_input_schema(\\n        self, config: Optional[RunnableConfig] = None\\n    ) -> Type[BaseModel]:\\n        func = getattr(self, \"func\", None) or getattr(self, \"afunc\")\\n\\n        if isinstance(func, itemgetter):\\n\\n\\n            items = str(func).replace(\"operator.itemgetter(\", \"\")[:-1].split(\", \")\\n            if all(\\n                item[0] == \"\\'\" and item[-1] == \"\\'\" and len(item) > 2 for item in items\\n            ):\\n\\n                return create_model(\\n                    \"RunnableLambdaInput\",\\n                    **{item[1:-1]: (Any, None) for item in items},\\n                )\\n            else:\\n                return create_model(\"RunnableLambdaInput\", __root__=(List[Any], None))\\n\\n        if self.InputType != Any:\\n            return super().get_input_schema(config)\\n\\n        if dict_keys := get_function_first_arg_dict_keys(func):\\n            return create_model(\\n                \"RunnableLambdaInput\",\\n                **{key: (Any, None) for key in dict_keys},\\n            )\\n\\n        return super().get_input_schema(config)',\n",
       "   'd': 'Run the tool.',\n",
       "   'l': False,\n",
       "   'g': ['Stop all the jobs in the current session.\\n  :return:',\n",
       "    'Stop all jobs in all sessions.',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': '    def batch(\\n        self,\\n        inputs: List[Input],\\n        config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,\\n        **kwargs: Optional[Any],\\n    ) -> List[Output]:\\n        configs = self._get_config_list(config, len(inputs))\\n\\n\\n        if len(inputs) == 1:\\n            return [self.invoke(inputs[0], configs[0], **kwargs)]\\n\\n        with get_executor_for_config(configs[0]) as executor:\\n            return list(\\n                executor.map(\\n                    partial(self.invoke, **kwargs),\\n                    inputs,\\n                    (patch_config(c, executor=executor) for c in configs),\\n                )\\n            )',\n",
       "   'd': 'Default implementation of batch, which calls invoke N times.\\nSubclasses should override this method if they can batch more efficiently.',\n",
       "   'l': True,\n",
       "   'g': ['(self):', '(self):', '(self):', '(self):']},\n",
       "  {'c': '    def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n        from momento.responses import CacheGet\\n\\n        generations: RETURN_VAL_TYPE = []\\n\\n        get_response = self.cache_client.get(\\n            self.cache_name, self.__key(prompt, llm_string)\\n        )\\n        if isinstance(get_response, CacheGet.Hit):\\n            value = get_response.value_string\\n            generations = _load_generations_from_json(value)\\n        elif isinstance(get_response, CacheGet.Miss):\\n            pass\\n        elif isinstance(get_response, CacheGet.Error):\\n            raise get_response.inner_exception\\n        return generations if generations else None',\n",
       "   'd': 'Call out to an AzureML Managed Online endpoint.\\nArgs:\\n    prompt: The prompt to pass into the model.\\n    stop: Optional list of stop words to use when generating.\\nReturns:\\n    The string generated by the model.\\nExample:\\n    .. code-block:: python\\n        response = azureml_model(\"Tell me a joke.\")',\n",
       "   'l': False,\n",
       "   'g': ['_init_ -', '_init_ -', '_init__', '_init_ -']},\n",
       "  {'c': '    def similarity_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        pre_filter: Optional[dict] = None,\\n        post_filter_pipeline: Optional[List[Dict]] = None,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        docs_and_scores = self.similarity_search_with_score(\\n            query,\\n            k=k,\\n            pre_filter=pre_filter,\\n            post_filter_pipeline=post_filter_pipeline,\\n        )\\n        return [doc for doc, _ in docs_and_scores]',\n",
       "   'd': 'Use the tool.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'Sets the output keys for all the datasets.\\n\\n    Args:\\n        datasets: list of datasets.\\n        output_keys: list of output keys.\\n\\n    Returns:\\n        None.',\n",
       "    'Sets the output keys for the given datasets.\\n\\n    :param datasets: list of datasets to set the output keys for\\n    :param output_keys: list of output keys to set for the given datasets\\n    :return:']},\n",
       "  {'c': 'def _call(\\n    self,\\n    inputs: Dict[str, List[Document]],\\n    run_manager: Optional[CallbackManagerForChainRun] = None,\\n) -> Dict[str, str]:\\n    _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n    docs = inputs[self.input_key]\\n\\n    other_keys = {k: v for k, v in inputs.items() if k != self.input_key}\\n    output, extra_return_dict = self.combine_docs(\\n        docs, callbacks=_run_manager.get_child(), **other_keys\\n    )\\n    extra_return_dict[self.output_key] = output\\n    return extra_return_dict',\n",
       "   'd': 'Format the prompt with the inputs.\\n\\nArgs:\\n    **kwargs: Any arguments to be passed to the prompt template.\\n\\nReturns:\\n    A formatted string.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    prompt.format(variable1=\"foo\")',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def on_chain_error(\\n        self,\\n        error: BaseException,\\n        **kwargs: Any,\\n    ) -> None:\\n        self.ended = True\\n        return self.parent_run_manager.on_chain_error(error, **kwargs)',\n",
       "   'd': 'Run when chain errors.\\n\\nArgs:\\n    error (Exception or KeyboardInterrupt): The error.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def input_keys(self) -> List[str]:\\n    return [\"user_input\", \"context\", \"response\"]',\n",
       "   'd': \"Instantiate a prompt cache using Momento as a backend.\\n\\nNote: to instantiate the cache client passed to MomentoCache,\\nyou must have a Momento account. See https://gomomento.com/.\\n\\nArgs:\\n    cache_client (CacheClient): The Momento cache client.\\n    cache_name (str): The name of the cache to use to store the data.\\n    ttl (Optional[timedelta], optional): The time to live for the cache items.\\n        Defaults to None, ie use the client default TTL.\\n    ensure_cache_exists (bool, optional): Create the cache if it doesn't\\n        exist. Defaults to True.\\n\\nRaises:\\n    ImportError: Momento python package is not installed.\\n    TypeError: cache_client is not of type momento.CacheClientObject\\n    ValueError: ttl is non-null and non-negative\",\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def test_instruct_prompt() -> None:\\n    llm = MosaicML(inject_instruction_format=True, model_kwargs={\"do_sample\": False})\\n    instruction = \"Repeat the word foo\"\\n    prompt = llm._transform_prompt(instruction)\\n    expected_prompt = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction)\\n    assert prompt == expected_prompt\\n    output = llm(prompt)\\n    assert isinstance(output, str)',\n",
       "   'd': \"Decorator to mark a function, a class, or a property as deprecated.\\n\\nWhen deprecating a classmethod, a staticmethod, or a property, the\\n``@deprecated`` decorator should go *under* ``@classmethod`` and\\n``@staticmethod`` (i.e., `deprecated` should directly decorate the\\nunderlying callable), but *over* ``@property``.\\n\\nWhen deprecating a class ``C`` intended to be used as a base class in a\\nmultiple inheritance hierarchy, ``C`` *must* define an ``__init__`` method\\n(if ``C`` instead inherited its ``__init__`` from its own base class, then\\n``@deprecated`` would mess up ``__init__`` inheritance when installing its\\nown (deprecation-emitting) ``C.__init__``).\\n\\nParameters are the same as for `warn_deprecated`, except that *obj_type*\\ndefaults to 'class' if decorating a class, 'attribute' if decorating a\\nproperty, and 'function' otherwise.\\n\\nArguments:\\n    since : str\\n        The release at which this API became deprecated.\\n    message : str, optional\\n        Override the default deprecation message. The %(since)s,\\n        %(name)s, %(alternative)s, %(obj_type)s, %(addendum)s,\\n        and %(removal)s format specifiers will be replaced by the\\n        values of the respective arguments passed to this function.\\n    name : str, optional\\n        The name of the deprecated object.\\n    alternative : str, optional\\n        An alternative API that the user may use in place of the\\n        deprecated API. The deprecation warning will tell the user\\n        about this alternative if provided.\\n    pending : bool, optional\\n        If True, uses a PendingDeprecationWarning instead of a\\n        DeprecationWarning. Cannot be used together with removal.\\n    obj_type : str, optional\\n        The object type being deprecated.\\n    addendum : str, optional\\n        Additional text appended directly to the final message.\\n    removal : str, optional\\n        The expected removal version. With the default (an empty\\n        string), a removal version is automatically computed from\\n        since. Set to other Falsy values to not schedule a removal\\n        date. Cannot be used together with pending.\\n\\nExamples\\n--------\\n\\n    .. code-block:: python\\n\\n        @deprecated('1.4.0')\\n        def the_function_to_deprecate():\\n            pass\",\n",
       "   'l': False,\n",
       "   'g': [':param session:\\n      :param pool_connections:\\n      :param pool_maxsize:\\n      :param max_retries:\\n      :return:\\n      :rtype:',\n",
       "    ':param session:\\n      :param pool_connections:\\n      :param pool_maxsize:\\n      :param max_retries:\\n      :return:\\n      :rtype: object',\n",
       "    '',\n",
       "    \"'Enhance' the connection to the remote server.\"]},\n",
       "  {'c': 'def test_vectara_add_documents() -> None:\\n    texts = [\"grounded generation\", \"retrieval augmented generation\", \"data privacy\"]\\n    docsearch: Vectara = Vectara.from_texts(\\n        texts,\\n        embedding=FakeEmbeddings(),\\n        metadatas=[\\n            {\"abbr\": \"gg\", \"test_num\": \"1\"},\\n            {\"abbr\": \"rag\", \"test_num\": \"1\"},\\n            {\"abbr\": \"dp\", \"test_num\": \"1\"},\\n        ],\\n        doc_metadata={\"test_num\": \"1\"},\\n    )\\n\\n\\n    new_texts = [\"large language model\", \"information retrieval\", \"question answering\"]\\n    docsearch.add_documents(\\n        [Document(page_content=t, metadata={\"abbr\": get_abbr(t)}) for t in new_texts],\\n        doc_metadata={\"test_num\": \"1\"},\\n    )\\n\\n\\n    output = docsearch.similarity_search(\\n        \"large language model\",\\n        k=2,\\n        n_sentence_context=0,\\n        filter=\"doc.test_num = 1\",\\n    )\\n    assert output[0].page_content == \"large language model\"\\n    assert output[0].metadata == {\"abbr\": \"llm\"}\\n    assert output[1].page_content == \"information retrieval\"\\n    assert output[1].metadata == {\"abbr\": \"ir\"}',\n",
       "   'd': 'Test end to end construction and search.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        client: marqo.Client,\\n        index_name: str,\\n        add_documents_settings: Optional[Dict[str, Any]] = None,\\n        searchable_attributes: Optional[List[str]] = None,\\n        page_content_builder: Optional[Callable[[Dict[str, Any]], str]] = None,\\n    ):\\n        try:\\n            import marqo\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import marqo python package. \"\\n                \"Please install it with `pip install marqo`.\"\\n            )\\n        if not isinstance(client, marqo.Client):\\n            raise ValueError(\\n                f\"client should be an instance of marqo.Client, got {type(client)}\"\\n            )\\n        self._client = client\\n        self._index_name = index_name\\n        self._add_documents_settings = (\\n            {} if add_documents_settings is None else add_documents_settings\\n        )\\n        self._searchable_attributes = searchable_attributes\\n        self.page_content_builder = page_content_builder\\n\\n        self.tensor_fields = [\"text\"]\\n\\n        self._document_batch_size = 1024',\n",
       "   'd': 'Helper method to transform an Iterator of Input values into an Iterator of\\nOutput values, with callbacks.\\nUse this to implement `stream()` or `transform()` in Runnable subclasses.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    \"_cfg_file = os.path.join(self.cfg_dir, 'config.yml')\\n    if os.path.exists(_cfg_file):\\n        with open(_cfg_file, 'r') as f:\\n            cfg = yaml.load(f)\",\n",
       "    '_cfg = cfg.get_config()\\n        # TODO: \\n        # 1. add some default value\\n        # 2. add some config check\\n        # 3. add some config check for some config\\n        # 4. add some config check for some config\\n        # 5. add some config check for some config\\n        # 6. add some config check for some config\\n        # 7. add some config check for some config\\n        # 8. add some config check for some config\\n        # 9. add some config check for some config\\n        # 10. add some config check for some config\\n        # 11. add some config check for some config\\n        # 12. add some config check for some config\\n        # 13. add some config check for some config\\n        # 14. add some config check for some config\\n        # 15. add some config check for some config\\n        # 16. add some config check for some config\\n        # 17. add some config check for some config\\n        # 18. add some config check for some config\\n        #',\n",
       "    '_init_function_ = lambda self, cfg: self.init_function(cfg)\\n\\n        return self.init_function_']},\n",
       "  {'c': '    def run(self, command: str, fetch: str = \"all\") -> str:\\n        result = self._execute(command, fetch)\\n\\n\\n        if not result:\\n            return \"\"\\n        elif isinstance(result, list):\\n            res: Sequence = [\\n                tuple(truncate_word(c, length=self._max_string_length) for c in r)\\n                for r in result\\n            ]\\n        else:\\n            res = tuple(\\n                truncate_word(c, length=self._max_string_length) for c in result\\n            )\\n        return str(res)',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': False,\n",
       "   'g': ['_get_loaded_models_info_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model_info_dict_by_key_and_model',\n",
       "    '_get_loaded_models_info() -> str:',\n",
       "    '_get_loaded_models_info() -> str:',\n",
       "    '_get_loaded_models_info_result_type_dict_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict_str_dict']},\n",
       "  {'c': '    def similarity_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        embedding = self.embedding.embed_query(text=query)\\n        return self.similarity_search_by_vector(\\n            embedding=embedding,\\n            k=k,\\n        )',\n",
       "   'd': 'Run similarity search with Neo4jVector.\\n\\nArgs:\\n    query (str): Query text to search for.\\n    k (int): Number of results to return. Defaults to 4.\\n\\nReturns:\\n    List of Documents most similar to the query.',\n",
       "   'l': True,\n",
       "   'g': ['.get_dev_examples(self, data_dir)', '.', '.get_dev_examples', '.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        dataset_path: str = _LANGCHAIN_DEFAULT_DEEPLAKE_PATH,\\n        token: Optional[str] = None,\\n        embedding: Optional[Embeddings] = None,\\n        embedding_function: Optional[Embeddings] = None,\\n        read_only: bool = False,\\n        ingestion_batch_size: int = 1000,\\n        num_workers: int = 0,\\n        verbose: bool = True,\\n        exec_option: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        self.ingestion_batch_size = ingestion_batch_size\\n        self.num_workers = num_workers\\n        self.verbose = verbose\\n\\n        if _DEEPLAKE_INSTALLED is False:\\n            raise ImportError(\\n                \"Could not import deeplake python package. \"\\n                \"Please install it with `pip install deeplake[enterprise]`.\"\\n            )\\n\\n        if (\\n            kwargs.get(\"runtime\") == {\"tensor_db\": True}\\n            and version_compare(deeplake.__version__, \"3.6.7\") == -1\\n        ):\\n            raise ImportError(\\n                \"To use tensor_db option you need to update deeplake to `3.6.7`. \"\\n                f\"Currently installed deeplake version is {deeplake.__version__}. \"\\n            )\\n\\n        self.dataset_path = dataset_path\\n\\n        if embedding_function:\\n            logger.warning(\\n                \"Using embedding function is deprecated and will be removed \"\\n                \"in the future. Please use embedding instead.\"\\n            )\\n\\n        self.vectorstore = DeepLakeVectorStore(\\n            path=self.dataset_path,\\n            embedding_function=embedding_function or embedding,\\n            read_only=read_only,\\n            token=token,\\n            exec_option=exec_option,\\n            verbose=verbose,\\n            **kwargs,\\n        )\\n\\n        self._embedding_function = embedding_function or embedding\\n        self._id_tensor_name = \"ids\" if \"ids\" in self.vectorstore.tensors() else \"id\"',\n",
       "   'd': 'Get documents relevant for a query.',\n",
       "   'l': False,\n",
       "   'g': ['(function)',\n",
       "    '(url, method, headers, files, data, params, auth, cookies, hooks, json) -> None:',\n",
       "    '(function)',\n",
       "    '(str|None) -> None']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        init_func: Union[\\n            Callable[[Any, str], None], Callable[[Any], None], None\\n        ] = None,\\n    ):\\n        try:\\n            import gptcache\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import gptcache python package. \"\\n                \"Please install it with `pip install gptcache`.\"\\n            )\\n\\n        self.init_gptcache_func: Union[\\n            Callable[[Any, str], None], Callable[[Any], None], None\\n        ] = init_func\\n        self.gptcache_dict: Dict[str, Any] = {}',\n",
       "   'd': 'Initialize by passing in init function (default: `None`).\\n\\nArgs:\\n    init_func (Optional[Callable[[Any], None]]): init `GPTCache` function\\n    (default: `None`)\\n\\nExample:\\n.. code-block:: python\\n\\n    # Initialize GPTCache with a custom init function\\n    import gptcache\\n    from gptcache.processor.pre import get_prompt\\n    from gptcache.manager.factory import get_data_manager\\n\\n    # Avoid multiple caches using the same file,\\n    causing different llm model caches to affect each other\\n\\n    def init_gptcache(cache_obj: gptcache.Cache, llm str):\\n        cache_obj.init(\\n            pre_embedding_func=get_prompt,\\n            data_manager=manager_factory(\\n                manager=\"map\",\\n                data_dir=f\"map_cache_{llm}\"\\n            ),\\n        )\\n\\n    langchain.llm_cache = GPTCache(init_gptcache)',\n",
       "   'l': True,\n",
       "   'g': ['(form_class, form_object, form_name, content_object, request)',\n",
       "    '_process_all_forms',\n",
       "    '_form_view_function_wrapper(self, form):',\n",
       "    '_process_all_forms']},\n",
       "  {'c': 'def lazy_load(self) -> Iterator[Document]:\\n    if self.use_async:\\n        results = asyncio.run(self._async_get_child_links_recursive(self.url))\\n        if results is None:\\n            return iter([])\\n        else:\\n            return iter(results)\\n    else:\\n        return self._get_child_links_recursive(self.url)',\n",
       "   'd': 'Update cache.\\nFirst, retrieve the corresponding cache object using the `llm_string` parameter,\\nand then store the `prompt` and `return_val` in the cache object.',\n",
       "   'l': False,\n",
       "   'g': [\"_time(self, time_func, apply_to=['mask', 'audio'],\\n                keep_duration=False):\",\n",
       "    \"_time(self, time_func, apply_to=['mask', 'audio'],\\n                keep_duration=False):\",\n",
       "    \"_time(self, time_func, apply_to=['mask', 'audio'],\\n                keep_duration=False):\",\n",
       "    \"_time(self, time_func, apply_to=['mask', 'audio'],\\n                keep_duration=False):\"]},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        openai_api_key = get_from_dict_or_env(\\n            values, \"openai_api_key\", \"OPENAI_API_KEY\"\\n        )\\n        openai_api_base = get_from_dict_or_env(\\n            values,\\n            \"openai_api_base\",\\n            \"OPENAI_API_BASE\",\\n            default=\"\",\\n        )\\n        openai_proxy = get_from_dict_or_env(\\n            values,\\n            \"openai_proxy\",\\n            \"OPENAI_PROXY\",\\n            default=\"\",\\n        )\\n        openai_organization = get_from_dict_or_env(\\n            values, \"openai_organization\", \"OPENAI_ORGANIZATION\", default=\"\"\\n        )\\n        try:\\n            import openai\\n\\n            openai.api_key = openai_api_key\\n            if openai_api_base:\\n                openai.api_base = openai_api_base\\n            if openai_organization:\\n                openai.organization = openai_organization\\n            if openai_proxy:\\n                openai.proxy = {\"http\": openai_proxy, \"https\": openai_proxy}\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import openai python package. \"\\n                \"Please install it with `pip install openai`.\"\\n            )\\n        try:\\n            values[\"client\"] = openai.ChatCompletion\\n        except AttributeError:\\n            raise ValueError(\\n                \"`openai` has no `ChatCompletion` attribute, this is likely \"\\n                \"due to an old version of the openai package. Try upgrading it \"\\n                \"with `pip install --upgrade openai`.\"\\n            )\\n        warnings.warn(\\n            \"You are trying to use a chat model. This way of initializing it is \"\\n            \"no longer supported. Instead, please use: \"\\n            \"`from langchain.chat_models import ChatOpenAI`\"\\n        )\\n        return values',\n",
       "   'd': 'Clear semantic cache for a given llm_string.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'Load the library item into the record.\\n\\n  :return: None.',\n",
       "    'Load the current song into the current song object.\\n  :return: None']},\n",
       "  {'c': '    def load(self) -> List[Document]:\\n        parser = PDFPlumberParser(text_kwargs=self.text_kwargs, dedupe=self.dedupe)\\n        blob = Blob.from_path(self.file_path)\\n        return parser.parse(blob)',\n",
       "   'd': 'Generate the reference.rst file for each package.',\n",
       "   'l': False,\n",
       "   'g': ['', '', ':param date_string: \\n    :return:', '']},\n",
       "  {'c': 'def try_load_from_hub(\\n    path: Union[str, Path],\\n    loader: Callable[[str], T],\\n    valid_prefix: str,\\n    valid_suffixes: Set[str],\\n    **kwargs: Any,\\n) -> Optional[T]:\\n    if not isinstance(path, str) or not (match := HUB_PATH_RE.match(path)):\\n        return None\\n    ref, remote_path_str = match.groups()\\n    ref = ref[1:] if ref else DEFAULT_REF\\n    remote_path = Path(remote_path_str)\\n    if remote_path.parts[0] != valid_prefix:\\n        return None\\n    if remote_path.suffix[1:] not in valid_suffixes:\\n        raise ValueError(f\"Unsupported file type, must be one of {valid_suffixes}.\")\\n\\n\\n\\n\\n\\n\\n    full_url = urljoin(URL_BASE.format(ref=ref), PurePosixPath(remote_path).__str__())\\n\\n    r = requests.get(full_url, timeout=5)\\n    if r.status_code != 200:\\n        raise ValueError(f\"Could not find file at {full_url}\")\\n    with tempfile.TemporaryDirectory() as tmpdirname:\\n        file = Path(tmpdirname) / remote_path.name\\n        with open(file, \"wb\") as f:\\n            f.write(r.content)\\n        return loader(str(file), **kwargs)',\n",
       "   'd': 'Load configuration from hub.  Returns None if path is not a hub path.',\n",
       "   'l': True,\n",
       "   'g': ['Test that the default mode is set on a file.\\n\\n    This is a simple test that the default mode is set on a file.',\n",
       "    'Tests that the default mode is set on a file created by dotbot.\\n\\n    :param home: The home directory.\\n    :param dotfiles: The dotfiles object.\\n    :param run_dotbot: The run_dotbot function.\\n    :return: None.',\n",
       "    'Test default mode',\n",
       "    'Test that a file created with defaults is created with the correct\\n    mode.\\n\\n    :param home:\\n    :param dotfiles:\\n    :param run_dotbot:\\n    :return:\\n    :rtype: None\\n    :raises: :class:`~dotbot.DotbotError`']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: List[str],\\n        metadatas: Optional[List[dict]] = None,\\n        doc_metadata: Optional[dict] = None,\\n    ) -> None:\\n        self.vectorstore.add_texts(texts, metadatas, doc_metadata or {})',\n",
       "   'd': 'Add text to the Vectara vectorstore.\\n\\nArgs:\\n    texts (List[str]): The text\\n    metadatas (List[dict]): Metadata dicts, must line up with existing store',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def _create_retry_decorator(\\n    llm: ChatFireworks,\\n    run_manager: Optional[\\n        Union[AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun]\\n    ] = None,\\n) -> Callable[[Any], Any]:\\n    errors = [\\n        fireworks.client.error.RateLimitError,\\n        fireworks.client.error.ServiceUnavailableError,\\n    ]\\n    return create_base_retry_decorator(\\n        error_types=errors, max_retries=llm.max_retries, run_manager=run_manager\\n    )',\n",
       "   'd': 'Define retry mechanism.',\n",
       "   'l': True,\n",
       "   'g': ['Creates a simple fully-connected network.',\n",
       "    'Returns a network_fn that implements a fully-connected\\n  multi-layer perceptron (MLP).',\n",
       "    '- fully-connected (fc) layers with',\n",
       "    '']},\n",
       "  {'c': 'def __init__(\\n    self,\\n    separators: Optional[List[str]] = None,\\n    keep_separator: bool = True,\\n    is_separator_regex: bool = False,\\n    **kwargs: Any,\\n) -> None:\\n    super().__init__(keep_separator=keep_separator, **kwargs)\\n    self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n    self._is_separator_regex = is_separator_regex',\n",
       "   'd': 'Initialize a PythonCodeTextSplitter.',\n",
       "   'l': False,\n",
       "   'g': ['Sets the display name of the user.\\n\\n  Args:\\n    display_name: The display name of the user.\\n  Returns:\\n    None.',\n",
       "    '',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': '    def from_function(\\n        cls,\\n        func: Optional[Callable] = None,\\n        coroutine: Optional[Callable[..., Awaitable[Any]]] = None,\\n        name: Optional[str] = None,\\n        description: Optional[str] = None,\\n        return_direct: bool = False,\\n        args_schema: Optional[Type[BaseModel]] = None,\\n        infer_schema: bool = True,\\n        **kwargs: Any,\\n    ) -> StructuredTool:\\n        if func is not None:\\n            source_function = func\\n        elif coroutine is not None:\\n            source_function = coroutine\\n        else:\\n            raise ValueError(\"Function and/or coroutine must be provided\")\\n        name = name or source_function.__name__\\n        description = description or source_function.__doc__\\n        if description is None:\\n            raise ValueError(\\n                \"Function must have a docstring if description not provided.\"\\n            )\\n\\n\\n\\n        sig = signature(source_function)\\n        description = f\"{name}{sig} - {description.strip()}\"\\n        _args_schema = args_schema\\n        if _args_schema is None and infer_schema:\\n            _args_schema = create_schema_from_function(f\"{name}Schema\", source_function)\\n        return cls(\\n            name=name,\\n            func=func,\\n            coroutine=coroutine,\\n            args_schema=_args_schema,\\n            description=description,\\n            return_direct=return_direct,\\n            **kwargs,\\n        )',\n",
       "   'd': 'Create tool from a given function.\\n\\nA classmethod that helps to create a tool from a function.\\n\\nArgs:\\n    func: The function from which to create a tool\\n    coroutine: The async function from which to create a tool\\n    name: The name of the tool. Defaults to the function name\\n    description: The description of the tool. Defaults to the function docstring\\n    return_direct: Whether to return the result directly or as a callback\\n    args_schema: The schema of the tool\\'s input arguments\\n    infer_schema: Whether to infer the schema from the function\\'s signature\\n    **kwargs: Additional arguments to pass to the tool\\n\\nReturns:\\n    The tool\\n\\nExamples:\\n\\n    .. code-block:: python\\n\\n        def add(a: int, b: int) -> int:\\n            \"\"\"Add two numbers\"\"\"\\n            return a + b\\n        tool = StructuredTool.from_function(add)\\n        tool.run(1, 2) # 3',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Test that an individual boss on unit test is working as expected.\\n\\n  :param X_train:\\n  :param y_train:\\n  :param X_test:\\n  :param y_test:\\n  :return:',\n",
       "    'Test individual_boss_unit_test_probas',\n",
       "    '']},\n",
       "  {'c': '        def _create_doc(node: Any, text: str) -> Document:\\n            metadata = {\\n                XPATH_KEY: _xpath_for_chunk(node),\\n                DOCUMENT_ID_KEY: document[\"id\"],\\n                DOCUMENT_NAME_KEY: document[\"name\"],\\n                STRUCTURE_KEY: node.attrib.get(\"structure\", \"\"),\\n                TAG_KEY: re.sub(r\"\\\\{.*\\\\}\", \"\", node.tag),\\n            }\\n\\n            if doc_metadata:\\n                metadata.update(doc_metadata)\\n\\n            return Document(\\n                page_content=text,\\n                metadata=metadata,\\n            )',\n",
       "   'd': 'Create a Document from a node and text.',\n",
       "   'l': True,\n",
       "   'g': [':return:', ':return:', ':return:', ':return:']},\n",
       "  {'c': '    def __init__(self, redis_: Any, *, ttl: Optional[int] = None):\\n        try:\\n            from redis import Redis\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import redis python package. \"\\n                \"Please install it with `pip install redis`.\"\\n            )\\n        if not isinstance(redis_, Redis):\\n            raise ValueError(\"Please pass in Redis object.\")\\n        self.redis = redis_\\n        self.ttl = ttl',\n",
       "   'd': 'Initialize an instance of RedisCache.\\n\\nThis method initializes an object with Redis caching capabilities.\\nIt takes a `redis_` parameter, which should be an instance of a Redis\\nclient class, allowing the object to interact with a Redis\\nserver for caching purposes.\\n\\nParameters:\\n    redis_ (Any): An instance of a Redis client class\\n        (e.g., redis.Redis) used for caching.\\n        This allows the object to communicate with a\\n        Redis server for caching operations.\\n    ttl (int, optional): Time-to-live (TTL) for cached items in seconds.\\n        If provided, it sets the time duration for how long cached\\n        items will remain valid. If not provided, cached items will not\\n        have an automatic expiration.',\n",
       "   'l': True,\n",
       "   'g': ['_extract(self, file_path):\\n    Extracts a tar file to the data directory.\\n\\n    Parameters\\n    ----------\\n    file_path : str\\n        The path to the tar file to extract.\\n\\n    Returns\\n    -------\\n    bool\\n        True if the extraction was successful, False otherwise.',\n",
       "    '_extract(self, file_path):\\n    Extract a tar file to the data directory.\\n\\n    :param file_path: The file path to extract.\\n    :return: True if the file was extracted, False otherwise.',\n",
       "    '_extract(self, file_path):\\n\\n    Extract a tar.gz file.\\n\\n    Parameters\\n    ----------\\n    file_path : str\\n        Path to the tar.gz file to extract.\\n\\n    Returns\\n    -------\\n    bool\\n        True if extraction is successful, False otherwise.\\n\\n    See Also\\n    --------\\n    extract_file(self, file_path):\\n        Extract a file from a tar.gz file.',\n",
       "    '_extract(self, file_path):']},\n",
       "  {'c': '    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        self.return_each_line = return_each_line\\n\\n\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )',\n",
       "   'd': 'Create a question answering chain that returns an answer with sources.\\n\\nArgs:\\n    llm: Language model to use for the chain.\\n    **kwargs: Keyword arguments to pass to `create_qa_with_structure_chain`.\\n\\nReturns:\\n    Chain (LLMChain) that can be used to answer questions with citations.',\n",
       "   'l': False,\n",
       "   'g': ['(str shareName, str pathName) -> bool',\n",
       "    '(str shareName, str pathName) -> bool\\n\\n    :param shareName:\\n    :param pathName:\\n    :return:',\n",
       "    '(shareName, pathName) -> bool\\n\\n    Delete a file from a share.\\n\\n    :param shareName: The share name.\\n    :param pathName: The path name.\\n    :return: True if the file was deleted successfully, False otherwise.\\n    :raises: SessionError if the session is not connected to a server.',\n",
       "    '(str shareName, str pathName) -> (str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str']},\n",
       "  {'c': 'def clear(self, **kwargs: Any) -> None:\\n    with Session(self.engine) as session:\\n        session.query(self.cache_schema).delete()\\n        session.commit()',\n",
       "   'd': 'Return response when agent has been stopped due to max iterations.',\n",
       "   'l': False,\n",
       "   'g': ['Clear the current jobs.', '', '', 'Clear the jobs.']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        google_api_key = get_from_dict_or_env(\\n            values, \"google_api_key\", \"GOOGLE_API_KEY\"\\n        )\\n        values[\"google_api_key\"] = google_api_key\\n\\n        google_cse_id = get_from_dict_or_env(values, \"google_cse_id\", \"GOOGLE_CSE_ID\")\\n        values[\"google_cse_id\"] = google_cse_id\\n\\n        try:\\n            from googleapiclient.discovery import build\\n\\n        except ImportError:\\n            raise ImportError(\\n                \"google-api-python-client is not installed. \"\\n                \"Please install it with `pip install google-api-python-client\"\\n                \">=2.100.0`\"\\n            )\\n\\n        service = build(\"customsearch\", \"v1\", developerKey=google_api_key)\\n        values[\"search_engine\"] = service\\n\\n        return values',\n",
       "   'd': 'Validate that api key and python package exists in environment.',\n",
       "   'l': False,\n",
       "   'g': ['_contains__',\n",
       "    '(self, other) -> bool\\n\\n    Return True if self is a subset of other.',\n",
       "    '_contains__',\n",
       "    '(x, y) in (0, 1)']},\n",
       "  {'c': 'def args(self) -> dict:\\n    if self.args_schema is not None:\\n        return self.args_schema.schema()[\"properties\"]\\n\\n\\n    return {\"tool_input\": {\"type\": \"string\"}}',\n",
       "   'd': \"Run an Actor on the Apify platform and wait for results to be ready.\\nArgs:\\n    actor_id (str): The ID or name of the Actor on the Apify platform.\\n    run_input (Dict): The input object of the Actor that you're trying to run.\\n    dataset_mapping_function (Callable): A function that takes a single\\n        dictionary (an Apify dataset item) and converts it to an\\n        instance of the Document class.\\n    build (str, optional): Optionally specifies the actor build to run.\\n        It can be either a build tag or build number.\\n    memory_mbytes (int, optional): Optional memory limit for the run,\\n        in megabytes.\\n    timeout_secs (int, optional): Optional timeout for the run, in seconds.\\nReturns:\\n    ApifyDatasetLoader: A loader that will fetch the records from the\\n        Actor run's default dataset.\",\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Handle the mouse press event.  If the middle button is\\n    pressed, then close the tab that is under the\\n    mouse.  If the tab is closed, then also close the\\n    window.\\n\\n  :param event:  Qt.MouseEvent  the mouse event to handle\\n  :return:  None',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': '    def marqo_bulk_similarity_search(\\n        self, queries: Iterable[Union[str, Dict[str, float]]], k: int = 4\\n    ) -> Dict[str, List[Dict[str, List[Dict[str, str]]]]]:\\n        bulk_results = self._client.bulk_search(\\n            [\\n                {\\n                    \"index\": self._index_name,\\n                    \"q\": query,\\n                    \"searchableAttributes\": self._searchable_attributes,\\n                    \"limit\": k,\\n                }\\n                for query in queries\\n            ]\\n        )\\n        return bulk_results',\n",
       "   'd': 'Construct OpenSearchVectorSearch wrapper from raw documents.\\n\\nExample:\\n    .. code-block:: python\\n\\n        from langchain import OpenSearchVectorSearch\\n        from langchain.embeddings import OpenAIEmbeddings\\n        embeddings = OpenAIEmbeddings()\\n        opensearch_vector_search = OpenSearchVectorSearch.from_texts(\\n            texts,\\n            embeddings,\\n            opensearch_url=\"http://localhost:9200\"\\n        )\\n\\nOpenSearch by default supports Approximate Search powered by nmslib, faiss\\nand lucene engines recommended for large datasets. Also supports brute force\\nsearch through Script Scoring and Painless Scripting.\\n\\nOptional Args:\\n    vector_field: Document field embeddings are stored in. Defaults to\\n    \"vector_field\".\\n\\n    text_field: Document field the text of the document is stored in. Defaults\\n    to \"text\".\\n\\nOptional Keyword Args for Approximate Search:\\n    engine: \"nmslib\", \"faiss\", \"lucene\"; default: \"nmslib\"\\n\\n    space_type: \"l2\", \"l1\", \"cosinesimil\", \"linf\", \"innerproduct\"; default: \"l2\"\\n\\n    ef_search: Size of the dynamic list used during k-NN searches. Higher values\\n    lead to more accurate but slower searches; default: 512\\n\\n    ef_construction: Size of the dynamic list used during k-NN graph creation.\\n    Higher values lead to more accurate graph but slower indexing speed;\\n    default: 512\\n\\n    m: Number of bidirectional links created for each new element. Large impact\\n    on memory consumption. Between 2 and 100; default: 16\\n\\nKeyword Args for Script Scoring or Painless Scripting:\\n    is_appx_search: False',\n",
       "   'l': False,\n",
       "   'g': ['_authenticate_user() -> AuthResponse:',\n",
       "    '_authenticate_user:\\n    :returns: AuthResponse\\n    :raises: 500, 401, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 422, 423, 424, 426, 428, 429, 431, 444, 451, 500, 501, 502, 503, 504, 505, 506, 507, 508, 510, 511, 598, 599\\n    :raises: 500, 401, 403, 404, 405, 406, 407,',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': '    def load(self) -> List[Document]:\\n        from playwright.sync_api import sync_playwright\\n\\n        docs: List[Document] = list()\\n\\n        with sync_playwright() as p:\\n            browser = p.chromium.launch(headless=self.headless)\\n            for url in self.urls:\\n                try:\\n                    page = browser.new_page()\\n                    response = page.goto(url)\\n                    text = self.evaluator.evaluate(page, browser, response)\\n                    metadata = {\"source\": url}\\n                    docs.append(Document(page_content=text, metadata=metadata))\\n                except Exception as e:\\n                    if self.continue_on_failure:\\n                        logger.error(\\n                            f\"Error fetching or processing {url}, exception: {e}\"\\n                        )\\n                    else:\\n                        raise e\\n            browser.close()\\n        return docs',\n",
       "   'd': 'Load the specified URLs using Playwright and create Document instances.\\n\\nReturns:\\n    List[Document]: A list of Document instances with loaded content.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        web_path: str,\\n        filter_urls: Optional[List[str]] = None,\\n        parsing_function: Optional[Callable] = None,\\n        blocksize: Optional[int] = None,\\n        blocknum: int = 0,\\n        meta_function: Optional[Callable] = None,\\n        is_local: bool = False,\\n        continue_on_failure: bool = False,\\n    ):\\n        if blocksize is not None and blocksize < 1:\\n            raise ValueError(\"Sitemap blocksize should be at least 1\")\\n\\n        if blocknum < 0:\\n            raise ValueError(\"Sitemap blocknum can not be lower then 0\")\\n\\n        try:\\n            import lxml\\n        except ImportError:\\n            raise ImportError(\\n                \"lxml package not found, please install it with \" \"`pip install lxml`\"\\n            )\\n\\n        super().__init__(web_path)\\n\\n        self.filter_urls = filter_urls\\n        self.parsing_function = parsing_function or _default_parsing_function\\n        self.meta_function = meta_function or _default_meta_function\\n        self.blocksize = blocksize\\n        self.blocknum = blocknum\\n        self.is_local = is_local\\n        self.continue_on_failure = continue_on_failure',\n",
       "   'd': 'Initialize with webpage path and optional filter URLs.\\n\\nArgs:\\n    web_path: url of the sitemap. can also be a local path\\n    filter_urls: list of strings or regexes that will be applied to filter the\\n        urls that are parsed and loaded\\n    parsing_function: Function to parse bs4.Soup output\\n    blocksize: number of sitemap locations per block\\n    blocknum: the number of the block that should be loaded - zero indexed.\\n        Default: 0\\n    meta_function: Function to parse bs4.Soup output for metadata\\n        remember when setting this method to also copy metadata[\"loc\"]\\n        to metadata[\"source\"] if you are using this field\\n    is_local: whether the sitemap is a local file. Default: False\\n    continue_on_failure: whether to continue loading the sitemap if an error\\n        occurs loading a url, emitting a warning instead of raising an\\n        exception. Setting this to True makes the loader more robust, but also\\n        may result in missing data. Default: False',\n",
       "   'l': True,\n",
       "   'g': ['Create a new game with the number to guess.\\n\\n  Args:\\n      number_to_guess (int): The number to guess.',\n",
       "    '',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': 'def _get_docs(\\n    self,\\n    question: str,\\n    *,\\n    run_manager: CallbackManagerForChainRun,\\n) -> List[Document]:\\n    return self.retriever.get_relevant_documents(\\n        question, callbacks=run_manager.get_child()\\n    )',\n",
       "   'd': 'Return values of the agent.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '_pyro_param(self, msg) -> ret', '']},\n",
       "  {'c': 'def _run(\\n    self,\\n    query: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> str:\\n    return self.llm_chain.predict(query=query, dialect=self.db.dialect)',\n",
       "   'd': 'Process the media type of the request body.',\n",
       "   'l': False,\n",
       "   'g': ['(Constructor)',\n",
       "    '_build: build the model.\\n\\n    :param blocks_args: a list of blocks args.\\n    :param global_params: a global params.\\n    :return:',\n",
       "    '(constructor)\\n    Parameters\\n    ----------\\n    blocks_args: list of dict\\n        List of dictionary containing parameters for each block.\\n    global_params: dict\\n        Global parameters for the model.',\n",
       "    '(blocks_args, global_params) -> Model:\\n\\n    :param blocks_args: list of dictionaries, one dictionary per block\\n    :param global_params: global parameters for all blocks, e.g. number of\\n        layers in the entire model.']},\n",
       "  {'c': 'def load(self) -> List[Document]:\\n    loader = UnstructuredPDFLoader(str(self.file_path))\\n    return loader.load()',\n",
       "   'd': 'Load documents.',\n",
       "   'l': True,\n",
       "   'g': ['Find all snippet files in a given directory.\\n\\n  :param ft:  The snippet type.\\n  :param directory:  The snippet directory.\\n\\n  :return:  A set of snippet files.',\n",
       "    'Find all snippet files in a given directory.\\n\\n  :param ft: The name of the file type to find snippets for.\\n  :param directory: The directory to find snippet files in.\\n\\n  :return: A set of snippet files.',\n",
       "    ':param ft:  the name of the file to search for\\n  :param directory:  the directory to search in\\n  :return:  the list of files that match the name and the directory',\n",
       "    '']},\n",
       "  {'c': 'def __new__(cls, **data: Any) -> Union[OpenAIChat, BaseOpenAI]:\\n    model_name = data.get(\"model_name\", \"\")\\n    if model_name.startswith(\"gpt-3.5-turbo\") or model_name.startswith(\"gpt-4\"):\\n        warnings.warn(\\n            \"You are trying to use a chat model. This way of initializing it is \"\\n            \"no longer supported. Instead, please use: \"\\n            \"`from langchain.chat_models import ChatOpenAI`\"\\n        )\\n        return OpenAIChat(**data)\\n    return super().__new__(cls)',\n",
       "   'd': 'PUT the URL and return the text.',\n",
       "   'l': False,\n",
       "   'g': ['_set_validation(self, batch_size, X_val, Y_val, trigger, val_method=None):',\n",
       "    '_set_validation',\n",
       "    '_set_validation(self, batch_size, X_val, Y_val, trigger, val_method=None):\\n    \\t\\n    \\tSet validation',\n",
       "    '_set_validation']},\n",
       "  {'c': 'def clear(self, **kwargs: Any) -> None:\\n    asynchronous = kwargs.get(\"asynchronous\", False)\\n    self.redis.flushdb(asynchronous=asynchronous, **kwargs)',\n",
       "   'd': 'Input variables for this prompt template.\\n\\nReturns:\\n    List of input variables.',\n",
       "   'l': False,\n",
       "   'g': ['_to_json_string() - Returns the string representation of the object\\n    in JSON format.\\n\\n    :return: The string representation of the object in JSON format.',\n",
       "    '_to_json_string:\\n    Return a JSON string of the configuration.\\n    :return: a JSON string of the configuration.\\n    :rtype: str\\n    :raises',\n",
       "    '_internal_dict is a dictionary of all the configuration options for this config object.\\n_internal_dict is a dictionary of all the configuration options for this config object.\\n_internal_dict is a dictionary of all the configuration options for this config object.\\n_internal_dict is a dictionary of all the configuration options for this config object.\\n_internal_dict is a dictionary of all the configuration options for this config object.\\n_internal_dict is a dictionary of all the configuration options for this config object.\\n_internal_dict is a dictionary of all the configuration options for this config object.\\n_internal_dict is a dictionary of all the configuration options for this config object.\\n_internal_dict is a dictionary of all the configuration options for this config object.\\n_internal_dict is a dictionary of all the configuration options for this config object.\\n_internal_dict is a dictionary of all the configuration options for this config object.\\n_internal_dict is a dictionary of all the configuration options for this config object.\\n_internal_dict is a dictionary of all the configuration options for this config object.\\n_internal_dict is a dictionary of all the configuration options for this config object.\\n_internal_dict',\n",
       "    '_to_json_string()']},\n",
       "  {'c': 'def split_text(self, text: str) -> List[str]:\\n    separator = (\\n        self._separator if self._is_separator_regex else re.escape(self._separator)\\n    )\\n    splits = _split_text_with_regex(text, separator, self._keep_separator)\\n    _separator = \"\" if self._keep_separator else self._separator\\n    return self._merge_splits(splits, _separator)',\n",
       "   'd': 'Split incoming text and return chunks.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    ':param item_type: The type of each item in the tuple.\\n  :param iterable: The iterable to be converted into a tuple.\\n  :param _tuple_cache: The cache to be used by the compiler.\\n  :return: The tuple.\\n  :rtype: :class:`tuple`.\\n  :raises: :class:`TypeError` if the item type is not a valid type.\\n  :raises: :class:`TypeError` if the iterable is not a valid iterable.\\n  :raises: :class:`TypeError` if the iterable is not a tuple.\\n  :raises: :class:`TypeError` if the iterable is not a tuple of the same type as the item type.\\n  :raises: :class:`TypeError` if the iterable is not a tuple of the same type as the item type.\\n  :raises: :class:`TypeError` if the iterable is not a tuple of the same type as the item type.\\n  :raises: :class:`TypeError` if the iterable is not a tuple of the same type as the item type.\\n  :raises: :class:`TypeError` if the iterable is not a tuple of the same type as the item type.',\n",
       "    '',\n",
       "    ':param item_type: The type of the elements in the iterable.\\n  :param iterable: The iterable to be converted to a typed tuple.\\n  :param _tuple_cache: The cache of compiled typed tuples.\\n  :return: A typed tuple.\\n  :raises TypeError: If the item_type is not a type.']},\n",
       "  {'c': 'def get_extraction_chain(\\n    allowed_nodes: Optional[List[str]] = None, allowed_rels: Optional[List[str]] = None\\n):\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                f\"\"\"\\n\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\n- **Nodes** represent entities and concepts. They\\'re akin to Wikipedia nodes.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\\n\\n- **Consistency**: Ensure you use basic or elementary types for node labels.\\n  - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\\n- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n{\\'- **Allowed Node Labels:**\\' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\\n{\\'- **Allowed Relationship Types**:\\' + \", \".join(allowed_rels) if allowed_rels else \"\"}\\n\\n- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\\n- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\\n- **Property Format**: Properties must be in a key-value format.\\n- **Quotation Marks**: Never use escaped single or double quotes within property values.\\n- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\\n\\n- **Maintain Entity Consistency**: When extracting entities, it\\'s vital to ensure consistency.\\nIf an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),\\nalways use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n\\nAdhere to the rules strictly. Non-compliance will result in termination.\\n          \"\"\",\\n            ),\\n            (\\n                \"human\",\\n                \"Use the given format to extract information from the following input: {input}\",\\n            ),\\n            (\"human\", \"Tip: Make sure to answer in the correct format\"),\\n        ]\\n    )\\n    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)',\n",
       "   'd': 'Get an OpenAPI spec from a text.',\n",
       "   'l': False,\n",
       "   'g': ['Test that the hosts decorator overrides the env_hosts.',\n",
       "    'Test that the hosts decorator overrides the env_hosts.',\n",
       "    '',\n",
       "    ':param str hosts:']},\n",
       "  {'c': 'def _run(\\n    self, text: str, run_manager: Optional[CallbackManagerForToolRun] = None\\n) -> str:\\n    try:\\n        data = _parse_input(text)\\n        return self.requests_wrapper.patch(_clean_url(data[\"url\"]), data[\"data\"])\\n    except Exception as e:\\n        return repr(e)',\n",
       "   'd': 'Run more texts through the embeddings and add to the vectorstore.\\n\\nArgs:\\n    texts: Iterable of strings to add to the vectorstore.\\n    metadatas: Optional list of metadatas associated with the texts.\\n    ids: Optional list of ids to associate with the texts.\\n    bulk_size: Bulk API request count; Default: 500\\n\\nReturns:\\n    List of ids from adding the texts into the vectorstore.\\n\\nOptional Args:\\n    vector_field: Document field embeddings are stored in. Defaults to\\n    \"vector_field\".\\n\\n    text_field: Document field the text of the document is stored in. Defaults\\n    to \"text\".',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def step(\\n    self, inputs: dict, callbacks: Callbacks = None, **kwargs: Any\\n) -> StepResponse:\\n',\n",
       "   'd': \"Load QA Eval Chain from LLM.\\n\\nArgs:\\n    llm (BaseLanguageModel): the base language model to use.\\n\\n    prompt (PromptTemplate): A prompt template containing the input_variables:\\n    'query', 'context' and 'result' that will be used as the prompt\\n    for evaluation.\\n    Defaults to PROMPT.\\n\\n    **kwargs: additional keyword arguments.\\n\\nReturns:\\n    ContextQAEvalChain: the loaded QA eval chain.\",\n",
       "   'l': False,\n",
       "   'g': ['(str)\\n    Command to run after the post release.\\n    :param str command:\\n        The command to run after the post release.\\n        If the command is empty, the post release will not be executed.\\n        The command will be executed in the waller environment.\\n        The command will be executed in the waller environment.\\n        The command will be executed in the waller environment.\\n        The command will be executed in the waller environment.\\n        The command will be executed in the waller environment.\\n        The command will be executed in the waller environment.\\n        The command will be executed in the waller environment.\\n        The command will be executed in the waller environment.\\n        The command will be executed in the waller environment.\\n        The command will be executed in the waller environment.\\n        The command will be executed in the waller environment.\\n        The command will be executed in the waller environment.\\n        The command will be executed in the waller environment.\\n        The command will be executed in the waller environment.\\n        The command will be executed in the waller environment.\\n        The command will be executed in the waller environment.',\n",
       "    '(description)\\n\\n    :param self:\\n    :param waller:\\n    :return:\\n    :rtype: None or str',\n",
       "    '(str) post_release command.\\n\\n    Returns:\\n        str: post_release command.\\n        None: post_release command is not set.\\n\\n    See Also:\\n        :func:`post_release_service`\\n\\n    :param str command: post_release command.\\n        None: post_release command is not set.\\n        :func:`post_release_service`\\n\\n    :rtype: str\\n        post_release command.\\n        None: post_release command is not set.\\n        :func:`post_release_service`\\n\\n    :raises: :class:`~wouter.errors.WouterError`\\n        - when post_release command is not set.\\n        - when post_release command is not valid.\\n        - when post_release command is not executable.\\n        - when post_release command is not found.\\n        - when post_release command is not found in the current environment.\\n        - when post_release command is not found in the current environment.\\n        - when post_release command is not found in the current environment.\\n        - when post_release command is not found in the current environment.\\n        - when post_',\n",
       "    '_post_release']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        doc_metadata: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        doc_hash = md5()\\n        for t in texts:\\n            doc_hash.update(t.encode())\\n        doc_id = doc_hash.hexdigest()\\n        if metadatas is None:\\n            metadatas = [{} for _ in texts]\\n        if doc_metadata:\\n            doc_metadata[\"source\"] = \"langchain\"\\n        else:\\n            doc_metadata = {\"source\": \"langchain\"}\\n        doc = {\\n            \"document_id\": doc_id,\\n            \"metadataJson\": json.dumps(doc_metadata),\\n            \"section\": [\\n                {\"text\": text, \"metadataJson\": json.dumps(md)}\\n                for text, md in zip(texts, metadatas)\\n            ],\\n        }\\n\\n        success_str = self._index_doc(doc)\\n        if success_str == \"E_ALREADY_EXISTS\":\\n            self._delete_doc(doc_id)\\n            self._index_doc(doc)\\n        elif success_str == \"E_NO_PERMISSIONS\":\\n            print(\\n                \"\"\"No permissions to add document to Vectara.\\n                Check your corpus ID, customer ID and API key\"\"\"\\n            )\\n        return [doc_id]',\n",
       "   'd': 'Load the specified URLs using Playwright and create Document instances.\\n\\nReturns:\\n    List[Document]: A list of Document instances with loaded content.',\n",
       "   'l': False,\n",
       "   'g': [':rtype: list of :class:`~django.db.models.Field`',\n",
       "    ':type self: object\\n    :rtype: list',\n",
       "    ':type self: Model\\n  :rtype: list\\n  :return: list of fields to be displayed in the form',\n",
       "    ':return: list of all fields that are defined in the class.']},\n",
       "  {'c': '    def embed_query(self, text: str) -> List[float]:\\n        text = text.replace(\"\\\\n\", \" \")\\n        embedding = self.client.encode(\\n            self.query_instruction + text, **self.encode_kwargs\\n        )\\n        return embedding.tolist()',\n",
       "   'd': 'Compute query embeddings using a HuggingFace transformer model.\\n\\nArgs:\\n    text: The text to embed.\\n\\nReturns:\\n    Embeddings for the text.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def output_keys(self) -> List[str]:\\n        return [self.output_key]',\n",
       "   'd': 'Parse a single string model output into some structure.\\n\\nArgs:\\n    text: String output of a language model.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': False,\n",
       "   'g': ['Search for the given term in the index.\\n\\n  :param cls: the class to search in.\\n  :param term: the term to search for.\\n  :param weights: a list of weights for each field in the index.\\n  :param with_score: if True, return the score of the match.\\n  :param score_alias: the alias for the score field.\\n  :param explicit_ordering: if True, return the score and the ordering\\n    of the match.\\n\\n  :return: a list of the matches, with the score of the match and\\n    the ordering of the match if the score is requested.',\n",
       "    '',\n",
       "    '',\n",
       "    'Returns a list of documents and their scores for a given term.\\n\\n    :param cls: The class to use for the search.\\n    :param term: The term to search for.\\n    :param weights: A list of weights to use for each document.\\n    :param with_score: Whether to include the score in the result.\\n    :param score_alias: The alias to use for the score.\\n    :param explicit_ordering: Whether to use the explicit ordering.\\n\\n    :return: A list of documents and their scores.\\n    :rtype: list of (int, float)']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        path: Union[str, Path],\\n        *,\\n        glob: str = \"**/[!.]*\",\\n        exclude: Sequence[str] = (),\\n        suffixes: Optional[Sequence[str]] = None,\\n        show_progress: bool = False,\\n    ) -> None:\\n        if isinstance(path, Path):\\n            _path = path\\n        elif isinstance(path, str):\\n            _path = Path(path)\\n        else:\\n            raise TypeError(f\"Expected str or Path, got {type(path)}\")\\n\\n        self.path = _path.expanduser()\\n        self.glob = glob\\n        self.suffixes = set(suffixes or [])\\n        self.show_progress = show_progress\\n        self.exclude = exclude',\n",
       "   'd': 'Initialize with a path to directory and how to glob over it.\\n\\nArgs:\\n    path: Path to directory to load from\\n    glob: Glob pattern relative to the specified path\\n          by default set to pick up all non-hidden files\\n    exclude: patterns to exclude from results, use glob syntax\\n    suffixes: Provide to keep only files with these suffixes\\n              Useful when wanting to keep files with different suffixes\\n              Suffixes must include the dot, e.g. \".txt\"\\n    show_progress: If true, will show a progress bar as the files are loaded.\\n                   This forces an iteration through all matching files\\n                   to count them prior to loading them.\\n\\nExamples:\\n\\n    .. code-block:: python\\n\\n        # Recursively load all text files in a directory.\\n        loader = FileSystemBlobLoader(\"/path/to/directory\", glob=\"**/*.txt\")\\n\\n        # Recursively load all non-hidden files in a directory.\\n        loader = FileSystemBlobLoader(\"/path/to/directory\", glob=\"**/[!.]*\")\\n\\n        # Load all files in a directory without recursion.\\n        loader = FileSystemBlobLoader(\"/path/to/directory\", glob=\"*\")\\n\\n        # Recursively load all files in a directory, except for py or pyc files.\\n        loader = FileSystemBlobLoader(\\n            \"/path/to/directory\",\\n            glob=\"**/*.txt\",\\n            exclude=[\"**/*.py\", \"**/*.pyc\"]\\n        )',\n",
       "   'l': True,\n",
       "   'g': ['Return a dictionary of the current counter.\\n\\n  :param get_value:  A function to get the value of a counter.  Defaults to None.\\n  :return:  A dictionary of the current counter.  The values are either the values of the\\n            counters or the values of the functions.  Defaults to None.\\n  :rtype:  dict\\n  :Example:  ```\\n            >>> counter = Counter(1, 2, 3)\\n            >>> counter.to_dict()\\n            {1: 1, 2: 2, 3: 3}\\n            >>> counter = Counter(1, 2, 3)\\n            >>> counter.to_dict(get_value=lambda x: x.to_dict())\\n            {1: 1, 2: 2, 3: 3}',\n",
       "    ':param get_value: the key to get the value from the BaseCounter\\n    :return: a dictionary of the values of the counters.',\n",
       "    \":return: a dict of the current counter's values.\\n      :rtype: dict\",\n",
       "    'Convert a Counter to a dictionary.\\n\\n  :param get_value: a function that returns a value from a counter.\\n  :type get_value: function\\n  :return: a dictionary of the counter.\\n  :rtype: dict\\n  :raises: :class:`~pylons.decorators.decorators.counter.CounterError`\\n  :raises: :class:`~pylons.decorators.decorators.counter.CounterError`\\n  :raises: :class:`~pylons.decorators.decorators.counter.CounterError`\\n  :raises: :class:`~pylons.decorators.decorators.counter.CounterError`\\n  :raises: :class:`~pylons.decorators.decorators.counter.CounterError`\\n  :raises: :class:`~pylons.decorators.decorators.counter.CounterError`\\n  :raises: :class:`~pylons.decorators.decorators.counter.CounterError`\\n  :raises: :class:`~pylons.decorators.decorators.counter.CounterError`\\n  :raises: :class:`~pylons.decorators.decorators.counter.CounterError`\\n  :raises: :class:`~pylons.decorators.decorators']},\n",
       "  {'c': 'def split_text(self, text: str) -> List[str]:\\n    splits = (s.text for s in self._tokenizer(text).sents)\\n    return self._merge_splits(splits, self._separator)',\n",
       "   'd': 'Call the textgen web API and return the output.\\n\\nArgs:\\n    prompt: The prompt to use for generation.\\n    stop: A list of strings to stop generation when encountered.\\n\\nReturns:\\n    The generated text.\\n\\nExample:\\n    .. code-block:: python\\n\\n        from langchain.llms import TextGen\\n        llm = TextGen(model_url=\"http://localhost:5000\")\\n        llm(\"Write a story about llamas.\")',\n",
       "   'l': False,\n",
       "   'g': ['(cls, id_, name, sizes, min_size, max_size, default_size)',\n",
       "    '(cls, id_, name, sizes, default_size, default_count, default_value)',\n",
       "    '(cls, id_, name, sizes, parent, children, children_map)',\n",
       "    '(cls, id_, name, sizes, default_size, default_value, default_value_type)']},\n",
       "  {'c': '    def from_template(\\n        cls,\\n        template: str,\\n        *,\\n        template_format: str = \"f-string\",\\n        partial_variables: Optional[Dict[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> PromptTemplate:\\n        if template_format == \"jinja2\":\\n\\n            input_variables = _get_jinja2_variables_from_template(template)\\n        elif template_format == \"f-string\":\\n            input_variables = {\\n                v for _, v, _, _ in Formatter().parse(template) if v is not None\\n            }\\n        else:\\n            raise ValueError(f\"Unsupported template format: {template_format}\")\\n\\n        _partial_variables = partial_variables or {}\\n\\n        if _partial_variables:\\n            input_variables = {\\n                var for var in input_variables if var not in _partial_variables\\n            }\\n\\n        return cls(\\n            input_variables=sorted(input_variables),\\n            template=template,\\n            template_format=template_format,\\n            partial_variables=_partial_variables,\\n            **kwargs,\\n        )',\n",
       "   'd': 'Given input, decided what to do.\\n\\nArgs:\\n    intermediate_steps: Steps the LLM has taken to date,\\n        along with the observations.\\n    callbacks: Callbacks to run.\\n    **kwargs: User inputs.\\n\\nReturns:\\n    Action specifying what tool to use.',\n",
       "   'l': False,\n",
       "   'g': ['', '', ':param ctext:', '']},\n",
       "  {'c': 'def type(self) -> str:\\n',\n",
       "   'd': 'Type of the Message, used for serialization.',\n",
       "   'l': True,\n",
       "   'g': ['(nodes, thunks, pre_call_clear):',\n",
       "    '(nodes, thunks, pre_call_clear):',\n",
       "    '(nodes, thunks, pre_call_clear):',\n",
       "    '(nodes, thunks, pre_call_clear)\\n    :param nodes: list of nodes\\n    :param thunks: list of thunks\\n    :param pre_call_clear: list of clear functions to call before calling the thunk']},\n",
       "  {'c': 'def clear(self, **kwargs: Any) -> None:\\n    with Session(self.engine) as session:\\n        session.query(self.cache_schema).delete()\\n        session.commit()',\n",
       "   'd': 'Add text to the Vectara vectorstore.\\n\\nArgs:\\n    texts (List[str]): The text\\n    metadatas (List[dict]): Metadata dicts, must line up with existing store',\n",
       "   'l': False,\n",
       "   'g': [':param filename:\\n    :param mode:\\n    :return:',\n",
       "    ':param filename:\\n    :param mode:\\n    :return:',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        dataset_name: str,\\n        workspace_name: Optional[str] = None,\\n        api_url: Optional[str] = None,\\n        api_key: Optional[str] = None,\\n    ) -> None:\\n        super().__init__()\\n\\n\\n        try:\\n            import argilla as rg\\n\\n            self.ARGILLA_VERSION = rg.__version__\\n        except ImportError:\\n            raise ImportError(\\n                \"To use the Argilla callback manager you need to have the `argilla` \"\\n                \"Python package installed. Please install it with `pip install argilla`\"\\n            )\\n\\n\\n        if parse(self.ARGILLA_VERSION) < parse(\"1.8.0\"):\\n            raise ImportError(\\n                f\"The installed `argilla` version is {self.ARGILLA_VERSION} but \"\\n                \"`ArgillaCallbackHandler` requires at least version 1.8.0. Please \"\\n                \"upgrade `argilla` with `pip install --upgrade argilla`.\"\\n            )\\n\\n\\n        if api_url is None and os.getenv(\"ARGILLA_API_URL\") is None:\\n            warnings.warn(\\n                (\\n                    \"Since `api_url` is None, and the env var `ARGILLA_API_URL` is not\"\\n                    f\" set, it will default to `{self.DEFAULT_API_URL}`, which is the\"\\n                    \" default API URL in Argilla Quickstart.\"\\n                ),\\n            )\\n            api_url = self.DEFAULT_API_URL\\n\\n        if api_key is None and os.getenv(\"ARGILLA_API_KEY\") is None:\\n            self.DEFAULT_API_KEY = (\\n                \"admin.apikey\"\\n                if parse(self.ARGILLA_VERSION) < parse(\"1.11.0\")\\n                else \"owner.apikey\"\\n            )\\n\\n            warnings.warn(\\n                (\\n                    \"Since `api_key` is None, and the env var `ARGILLA_API_KEY` is not\"\\n                    f\" set, it will default to `{self.DEFAULT_API_KEY}`, which is the\"\\n                    \" default API key in Argilla Quickstart.\"\\n                ),\\n            )\\n            api_url = self.DEFAULT_API_URL\\n\\n\\n        try:\\n            rg.init(api_key=api_key, api_url=api_url)\\n        except Exception as e:\\n            raise ConnectionError(\\n                f\"Could not connect to Argilla with exception: \\'{e}\\'.\\\\n\"\\n                \"Please check your `api_key` and `api_url`, and make sure that \"\\n                \"the Argilla server is up and running. If the problem persists \"\\n                f\"please report it to {self.ISSUES_URL} as an `integration` issue.\"\\n            ) from e\\n\\n\\n        self.dataset_name = dataset_name\\n        self.workspace_name = workspace_name or rg.get_workspace()\\n\\n\\n        try:\\n            extra_args = {}\\n            if parse(self.ARGILLA_VERSION) < parse(\"1.14.0\"):\\n                warnings.warn(\\n                    f\"You have Argilla {self.ARGILLA_VERSION}, but Argilla 1.14.0 or\"\\n                    \" higher is recommended.\",\\n                    UserWarning,\\n                )\\n                extra_args = {\"with_records\": False}\\n            self.dataset = rg.FeedbackDataset.from_argilla(\\n                name=self.dataset_name,\\n                workspace=self.workspace_name,\\n                **extra_args,\\n            )\\n        except Exception as e:\\n            raise FileNotFoundError(\\n                f\"`FeedbackDataset` retrieval from Argilla failed with exception `{e}`.\"\\n                f\"\\\\nPlease check that the dataset with name={self.dataset_name} in the\"\\n                f\" workspace={self.workspace_name} exists in advance. If you need help\"\\n                \" on how to create a `langchain`-compatible `FeedbackDataset` in\"\\n                f\" Argilla, please visit {self.BLOG_URL}. If the problem persists\"\\n                f\" please report it to {self.ISSUES_URL} as an `integration` issue.\"\\n            ) from e\\n\\n        supported_fields = [\"prompt\", \"response\"]\\n        if supported_fields != [field.name for field in self.dataset.fields]:\\n            raise ValueError(\\n                f\"`FeedbackDataset` with name={self.dataset_name} in the workspace=\"\\n                f\"{self.workspace_name} had fields that are not supported yet for the\"\\n                f\"`langchain` integration. Supported fields are: {supported_fields},\"\\n                f\" and the current `FeedbackDataset` fields are {[field.name for field in self.dataset.fields]}.\"\\n                \" For more information on how to create a `langchain`-compatible\"\\n                f\" `FeedbackDataset` in Argilla, please visit {self.BLOG_URL}.\"\\n            )\\n\\n        self.prompts: Dict[str, List[str]] = {}\\n\\n        warnings.warn(\\n            (\\n                \"The `ArgillaCallbackHandler` is currently in beta and is subject to\"\\n                \" change based on updates to `langchain`. Please report any issues to\"\\n                f\" {self.ISSUES_URL} as an `integration` issue.\"\\n            ),\\n        )',\n",
       "   'd': 'Split markdown file\\nArgs:\\n    text: Markdown file',\n",
       "   'l': False,\n",
       "   'g': ['2010-07-12 18:47:15.988342000 +0000 -07:00 \\n    \\n    :param loops: \\n    :return:',\n",
       "    '2111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111',\n",
       "    '2117708148076857758362089240872387245711970733268682457289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289716289',\n",
       "    '21ff0b  Netscape2.0  loops=2^16-1']},\n",
       "  {'c': '    def get_child(self, tag: Optional[str] = None) -> AsyncCallbackManager:\\n        manager = AsyncCallbackManager(handlers=[], parent_run_id=self.run_id)\\n        manager.set_handlers(self.inheritable_handlers)\\n        manager.add_tags(self.inheritable_tags)\\n        manager.add_metadata(self.inheritable_metadata)\\n        if tag is not None:\\n            manager.add_tags([tag], False)\\n        return manager',\n",
       "   'd': 'Parse a list of candidate model Generations into a specific format.\\n\\nArgs:\\n    result: A list of Generations to be parsed. The Generations are assumed\\n        to be different candidate outputs for a single model input.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': False,\n",
       "   'g': ['_loglikeobs(self, params):\\n    Return the log-likelihood of the given parameters.',\n",
       "    '_loglikeobs(self, params):\\n    Returns the log-likelihood of the data given the parameters.',\n",
       "    '_loglikeobs(self, params):',\n",
       "    '(']},\n",
       "  {'c': 'def test_faiss_add_texts_not_supported() -> None:\\n    docsearch = FAISS(FakeEmbeddings().embed_query, None, FakeDocstore(), {})\\n    with pytest.raises(ValueError):\\n        docsearch.add_texts([\"foo\"])',\n",
       "   'd': 'Load documents.',\n",
       "   'l': False,\n",
       "   'g': ['_summary_',\n",
       "    '_summary_',\n",
       "    '(int):\\n    The number of connected components in a graph.',\n",
       "    '_summary_']},\n",
       "  {'c': '    def format_messages(self, **kwargs: Any) -> List[BaseMessage]:\\n        value = kwargs[self.variable_name]\\n        if not isinstance(value, list):\\n            raise ValueError(\\n                f\"variable {self.variable_name} should be a list of base messages, \"\\n                f\"got {value}\"\\n            )\\n        for v in value:\\n            if not isinstance(v, BaseMessage):\\n                raise ValueError(\\n                    f\"variable {self.variable_name} should be a list of base messages,\"\\n                    f\" got {value}\"\\n                )\\n        return value',\n",
       "   'd': 'Format messages from kwargs.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    List of BaseMessage.',\n",
       "   'l': True,\n",
       "   'g': ['Choose the best item from the list of items.\\n    :param session: the session to log the choice in.\\n    :return: the item that was chosen.',\n",
       "    ':param session: The session that is currently active.\\n    :return: The choice that was chosen by the user.',\n",
       "    '',\n",
       "    'Choose an item from the list of items.\\n    :param session: The session that the item is being chosen for.\\n    :return: The item that was chosen.']},\n",
       "  {'c': '    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        import sentence_transformers\\n\\n        texts = list(map(lambda x: x.replace(\"\\\\n\", \" \"), texts))\\n        if self.multi_process:\\n            pool = self.client.start_multi_process_pool()\\n            embeddings = self.client.encode_multi_process(texts, pool)\\n            sentence_transformers.SentenceTransformer.stop_multi_process_pool(pool)\\n        else:\\n            embeddings = self.client.encode(texts, **self.encode_kwargs)\\n\\n        return embeddings.tolist()',\n",
       "   'd': 'Compute doc embeddings using a HuggingFace transformer model.\\n\\nArgs:\\n    texts: The list of texts to embed.\\n\\nReturns:\\n    List of embeddings, one for each text.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', ':param loader:\\n    :param model:\\n    :return:']},\n",
       "  {'c': 'def args(self) -> dict:\\n    return self.args_schema.schema()[\"properties\"]',\n",
       "   'd': 'Return the output parser type for serialization.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    ':param topic: \\n    :param request_options:',\n",
       "    ':param topic: \\n    :param request_options:',\n",
       "    'Returns a dict with the answer and format for the given page.\\n\\n  :param topic: The page to get the answer for.\\n  :param request_options: A dictionary of options to pass to the adapter.\\n\\n  :return: A dict with the answer and format for the given page.']},\n",
       "  {'c': '    def clear(self, **kwargs: Any) -> None:\\n        from gptcache import Cache\\n\\n        for gptcache_instance in self.gptcache_dict.values():\\n            gptcache_instance = cast(Cache, gptcache_instance)\\n            gptcache_instance.flush()\\n\\n        self.gptcache_dict.clear()',\n",
       "   'd': 'Clear cache.',\n",
       "   'l': True,\n",
       "   'g': ['(dataloader, current_iter, tb_logger, save_img) -> None',\n",
       "    '(dataloader, current_iter, tb_logger, save_img) -> None',\n",
       "    '_validation(self, dataloader, current_iter, tb_logger, save_img=False):\\n\\n    _validation_nondist(self, dataloader, current_iter, tb_logger,\\n                        save_img)\\n\\n_validation_nondist(self, dataloader, current_iter, tb_logger,\\n                    save_img)\\n\\n    _validation_dist(self, dataloader, current_iter, tb_logger,\\n                    save_img)\\n\\n_validation_dist(self, dataloader, current_iter, tb_logger,\\n                    save_img)',\n",
       "    '(dataloader, current_iter, tb_logger, save_img) -> None:\\n\\n    :param dataloader:\\n    :param current_iter:\\n    :param tb_logger:\\n    :param save_img:\\n    :return: None:']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        config: Mapping[str, Any],\\n        stream_name: str,\\n        record_handler: Optional[RecordHandler] = None,\\n        state: Optional[Any] = None,\\n    ) -> None:\\n        source_class = guard_import(\\n            \"source_gong\", pip_name=\"airbyte-source-gong\"\\n        ).SourceGong\\n        super().__init__(\\n            config=config,\\n            source_class=source_class,\\n            stream_name=stream_name,\\n            record_handler=record_handler,\\n            state=state,\\n        )',\n",
       "   'd': 'Initializes the loader.\\n\\nArgs:\\n    config: The config to pass to the source connector.\\n    stream_name: The name of the stream to load.\\n    record_handler: A function that takes in a record and an optional id and\\n        returns a Document. If None, the record will be used as the document.\\n        Defaults to None.\\n    state: The state to pass to the source connector. Defaults to None.',\n",
       "   'l': True,\n",
       "   'g': ['_summary_',\n",
       "    '_summary_',\n",
       "    '_Convert raw features to features.\\n    Args:\\n      raw_features: A `tf.train.Example` or `features.FeatureDict`\\n        containing the features to be converted.\\n      random_seed: A `int` used to seed the `np_random` for the\\n        corresponding `raw_features`.\\n    Returns:\\n      A `features.FeatureDict` containing the converted features.',\n",
       "    '- Parameters:\\n      - `raw_features`: The raw features to be converted to features.\\n        - `random_seed`: The random seed used to generate the features.\\n\\n- Returns:\\n  - `features`: The features corresponding to the raw features.']},\n",
       "  {'c': 'def test_warn_deprecated(kwargs: Dict[str, Any], expected_message: str) -> None:\\n    with warnings.catch_warnings(record=True) as warning_list:\\n        warnings.simplefilter(\"always\")\\n\\n        warn_deprecated(**kwargs)\\n\\n        assert len(warning_list) == 1\\n        warning = warning_list[0].message\\n        assert str(warning) == expected_message',\n",
       "   'd': 'Save the agent.\\n\\nArgs:\\n    file_path: Path to file to save the agent to.\\n\\nExample:\\n.. code-block:: python\\n\\n    # If working with agent executor\\n    agent.agent.save(file_path=\"path/agent.yaml\")',\n",
       "   'l': False,\n",
       "   'g': ['Decorator for model registration.\\n\\n  Args:\\n      name (str, optional): The name of the model. If None, the name of the model class will be used.\\n\\n  Returns:\\n      Type: The model class.\\n\\n  Example:\\n      ```\\n      @register(name=\"my_model\")\\n      class MyModel(nn.Module):\\n          ...\\n      ```',\n",
       "    \"Register a model class.\\n\\n  Args:\\n      name (str): The name of the model.\\n\\n  Returns:\\n      Type: The model class.\\n\\n  Examples:\\n      >>> @register('model_name')\\n      ... def model_name(cls, *args, **kwargs):\\n      ...     # do something with the model class\\n      ...     return cls\",\n",
       "    '',\n",
       "    ':param name:']},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'Test that the same padding is used when output_padding is None.',\n",
       "    'Test that same padding without output padding is the same as the same padding with output padding.']},\n",
       "  {'c': '    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        vectors = np.array(\\n            self.embeddings.embed_documents(\\n                [inputs[\"prediction\"], inputs[\"prediction_b\"]]\\n            )\\n        )\\n        score = self._compute_score(vectors)\\n        return {\"score\": score}',\n",
       "   'd': 'The keys to extract from the run.',\n",
       "   'l': False,\n",
       "   'g': ['.encode(x, n_q=None, st=None) -> torch.Tensor:',\n",
       "    '.encode(x, n_q=None, st=0) -> torch.Tensor:',\n",
       "    '.encode(x, n_q=None, st=None) -> torch.Tensor:\\n\\n    Encode x using the same encoding as the model.\\n\\n    Parameters\\n    ----------\\n    x : torch.Tensor\\n        The input tensor.\\n    n_q : int, optional\\n        The number of codes to encode.\\n    st : int, optional\\n        The start index of the codes to encode.\\n\\n    Returns\\n    -------\\n    torch.Tensor\\n        The encoded tensor.',\n",
       "    '.encode(x, n_q=None, st=None) -> torch.Tensor:']},\n",
       "  {'c': '    def load(self, query: str) -> List[Document]:\\n        try:\\n            import fitz\\n        except ImportError:\\n            raise ImportError(\\n                \"PyMuPDF package not found, please install it with \"\\n                \"`pip install pymupdf`\"\\n            )\\n\\n        try:\\n\\n            query = query.replace(\":\", \"\").replace(\"-\", \"\")\\n            if self.is_arxiv_identifier(query):\\n                results = self.arxiv_search(\\n                    id_list=query[: self.ARXIV_MAX_QUERY_LENGTH].split(),\\n                    max_results=self.load_max_docs,\\n                ).results()\\n            else:\\n                results = self.arxiv_search(\\n                    query[: self.ARXIV_MAX_QUERY_LENGTH], max_results=self.load_max_docs\\n                ).results()\\n        except self.arxiv_exceptions as ex:\\n            logger.debug(\"Error on arxiv: %s\", ex)\\n            return []\\n\\n        docs: List[Document] = []\\n        for result in results:\\n            try:\\n                doc_file_name: str = result.download_pdf()\\n                with fitz.open(doc_file_name) as doc_file:\\n                    text: str = \"\".join(page.get_text() for page in doc_file)\\n            except (FileNotFoundError, fitz.fitz.FileDataError) as f_ex:\\n                logger.debug(f_ex)\\n                continue\\n            if self.load_all_available_meta:\\n                extra_metadata = {\\n                    \"entry_id\": result.entry_id,\\n                    \"published_first_time\": str(result.published.date()),\\n                    \"comment\": result.comment,\\n                    \"journal_ref\": result.journal_ref,\\n                    \"doi\": result.doi,\\n                    \"primary_category\": result.primary_category,\\n                    \"categories\": result.categories,\\n                    \"links\": [link.href for link in result.links],\\n                }\\n            else:\\n                extra_metadata = {}\\n            metadata = {\\n                \"Published\": str(result.updated.date()),\\n                \"Title\": result.title,\\n                \"Authors\": \", \".join(a.name for a in result.authors),\\n                \"Summary\": result.summary,\\n                **extra_metadata,\\n            }\\n            doc = Document(\\n                page_content=text[: self.doc_content_chars_max], metadata=metadata\\n            )\\n            docs.append(doc)\\n            os.remove(doc_file_name)\\n        return docs',\n",
       "   'd': 'Run Arxiv search and get the article texts plus the article meta information.\\nSee https://lukasschwab.me/arxiv.py/index.html#Search\\n\\nReturns: a list of documents with the document.page_content in text format\\n\\nPerforms an arxiv search, downloads the top k results as PDFs, loads\\nthem as Documents, and returns them in a List.\\n\\nArgs:\\n    query: a plaintext search query',\n",
       "   'l': True,\n",
       "   'g': ['Get all children of the element.\\n  \\n  :param tag:  The tag name to filter by.\\n  :param recursive:  If True, return all children of all children.\\n  :return:  A list of the children elements.',\n",
       "    'Return a list of the children of the current element.\\n  :param tag: (optional) the tag name of the elements to return\\n  :param recursive: (optional) if True, return all the children of all\\n    the children of the current element\\n  :return: a list of the children of the current element.',\n",
       "    'Return all children of the element.\\n  :param tag: The tag name of the children to return.\\n  :param recursive: Whether to return all children, or only direct children.\\n  :return: The children of the element.',\n",
       "    'Return all children of the element.\\n  :param tag: The tag name of the element to filter.\\n  :param recursive: Whether to include all children of children.\\n  :return: A list of selenium.webdriver.remote.webelement.WebElement.\\n  :rtype: list[selenium.webdriver.remote.webelement.WebElement]\\n  :raises: selenium.common.exceptions.StaleElementReferenceException if the element is no longer present in the DOM.\\n  :raises: selenium.common.exceptions.NoSuchElementException if the element is not found in the DOM.']},\n",
       "  {'c': '    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[PromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> QAEvalChain:\\n        prompt = prompt or PROMPT\\n        expected_input_vars = {\"query\", \"answer\", \"result\"}\\n        if expected_input_vars != set(prompt.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt.input_variables}\"\\n            )\\n        return cls(llm=llm, prompt=prompt, **kwargs)',\n",
       "   'd': 'Constructs a new RocksetChatMessageHistory.\\n\\nArgs:\\n    - session_id: The ID of the chat session\\n    - client: The RocksetClient object to use to query\\n    - collection: The name of the collection to use to store chat\\n                  messages. If a collection with the given name\\n                  does not exist in the workspace, it is created.\\n    - workspace: The workspace containing `collection`. Defaults\\n                 to `\"commons\"`\\n    - messages_key: The DB column containing message history.\\n                    Defaults to `\"messages\"`\\n    - sync: Whether to wait for messages to be added. Defaults\\n            to `False`. NOTE: setting this to `True` will slow\\n            down performance.\\n    - message_uuid_method: The method that generates message IDs.\\n            If set, all messages will have an `id` field within the\\n            `additional_kwargs` property. If this param is not set\\n            and `sync` is `False`, message IDs will not be created.\\n            If this param is not set and `sync` is `True`, the\\n            `uuid.uuid4` method will be used to create message IDs.',\n",
       "   'l': False,\n",
       "   'g': ['(Optional) Use the parameters in the current pipeline.\\n\\n    :param params: The parameters to use in the pipeline.\\n    :type params: dict',\n",
       "    '(see: :class:`~pipelines.Pipeline.use_params`):',\n",
       "    '_use_params(self, params, **cfg):\\n    Use the given params to produce a new context.\\n\\n    :param params: The parameters to use.\\n    :param cfg: The configuration to use.\\n    :return: The new context.\\n    :rtype: Generator[Any, Any, Any]\\n\\n    :raises: :class:`~pipelines.exceptions.PipelineException` if the\\n        parameters are not valid.\\n\\n    :raises: :class:`~pipelines.exceptions.PipelineException` if the\\n        parameters are not valid.\\n\\n    :raises: :class:`~pipelines.exceptions.PipelineException` if the\\n        parameters are not valid.\\n\\n    :raises: :class:`~pipelines.exceptions.PipelineException` if the\\n        parameters are not valid.\\n\\n    :raises: :class:`~pipelines.exceptions.PipelineException` if the\\n        parameters are not valid.\\n\\n    :raises: :class:`~pipelines.exceptions.PipelineException` if the\\n        parameters are not valid.\\n\\n    :raises: :class:`~pipelines.exceptions.PipelineException` if the\\n        parameters are not valid.\\n\\n    :raises: :',\n",
       "    '(param_dict, **cfg) -> (self, param_dict, **cfg)']},\n",
       "  {'c': '    def split_text(self, text: str) -> List[Document]:\\n        lines = text.split(\"\\\\n\")\\n\\n        lines_with_metadata: List[LineType] = []\\n\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n\\n\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            for sep, name in self.headers_to_split_on:\\n\\n                if stripped_line.startswith(sep) and (\\n\\n\\n                    len(stripped_line) == len(sep)\\n                    or stripped_line[len(sep)] == \" \"\\n                ):\\n\\n                    if name is not None:\\n\\n                        current_header_level = sep.count(\"\\n\\n\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n\\n\\n                            popped_header = header_stack.pop()\\n\\n\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])\\n\\n\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n\\n                        initial_metadata[name] = header[\"data\"]\\n\\n\\n\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n\\n\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
       "   'd': 'Return whether the chain requires a reference.\\n\\nReturns:\\n    bool: True if the chain requires a reference, False otherwise.',\n",
       "   'l': False,\n",
       "   'g': ['.  :param int size:  :param str encoding:  :return:  :rtype:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises:  :type:  :raises',\n",
       "    '_init_',\n",
       "    '_init_',\n",
       "    '.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n    ) -> None:\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index',\n",
       "   'd': \"Create a new TextSplitter.\\n\\nArgs:\\n    chunk_size: Maximum size of chunks to return\\n    chunk_overlap: Overlap in characters between chunks\\n    length_function: Function that measures the length of given chunks\\n    keep_separator: Whether to keep the separator in the chunks\\n    add_start_index: If `True`, includes chunk's start index in metadata\",\n",
       "   'l': True,\n",
       "   'g': ['Close the writer.',\n",
       "    '',\n",
       "    'Close the tensorboard writer.',\n",
       "    'Close the writer.\\n\\n    Args:\\n        None\\n\\n    Returns:\\n        None.']},\n",
       "  {'c': '    def similarity_search_with_score(\\n        self,\\n        query: str,\\n        k: int = DEFAULT_TOPN,\\n        text_in_page_content: Optional[str] = None,\\n        meta_filter: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> List[Tuple[Document, float]]:\\n        if self.awadb_client is None:\\n            raise ValueError(\"AwaDB client is None!!!\")\\n\\n        embedding = None\\n        if self.using_table_name in self.table2embeddings:\\n            embedding = self.table2embeddings[self.using_table_name].embed_query(query)\\n        else:\\n            from awadb import AwaEmbedding\\n\\n            embedding = AwaEmbedding().Embedding(query)\\n\\n        results: List[Tuple[Document, float]] = []\\n\\n        not_include_fields: Set[str] = {\"text_embedding\", \"_id\"}\\n        retrieval_docs = self.similarity_search_by_vector(\\n            embedding,\\n            k,\\n            text_in_page_content=text_in_page_content,\\n            meta_filter=meta_filter,\\n            not_include_fields_in_metadata=not_include_fields,\\n        )\\n\\n        for doc in retrieval_docs:\\n            score = doc.metadata[\"score\"]\\n            del doc.metadata[\"score\"]\\n            doc_tuple = (doc, score)\\n            results.append(doc_tuple)\\n\\n        return results',\n",
       "   'd': 'Extract images from page and get the text with RapidOCR.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    ':param model:\\n        :return:\\n        :rtype: int',\n",
       "    'Returns the token limit for a given model.\\n    \\n    Args:\\n        model (str): The model to get the token limit for.\\n\\n    Returns:\\n        int: The token limit for the model.',\n",
       "    ':param model:\\n    :return:\\n    :']},\n",
       "  {'c': 'def _llm_type(self) -> str:\\n    return \"azure\"',\n",
       "   'd': 'Update based on prompt and llm_string.',\n",
       "   'l': False,\n",
       "   'g': ['_get_param_groups()',\n",
       "    '(str, List[Parameter]) -> Dict[str, List[Parameter]',\n",
       "    '_get_param_groups() - returns a dictionary of all the parameters that are used by the model and the datamanager.\\n        - datamanager_params - a dictionary of all the parameters that are used by the datamanager.\\n        - model_params - a dictionary of all the parameters that are used by the model.\\n\\n:return: a dictionary of all the parameters that are used by the model and the datamanager.\\n:rtype: Dict[str, List[Parameter]',\n",
       "    '(str, List[Parameter]) -> Dict[str, List[Parameter]']},\n",
       "  {'c': 'def template_is_valid(cls, values: Dict) -> Dict:\\n    if values[\"validate_template\"]:\\n        check_valid_template(\\n            values[\"prefix\"] + values[\"suffix\"],\\n            values[\"template_format\"],\\n            values[\"input_variables\"] + list(values[\"partial_variables\"]),\\n        )\\n    elif values.get(\"template_format\"):\\n        values[\"input_variables\"] = [\\n            var\\n            for var in get_template_variables(\\n                values[\"prefix\"] + values[\"suffix\"], values[\"template_format\"]\\n            )\\n            if var not in values[\"partial_variables\"]\\n        ]\\n    return values',\n",
       "   'd': 'Run on new LLM token. Only available when streaming is enabled.',\n",
       "   'l': False,\n",
       "   'g': ['(bool)',\n",
       "    '(self, leaf=False):',\n",
       "    '- Returns True if the node is a leaf.\\n     *\\n     * @return {Boolean}',\n",
       "    '(bool)\\n    Return True if the node has no children, False otherwise.\\n\\n    Returns:\\n        bool: True if the node has no children, False otherwise.']},\n",
       "  {'c': 'def on_tool_error(\\n    self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\\n) -> None:\\n    self.step += 1\\n    self.errors += 1',\n",
       "   'd': 'Clear session memory from DynamoDB',\n",
       "   'l': False,\n",
       "   'g': [':param number: The number of the nth James number to calculate.\\n    :type number: int\\n    :return: The number of the nth John number.\\n    :rtype: int\\n    :raises ValueError: If the input value of [n] is not an integer or if the input value of [n] is < 1.\\n    :raises ValueError: If the input value of [n] is 1.\\n    :raises ValueError: If the input value of [n] is 2.\\n    :raises ValueError: If the input value of [n] is 3.\\n    :raises ValueError: If the input value of [n] is 4.\\n    :raises ValueError: If the input value of [n] is 5.\\n    :raises ValueError: If the input value of [n] is 6.\\n    :raises ValueError: If the input value of [n] is 7.\\n    :raises ValueError: If the input value of [n] is 8.\\n    :raises ValueError: If the input value of [n] is 9.\\n    :raises ValueError: If the input value of [n] is 10.',\n",
       "    ':param number: the number of syllables to return\\n    :return: the number of syllables in the given number of syllables\\n    :raises ValueError: if the input value is not an integer or less than 1\\n    :raises ValueError: if the input value is 1\\n    :raises ValueError: if the input value is 2 or less',\n",
       "    'Returns the number of Syllables in a given number of words.\\n\\n    Parameters\\n    ----------\\n    number : int\\n        Number of words in a sentence.\\n\\n    Returns\\n    -------\\n    int\\n        Number of Syllables in a given number of words.',\n",
       "    'Parameters\\n    ----------\\n    number : int\\n        The number of terms in the sequence.\\n\\n    Returns\\n    -------\\n    int\\n        The number of terms in the sequence.']},\n",
       "  {'c': '    def input_keys(self) -> List[str]:\\n        return [self.input_docs_key, self.question_key]',\n",
       "   'd': 'Create a new TextSplitter.',\n",
       "   'l': False,\n",
       "   'g': ['context_str = self._generate_context(message)\\n        return self._llm_chain.run(\\n            context=context_str,\\n            input=message,\\n        )',\n",
       "    '(',\n",
       "    '_generate_context(self, message: str) -> str:',\n",
       "    '_generate_context(self, message: str) -> str:\\n\\n    Generate a context for a message.\\n\\n    Args:\\n        message (str): The message to generate a context for.\\n\\n    Returns:\\n        str: The context for the message.']},\n",
       "  {'c': 'def requires_input(self) -> bool:\\n    return False',\n",
       "   'd': 'Whether this evaluator requires an input string.',\n",
       "   'l': True,\n",
       "   'g': ['_expert_devices: list of devices to send expert data to\\n  return: list of lists of expert data to send to each device',\n",
       "    '.ExpertToGates() -> tf.Tensor\\n\\n    Returns a tensor of expert-to-gate functions for each of the\\n    self._num_experts experts.',\n",
       "    '',\n",
       "    '_expert_devices: list of devices to use for expert']},\n",
       "  {'c': 'def __init__(\\n    self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n) -> None:\\n    super().__init__(**kwargs)\\n    self._separator = separator\\n    self._is_separator_regex = is_separator_regex',\n",
       "   'd': 'Create a new TextSplitter.',\n",
       "   'l': True,\n",
       "   'g': ['run the test', 'run', 'run the test', 'run the test']},\n",
       "  {'c': '    def _run(\\n        self,\\n        *args: Any,\\n        **kwargs: Any,\\n    ) -> Any:\\n',\n",
       "   'd': 'Validate environment.\\nValidate that an Apify API token is set and the apify-client\\nPython package exists in the current environment.',\n",
       "   'l': False,\n",
       "   'g': [':param ctx: :class:`click.Context`\\n  :param args: list of arguments to be parsed\\n\\n  :return: list of parsed arguments\\n  :rtype: list\\n  :raises: :class:`click.',\n",
       "    'Parse the arguments.\\n\\n  :param ctx: The context.\\n  :param args: The arguments to parse.',\n",
       "    'Parse the arguments.\\n\\n  :param ctx: The current context.\\n  :param args: The arguments to parse.',\n",
       "    '']},\n",
       "  {'c': '    def messages(self) -> List[BaseMessage]:\\n        from botocore.exceptions import ClientError\\n\\n        response = None\\n        try:\\n            response = self.table.get_item(Key={\"SessionId\": self.session_id})\\n        except ClientError as error:\\n            if error.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":\\n                logger.warning(\"No record found with session id: %s\", self.session_id)\\n            else:\\n                logger.error(error)\\n\\n        if response and \"Item\" in response:\\n            items = response[\"Item\"][\"History\"]\\n        else:\\n            items = []\\n\\n        messages = messages_from_dict(items)\\n        return messages',\n",
       "   'd': 'Retrieve the messages from DynamoDB',\n",
       "   'l': True,\n",
       "   'g': ['(sample: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:',\n",
       "    '(sample: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:',\n",
       "    '(torch.FloatTensor) -> (torch.FloatTensor | (torch.FloatTensor, torch.FloatTensor))',\n",
       "    '(sample: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:']},\n",
       "  {'c': 'def validate_channel_or_videoIds_is_set(\\n    cls, values: Dict[str, Any]\\n) -> Dict[str, Any]:\\n    if not values.get(\"channel_name\") and not values.get(\"video_ids\"):\\n        raise ValueError(\"Must specify either channel_name or video_ids\")\\n    return values',\n",
       "   'd': 'Validate that either folder_id or document_ids is set, but not both.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def dict(self, **kwargs: Any) -> Dict:\\n    _dict = super().dict()\\n    del _dict[\"output_parser\"]\\n    return _dict',\n",
       "   'd': 'Checks if a link is working.\\n\\nIf an error is identified when the request for the link occurs,\\nthe return will be a tuple with the first value True and the second\\nvalue a string containing the error message.\\n\\nIf no errors are identified, the return will be a tuple with the\\nfirst value False and the second an empty string.',\n",
       "   'l': False,\n",
       "   'g': [':param out_dir:\\n    :type out_dir: Path\\n    :return:\\n    :rtype: None\\n    :raises:\\n    :',\n",
       "    '',\n",
       "    'Runs the shell_plus in the output directory.',\n",
       "    'Call django shell_plus.']},\n",
       "  {'c': '    def from_template(cls, template: str, **kwargs: Any) -> ChatPromptTemplate:\\n        prompt_template = PromptTemplate.from_template(template, **kwargs)\\n        message = HumanMessagePromptTemplate(prompt=prompt_template)\\n        return cls.from_messages([message])',\n",
       "   'd': 'Create a class from a template.\\n\\nArgs:\\n    template: template string.\\n    **kwargs: keyword arguments to pass to the constructor.\\n\\nReturns:\\n    A new instance of this class.',\n",
       "   'l': True,\n",
       "   'g': ['_clients = {}\\n\\ndef _get_client(service: str, session: boto3.session.Session, region: str = None):',\n",
       "    '_clients = {}\\n\\n_clients = {}\\ndef get_client(service: str, session: boto3.session.Session, region: str = None):',\n",
       "    '_clients = {}\\n\\n_clients_lock = threading.Lock()',\n",
       "    '_clients = {}\\n\\n_clients = {}']},\n",
       "  {'c': 'def load(self) -> List[Document]:\\n    return list(self.lazy_load())',\n",
       "   'd': 'Load given path as pages.',\n",
       "   'l': True,\n",
       "   'g': ['_run(self, trial_parameters, workspace, callback, **kwargs)\\n    def run(self, trial_parameters, workspace, callback, **kwargs):\\n    def run(self, trial_parameters, workspace, callback, **kwargs):\\n    def run(self, trial_parameters, workspace, callback, **kwargs):\\n    def run(self, trial_parameters, workspace, callback, **kwargs):\\n    def run(self, trial_parameters, workspace, callback, **kwargs):\\n    def run(self, trial_parameters, workspace, callback, **kwargs):\\n    def run(self, trial_parameters, workspace, callback, **kwargs):\\n    def run(self, trial_parameters, workspace, callback, **kwargs):\\n    def run(self, trial_parameters, workspace, callback, **kwargs):\\n    def run(self, trial_parameters, workspace, callback, **kwargs):\\n    def run(self, trial_parameters, workspace, callback, **kwargs):\\n    def run(self, trial_parameters, workspace, callback, **kwargs):\\n    def run(self, trial_parameters, workspace, callback, **kwargs):\\n    def run(self',\n",
       "    '_run(self, trial_parameters, workspace, callback, **kwargs)\\n    def _run(self, trial_parameters, workspace, callback, **kwargs):\\n    def _run_flow(self, trial_parameters, workspace, callback, **kwargs):\\n    def _run_flow_with_flow_id(self, trial_parameters, workspace, callback, **kwargs):\\n    def _run_flow_with_flow_id_and_flow_name(self, trial_parameters, workspace, callback, **kwargs):\\n    def _run_flow_with_flow_id_and_flow_name_and_flow_type(self, trial_parameters, workspace, callback, **kwargs):\\n    def _run_flow_with_flow_id_and_flow_name_and_flow_type_and_flow_kwargs(self, trial_parameters, workspace, callback, **kwargs):\\n    def _run_flow_with_flow_id_and_flow_name_and_flow_type_and_flow_kwargs_and_flow_type_kwargs(self, trial_parameters, workspace, callback, **kwargs):\\n    def',\n",
       "    '_run(self, trial_parameters: dict, workspace: str, callback=None, **kwargs):',\n",
       "    '_run(self, trial_parameters, workspace, callback, **kwargs)\\n    _run_flow(self, trial_parameters, workspace, callback, **kwargs)\\n    _run_flow_with_custom_logger(self, trial_parameters, workspace, callback, **kwargs)\\n    _run_flow_with_custom_logger_and_callback(self, trial_parameters, workspace, callback, **kwargs)\\n    _run_flow_with_custom_logger_and_callback_and_callback(self, trial_parameters, workspace, callback, **kwargs)\\n    _run_flow_with_custom_logger_and_callback_and_callback_and_callback(self, trial_parameters, workspace, callback, **kwargs)\\n    _run_flow_with_custom_logger_and_callback_and_callback_and_callback_and_callback(self, trial_parameters, workspace, callback, **kwargs)\\n    _run_flow_with_custom_logger_and_callback_and_callback_and_callback_and_callback_and_callback(self, trial_parameters, workspace, callback, **kwargs)\\n    _run']},\n",
       "  {'c': 'def _get_combined_score(\\n    self,\\n    document: Document,\\n    vector_relevance: Optional[float],\\n    current_time: datetime.datetime,\\n) -> float:\\n    hours_passed = _get_hours_passed(\\n        current_time,\\n        document.metadata[\"last_accessed_at\"],\\n    )\\n    score = (1.0 - self.decay_rate) ** hours_passed\\n    for key in self.other_score_keys:\\n        if key in document.metadata:\\n            score += document.metadata[key]\\n    if vector_relevance is not None:\\n        score += vector_relevance\\n    return score',\n",
       "   'd': 'Return the combined score for a document.',\n",
       "   'l': True,\n",
       "   'g': ['_goes_first(self.is_main_process):',\n",
       "    '_goes_first(func):\\n    if func():\\n        return',\n",
       "    '_goes_first(self.is_main_process):',\n",
       "    '_goes_first(self, func):']},\n",
       "  {'c': 'def __init__(\\n    self,\\n    separators: Optional[List[str]] = None,\\n    keep_separator: bool = True,\\n    is_separator_regex: bool = False,\\n    **kwargs: Any,\\n) -> None:\\n    super().__init__(keep_separator=keep_separator, **kwargs)\\n    self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n    self._is_separator_regex = is_separator_regex',\n",
       "   'd': 'Create a new TextSplitter.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> BaseMessage:\\n        text = self.prompt.format(**kwargs)\\n        return AIMessage(content=text, additional_kwargs=self.additional_kwargs)',\n",
       "   'd': 'Format the prompt template.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    Formatted message.',\n",
       "   'l': True,\n",
       "   'g': [':return: The environment to use for the current request.',\n",
       "    'Returns the environment variables for the current user.\\n\\n  :return: The environment variables for the current user.\\n  :rtype: dict\\n  :see: :func:`~django_user_env.get_env`',\n",
       "    ':return: The environment to use for this request.',\n",
       "    '']},\n",
       "  {'c': 'def return_stopped_response(\\n    self,\\n    early_stopping_method: str,\\n    intermediate_steps: List[Tuple[AgentAction, str]],\\n    **kwargs: Any,\\n) -> AgentFinish:\\n    if early_stopping_method == \"force\":\\n\\n        return AgentFinish(\\n            {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\\n        )\\n    elif early_stopping_method == \"generate\":\\n\\n        thoughts = \"\"\\n        for action, observation in intermediate_steps:\\n            thoughts += action.log\\n            thoughts += (\\n                f\"\\\\n{self.observation_prefix}{observation}\\\\n{self.llm_prefix}\"\\n            )\\n\\n        thoughts += (\\n            \"\\\\n\\\\nI now need to return a final answer based on the previous steps:\"\\n        )\\n        new_inputs = {\"agent_scratchpad\": thoughts, \"stop\": self._stop}\\n        full_inputs = {**kwargs, **new_inputs}\\n        full_output = self.llm_chain.predict(**full_inputs)\\n\\n        parsed_output = self.output_parser.parse(full_output)\\n        if isinstance(parsed_output, AgentFinish):\\n\\n            return parsed_output\\n        else:\\n\\n\\n            return AgentFinish({\"output\": full_output}, full_output)\\n    else:\\n        raise ValueError(\\n            \"early_stopping_method should be one of `force` or `generate`, \"\\n            f\"got {early_stopping_method}\"\\n        )',\n",
       "   'd': 'Return response when agent has been stopped due to max iterations.',\n",
       "   'l': True,\n",
       "   'g': ['Print message to console and exit with exit_code.\\n\\n    :param message: The message to print.\\n    :param exit_code: The exit code to exit with.\\n    :return: None.',\n",
       "    'Print message to log and exit with given exit code.\\n    :param message: message to print\\n    :param exit_code: exit code to exit with\\n    :return: None\\n    :rtype: None\\n    :Example:\\n    >>> import sys\\n    >>> from tools import print_log\\n    >>> print_log(\"Hello world\", RED, BOLD)\\n    >>> sys.exit(0)\\n    :Example:\\n    >>> import sys\\n    >>> from tools import print_log\\n    >>> print_log(\"Hello world\", RED, BOLD)\\n    :return: None\\n    :rtype: None\\n    :Example:\\n    >>> import sys\\n    >>> from tools import print_log\\n    >>> print_log(\"Hello world\", RED, BOLD)\\n    :return: None\\n    :rtype: None\\n    :Example:\\n    >>> import sys\\n    >>> from tools import print_log\\n    >>> print_log(\"Hello world\", RED, BOLD)\\n    :return: None\\n    :rtype: None\\n    :Example:\\n    >>> import sys\\n    >>> from tools import print_log\\n    >>> print_log(\"Hello world\", RED, BOLD)',\n",
       "    '',\n",
       "    '() -> None']},\n",
       "  {'c': '    def output_keys(self) -> List[str]:\\n        return self.combine_docs_chain.output_keys',\n",
       "   'd': 'Return output key.\\n\\n:meta private:',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    ':param deployment: The deployment to print\\n    :return: The deployment to print\\n    :rtype: dict',\n",
       "    '',\n",
       "    ':param deployment:']},\n",
       "  {'c': '    def format_messages(self, **kwargs: Any) -> List[BaseMessage]:\\n        return [self.format(**kwargs)]',\n",
       "   'd': 'Format messages from kwargs.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    List of BaseMessages.',\n",
       "   'l': True,\n",
       "   'g': ['', ':param v:\\n    :return:\\n    :rtype:', '', '']},\n",
       "  {'c': '    def requires_reference(self) -> bool:\\n        return False',\n",
       "   'd': 'Return whether the chain requires a reference.\\n\\nReturns:\\n    bool: True if the chain requires a reference, False otherwise.',\n",
       "   'l': True,\n",
       "   'g': ['_close(self):', '_close(self):', '_close(self):', '(self):']},\n",
       "  {'c': 'def parse(self, text: str) -> List[str]:\\n    return text.strip().split(\", \")',\n",
       "   'd': 'Pushes an object to the hub and returns the URL.',\n",
       "   'l': False,\n",
       "   'g': ['_get_competent_detectors(self, scores)',\n",
       "    '_get_competent_detectors(self, scores):',\n",
       "    '_get_competent_detectors(self, scores):',\n",
       "    '_get_competent_detectors(self, scores):']},\n",
       "  {'c': 'def test_invalid_suffix() -> None:\\n    path = \"lc://chains/path.invalid\"\\n    loader = Mock()\\n    valid_suffixes = {\"json\"}\\n\\n    with pytest.raises(ValueError, match=\"Unsupported file type.\"):\\n        try_load_from_hub(path, loader, \"chains\", valid_suffixes)\\n\\n    loader.assert_not_called()',\n",
       "   'd': 'Test that a hub path with an invalid suffix raises an error.',\n",
       "   'l': True,\n",
       "   'g': ['Create a waiter action.\\n\\n    :param self: The waiter action.\\n    :param args: The arguments to pass to the waiter action.\\n    :param kwargs: The keyword arguments to pass to the waiter action.\\n    :return: The waiter action.\\n    :rtype: :class:`~django.db.models.signals.',\n",
       "    \"_create_waiter(self, *args, **kwargs) -> 'TODO':\",\n",
       "    '_create_waiter(factory_self, waiter_model)',\n",
       "    '_create_waiter(factory_self, waiter_model)\\n\\n    Create a new waiter action for the given waiter model.\\n\\n    :param factory_self: The factory object.\\n    :param waiter_model: The waiter model.\\n    :return: A function that can be used to invoke the waiter action.']},\n",
       "  {'c': 'def requires_input(self) -> bool:\\n    return False',\n",
       "   'd': 'Whether this evaluator requires an input string.',\n",
       "   'l': True,\n",
       "   'g': [\"'\",\n",
       "    'text: str\\n    min_rarity: int\\n    max_rarity: int\\n    included_tags: list\\n    excluded_tags: list\\n    return self.id.identify(\\n        text,\\n        min_rarity=min_rarity, max_rarity=max_rarity,\\n        included_tags=included_tags, excluded_tags=excluded_tags)',\n",
       "    '',\n",
       "    \"'\"]},\n",
       "  {'c': '    def __init__(\\n        self,\\n        url: str,\\n        max_depth: Optional[int] = 2,\\n        use_async: Optional[bool] = None,\\n        extractor: Optional[Callable[[str], str]] = None,\\n        metadata_extractor: Optional[Callable[[str, str], str]] = None,\\n        exclude_dirs: Optional[Sequence[str]] = (),\\n        timeout: Optional[int] = 10,\\n        prevent_outside: Optional[bool] = True,\\n        link_regex: Union[str, re.Pattern, None] = None,\\n        headers: Optional[dict] = None,\\n        check_response_status: bool = False,\\n    ) -> None:\\n        self.url = url\\n        self.max_depth = max_depth if max_depth is not None else 2\\n        self.use_async = use_async if use_async is not None else False\\n        self.extractor = extractor if extractor is not None else lambda x: x\\n        self.metadata_extractor = (\\n            metadata_extractor\\n            if metadata_extractor is not None\\n            else _metadata_extractor\\n        )\\n        self.exclude_dirs = exclude_dirs if exclude_dirs is not None else ()\\n\\n        if any(url.startswith(exclude_dir) for exclude_dir in self.exclude_dirs):\\n            raise ValueError(\\n                f\"Base url is included in exclude_dirs. Received base_url: {url} and \"\\n                f\"exclude_dirs: {self.exclude_dirs}\"\\n            )\\n\\n        self.timeout = timeout\\n        self.prevent_outside = prevent_outside if prevent_outside is not None else True\\n        self.link_regex = link_regex\\n        self._lock = asyncio.Lock() if self.use_async else None\\n        self.headers = headers\\n        self.check_response_status = check_response_status',\n",
       "   'd': 'Compute query embeddings using a HuggingFace instruct model.\\n\\nArgs:\\n    text: The text to embed.\\n\\nReturns:\\n    Embeddings for the text.',\n",
       "   'l': False,\n",
       "   'g': ['', '(result) ->', '', '(override)']},\n",
       "  {'c': '    def _call(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        chain_input = {**inputs}\\n        if self.agent_tools:\\n            chain_input[\"tool_descriptions\"] = self._tools_description\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        raw_output = self.eval_chain.run(\\n            chain_input, callbacks=_run_manager.get_child()\\n        )\\n        parsed_output = self.output_parser.parse(raw_output)\\n\\n        if self.return_reasoning:\\n            return {\"score\": parsed_output.score, \"reasoning\": parsed_output.reasoning}\\n\\n        return {\"score\": parsed_output.score}',\n",
       "   'd': 'Wait for the given futures to complete.',\n",
       "   'l': False,\n",
       "   'g': [':param total_loss: \\n    :return: \\n    :rtype:',\n",
       "    'Add the loss to the scalar_summary.\\n\\n  Args:\\n    total_loss: Total loss scalar.\\n\\n  Returns:\\n    loss_averages_op: The scalar_summary to be collected and\\n      summarized in the Summary.',\n",
       "    \"Add loss summaries to the train_op.\\n\\n  Args:\\n    total_loss: loss for all of the model's train_ops\",\n",
       "    'Args:\\n        total_loss: \\n    Returns:']}],\n",
       " 'google/codegemma-2b': [{'c': '    def __init__(self, file_path: str) -> None:\\n        try:\\n            from pdfminer.high_level import extract_text\\n        except ImportError:\\n            raise ImportError(\\n                \"`pdfminer` package not found, please install it with \"\\n                \"`pip install pdfminer.six`\"\\n            )\\n\\n        super().__init__(file_path)\\n        self.parser = PDFMinerParser()',\n",
       "   'd': 'Initialize with file path.',\n",
       "   'l': True,\n",
       "   'g': ['.',\n",
       "    '.\\n    Yields:\\n        ObjectReferencePart.',\n",
       "    'for each element in self.recursive_crawl(\"identifier\"),\\n        yield an ObjectReferencePart object.',\n",
       "    '.\\n    Iterate over all references in the raw data.\\n\\n    :return: Generator over all references.']},\n",
       "  {'c': '    def __call__(\\n        self,\\n        inputs: Union[Dict[str, Any], Any],\\n        return_only_outputs: bool = False,\\n        callbacks: Callbacks = None,\\n        *,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        run_id: Optional[UUID] = None,\\n        run_name: Optional[str] = None,\\n        include_run_info: bool = False,\\n    ) -> Dict[str, Any]:\\n        inputs = self.prep_inputs(inputs)\\n        callback_manager = CallbackManager.configure(\\n            callbacks,\\n            self.callbacks,\\n            self.verbose,\\n            tags,\\n            self.tags,\\n            metadata,\\n            self.metadata,\\n        )\\n        new_arg_supported = inspect.signature(self._call).parameters.get(\"run_manager\")\\n        run_manager = callback_manager.on_chain_start(\\n            dumpd(self),\\n            inputs,\\n            run_id=run_id,\\n            name=run_name,\\n        )\\n        try:\\n            outputs = (\\n                self._call(inputs, run_manager=run_manager)\\n                if new_arg_supported\\n                else self._call(inputs)\\n            )\\n        except (KeyboardInterrupt, Exception) as e:\\n            run_manager.on_chain_error(e)\\n            raise e\\n        run_manager.on_chain_end(outputs)\\n        final_outputs: Dict[str, Any] = self.prep_outputs(\\n            inputs, outputs, return_only_outputs\\n        )\\n        if include_run_info:\\n            final_outputs[RUN_KEY] = RunInfo(run_id=run_manager.run_id)\\n        return final_outputs',\n",
       "   'd': \"Execute the chain.\\n\\nArgs:\\n    inputs: Dictionary of inputs, or single input if chain expects\\n        only one param. Should contain all inputs specified in\\n        `Chain.input_keys` except for inputs that will be set by the chain's\\n        memory.\\n    return_only_outputs: Whether to return only outputs in the\\n        response. If True, only new keys generated by this chain will be\\n        returned. If False, both input keys and new keys generated by this\\n        chain will be returned. Defaults to False.\\n    callbacks: Callbacks to use for this chain run. These will be called in\\n        addition to callbacks passed to the chain during construction, but only\\n        these runtime callbacks will propagate to calls to other objects.\\n    tags: List of string tags to pass to all callbacks. These will be passed in\\n        addition to tags passed to the chain during construction, but only\\n        these runtime tags will propagate to calls to other objects.\\n    metadata: Optional metadata associated with the chain. Defaults to None\\n    include_run_info: Whether to include run info in the response. Defaults\\n        to False.\\n\\nReturns:\\n    A dict of named outputs. Should contain all outputs specified in\\n        `Chain.output_keys`.\",\n",
       "   'l': True,\n",
       "   'g': ['(self, x)\\n\\n    Combine two lists',\n",
       "    '(self, x)',\n",
       "    '(self, x)\\n\\n    Combine two dictionaries.',\n",
       "    '.\\n    Combine the two dictionaries']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        file_path: str,\\n        textract_features: Optional[Sequence[str]] = None,\\n        client: Optional[Any] = None,\\n        credentials_profile_name: Optional[str] = None,\\n        region_name: Optional[str] = None,\\n        endpoint_url: Optional[str] = None,\\n        headers: Optional[Dict] = None,\\n    ) -> None:\\n        super().__init__(file_path, headers=headers)\\n\\n        try:\\n            import textractcaller as tc\\n        except ImportError:\\n            raise ModuleNotFoundError(\\n                \"Could not import amazon-textract-caller python package. \"\\n                \"Please install it with `pip install amazon-textract-caller`.\"\\n            )\\n        if textract_features:\\n            features = [tc.Textract_Features[x] for x in textract_features]\\n        else:\\n            features = []\\n\\n        if credentials_profile_name or region_name or endpoint_url:\\n            try:\\n                import boto3\\n\\n                if credentials_profile_name is not None:\\n                    session = boto3.Session(profile_name=credentials_profile_name)\\n                else:\\n\\n                    session = boto3.Session()\\n\\n                client_params = {}\\n                if region_name:\\n                    client_params[\"region_name\"] = region_name\\n                if endpoint_url:\\n                    client_params[\"endpoint_url\"] = endpoint_url\\n\\n                client = session.client(\"textract\", **client_params)\\n\\n            except ImportError:\\n                raise ModuleNotFoundError(\\n                    \"Could not import boto3 python package. \"\\n                    \"Please install it with `pip install boto3`.\"\\n                )\\n            except Exception as e:\\n                raise ValueError(\\n                    \"Could not load credentials to authenticate with AWS client. \"\\n                    \"Please check that credentials in the specified \"\\n                    \"profile name are valid.\"\\n                ) from e\\n        self.parser = AmazonTextractPDFParser(textract_features=features, client=client)',\n",
       "   'd': 'Return response when agent has been stopped due to max iterations.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n    return self._cache.get((prompt, llm_string), None)',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': ['_forward',\n",
       "    'Forward pass of the generator\\n\\n    :param x: input image\\n    :return: sine_merge, noise, uv',\n",
       "    '_forward_impl(self, x):\\n        See base class.',\n",
       "    '(self, x):']},\n",
       "  {'c': 'def _llm_type(self) -> str:\\n    return \"fireworks-chat\"',\n",
       "   'd': 'Return type of llm.',\n",
       "   'l': True,\n",
       "   'g': ['Stops the Scalene application.',\n",
       "    'Stops the Scalene program.',\n",
       "    'Stops the program',\n",
       "    'Stops the Scalene server']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        messages = self.format_messages(**kwargs)\\n        return get_buffer_string(messages)',\n",
       "   'd': 'Format the prompt with inputs generating a string.\\n\\nUse this method to generate a string representation of a prompt consisting\\nof chat messages.\\n\\nUseful for feeding into a string based completion language model or debugging.\\n\\nArgs:\\n    **kwargs: keyword arguments to use for formatting.\\n\\nReturns:\\n    A string representation of the prompt',\n",
       "   'l': True,\n",
       "   'g': ['Function to compute the ratio and resize the image\\n    \\n    Args:\\n        img (numpy.ndarray): Input image\\n        width (int): Width of the image\\n        height (int): Height of the image\\n        model_height (int): Height of the model\\n        \\n    Returns:\\n        img (numpy.ndarray): Resized image\\n        ratio (float): Ratio of the image',\n",
       "    'Compute the ratio of the image and resize it to fit the model\\n    \\n    Parameters\\n    ----------\\n    img : numpy array\\n        The image to resize\\n    width : int\\n        The width of the image\\n    height : int\\n        The height of the image\\n    model_height : int\\n        The height of the model\\n    \\n    Returns\\n    -------\\n    img : numpy array\\n        The resized image\\n    ratio : float\\n        The ratio of the image',\n",
       "    'Compute the ratio and resize the image to the model height\\n    :param img:\\n    :param width:\\n    :param height:\\n    :param model_height:\\n    :return:',\n",
       "    'compute the ratio of the image and resize the image']},\n",
       "  {'c': '    def __call__(\\n        self,\\n        inputs: Union[Dict[str, Any], Any],\\n        return_only_outputs: bool = False,\\n        callbacks: Callbacks = None,\\n        *,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n    ) -> Dict[str, Any]:\\n        inputs = self.prep_inputs(inputs)\\n        callback_manager = CallbackManager.configure(\\n            callbacks,\\n            self.callbacks,\\n            self.verbose,\\n            tags,\\n            self.tags,\\n            metadata,\\n            self.metadata,\\n        )\\n        new_arg_supported = inspect.signature(self._call).parameters.get(\"run_manager\")\\n        run_manager = callback_manager.on_chain_start(\\n            dumpd(self),\\n            inputs,\\n        )\\n        try:\\n            outputs = (\\n                self._call(inputs, run_manager=run_manager)\\n                if new_arg_supported\\n                else self._call(inputs)\\n            )\\n        except (KeyboardInterrupt, Exception) as e:\\n            run_manager.on_chain_error(e)\\n            raise e\\n        run_manager.on_chain_end(outputs)\\n        final_outputs: Dict[str, Any] = self.prep_outputs(\\n            inputs, outputs, return_only_outputs\\n        )\\n        if include_run_info:\\n            final_outputs[RUN_KEY] = RunInfo(run_id=run_manager.run_id)\\n        return final_outputs',\n",
       "   'd': \"Execute the chain.\\n\\nArgs:\\n    inputs: Dictionary of inputs, or single input if chain expects\\n        only one param. Should contain all inputs specified in\\n        `Chain.input_keys` except for inputs that will be set by the chain's\\n        memory.\\n    return_only_outputs: Whether to return only outputs in the\\n        response. If True, only new keys generated by this chain will be\\n        returned. If False, both input keys and new keys generated by this\\n        chain will be returned. Defaults to False.\\n    callbacks: Callbacks to use for this chain run. These will be called in\\n        addition to callbacks passed to the chain during construction, but only\\n        these runtime callbacks will propagate to calls to other objects.\\n    tags: List of string tags to pass to all callbacks. These will be passed in\\n        addition to tags passed to the chain during construction, but only\\n        these runtime tags will propagate to calls to other objects.\\n    metadata: Optional metadata associated with the chain. Defaults to None\\n    include_run_info: Whether to include run info in the response. Defaults\\n        to False.\\n\\nReturns:\\n    A dict of named outputs. Should contain all outputs specified in\\n        `Chain.output_keys`.\",\n",
       "   'l': True,\n",
       "   'g': ['to predict the target value of the input X. \\n    \\n    Args:\\n        X (ndarray): input data.\\n        \\n    Returns:\\n        ndarray: predicted target values.\\n        \\n    Raises:\\n        NotImplementedError: if the method is not implemented.',\n",
       "    '(X) -> (y)',\n",
       "    ',',\n",
       "    '(X, y) -> (X, y)\\n    \\n    Make predictions for X.\\n    \\n    Parameters\\n    ----------\\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\\n        The input data.\\n    \\n    Returns\\n    -------\\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\\n        The input data.']},\n",
       "  {'c': '    def on_tool_error(\\n        self,\\n        error: Union[Exception, KeyboardInterrupt],\\n        *,\\n        run_id: UUID,\\n        **kwargs: Any,\\n    ) -> None:\\n        if not run_id:\\n            raise TracerException(\"No run_id provided for on_tool_error callback.\")\\n        tool_run = self.run_map.get(str(run_id))\\n        if tool_run is None or tool_run.run_type != \"tool\":\\n            raise TracerException(\"No tool Run found to be traced\")\\n\\n        tool_run.error = repr(error)\\n        tool_run.end_time = datetime.utcnow()\\n        tool_run.events.append({\"name\": \"error\", \"time\": tool_run.end_time})\\n        self._end_trace(tool_run)\\n        self._on_tool_error(tool_run)',\n",
       "   'd': 'Handle an error for a tool run.',\n",
       "   'l': True,\n",
       "   'g': ['Returns:\\n        dict: dictionary of scores for each algorithm\\n        np.ndarray: number of steps for each algorithm\\n        np.ndarray: score thresholds',\n",
       "    'Returns a dict of the test scores and the number of steps taken in each episode.\\n    \\n    Parameters\\n    ----------\\n    algo_name : str, optional\\n        The name of the algorithm. If None, the name of the experiment directory is used.\\n    score_thresholds : np.ndarray, optional\\n        The thresholds to use for the score. If None, the thresholds are determined by the score_thresholds parameter.\\n\\n    Returns\\n    -------\\n    dict\\n        A dictionary containing the test scores and the number of steps taken in each episode.',\n",
       "    'Args:\\n        algo_name (str, optional): Name of the algorithm. Defaults to None.\\n        score_thresholds (np.ndarray, optional): Thresholds for the score. Defaults to None.\\n\\n    Returns:\\n        dict: Dictionary containing the scores.\\n        np.ndarray: Array of the number of steps.\\n        np.ndarray: Array of the score thresholds.',\n",
       "    'Get the data for the score plots.\\n    \\n    Args:\\n        algo_name (str, optional): The name of the algorithm. Defaults to None.\\n        score_thresholds (np.ndarray, optional): The thresholds to use for the score plots. Defaults to None.\\n    \\n    Returns:\\n        dict: The dictionary containing the score data.\\n        np.ndarray: The episode steps data.\\n        np.ndarray: The score thresholds.']},\n",
       "  {'c': 'def foo(bar: str, callbacks: Optional[CallbackManagerForToolRun] = None) -> str:\\n    assert callbacks is not None\\n    return \"foo\" + bar',\n",
       "   'd': 'Formats the request according the the chosen api',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Static file serving.\\n    \\n    This is a very simple file serving middleware. It serves static files\\n    from a directory or a file.\\n    \\n    The file serving is done by opening the file and reading it.\\n    \\n    The file is served with the Content-Type header set to the mimetype\\n    of the file.\\n    \\n    If the file is a directory, it is served with a list of files in the\\n    directory.\\n    \\n    If the file is a symlink, it is served by following the symlink.\\n    \\n    If the file is a directory and the directory is empty, an empty file\\n    is served.\\n    \\n    If the file is a directory and the directory is not empty, an error is\\n    raised.\\n    \\n    If the file is a directory and the directory is not empty and the\\n    directory is not a symlink, a 404 error is raised.\\n    \\n    If the file is a directory and the directory is not empty and the\\n    directory is a symlink, a 403 error is raised.\\n    \\n    If the file is a directory and the directory is not empty',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': '    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        self.return_each_line = return_each_line\\n\\n\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )',\n",
       "   'd': 'Create a new MarkdownHeaderTextSplitter.\\n\\nArgs:\\n    headers_to_split_on: Headers we want to track\\n    return_each_line: Return each line w/ associated headers',\n",
       "   'l': True,\n",
       "   'g': ['to enable iteration over the buffers',\n",
       "    'to iterate over the buffers',\n",
       "    'to iterate over the buffers',\n",
       "    'to iterate over the buffers in the buffer pool']},\n",
       "  {'c': '    def __init__(self, text_kwargs: Optional[Mapping[str, Any]] = None) -> None:\\n        self.text_kwargs = text_kwargs or {}',\n",
       "   'd': 'Initialize the parser.\\n\\nArgs:\\n    text_kwargs: Keyword arguments to pass to ``fitz.Page.get_text()``.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    '',\n",
       "    'Function to predict the image class\\n    \\n    Parameters\\n    ----------\\n    image : ndarray\\n        The image to predict\\n    \\n    Returns\\n    -------\\n    prediction : str\\n        The predicted class of the image']},\n",
       "  {'c': '    def input_keys(self) -> List[str]:\\n        return [self.input_key]',\n",
       "   'd': 'Expect input key.\\n\\n:meta private:',\n",
       "   'l': True,\n",
       "   'g': ['Returns the coordinates of the point at the given direction.\\n    \\n    Args:\\n        dim: The dimension of the point to return.\\n        direction: The direction of the point to return.\\n    \\n    Returns:\\n        The coordinates of the point at the given direction.',\n",
       "    'Returns the coordinate of the extremum along a given dimension.',\n",
       "    'Returns the coordinate of the extremum along the given dimension.',\n",
       "    'Get the coordinates of the extremum along a given dimension. \\n    \\n    Parameters\\n    ----------\\n    dim : int\\n        The dimension along which to get the extremum.\\n    direction : tuple, optional\\n        The direction of the extremum. The default is ORIGIN.\\n\\n    Returns\\n    -------\\n    tuple\\n        The coordinates of the extremum along the given dimension.']},\n",
       "  {'c': 'def test_invalid_suffix() -> None:\\n    path = \"lc://chains/path.invalid\"\\n    loader = Mock()\\n    valid_suffixes = {\"json\"}\\n\\n    with pytest.raises(\\n        ValueError, match=f\"Unsupported file type, must be one of {valid_suffixes}.\"\\n    ):\\n        try_load_from_hub(path, loader, \"chains\", valid_suffixes)\\n\\n    loader.assert_not_called()',\n",
       "   'd': 'Test that a hub path with an invalid suffix raises an error.',\n",
       "   'l': True,\n",
       "   'g': ['(str) -> None', '(result)', '(result)', '(result) -> NoneType']},\n",
       "  {'c': 'def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\\n    if self.input_keys:\\n        input_variables = {key: input_variables[key] for key in self.input_keys}\\n    query = \" \".join(sorted_values(input_variables))\\n    example_docs = self.vectorstore.similarity_search(query, k=self.k)\\n\\n\\n    examples = [dict(e.metadata) for e in example_docs]\\n\\n    if self.example_keys:\\n        examples = [{k: eg[k] for k in self.example_keys} for eg in examples]\\n    return examples',\n",
       "   'd': 'Select which examples to use based on semantic similarity.',\n",
       "   'l': True,\n",
       "   'g': ['Convert a string to a native Python type.',\n",
       "    \"Native string encoding function. \\n    \\n    This function is used to convert a string to a native string encoding. \\n    \\n    Parameters\\n    ----------\\n    s : str\\n        A string to be converted to a native string encoding.\\n    encoding : str, optional\\n        The encoding to be used for conversion. Default is 'latin-1'.\\n    errors : str, optional\\n        The error handling strategy to be used for conversion. Default is 'strict'.\\n    \\n    Returns\\n    -------\\n    str\\n        The native string encoding of the input string.\",\n",
       "    'Converts a string to a native Python type.',\n",
       "    '']},\n",
       "  {'c': 'def test_json_spec_value() -> None:\\n    spec = JsonSpec(dict_={\"foo\": \"bar\", \"baz\": {\"test\": {\"foo\": [1, 2, 3]}}})\\n    assert spec.value(\"data\") == \"{\\'foo\\': \\'bar\\', \\'baz\\': {\\'test\\': {\\'foo\\': [1, 2, 3]}}}\"\\n    assert spec.value(\\'data[\"foo\"]\\') == \"bar\"\\n    assert spec.value(\\'data[\"baz\"]\\') == \"{\\'test\\': {\\'foo\\': [1, 2, 3]}}\"\\n    assert spec.value(\\'data[\"baz\"][\"test\"]\\') == \"{\\'foo\\': [1, 2, 3]}\"\\n    assert spec.value(\\'data[\"baz\"][\"test\"][\"foo\"]\\') == \"[1, 2, 3]\"\\n    assert spec.value(\"data[\\'foo\\']\") == \"bar\"\\n    assert spec.value(\"data[\\'baz\\']\") == \"{\\'test\\': {\\'foo\\': [1, 2, 3]}}\"\\n    assert spec.value(\"data[\\'baz\\'][\\'test\\']\") == \"{\\'foo\\': [1, 2, 3]}\"\\n    assert spec.value(\"data[\\'baz\\'][\\'test\\'][\\'foo\\']\") == \"[1, 2, 3]\"',\n",
       "   'd': 'Lazily parse the blob.',\n",
       "   'l': False,\n",
       "   'g': ['Initialize a `DataFrame` object.\\n\\n    Parameters\\n    ----------\\n    args : sequence of arrays or 2D `DataFrame` objects\\n        The data to be stored in the `DataFrame`.\\n    concat_rows : bool, optional\\n        If `True`, then the rows of the `DataFrame` will be concatenated\\n        into a single row.\\n    **kwargs :\\n        Additional keyword arguments.',\n",
       "    'Args:\\n        args (Any): \\n        concat_rows (bool, optional): \\n        kwargs (Any):',\n",
       "    '',\n",
       "    'Initialize the DataFrame class.\\n\\n    Parameters:\\n        *args: Variable length argument list.\\n        **kwargs: Arbitrary keyword arguments.']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        return self.format_prompt(**kwargs).to_string()',\n",
       "   'd': 'Format the chat template into a string.\\n\\nArgs:\\n    **kwargs: keyword arguments to use for filling in template variables\\n              in all the template messages in this chat template.\\n\\nReturns:\\n    formatted string',\n",
       "   'l': True,\n",
       "   'g': ['_summary_\\n\\n    Args:\\n        store_type (str): _description_\\n\\n    Returns:\\n        str: _description_',\n",
       "    '_summary_\\n\\n    Args:\\n        store_type (str): _description_\\n\\n    Returns:\\n        str: _description_',\n",
       "    '_summary_\\n\\n    Args:\\n        store_type (str): _description_\\n\\n    Returns:\\n        str: _description_',\n",
       "    '_make_save_file_name_\\n\\n    Args:\\n        store_type (str): _description_\\n\\n    Returns:\\n        str: _description_']},\n",
       "  {'c': '    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        all_required_field_names = {field.alias for field in cls.__fields__.values()}\\n\\n        extra = values.get(\"model_kwargs\", {})\\n        for field_name in list(values):\\n            if field_name not in all_required_field_names:\\n                if field_name in extra:\\n                    raise ValueError(f\"Found {field_name} supplied twice.\")\\n                logger.warning(\\n                    f\"\"\"{field_name} was transferred to model_kwargs.\\n                    Please confirm that {field_name} is what you intended.\"\"\"\\n                )\\n                extra[field_name] = values.pop(field_name)\\n        values[\"model_kwargs\"] = extra\\n        return values',\n",
       "   'd': 'Build extra kwargs from additional params that were passed in.',\n",
       "   'l': True,\n",
       "   'g': ['TODO: refactor this function to return the path to the log file.',\n",
       "    'TODO: Refactor this method to use pathlib.Path instead of string',\n",
       "    'TODO: refactor',\n",
       "    '_summary_\\n    This function returns the path to the log file.']},\n",
       "  {'c': '    def lazy_parse(self, blob: Blob) -> Iterator[Document]:\\n        import fitz\\n\\n        with blob.as_bytes_io() as file_path:\\n            doc = fitz.open(file_path)\\n\\n            yield from [\\n                Document(\\n                    page_content=page.get_text(**self.text_kwargs),\\n                    metadata=dict(\\n                        {\\n                            \"source\": blob.source,\\n                            \"file_path\": blob.source,\\n                            \"page\": page.number,\\n                            \"total_pages\": len(doc),\\n                        },\\n                        **{\\n                            k: doc.metadata[k]\\n                            for k in doc.metadata\\n                            if type(doc.metadata[k]) in [str, int]\\n                        },\\n                    ),\\n                )\\n                for page in doc\\n            ]',\n",
       "   'd': 'Run when chain errors.\\n\\nArgs:\\n    error (Exception or KeyboardInterrupt): The error.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    Args:\\n        xs: [batch_size, seq_len, dim]\\n        masks: [batch_size, seq_len]\\n    Returns:\\n        [batch_size, seq_len, dim]',\n",
       "    '.\\n    Args:\\n        xs: [batch_size, seq_len, embed_size]\\n        masks: [batch_size, seq_len]\\n\\n    Returns:\\n        [batch_size, seq_len, embed_size]',\n",
       "    '.\\n    Args:\\n        xs (paddle.Tensor):\\n        masks (paddle.Tensor):\\n    Returns:\\n        paddle.Tensor:',\n",
       "    '.\\n    Args:\\n        xs (paddle.Tensor): [description]\\n        masks (paddle.Tensor): [description]\\n\\n    Returns:\\n        [type]: [description]']},\n",
       "  {'c': '    def format_messages(self, **kwargs: Any) -> List[BaseMessage]:\\n        return [self.format(**kwargs)]',\n",
       "   'd': 'Format messages from kwargs.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    List of BaseMessages.',\n",
       "   'l': True,\n",
       "   'g': ['Returns a list of corners and center of the region',\n",
       "    'Returns the four corners of the region.',\n",
       "    'Returns the four corners of the current region as a list of points',\n",
       "    'Return the corners and center of the region.\\n    \\n    :return: (pt1, pt2, pt3, pt4, center)']},\n",
       "  {'c': 'def from_function(\\n    cls,\\n    func: Callable,\\n    name: str,\\n    description: str,\\n    return_direct: bool = False,\\n    args_schema: Optional[Type[BaseModel]] = None,\\n    **kwargs: Any,\\n) -> Tool:\\n    return cls(\\n        name=name,\\n        func=func,\\n        description=description,\\n        return_direct=return_direct,\\n        args_schema=args_schema,\\n        **kwargs,\\n    )',\n",
       "   'd': 'Initialize with file path.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'This function adds two numbers together.\\n    \\n    Parameters\\n    ----------\\n    a : int\\n        The first number to add.\\n    b : int\\n        The second number to add.\\n    \\n    Returns\\n    -------\\n    int\\n        The sum of the two numbers.',\n",
       "    '',\n",
       "    'A test function.']},\n",
       "  {'c': 'def _llm_type(self) -> str:\\n    return \"azure\"',\n",
       "   'd': 'Clear cache.',\n",
       "   'l': False,\n",
       "   'g': ['Pad the data to be a multiple of 64 bytes.',\n",
       "    'Pad the data to a multiple of 64 bytes',\n",
       "    'padding function',\n",
       "    'Padded data.']},\n",
       "  {'c': '    def with_listeners(\\n        self,\\n        *,\\n        on_start: Optional[Listener] = None,\\n        on_end: Optional[Listener] = None,\\n        on_error: Optional[Listener] = None,\\n    ) -> Runnable[Input, Output]:\\n        from langchain.callbacks.tracers.root_listeners import RootListenersTracer\\n\\n        return self.__class__(\\n            bound=self.bound,\\n            kwargs=self.kwargs,\\n            config=self.config,\\n            config_factories=[\\n                lambda config: {\\n                    \"callbacks\": [\\n                        RootListenersTracer(\\n                            config=config,\\n                            on_start=on_start,\\n                            on_end=on_end,\\n                            on_error=on_error,\\n                        )\\n                    ],\\n                }\\n            ],\\n            custom_input_type=self.custom_input_type,\\n            custom_output_type=self.custom_output_type,\\n        )',\n",
       "   'd': 'Bind lifecycle listeners to a Runnable, returning a new Runnable.\\n\\non_start: Called before the runnable starts running, with the Run object.\\non_end: Called after the runnable finishes running, with the Run object.\\non_error: Called if the runnable throws an error, with the Run object.\\n\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.',\n",
       "   'l': True,\n",
       "   'g': [', checks if the image is fully within the image.\\n\\n    Args:\\n        image (Image): the image to check.\\n\\n    Returns:\\n        bool: True if the image is fully within the image, False otherwise.',\n",
       "    '.\\n    Checks if an image is fully within the image.',\n",
       "    '.\\n    Checks if the image is fully within the image.',\n",
       "    '.\\n    Checks if a given image is fully within the image.\\n\\n    :param image: the image to check\\n    :return: True if the image is fully within the image, False otherwise']},\n",
       "  {'c': 'def _run(\\n    self,\\n    table_names: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> str:\\n    return self.db.get_table_info_no_throw(table_names.split(\", \"))',\n",
       "   'd': 'Get the schema for tables in a comma-separated list.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    '',\n",
       "    'Show an error bubble.\\n    \\n    :param str error: The error text to show.\\n    :param str width: The width of the bubble.\\n    :param tuple pos: The position of the bubble.\\n    :param tuple arrow_pos: The position of the arrow.\\n    :param bool exit: Whether to exit the app.\\n    :param str icon: The icon to show.\\n    :param int duration: The duration of the bubble.\\n    :returns: None']},\n",
       "  {'c': '    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        texts = [t.replace(\"\\\\n\", \" \") for t in texts]\\n        embeddings = self.client.encode(texts, **self.encode_kwargs)\\n        return embeddings.tolist()',\n",
       "   'd': 'Format the prompt template.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    Formatted message.',\n",
       "   'l': False,\n",
       "   'g': ['Get the parameters of the model.\\n    \\n    Returns\\n    -------\\n    list\\n        List of parameters.',\n",
       "    'Return the parameters of the model.',\n",
       "    'Return the parameters for the current model. \\n    \\n    Returns\\n    -------\\n    parameters : list\\n        A list of the parameters for the current model.',\n",
       "    'Returns the parameters for the model.']},\n",
       "  {'c': '    def clear(self) -> None:\\n        from botocore.exceptions import ClientError\\n\\n        try:\\n            self.table.delete_item(self.key)\\n        except ClientError as err:\\n            logger.error(err)',\n",
       "   'd': 'Get the schema for tables in a comma-separated list.',\n",
       "   'l': False,\n",
       "   'g': ['Start a process that will be detached from the parent process.',\n",
       "    'Start a process that will be detached from the main process.\\n\\n    Parameters\\n    ----------\\n    executable : str\\n        Path to the executable to be started.\\n    *args : list\\n        Arguments to pass to the executable.\\n\\n    Returns\\n    -------\\n    int\\n        Process ID of the started process.',\n",
       "    'Start a detached process and return the process ID.',\n",
       "    '_start_detached_process_internal']},\n",
       "  {'c': '    def __call__(\\n        self,\\n        inputs: Union[Dict[str, Any], Any],\\n        return_only_outputs: bool = False,\\n        callbacks: Callbacks = None,\\n        *,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        run_id: Optional[UUID] = None,\\n        run_name: Optional[str] = None,\\n        include_run_info: bool = False,\\n    ) -> Dict[str, Any]:\\n        inputs = self.prep_inputs(inputs)\\n        callback_manager = CallbackManager.configure(\\n            callbacks,\\n            self.callbacks,\\n            self.verbose,\\n            tags,\\n            self.tags,\\n            metadata,\\n            self.metadata,\\n        )\\n        new_arg_supported = inspect.signature(self._call).parameters.get(\"run_manager\")\\n        run_manager = callback_manager.on_chain_start(\\n            dumpd(self),\\n            inputs,\\n            run_id=run_id,\\n            name=run_name,\\n        )\\n        try:\\n            outputs = (\\n                self._call(inputs, run_manager=run_manager)\\n                if new_arg_supported\\n                else self._call(inputs)\\n            )\\n        except (KeyboardInterrupt, Exception) as e:\\n            run_manager.on_chain_error(e)\\n            raise e\\n        run_manager.on_chain_end(outputs)\\n        final_outputs: Dict[str, Any] = self.prep_outputs(\\n            inputs, outputs, return_only_outputs\\n        )\\n        if include_run_info:\\n            final_outputs[RUN_KEY] = RunInfo(run_id=run_manager.run_id)\\n        return final_outputs',\n",
       "   'd': 'Lazily parse the blob.',\n",
       "   'l': False,\n",
       "   'g': ['Destroy the object',\n",
       "    'Destroy the object.',\n",
       "    'Destroys the game object.',\n",
       "    'Destroy the game']},\n",
       "  {'c': '    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        vectors = np.array(\\n            self.embeddings.embed_documents([inputs[\"prediction\"], inputs[\"reference\"]])\\n        )\\n        score = self._compute_score(vectors)\\n        return {\"score\": score}',\n",
       "   'd': 'Compute the score for a prediction and reference.\\n\\nArgs:\\n    inputs (Dict[str, Any]): The input data.\\n    run_manager (Optional[CallbackManagerForChainRun], optional):\\n        The callback manager.\\n\\nReturns:\\n    Dict[str, Any]: The computed score.',\n",
       "   'l': True,\n",
       "   'g': ['(x, style, skip) -> out',\n",
       "    '(self, x, style, skip=None)\\n    Args:\\n        x: input tensor\\n        style: style tensor\\n        skip: skip tensor\\n    Returns:\\n        out: output tensor',\n",
       "    '(x, style, skip=None) -> torch.Tensor\\n        x: input tensor of shape (N, C, H, W)\\n        style: style tensor of shape (N, C, H, W)\\n        skip: skip tensor of shape (N, C, H, W)\\n        return: output tensor of shape (N, C, H, W)',\n",
       "    '(x, style, skip=None) -> out:\\n\\n    Args:\\n        x: input image.\\n        style: style image.\\n        skip: skip connection.\\n\\n    Returns:\\n        out: output image.']},\n",
       "  {'c': 'def serialize_chat_messages(self, messages: List[Dict]) -> str:\\n    chat_messages = _get_messages_from_run_dict(messages)\\n    return get_buffer_string(chat_messages)',\n",
       "   'd': 'Extract the input messages from the run.',\n",
       "   'l': True,\n",
       "   'g': ['(Constructor)',\n",
       "    \"for a target function f(x) = 0.5 * x' * A * x,\\n    A is a symmetric positive definite matrix.\",\n",
       "    'for i in range(len(self._keys)):\\n            self._cache[self._keys[i]] = self._bounds[i]',\n",
       "    'for a given target function,\\n    this class will be used to initialize a\\n    random search algorithm.']},\n",
       "  {'c': 'def load(self) -> List[Document]:\\n    loader = UnstructuredPDFLoader(str(self.file_path))\\n    return loader.load()',\n",
       "   'd': 'Maps the Run to a dictionary.',\n",
       "   'l': False,\n",
       "   'g': ['(self, output_file=None,\\n                 resume=False, progress_file=sys.stderr)\\n\\n    Create a new progress reporter.\\n\\n    :param output_file: If given, the progress will be written to this file.\\n    :param resume: If True, the progress will be resumed from the last\\n        recorded status.\\n    :param progress_file: If given, the progress will be printed to this\\n        file.',\n",
       "    '.\\n    Args:\\n        output_file: The file to write the output to.\\n        resume: Whether to resume the job from the last checkpoint.\\n        progress_file: The file to write the progress to.',\n",
       "    '(self, output_file=None,\\n                 resume=False, progress_file=sys.stderr)\\n\\n    Initialize the progress bar.\\n\\n    Parameters\\n    ----------\\n    output_file : str, optional\\n        File to write progress to. Defaults to sys.stderr.\\n    resume : bool, optional\\n        Whether to resume from the last saved position. Defaults to False.\\n    progress_file : file-like, optional\\n        File to write progress to. Defaults to sys.stderr.\\n\\n    Returns\\n    -------\\n    None.',\n",
       "    '(self, output_file=None,\\n                 resume=False, progress_file=sys.stderr)\\n\\n    Initializes a new ``ProgressReporter`` instance.\\n\\n    :param output_file: The file to write the progress report to.\\n    :param resume: If ``True``, the progress report will be resumed from\\n        the last known position.\\n    :param progress_file: The file to write the progress report to.\\n        Defaults to ``sys.stderr``.']},\n",
       "  {'c': '    def clear(self, **kwargs: Any) -> None:\\n        from gptcache import Cache\\n\\n        for gptcache_instance in self.gptcache_dict.values():\\n            gptcache_instance = cast(Cache, gptcache_instance)\\n            gptcache_instance.flush()\\n\\n        self.gptcache_dict.clear()',\n",
       "   'd': 'Clear cache.',\n",
       "   'l': True,\n",
       "   'g': ['.log() is a decorator that logs values to WandB.\\n\\n    Args:\\n        values (dict): The values to log.\\n        step (Optional[int]): The step to log the values to.\\n        **kwargs: Additional keyword arguments to pass to the run.log() method.\\n\\n    Example:\\n        >>> @wandb.log\\n        ... def log(values: dict, step: Optional[int] = None, **kwargs):\\n        ...     logger.debug(\"Logging values: %s\", values)\\n        ...     return wandb.log(values, step=step, **kwargs)\\n        ...\\n        >>> log({\"a\": 1, \"b\": 2})',\n",
       "    '.\\n    Logs the values to WandB.\\n\\n    Args:\\n        values (dict): The values to log.\\n        step (Optional[int]): The step to log the values for. Defaults to None.\\n        **kwargs: Additional keyword arguments to pass to wandb.log.\\n\\n    Raises:\\n        Exception: If the wandb.run is not set.',\n",
       "    'for logging in wandb.',\n",
       "    '.\\n    Log the values to wandb.']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        cohere_api_key = get_from_dict_or_env(\\n            values, \"cohere_api_key\", \"COHERE_API_KEY\"\\n        )\\n        try:\\n            import cohere\\n\\n            values[\"client\"] = cohere.Client(cohere_api_key)\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import cohere python package. \"\\n                \"Please install it with `pip install cohere`.\"\\n            )\\n        return values',\n",
       "   'd': 'Validate that api key and python package exists in environment.',\n",
       "   'l': True,\n",
       "   'g': ['for testing',\n",
       "    '.',\n",
       "    'for json.dumps()',\n",
       "    '.\\n    Returns:\\n        dict: The dictionary representation of the object.']},\n",
       "  {'c': '    def execute(self, query: str, params: dict = {}, retry: int = 0) -> Any:\\n        from nebula3.Exception import IOErrorException, NoValidSessionException\\n        from nebula3.fbthrift.transport.TTransport import TTransportException\\n\\n        try:\\n            result = self.session_pool.execute_parameter(query, params)\\n            if not result.is_succeeded():\\n                logging.warning(\\n                    f\"Error executing query to NebulaGraph. \"\\n                    f\"Error: {result.error_msg()}\\\\n\"\\n                    f\"Query: {query} \\\\n\"\\n                )\\n            return result\\n\\n        except NoValidSessionException:\\n            logging.warning(\\n                f\"No valid session found in session pool. \"\\n                f\"Please consider increasing the session pool size. \"\\n                f\"Current size: {self.session_pool_size}\"\\n            )\\n            raise ValueError(\\n                f\"No valid session found in session pool. \"\\n                f\"Please consider increasing the session pool size. \"\\n                f\"Current size: {self.session_pool_size}\"\\n            )\\n\\n        except RuntimeError as e:\\n            if retry < RETRY_TIMES:\\n                retry += 1\\n                logging.warning(\\n                    f\"Error executing query to NebulaGraph. \"\\n                    f\"Retrying ({retry}/{RETRY_TIMES})...\\\\n\"\\n                    f\"query: {query} \\\\n\"\\n                    f\"Error: {e}\"\\n                )\\n                return self.execute(query, params, retry)\\n            else:\\n                raise ValueError(f\"Error executing query to NebulaGraph. Error: {e}\")\\n\\n        except (TTransportException, IOErrorException):\\n\\n            if retry < RETRY_TIMES:\\n                retry += 1\\n                logging.warning(\\n                    f\"Connection issue with NebulaGraph. \"\\n                    f\"Retrying ({retry}/{RETRY_TIMES})...\\\\n to recreate session pool\"\\n                )\\n                self.session_pool = self._get_session_pool()\\n                return self.execute(query, params, retry)',\n",
       "   'd': 'Split HTML text string\\n\\nArgs:\\n    text: HTML text',\n",
       "   'l': False,\n",
       "   'g': ['.format_tb(exc_traceback, limit=None, frame=None, file=sys.stderr)',\n",
       "    '.format_tb(exc_traceback, limit=None, frame=None)',\n",
       "    '.format_tb(exc_traceback, limit=None, frame=None,\\n                 source_path=None)',\n",
       "    '.format_tb(exc_traceback, limit=None)\\n\\n    Return a string containing the formatted traceback.']},\n",
       "  {'c': 'def load(self) -> List[Document]:\\n    loader = UnstructuredPDFLoader(str(self.file_path))\\n    return loader.load()',\n",
       "   'd': 'Load documents.',\n",
       "   'l': True,\n",
       "   'g': ['', '', 'Returns\\n    -------\\n    F.arange(0, len(self))', '']},\n",
       "  {'c': '    def delete(\\n        self,\\n        ids: Optional[List[str]] = None,\\n        **kwargs: Any,\\n    ) -> Optional[bool]:\\n        ret: Optional[bool] = None\\n        tmp_res = []\\n        if ids is None or ids.__len__() == 0:\\n            return ret\\n        for _id in ids:\\n            if self.flag:\\n                ret = self.vearch.delete(self.using_db_name, self.using_table_name, _id)\\n            else:\\n                ret = self.vearch.del_doc(_id)\\n            tmp_res.append(ret)\\n        ret = all(i == 0 for i in tmp_res)\\n        return ret',\n",
       "   'd': 'Format the prompt with inputs generating a string.\\n\\nUse this method to generate a string representation of a prompt consisting\\nof chat messages.\\n\\nUseful for feeding into a string based completion language model or debugging.\\n\\nArgs:\\n    **kwargs: keyword arguments to use for formatting.\\n\\nReturns:\\n    A string representation of the prompt',\n",
       "   'l': False,\n",
       "   'g': ['Generates a policy that allows the app to write logs to CloudWatch and \\n    attach itself to a VPC.\\n    \\n    Args:\\n        config (object): Configuration object\\n    \\n    Returns:\\n        policy (dict): Policy to be used by CloudFormation',\n",
       "    'Generate a CloudWatch Logs policy for the app.',\n",
       "    '_summary_',\n",
       "    '_summary_\\n    Generate the policy that will be applied to the app.']},\n",
       "  {'c': '    def _call(\\n        self,\\n        prompt: str,\\n        stop: Optional[List[str]] = None,\\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\\n        **kwargs: Any,\\n    ) -> str:\\n        try:\\n            from clarifai_grpc.grpc.api import (\\n                resources_pb2,\\n                service_pb2,\\n            )\\n            from clarifai_grpc.grpc.api.status import status_code_pb2\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import clarifai python package. \"\\n                \"Please install it with `pip install clarifai`.\"\\n            )\\n\\n\\n\\n\\n        post_model_outputs_request = service_pb2.PostModelOutputsRequest(\\n            user_app_id=self.userDataObject,\\n            model_id=self.model_id,\\n            version_id=self.model_version_id,\\n            inputs=[\\n                resources_pb2.Input(\\n                    data=resources_pb2.Data(text=resources_pb2.Text(raw=prompt))\\n                )\\n            ],\\n        )\\n        post_model_outputs_response = self.stub.PostModelOutputs(\\n            post_model_outputs_request\\n        )\\n\\n        if post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\\n            logger.error(post_model_outputs_response.status)\\n            first_model_failure = (\\n                post_model_outputs_response.outputs[0].status\\n                if len(post_model_outputs_response.outputs)\\n                else None\\n            )\\n            raise Exception(\\n                f\"Post model outputs failed, status: \"\\n                f\"{post_model_outputs_response.status}, first output failure: \"\\n                f\"{first_model_failure}\"\\n            )\\n\\n        text = post_model_outputs_response.outputs[0].data.text.raw\\n\\n\\n        if stop is not None:\\n            text = enforce_stop_tokens(text, stop)\\n        return text',\n",
       "   'd': '求阶乘',\n",
       "   'l': False,\n",
       "   'g': ['Sets the backend for all configuration files.\\n    \\n    :param backend: The backend to set.',\n",
       "    'Sets the backend for all configs.\\n    \\n    :param backend: backend to use',\n",
       "    'Sets the backend to use for this config.',\n",
       "    'Sets the backend for all configurations in this group.']},\n",
       "  {'c': '    def parse(self, text: str) -> T:\\n',\n",
       "   'd': 'Run the tool.',\n",
       "   'l': False,\n",
       "   'g': ['for unknown types, return application/octet-stream',\n",
       "    '- Return the MIME type of the file.',\n",
       "    'for unknown types',\n",
       "    '.']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        values[\"openai_api_key\"] = get_from_dict_or_env(\\n            values,\\n            \"openai_api_key\",\\n            \"OPENAI_API_KEY\",\\n        )\\n        values[\"openai_api_base\"] = get_from_dict_or_env(\\n            values,\\n            \"openai_api_base\",\\n            \"OPENAI_API_BASE\",\\n        )\\n        values[\"openai_api_version\"] = get_from_dict_or_env(\\n            values,\\n            \"openai_api_version\",\\n            \"OPENAI_API_VERSION\",\\n        )\\n        values[\"openai_api_type\"] = get_from_dict_or_env(\\n            values, \"openai_api_type\", \"OPENAI_API_TYPE\", default=\"azure\"\\n        )\\n        values[\"openai_organization\"] = get_from_dict_or_env(\\n            values,\\n            \"openai_organization\",\\n            \"OPENAI_ORGANIZATION\",\\n            default=\"\",\\n        )\\n        values[\"openai_proxy\"] = get_from_dict_or_env(\\n            values,\\n            \"openai_proxy\",\\n            \"OPENAI_PROXY\",\\n            default=\"\",\\n        )\\n        try:\\n            import openai\\n\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import openai python package. \"\\n                \"Please install it with `pip install openai`.\"\\n            )\\n        try:\\n            values[\"client\"] = openai.ChatCompletion\\n        except AttributeError:\\n            raise ValueError(\\n                \"`openai` has no `ChatCompletion` attribute, this is likely \"\\n                \"due to an old version of the openai package. Try upgrading it \"\\n                \"with `pip install --upgrade openai`.\"\\n            )\\n        if values[\"n\"] < 1:\\n            raise ValueError(\"n must be at least 1.\")\\n        if values[\"n\"] > 1 and values[\"streaming\"]:\\n            raise ValueError(\"n must be 1 when streaming.\")\\n        return values',\n",
       "   'd': 'Validate that api key and python package exists in environment.',\n",
       "   'l': True,\n",
       "   'g': ['@param {Object} kwargs',\n",
       "    ':param type: \\n    :param message: \\n    :param stack_trace: \\n    :param inner_exception: \\n    :param data: \\n    :param error_response:',\n",
       "    '_init__',\n",
       "    'Initializes the DebugInfoResponse class.\\n    \\n    Args:\\n        type (str): The type of exception.\\n        message (str): The message of the exception.\\n        stack_trace (str): The stack trace of the exception.\\n        inner_exception (Exception): The inner exception.\\n        data (dict): The data associated with the exception.\\n        error_response (dict): The error response associated with the exception.']},\n",
       "  {'c': 'def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': False,\n",
       "   'g': ['随机生成一个 1-9 之间的数字，并排除 0',\n",
       "    'Returns a random digit that is not null.',\n",
       "    'Returns a random digit, excluding zero.',\n",
       "    'Return a random digit, but not 0.']},\n",
       "  {'c': 'def format_messages(self, **kwargs: Any) -> List[BaseMessage]:\\n',\n",
       "   'd': 'Format kwargs into a list of messages.',\n",
       "   'l': True,\n",
       "   'g': ['to be implemented by subclasses',\n",
       "    'access to the QNetworkAccessManager for the application.',\n",
       "    'access the network access manager',\n",
       "    '.\\n    Returns:\\n        QNetworkAccessManager:']},\n",
       "  {'c': 'def _type(self) -> str:\\n    return \"default\"',\n",
       "   'd': 'Load a prompt template from a template.',\n",
       "   'l': False,\n",
       "   'g': ['Test that the command is executed on the local machine.',\n",
       "    'Test command execution.',\n",
       "    'Test that the command is executed on the local machine.',\n",
       "    '']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[Dict[Any, Any]]] = None,\\n        model_id: Optional[str] = None,\\n        refresh_indices: bool = False,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        if not self.client.indices.exists(index=self.index_name):\\n            dims = kwargs.get(\"dims\")\\n\\n            if dims is None:\\n                raise ValueError(\"ElasticKnnSearch requires \\'dims\\' parameter\")\\n\\n            similarity = kwargs.get(\"similarity\")\\n            optional_args = {}\\n\\n            if similarity is not None:\\n                optional_args[\"similarity\"] = similarity\\n\\n            mapping = self._default_knn_mapping(dims=dims, **optional_args)\\n            self.create_knn_index(mapping)\\n\\n        embeddings = self.embedding.embed_documents(list(texts))\\n\\n\\n        body: List[Mapping[str, Any]] = []\\n        for text, vector in zip(texts, embeddings):\\n            body.extend(\\n                [\\n                    {\"index\": {\"_index\": self.index_name}},\\n                    {\"text\": text, \"vector\": vector},\\n                ]\\n            )\\n\\n        responses = self.client.bulk(operations=body)\\n\\n        ids = [\\n            item[\"index\"][\"_id\"]\\n            for item in responses[\"items\"]\\n            if item[\"index\"][\"result\"] == \"created\"\\n        ]\\n\\n        if refresh_indices:\\n            self.client.indices.refresh(index=self.index_name)\\n\\n        return ids',\n",
       "   'd': 'Add a list of texts to the Elasticsearch index.\\n\\nArgs:\\n    texts (Iterable[str]): The texts to add to the index.\\n    metadatas (List[Dict[Any, Any]], optional): A list of metadata dictionaries\\n        to associate with the texts.\\n    model_id (str, optional): The ID of the model to use for transforming the\\n        texts into vectors.\\n    refresh_indices (bool, optional): Whether to refresh the Elasticsearch\\n        indices after adding the texts.\\n    **kwargs: Arbitrary keyword arguments.\\n\\nReturns:\\n    A list of IDs for the added texts.',\n",
       "   'l': True,\n",
       "   'g': ['This function returns the prompt embedding to be used in the model.\\n    \\n    Args:\\n        adapter_name (str): The name of the adapter to use.\\n    \\n    Returns:\\n        torch.Tensor: The prompt embedding.',\n",
       "    'Get the prompt embedding for a given adapter name. \\n    \\n    Args:\\n        adapter_name (str): The name of the adapter to get the prompt embedding for.\\n    \\n    Returns:\\n        torch.Tensor: The prompt embedding for the given adapter name.',\n",
       "    'Get the prompt embedding for the given adapter name.',\n",
       "    'Get the prompt embedding.\\n    \\n    Args:\\n        adapter_name (str): The adapter name.\\n    \\n    Returns:\\n        torch.Tensor: The prompt embedding.']},\n",
       "  {'c': 'def _get_combined_score(\\n    self,\\n    document: Document,\\n    vector_relevance: Optional[float],\\n    current_time: datetime.datetime,\\n) -> float:\\n    hours_passed = _get_hours_passed(\\n        current_time,\\n        document.metadata[\"last_accessed_at\"],\\n    )\\n    score = (1.0 - self.decay_rate) ** hours_passed\\n    for key in self.other_score_keys:\\n        if key in document.metadata:\\n            score += document.metadata[key]\\n    if vector_relevance is not None:\\n        score += vector_relevance\\n    return score',\n",
       "   'd': 'Test error is raised when name of input variable is wrong.',\n",
       "   'l': False,\n",
       "   'g': ['.clear()\\n    \\n    def _clear(self):',\n",
       "    '.\\n    Clears the screen.',\n",
       "    '.clear()\\n    \\n    def _clear(self):\\n        self._draw_area.clear()\\n        self._draw_area.fill((0, 0, 0))\\n        self._draw_area.set_source_rgb(1, 1, 1)\\n        self._draw_area.rectangle((0, 0, self._width, self._height))\\n        self._draw_area.stroke()\\n        self._draw_area.fill()\\n        self._draw_area.set_source_rgb(0, 0, 0)\\n        self._draw_area.set_font_size(20)\\n        self._draw_area.move_to(10, 10)\\n        self._draw_area.show_text(\"Hello, world!\")\\n        self._draw_area.fill()\\n    \\n    def _update(self):\\n        self._draw_area.show()\\n    \\n    def _draw_text(self, text):\\n        self._draw_area.show_text(text)\\n        self._update()\\n    \\n    def _draw_rect(self,',\n",
       "    '.\\n    \\n    Clears the screen and resets the cursor position.']},\n",
       "  {'c': '    def load(self) -> List[Document]:\\n        from pdfminer.high_level import extract_text_to_fp\\n        from pdfminer.layout import LAParams\\n        from pdfminer.utils import open_filename\\n\\n        output_string = StringIO()\\n        with open_filename(self.file_path, \"rb\") as fp:\\n            extract_text_to_fp(\\n                fp,\\n                output_string,\\n                codec=\"\",\\n                laparams=LAParams(),\\n                output_type=\"html\",\\n            )\\n        metadata = {\"source\": self.file_path}\\n        return [Document(page_content=output_string.getvalue(), metadata=metadata)]',\n",
       "   'd': 'Lazy load the chat sessions from the iMessage chat.db\\nand yield them in the required format.\\n\\nYields:\\n    ChatSession: Loaded chat session.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'ResNeXt101 32x8d\\n    Args:\\n        pretrained (bool): If True, returns a model pre-trained on ImageNet',\n",
       "    '']},\n",
       "  {'c': '    def input_variables(self) -> List[str]:\\n',\n",
       "   'd': 'Input variables for this prompt template.\\n\\nReturns:\\n    List of input variables.',\n",
       "   'l': True,\n",
       "   'g': ['Returns the center of the arc.',\n",
       "    'Get the center of the arc',\n",
       "    'Returns the center of the arc.',\n",
       "    'Returns the center of the arc.']},\n",
       "  {'c': '    def input_keys(self) -> List[str]:\\n        return self._input_keys',\n",
       "   'd': 'Return the input keys.\\n\\nReturns:\\n    List of input keys.',\n",
       "   'l': True,\n",
       "   'g': ['(async)\\n    Initialize the class with the given configuration and optional parameters.\\n\\n    Args:\\n        config (dict): The configuration dictionary.\\n        conversation_id (str, optional): The conversation ID. Defaults to None.\\n        parent_id (str, optional): The parent ID. Defaults to \"\".\\n        base_url (str, optional): The base URL. Defaults to \"\".\\n\\n    Returns:\\n        None',\n",
       "    '(\\n        self,\\n        config: dict,\\n        conversation_id: str | None = None,\\n        parent_id: str = \"\",\\n        base_url: str = \"\",\\n    ) -> None:',\n",
       "    '.',\n",
       "    '.\\n    Args:\\n        config (dict): Configuration for the client.\\n        conversation_id (str, optional): Conversation ID. Defaults to None.\\n        parent_id (str, optional): Parent ID. Defaults to \"\".\\n        base_url (str, optional): Base URL. Defaults to \"\".']},\n",
       "  {'c': 'def return_stopped_response(\\n    self,\\n    early_stopping_method: str,\\n    intermediate_steps: List[Tuple[AgentAction, str]],\\n    **kwargs: Any,\\n) -> AgentFinish:\\n    if early_stopping_method == \"force\":\\n\\n        return AgentFinish(\\n            {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\\n        )\\n    else:\\n        raise ValueError(\\n            f\"Got unsupported early_stopping_method `{early_stopping_method}`\"\\n        )',\n",
       "   'd': 'Return response when agent has been stopped due to max iterations.',\n",
       "   'l': True,\n",
       "   'g': ['This function takes in a GemmaConfig and a GroupQuantize object, and returns a tuple of a\\n    quantized model and a mapping from the original to quantized weights.',\n",
       "    'Args:\\n        model_config:\\n        quantization:\\n    Returns:\\n        model:\\n        quant_map:',\n",
       "    '',\n",
       "    'Group quantization model\\n    \\n    Args:\\n        model_config: GemmaConfig\\n        quantization: GroupQuantize\\n\\n    Returns:\\n        model: nn.Module\\n        quant_map: QuantizeMapping']},\n",
       "  {'c': '    def _add_vectors(\\n        client: supabase.client.Client,\\n        table_name: str,\\n        vectors: List[List[float]],\\n        documents: List[Document],\\n        ids: List[str],\\n    ) -> List[str]:\\n        rows: List[Dict[str, Any]] = [\\n            {\\n                \"id\": ids[idx],\\n                \"content\": documents[idx].page_content,\\n                \"embedding\": embedding,\\n                \"metadata\": documents[idx].metadata,\\n            }\\n            for idx, embedding in enumerate(vectors)\\n        ]\\n\\n\\n\\n        chunk_size = 500\\n        id_list: List[str] = []\\n        for i in range(0, len(rows), chunk_size):\\n            chunk = rows[i : i + chunk_size]\\n\\n            result = client.from_(table_name).upsert(chunk).execute()\\n\\n            if len(result.data) == 0:\\n                raise Exception(\"Error inserting: No rows added\")\\n\\n\\n            ids = [str(i.get(\"id\")) for i in result.data if i.get(\"id\")]\\n\\n            id_list.extend(ids)\\n\\n        return id_list',\n",
       "   'd': 'Add vectors to Supabase table.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Returns an iterable mapping each item to its normalized form.\\n\\n    .. versionadded:: 1.0',\n",
       "    '.\\n\\n    Returns a normalized iterable.',\n",
       "    '.\\n\\n    Returns a normalized version of the iterable.',\n",
       "    '.\\n\\n    Returns a normalized iterable.\\n\\n    .. note::\\n\\n        This method is a helper method for :meth:`~utils.normalize_iterable`.\\n\\n    .. warning::\\n\\n        This method is experimental.\\n\\n    :returns: A normalized iterable.\\n\\n    .. note::\\n\\n        This method is a helper method for :meth:`~utils.normalize_iterable`.\\n\\n    .. warning::\\n\\n        This method is experimental.\\n\\n    .. note::\\n\\n        This method is a helper method for :meth:`~utils.normalize_iterable`.\\n\\n    .. note::\\n\\n        This method is a helper method for :meth:`~utils.normalize_iterable`.\\n\\n    .. note::\\n\\n        This method is a helper method for :meth:`~utils.normalize_iterable`.\\n\\n    .. note::\\n\\n        This method is a helper method for :meth:`~utils.normalize_iterable`.\\n\\n    .. note::\\n\\n        This method is a helper method for :meth:`~utils.normalize_iterable`.\\n\\n    .. note::\\n\\n        This method is a helper method for :meth:`~utils.normalize_iterable`.\\n\\n    .. note::\\n\\n        This method is a helper method for :meth:`~utils.normalize_iterable']},\n",
       "  {'c': '    def input_schema(self) -> Type[BaseModel]:\\n        func = getattr(self, \"func\", None) or getattr(self, \"afunc\")\\n\\n        if isinstance(func, itemgetter):\\n\\n\\n            items = str(func).replace(\"operator.itemgetter(\", \"\")[:-1].split(\", \")\\n            if all(\\n                item[0] == \"\\'\" and item[-1] == \"\\'\" and len(item) > 2 for item in items\\n            ):\\n\\n                return create_model(\\n                    \"RunnableLambdaInput\",\\n                    **{item[1:-1]: (Any, None) for item in items},\\n                )\\n            else:\\n                return create_model(\"RunnableLambdaInput\", __root__=(List[Any], None))\\n\\n        if dict_keys := get_function_first_arg_dict_keys(func):\\n            return create_model(\\n                \"RunnableLambdaInput\",\\n                **{key: (Any, None) for key in dict_keys},\\n            )\\n\\n        return super().input_schema',\n",
       "   'd': 'Construct ElasticVectorSearch wrapper from raw documents.\\n\\nThis is a user-friendly interface that:\\n    1. Embeds documents.\\n    2. Creates a new index for the embeddings in the Elasticsearch instance.\\n    3. Adds the documents to the newly created Elasticsearch index.\\n\\nThis is intended to be a quick way to get started.\\n\\nExample:\\n    .. code-block:: python\\n\\n        from langchain import ElasticVectorSearch\\n        from langchain.embeddings import OpenAIEmbeddings\\n        embeddings = OpenAIEmbeddings()\\n        elastic_vector_search = ElasticVectorSearch.from_texts(\\n            texts,\\n            embeddings,\\n            elasticsearch_url=\"http://localhost:9200\"\\n        )',\n",
       "   'l': False,\n",
       "   'g': ['Download a file from a remote GDB server\\n    \\n    Parameters\\n    ----------\\n    target : str\\n        The remote file to download\\n    use_cache : bool\\n        Whether to use the local cache\\n    \\n    Returns\\n    -------\\n    local_name : str\\n        The local path of the downloaded file',\n",
       "    'Download a file from a remote target. \\n    \\n    Args:\\n        target (str): Remote target to download from.\\n        use_cache (bool): Whether to use the local cache.\\n    \\n    Returns:\\n        str: Path to the downloaded file.',\n",
       "    'Download file from remote and return local path.\\n    \\n    Parameters\\n    ----------\\n    target : str\\n        Remote file path.\\n    use_cache : bool\\n        Whether to use local cache.\\n    \\n    Returns\\n    -------\\n    local_name : str\\n        Local file path.',\n",
       "    'Download a file from a remote location and return the local path.\\n    \\n    Parameters\\n    ----------\\n    target : str\\n        The remote location of the file.\\n    use_cache : bool, optional\\n        If True, the file will be downloaded from the cache if it exists.\\n\\n    Returns\\n    -------\\n    local_name : str\\n        The local path of the downloaded file.']},\n",
       "  {'c': 'def _run_chain(\\n    chain: Union[Chain, Runnable],\\n    inputs: Dict[str, Any],\\n    callbacks: Callbacks,\\n    *,\\n    tags: Optional[List[str]] = None,\\n    input_mapper: Optional[Callable[[Dict], Any]] = None,\\n) -> Union[Dict, str]:\\n    inputs_ = inputs if input_mapper is None else input_mapper(inputs)\\n    if isinstance(chain, Chain) and isinstance(inputs_, dict) and len(inputs_) == 1:\\n        val = next(iter(inputs_.values()))\\n        output = chain(val, callbacks=callbacks, tags=tags)\\n    else:\\n        runnable_config = RunnableConfig(tags=tags or [], callbacks=callbacks)\\n        output = chain.invoke(inputs_, config=runnable_config)\\n    return output',\n",
       "   'd': 'Run a chain on inputs.',\n",
       "   'l': True,\n",
       "   'g': [\".\\n    Registers a new agent class for the given name.\\n\\n    Args:\\n        name (str): The name under which the agent class will be registered.\\n        agent_cls (Type['Agent']): The agent class to register.\\n\\n    Raises:\\n        ValueError: If the agent class is already registered under the given name.\",\n",
       "    \"(cls, name: str, agent_cls: Type['Agent']) -> None:\",\n",
       "    '.\\n    Registers an agent class with the given name.\\n\\n    Args:\\n        cls (type): The class to register.\\n        name (str): The name to associate with the agent class.\\n        agent_cls (Type[\\'Agent\\']): The agent class to register.\\n\\n    Raises:\\n        ValueError: If the agent class is already registered under the given name.\\n\\n    Example:\\n        class MyAgent(Agent):\\n            pass\\n\\n        @register(\"my_agent\", MyAgent)\\n        class MyAgent(Agent):\\n            pass\\n\\n        # Now, you can use the agent class as follows:\\n        agent = MyAgent()',\n",
       "    \"(cls, name: str, agent_cls: Type['Agent']) -> None:\"]},\n",
       "  {'c': '    def __init__(\\n        self,\\n        file: Union[IO, Sequence[IO]],\\n        mode: str = \"single\",\\n        url: str = \"https://api.unstructured.io/general/v0/general\",\\n        api_key: str = \"\",\\n        **unstructured_kwargs: Any,\\n    ):\\n        if isinstance(file, collections.abc.Sequence):\\n            validate_unstructured_version(min_unstructured_version=\"0.6.3\")\\n        if file:\\n            validate_unstructured_version(min_unstructured_version=\"0.6.2\")\\n\\n        self.url = url\\n        self.api_key = api_key\\n\\n        super().__init__(file=file, mode=mode, **unstructured_kwargs)',\n",
       "   'd': 'Initialize with file path.',\n",
       "   'l': True,\n",
       "   'g': ['This method is called when the widget is mounted.',\n",
       "    'Set the interval of the timer to 1/60th of a second.',\n",
       "    'This method is called when the widget is mounted.',\n",
       "    'Mount the component.']},\n",
       "  {'c': '    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        prompt = inputs[self.input_key]\\n\\n        _intent = self.sparql_intent_chain.run({\"prompt\": prompt}, callbacks=callbacks)\\n        intent = _intent.strip()\\n\\n        if \"SELECT\" not in intent and \"UPDATE\" not in intent:\\n            raise ValueError(\\n                \"I am sorry, but this prompt seems to fit none of the currently \"\\n                \"supported SPARQL query types, i.e., SELECT and UPDATE.\"\\n            )\\n        elif intent.find(\"SELECT\") < intent.find(\"UPDATE\"):\\n            sparql_generation_chain = self.sparql_generation_select_chain\\n            intent = \"SELECT\"\\n        else:\\n            sparql_generation_chain = self.sparql_generation_update_chain\\n            intent = \"UPDATE\"\\n\\n        _run_manager.on_text(\"Identified intent:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(intent, color=\"green\", end=\"\\\\n\", verbose=self.verbose)\\n\\n        generated_sparql = sparql_generation_chain.run(\\n            {\"prompt\": prompt, \"schema\": self.graph.get_schema}, callbacks=callbacks\\n        )\\n\\n        _run_manager.on_text(\"Generated SPARQL:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(\\n            generated_sparql, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n\\n        if intent == \"SELECT\":\\n            context = self.graph.query(generated_sparql)\\n\\n            _run_manager.on_text(\"Full Context:\", end=\"\\\\n\", verbose=self.verbose)\\n            _run_manager.on_text(\\n                str(context), color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n            )\\n            result = self.qa_chain(\\n                {\"prompt\": prompt, \"context\": context},\\n                callbacks=callbacks,\\n            )\\n            res = result[self.qa_chain.output_key]\\n        elif intent == \"UPDATE\":\\n            self.graph.update(generated_sparql)\\n            res = \"Successfully inserted triples into the graph.\"\\n        else:\\n            raise ValueError(\"Unsupported SPARQL query type.\")\\n        return {self.output_key: res}',\n",
       "   'd': 'Generate SPARQL query, use it to retrieve a response from the gdb and answer\\nthe question.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', 'This is the baz function']},\n",
       "  {'c': '    def from_messages(\\n        cls, messages: Sequence[Union[BaseMessagePromptTemplate, BaseMessage]]\\n    ) -> ChatPromptTemplate:\\n        input_vars = set()\\n        for message in messages:\\n            if isinstance(message, BaseMessagePromptTemplate):\\n                input_vars.update(message.input_variables)\\n        return cls(input_variables=list(input_vars), messages=messages)',\n",
       "   'd': 'Return dictionary representation of agent.',\n",
       "   'l': False,\n",
       "   'g': ['(self, buffers)\\n\\n    Initialize the buffers.',\n",
       "    '(object)',\n",
       "    '(self, buffers)\\n\\n    Initialize the class.\\n\\n    Args:\\n        buffers (list): List of buffers to be used.',\n",
       "    '(buffers)']},\n",
       "  {'c': '    def __init__(self, separator: str = \"\\\\n\\\\n\", **kwargs: Any) -> None:\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator',\n",
       "   'd': 'Test PDFMiner loader.',\n",
       "   'l': False,\n",
       "   'g': ['Parse a set statement.',\n",
       "    'Parse a set statement.',\n",
       "    'Parse a set statement.',\n",
       "    'Parse a set statement.']},\n",
       "  {'c': 'def _llm_type(self) -> str:\\n    return \"azure\"',\n",
       "   'd': \"Call out to Aleph Alpha's asymmetric, query embedding endpoint\\nArgs:\\n    text: The text to embed.\\n\\nReturns:\\n    Embeddings for the text.\",\n",
       "   'l': False,\n",
       "   'g': ['.',\n",
       "    '.',\n",
       "    '.\\n\\n    Args:\\n        path (str): path to the file.\\n        ext_map (Optional[Dict[str, List[str]]]): mapping of file extensions to their corresponding classes.\\n\\n    Returns:\\n        str: code representing the file.',\n",
       "    '.']},\n",
       "  {'c': '    def delete(\\n        self,\\n        ids: Optional[List[str]] = None,\\n        refresh_indices: Optional[bool] = True,\\n        **kwargs: Any,\\n    ) -> Optional[bool]:\\n        try:\\n            from elasticsearch.helpers import BulkIndexError, bulk\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import elasticsearch python package. \"\\n                \"Please install it with `pip install elasticsearch`.\"\\n            )\\n\\n        body = []\\n\\n        if ids is None:\\n            raise ValueError(\"ids must be provided.\")\\n\\n        for _id in ids:\\n            body.append({\"_op_type\": \"delete\", \"_index\": self.index_name, \"_id\": _id})\\n\\n        if len(body) > 0:\\n            try:\\n                bulk(self.client, body, refresh=refresh_indices, ignore_status=404)\\n                logger.debug(f\"Deleted {len(body)} texts from index\")\\n\\n                return True\\n            except BulkIndexError as e:\\n                logger.error(f\"Error deleting texts: {e}\")\\n                firstError = e.errors[0].get(\"index\", {}).get(\"error\", {})\\n                logger.error(f\"First error reason: {firstError.get(\\'reason\\')}\")\\n                raise e\\n\\n        else:\\n            logger.debug(\"No texts to delete from index\")\\n            return False',\n",
       "   'd': 'Expect input key.\\n\\n:meta private:',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Returns the distance and angle between two points.',\n",
       "    'Calculate the Euclidean distance between two points',\n",
       "    '']},\n",
       "  {'c': 'def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n    return {self.memory_key: self.buffer}',\n",
       "   'd': 'Return history buffer.',\n",
       "   'l': True,\n",
       "   'g': ['(self, *args, **kwargs):',\n",
       "    '(self, *args, **kwargs):',\n",
       "    '(self, *args, **kwargs):\\n        self.done = False\\n        self.order = None\\n        self.frame_count = 0\\n        self.portfolio = None\\n\\n        self.registered_transforms = {}\\n        self.transforms = []\\n        self.sources = []\\n\\n\\n        self.slippage = VolumeShareSlippage()\\n        self.commission = PerShare()\\n\\n\\n\\n        self.initialized = False\\n\\n\\n        self.initialize(*args, **kwargs)',\n",
       "    'Initialize the component.\\n\\n    Args:\\n        *args:\\n        **kwargs:']},\n",
       "  {'c': '    def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n        from gptcache.adapter.api import get\\n\\n        _gptcache = self._get_gptcache(llm_string)\\n\\n        res = get(prompt, cache_obj=_gptcache)\\n        if res:\\n            return [\\n                Generation(**generation_dict) for generation_dict in json.loads(res)\\n            ]\\n        return None',\n",
       "   'd': 'Initialize a PythonCodeTextSplitter.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n\\n    Args:\\n        regexes (List[Tuple[str, Regularizer]], optional): Defaults to None.',\n",
       "    '.\\n\\n    Parameters\\n    ----------\\n    regexes : List[Tuple[str, Regularizer]], optional\\n        List of tuples containing regular expressions and their corresponding regularizers.\\n        The default is None.\\n\\n    Returns\\n    -------\\n    None.',\n",
       "    '(str, Regularizer) -> NoneType\\n    \\n    Initializes the Regularizer object.\\n    \\n    Args:\\n        regexes (List[Tuple[str, Regularizer]]): A list of tuples containing the regular expression and the Regularizer.\\n    \\n    Returns:\\n        NoneType: None',\n",
       "    '(str, Regularizer) -> NoneType\\n    \\n    Initializes the regularizer.\\n    \\n    Args:\\n        regexes: A list of tuples of the form (regex, regularizer).']},\n",
       "  {'c': 'def _get_docs(\\n    self,\\n    question: str,\\n    inputs: Dict[str, Any],\\n    *,\\n    run_manager: CallbackManagerForChainRun,\\n) -> List[Document]:\\n',\n",
       "   'd': 'Return consistent embeddings for the text, if seen before, or a constant\\none if the text is unknown.',\n",
       "   'l': False,\n",
       "   'g': ['Sign the given payload with the testing key.\\n    \\n    :param payload: The payload to sign.\\n    :return: The signed JWT.',\n",
       "    'Sign the given payload using the RSA signing key.',\n",
       "    'Sign an ID token using the testing keys.',\n",
       "    'Create a signed JWT token for the given payload.\\n    \\n    :param payload: The payload to sign.\\n    :return: A signed JWT token.']},\n",
       "  {'c': 'def buffer(self) -> List[BaseMessage]:\\n    return self.chat_memory.messages',\n",
       "   'd': 'String buffer of memory.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Convert an object to a string.\\n    \\n    :param obj: The object to convert.\\n    :param encoding: The encoding to use.\\n    :param errors: The errors to use.\\n    :return: The string representation of the object.',\n",
       "    'Convert a string-like object to a string.\\n    \\n    Parameters\\n    ----------\\n    obj : str-like object\\n        The object to convert to a string.\\n    \\n    Returns\\n    -------\\n    str\\n        The converted string.',\n",
       "    \"Convert `obj` to a string.\\n    \\n    If `obj` is a string, return `obj`.\\n    If `obj` is an integer, return `str(obj)`.\\n    If `obj` is a float, return `str(obj)`.\\n    If `obj` is a boolean, return `str(obj)`.\\n    If `obj` is a `datetime.datetime` or `datetime.date`, return `obj.isoformat()`.\\n    If `obj` is a `datetime.time`, return `obj.isoformat(sep=':')`.\\n    If `obj` is a `datetime.timedelta`, return `obj.total_seconds()`.\\n    If `obj` is a `datetime.tzinfo`, return `obj.utcoffset(obj.dst()).isoformat()`.\\n    If `obj` is a `datetime.tzinfo` subclass, return `obj.tzname()`.\\n    If `obj` is a `datetime.tzinfo` subclass with `tzname` and `dst` attributes, return `obj.tzname()`.\\n    If `obj` is a `datetime.tzinfo\"]},\n",
       "  {'c': 'def serialize_chat_messages(self, messages: List[Dict]) -> str:\\n    chat_messages = _get_messages_from_run_dict(messages)\\n    return get_buffer_string(chat_messages)',\n",
       "   'd': 'An individual iterator of a :py:func:`~.tee`',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    Closes the connection to the database.',\n",
       "    'to close the database connection.',\n",
       "    '.',\n",
       "    '.']},\n",
       "  {'c': '    def clear(self, **kwargs: Any) -> None:\\n        from momento.responses import CacheFlush\\n\\n        flush_response = self.cache_client.flush_cache(self.cache_name)\\n        if isinstance(flush_response, CacheFlush.Success):\\n            pass\\n        elif isinstance(flush_response, CacheFlush.Error):\\n            raise flush_response.inner_exception',\n",
       "   'd': 'Clear the cache.\\n\\nRaises:\\n    SdkException: Momento service or network error',\n",
       "   'l': True,\n",
       "   'g': ['Returns the name of the type of the object.',\n",
       "    'Returns the type name of the current node.',\n",
       "    'Returns the type of the node.',\n",
       "    'Return the name of the type of the object.']},\n",
       "  {'c': '    def lazy_load(self) -> Iterator[chat_loaders.ChatSession]:\\n        import sqlite3\\n\\n        try:\\n            conn = sqlite3.connect(self.db_path)\\n        except sqlite3.OperationalError as e:\\n            raise ValueError(\\n                f\"Could not open iMessage DB file {self.db_path}.\\\\n\"\\n                \"Make sure your terminal emulator has disk access to this file.\\\\n\"\\n                \"   You can either copy the DB file to an accessible location\"\\n                \" or grant full disk access for your terminal emulator.\"\\n                \"  You can grant full disk access for your terminal emulator\"\\n                \" in System Settings > Security and Privacy > Full Disk Access.\"\\n            ) from e\\n        cursor = conn.cursor()\\n\\n\\n        query = \"\"\"SELECT chat_id\\n        FROM message\\n        JOIN chat_message_join ON message.ROWID = chat_message_join.message_id\\n        GROUP BY chat_id\\n        ORDER BY MAX(date) DESC;\"\"\"\\n        cursor.execute(query)\\n        chat_ids = [row[0] for row in cursor.fetchall()]\\n\\n        for chat_id in chat_ids:\\n            yield self._load_single_chat_session(cursor, chat_id)\\n\\n        conn.close()',\n",
       "   'd': 'Lazy load the chat sessions from the iMessage chat.db\\nand yield them in the required format.\\n\\nYields:\\n    ChatSession: Loaded chat session.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Send the status table to the chat.',\n",
       "    'Shows the status of the bot and its RPC.',\n",
       "    'Show the current status of the RPC.']},\n",
       "  {'c': '    def split_text(self, text: str) -> List[Document]:\\n        lines = text.split(\"\\\\n\")\\n\\n        lines_with_metadata: List[LineType] = []\\n\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n\\n\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            for sep, name in self.headers_to_split_on:\\n\\n                if stripped_line.startswith(sep) and (\\n\\n\\n                    len(stripped_line) == len(sep)\\n                    or stripped_line[len(sep)] == \" \"\\n                ):\\n\\n                    if name is not None:\\n\\n                        current_header_level = sep.count(\"\\n\\n\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n\\n\\n                            popped_header = header_stack.pop()\\n\\n\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])\\n\\n\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n\\n                        initial_metadata[name] = header[\"data\"]\\n\\n\\n\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n\\n\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
       "   'd': 'Split markdown file\\nArgs:\\n    text: Markdown file',\n",
       "   'l': True,\n",
       "   'g': ['(self, opt):',\n",
       "    '.\\n    Override the `__call__` method of the `Optimizer` class.',\n",
       "    '.\\n    Arguments:\\n        opt (OptimizerType): optimizer used during training',\n",
       "    \".\\n    Args:\\n        opt (dict): Configuration for training. It contains the following keys:\\n            - n_steps (int): Number of steps\\n            - lr_initial (float): Initial learning rate\\n            - lr_final (float): Final learning rate\\n            - n_epochs (int): Number of epochs\\n            - n_epochs_decay (int): Number of epochs to decay learning rate\\n            - n_epochs_warmup (int): Number of epochs to warm up learning rate\\n            - warmup_factor (float): Warmup factor\\n            - warmup_steps (int): Number of steps to warm up learning rate\\n            - decay_factor (float): Decay factor\\n            - decay_steps (int): Number of steps to decay learning rate\\n            - decay_steps_warmup (int): Number of steps to decay learning rate\\n            - decay_steps_decay (int): Number of steps to decay learning rate\\n            - decay_type (str): Decay type, either 'step' or 'cosine'\\n            - decay_by_epoch (bool): Decay learning rate by epoch\\n            - n_classes (int): Number of classes\\n            - n_channels (int): Number\"]},\n",
       "  {'c': '    def __add__(self, other: Any) -> ChatPromptTemplate:\\n        if isinstance(other, ChatPromptTemplate):\\n            return ChatPromptTemplate(messages=self.messages + other.messages)\\n        elif isinstance(\\n            other, (BaseMessagePromptTemplate, BaseMessage, BaseChatPromptTemplate)\\n        ):\\n            return ChatPromptTemplate(messages=self.messages + [other])\\n        elif isinstance(other, (list, tuple)):\\n            _other = ChatPromptTemplate.from_messages(other)\\n            return ChatPromptTemplate(messages=self.messages + _other.messages)\\n        elif isinstance(other, str):\\n            prompt = HumanMessagePromptTemplate.from_template(other)\\n            return ChatPromptTemplate(messages=self.messages + [prompt])\\n        else:\\n            raise NotImplementedError(f\"Unsupported operand type for +: {type(other)}\")',\n",
       "   'd': 'Combine two prompt templates.\\n\\nArgs:\\n    other: Another prompt template.\\n\\nReturns:\\n    Combined prompt template.',\n",
       "   'l': True,\n",
       "   'g': ['This method allows to set the scope of the bot command to a specific chat.\\n    \\n    Args:\\n        chat_id (str): Unique identifier for the target chat or username of the target channel (in the format @channelusername)\\n        user_id (int): User identifier of the target user\\n\\n    Returns:\\n        BotCommandScopeChatMember: The BotCommandScopeChatMember object',\n",
       "    '',\n",
       "    'Represents the scope of a bot command, covering a specific chat member.',\n",
       "    '']},\n",
       "  {'c': 'def put(self, url: str, data: Dict[str, Any], **kwargs: Any) -> str:\\n    return self.requests.put(url, data, **kwargs).text',\n",
       "   'd': 'Validate that chains are all single input/output.',\n",
       "   'l': False,\n",
       "   'g': ['.',\n",
       "    '.\\n    Returns a YowStack with the default layers for a given layer, axolotl, groups, media, privacy, profiles.\\n\\n    Args:\\n        layer (Layer): Layer to add to the stack.\\n        axolotl (bool): Whether to use the axolotl stack.\\n        groups (bool): Whether to use the groups stack.\\n        media (bool): Whether to use the media stack.\\n        privacy (bool): Whether to use the privacy stack.\\n        profiles (bool): Whether to use the profiles stack.\\n\\n    Returns:\\n        YowStack: YowStack with the default layers.',\n",
       "    '.',\n",
       "    '.']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\\n\\n        examples = self._get_examples(**kwargs)\\n        examples = [\\n            {k: e[k] for k in self.example_prompt.input_variables} for e in examples\\n        ]\\n\\n        example_strings = [\\n            self.example_prompt.format(**example) for example in examples\\n        ]\\n\\n        pieces = [self.prefix, *example_strings, self.suffix]\\n        template = self.example_separator.join([piece for piece in pieces if piece])\\n\\n\\n        return DEFAULT_FORMATTER_MAPPING[self.template_format](template, **kwargs)',\n",
       "   'd': 'Test end to end construction and search without metadata.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    Run the model and return the answer.',\n",
       "    '.\\n    Run the model with the given prompt.\\n\\n    Returns:\\n        str: The answer to the prompt.',\n",
       "    '.\\n    Runs the graph.\\n\\n    Returns:\\n        str: The answer to the prompt.',\n",
       "    '.\\n    Run the agent and return the answer.']},\n",
       "  {'c': 'def on_tool_error(\\n    self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\\n) -> None:\\n    self.step += 1\\n    self.errors += 1',\n",
       "   'd': 'Create a class from a string template.\\n\\nArgs:\\n    template: a template.\\n    template_format: format of the template.\\n    **kwargs: keyword arguments to pass to the constructor.\\n\\nReturns:\\n    A new instance of this class.',\n",
       "   'l': False,\n",
       "   'g': ['Forward pass of the network.\\n    \\n    Args:\\n        x: Input tensor.\\n    \\n    Returns:\\n        Output tensor.',\n",
       "    'Forward pass of the network\\n    \\n    Args:\\n        x: input tensor\\n\\n    Returns:\\n        out: output tensor',\n",
       "    'Forward function\\n    \\n    Parameters:\\n    -----------\\n    x : torch.Tensor\\n        Input tensor\\n\\n    Returns:\\n    --------\\n    torch.Tensor\\n        Output tensor',\n",
       "    'Forward pass of the network.\\n    \\n    Parameters:\\n    x (torch.Tensor): Input tensor.\\n\\n    Returns:\\n    torch.Tensor: Output tensor.']},\n",
       "  {'c': 'def _chain_type(self) -> str:\\n    return \"retrieval_qa\"',\n",
       "   'd': 'Return the chain type.',\n",
       "   'l': True,\n",
       "   'g': ['Return the Mel filters.\\n    \\n    Args:\\n        device: torch.device\\n        n_mels: int\\n    \\n    Returns:\\n        torch.Tensor',\n",
       "    'Get the Mel filters.\\n    \\n    Args:\\n        n_mels (int): Number of filters.\\n    \\n    Returns:\\n        torch.Tensor: Mel filters.',\n",
       "    'Create a Mel-filters tensor.\\n    \\n    Args:\\n        device (torch.device): The device to create the tensor on.\\n        n_mels (int, optional): The number of Mel-filters to create. Defaults to N_MELS.\\n    \\n    Returns:\\n        torch.Tensor: The Mel-filters tensor.',\n",
       "    'Create a John filterbank.\\n\\n    Parameters\\n    ----------\\n    device: torch.device\\n        Device to create the filterbank on.\\n    n_mels: int\\n        Number of filters in the filterbank.\\n    \\n    Returns\\n    -------\\n    torch.Tensor\\n        Tensor of shape (n_mels, n_fft // 2 + 1).']},\n",
       "  {'c': '    def clear(self, **kwargs: Any) -> None:\\n        from momento.responses import CacheFlush\\n\\n        flush_response = self.cache_client.flush_cache(self.cache_name)\\n        if isinstance(flush_response, CacheFlush.Success):\\n            pass\\n        elif isinstance(flush_response, CacheFlush.Error):\\n            raise flush_response.inner_exception',\n",
       "   'd': 'Return default prompt.',\n",
       "   'l': False,\n",
       "   'g': ['Returns the format profile for the given itag.',\n",
       "    'Get the format profile for the given itag. \\n    \\n    :param itag: the itag of the video to get the format profile for\\n    :return: a dictionary with the format profile',\n",
       "    'Get video format profile.\\n    \\n    :param itag: itag of video\\n    :type itag: int\\n    :return: dict',\n",
       "    'Get format profile for a given itag']},\n",
       "  {'c': 'def __init__(self, database_path: str = \".langchain.db\"):\\n    engine = create_engine(f\"sqlite:///{database_path}\")\\n    super().__init__(engine)',\n",
       "   'd': 'Initialize by creating the engine and all tables.',\n",
       "   'l': True,\n",
       "   'g': ['Adds words from a given URL to a given set of words.\\n    \\n    Args:\\n        word_set (set): The set of words to add to.\\n        url (str): The URL to read words from.\\n        line_function (callable): A function that takes a line of text and returns a word.',\n",
       "    'Adds words to a word set from a given url. \\n    \\n    :param word_set: The word set to add words to.\\n    :param url: The url to get words from.\\n    :param line_function: The function to use to process each line.\\n    :return: None',\n",
       "    'Adds words from the specified url to the word_set. \\n    The word_set must be a set. \\n    The url must be a string. \\n    The line_function must be a function that takes a string and returns a string.',\n",
       "    'Adds words from the given URL to the given word set.\\n\\n    Args:\\n        word_set (set): A set of words.\\n        url (str): The URL from which to read words.\\n        line_function (function): A function that takes a line from a URL and returns a word.\\n\\n    Returns:\\n        None']},\n",
       "  {'c': 'def _get_docs(\\n    self,\\n    inputs: Dict[str, Any],\\n    *,\\n    run_manager: CallbackManagerForChainRun,\\n) -> List[Document]:\\n',\n",
       "   'd': 'Get docs to run questioning over.',\n",
       "   'l': True,\n",
       "   'g': ['This method is used to run the code.',\n",
       "    '.\\n    Run the function.',\n",
       "    '.',\n",
       "    '.\\n    Runs the given x and y.\\n\\n    :param x: The first value.\\n    :param y: The second value.\\n    :return: The result of x and y.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\\n        embedding_function: Optional[Embeddings] = None,\\n        persist_directory: Optional[str] = None,\\n        client_settings: Optional[chromadb.config.Settings] = None,\\n        collection_metadata: Optional[Dict] = None,\\n        client: Optional[chromadb.Client] = None,\\n        relevance_score_fn: Optional[Callable[[float], float]] = None,\\n    ) -> None:\\n        try:\\n            import chromadb\\n            import chromadb.config\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import chromadb python package. \"\\n                \"Please install it with `pip install chromadb`.\"\\n            )\\n\\n        if client is not None:\\n            self._client_settings = client_settings\\n            self._client = client\\n            self._persist_directory = persist_directory\\n        else:\\n            if client_settings:\\n                _client_settings = client_settings\\n            elif persist_directory:\\n\\n                major, minor, _ = chromadb.__version__.split(\".\")\\n                if int(major) == 0 and int(minor) < 4:\\n                    _client_settings = chromadb.config.Settings(\\n                        chroma_db_impl=\"duckdb+parquet\",\\n                    )\\n                else:\\n                    _client_settings = chromadb.config.Settings(is_persistent=True)\\n                _client_settings.persist_directory = persist_directory\\n            else:\\n                _client_settings = chromadb.config.Settings()\\n            self._client_settings = _client_settings\\n            self._client = chromadb.Client(_client_settings)\\n            self._persist_directory = (\\n                _client_settings.persist_directory or persist_directory\\n            )\\n\\n        self._embedding_function = embedding_function\\n        self._collection = self._client.get_or_create_collection(\\n            name=collection_name,\\n            embedding_function=self._embedding_function.embed_documents\\n            if self._embedding_function is not None\\n            else None,\\n            metadata=collection_metadata,\\n        )\\n        self.override_relevance_score_fn = relevance_score_fn',\n",
       "   'd': 'Initialize with Chroma client.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Returns a list of users that the current user has requested to follow.',\n",
       "    'Lookup friendships between the current user and other users.\\n    \\n    :param user_id: The ID of the user to lookup\\n    :param screen_name: The screen name of the user to lookup\\n    :return: A list of :class:`~twitter.models.Relationship` objects',\n",
       "    'Lookup friendships between the current user and other users.']},\n",
       "  {'c': 'def type(self) -> str:\\n    return \"function\"',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': False,\n",
       "   'g': ['x\\n    :param jshost:\\n    :param echarts_template_dir:\\n    :param force_js_embed:\\n    :return:',\n",
       "    \"Configure the echarts-js-embed library.\\n\\n    :param jshost:\\n        The URL of the host that provides the echarts-js-embed library.\\n        Defaults to 'https://echarts.apache.org/assets/echarts.min.js'.\\n    :param echarts_template_dir:\\n        The directory where the echarts-js-embed library is located.\\n        Defaults to 'https://echarts.apache.org/assets/echarts.min.js'.\\n    :param force_js_embed:\\n        Whether to force the echarts-js-embed library to be embedded.\\n        Defaults to True.\",\n",
       "    '_summary_\\n\\n    Args:\\n        jshost (str, optional): _description_. Defaults to None.\\n        echarts_template_dir (str, optional): _description_. Defaults to None.\\n        force_js_embed (bool, optional): _description_. Defaults to None.\\n\\n    Returns:\\n        _type_: _description_',\n",
       "    'Configure the jshost, echarts_template_dir, and force_js_embed.\\n    \\n    Parameters\\n    ----------\\n    jshost : str\\n        The URL of the jshost.\\n    echarts_template_dir : str\\n        The directory containing the echarts template files.\\n    force_js_embed : bool\\n        Whether to force embedding the jshost.\\n    \\n    Returns\\n    -------\\n    None']},\n",
       "  {'c': 'def _run(\\n    self, text: str, run_manager: Optional[CallbackManagerForToolRun] = None\\n) -> str:\\n    try:\\n        data = _parse_input(text)\\n        return self.requests_wrapper.put(_clean_url(data[\"url\"]), data[\"data\"])\\n    except Exception as e:\\n        return repr(e)',\n",
       "   'd': 'Combine two prompt templates.\\n\\nArgs:\\n    other: Another prompt template.\\n\\nReturns:\\n    Combined prompt template.',\n",
       "   'l': False,\n",
       "   'g': ['Convert a SearchDoc into a SavedSearchDoc.\\n    \\n    Args:\\n        cls (SavedSearchDoc): The type of the SavedSearchDoc to return.\\n        search_doc (SearchDoc): The SearchDoc to convert.\\n        db_doc_id (int, optional): The id of the document in the database. Defaults to 0.\\n    \\n    Returns:\\n        SavedSearchDoc: The SavedSearchDoc.',\n",
       "    '_summary_\\n\\n    Args:\\n        cls (_type_): _description_\\n        search_doc (SearchDoc): _description_\\n        db_doc_id (int, optional): _description_. Defaults to 0.\\n\\n    Returns:\\n        _type_: _description_',\n",
       "    'Creates a SavedSearchDoc object from a SearchDoc object.\\n    \\n    Args:\\n        cls (SavedSearchDoc): The class to create the object from.\\n        search_doc (SearchDoc): The SearchDoc object to create the object from.\\n        db_doc_id (int, optional): The ID of the document in the database. Defaults to 0.\\n    \\n    Returns:\\n        SavedSearchDoc: The SavedSearchDoc object.',\n",
       "    'Convert a SearchDoc into a SavedSearchDoc. \\n    \\n    :param cls: The class of the SavedSearchDoc to create.\\n    :param search_doc: The SearchDoc to convert.\\n    :param db_doc_id: The ID of the document in the database.\\n    :return: A SavedSearchDoc.']},\n",
       "  {'c': 'def _run(\\n    self, url: str, run_manager: Optional[CallbackManagerForToolRun] = None\\n) -> str:\\n    return self.requests_wrapper.get(_clean_url(url))',\n",
       "   'd': 'Type of the message, used for serialization.',\n",
       "   'l': False,\n",
       "   'g': ['Returns the interval label for a given frequency.\\n    \\n    Parameters\\n    ----------\\n    x : float\\n        Frequency of the interval.\\n    pos : Optional[int], optional\\n        Position of the interval in the octave, by default None.\\n\\n    Returns\\n    -------\\n    str\\n        Interval label.',\n",
       "    'Returns the FJS label for the given pitch.',\n",
       "    \"'Call' a note with optional position.\\n\\n    Parameters\\n    ----------\\n    x: float\\n        The note number to call.\\n    pos: Optional[int]\\n        The octave position of the note.\\n\\n    Returns\\n    -------\\n    str\\n        The lab note.\\n\\n    Examples\\n    --------\\n    >>> from py_lab import Lab\\n    >>> lab = Lab()\\n    >>> lab(0)\\n    'C'\\n    >>> lab(0, 0)\\n    'C'\\n    >>> lab(0, 1)\\n    'D'\\n    >>> lab(0, 2)\\n    'E'\\n    >>> lab(0, 3)\\n    'F'\\n    >>> lab(0, 4)\\n    'G'\\n    >>> lab(0, 5)\\n    'A'\\n    >>> lab(0, 6)\\n    'B'\\n    >>> lab(0, 7)\\n    'C'\\n    >>> lab(1, 0)\\n    'C#'\\n    >>> lab(1, 1)\\n    'D#'\\n    >>> lab(1, 2)\",\n",
       "    'Return the lab representation of the interval.']},\n",
       "  {'c': '    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n        for gen in return_val:\\n            if not isinstance(gen, Generation):\\n                raise ValueError(\\n                    \"RedisCache only supports caching of normal LLM generations, \"\\n                    f\"got {type(gen)}\"\\n                )\\n\\n        key = self._key(prompt, llm_string)\\n\\n        with self.redis.pipeline() as pipe:\\n            pipe.hset(\\n                key,\\n                mapping={\\n                    str(idx): dumps(generation)\\n                    for idx, generation in enumerate(return_val)\\n                },\\n            )\\n            if self.ttl is not None:\\n                pipe.expire(key, self.ttl)\\n\\n            pipe.execute()',\n",
       "   'd': 'Update cache based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': ['forward function',\n",
       "    '.',\n",
       "    'Args:\\n\\n    inputs (torch.Tensor): input tensor\\n\\n    Returns:\\n\\n    torch.Tensor: output tensor',\n",
       "    '(str) -> str']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        aleph_alpha_api_key = get_from_dict_or_env(\\n            values, \"aleph_alpha_api_key\", \"ALEPH_ALPHA_API_KEY\"\\n        )\\n        try:\\n            import aleph_alpha_client\\n\\n            values[\"client\"] = aleph_alpha_client.Client(token=aleph_alpha_api_key)\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import aleph_alpha_client python package. \"\\n                \"Please install it with `pip install aleph_alpha_client`.\"\\n            )\\n        return values',\n",
       "   'd': 'Validate that api key and python package exists in environment.',\n",
       "   'l': True,\n",
       "   'g': ['Get all songs',\n",
       "    'Returns all songs in the database.',\n",
       "    'Gets all songs from the database',\n",
       "    'Returns all songs from the database.']},\n",
       "  {'c': 'def run_on_dataset(\\n    client: Client,\\n    dataset_name: str,\\n    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,\\n    *,\\n    evaluation: Optional[RunEvalConfig] = None,\\n    concurrency_level: int = 5,\\n    project_name: Optional[str] = None,\\n    verbose: bool = False,\\n    tags: Optional[List[str]] = None,\\n    input_mapper: Optional[Callable[[Dict], Any]] = None,\\n    **kwargs: Any,\\n) -> Dict[str, Any]:\\n    if kwargs:\\n        warnings.warn(\\n            \"The following arguments are deprecated and \"\\n            \"will be removed in a future release: \"\\n            f\"{kwargs.keys()}.\",\\n            DeprecationWarning,\\n        )\\n    wrapped_model, project_name, dataset, examples = _prepare_eval_run(\\n        client, dataset_name, llm_or_chain_factory, project_name\\n    )\\n    if concurrency_level in (0, 1):\\n        results = _run_on_examples(\\n            client,\\n            examples,\\n            wrapped_model,\\n            project_name=project_name,\\n            verbose=verbose,\\n            tags=tags,\\n            evaluation=evaluation,\\n            input_mapper=input_mapper,\\n            data_type=dataset.data_type,\\n        )\\n    else:\\n\\n        coro = _arun_on_examples(\\n            client,\\n            examples,\\n            wrapped_model,\\n            concurrency_level=concurrency_level,\\n            project_name=project_name,\\n            verbose=verbose,\\n            tags=tags,\\n            evaluation=evaluation,\\n            input_mapper=input_mapper,\\n            data_type=dataset.data_type,\\n        )\\n        results = _handle_coroutine(coro)\\n    return {\\n        \"project_name\": project_name,\\n        \"results\": results,\\n    }',\n",
       "   'd': 'Convert a raw function/class to an OpenAI function.\\n\\nArgs:\\n    function: Either a dictionary, a pydantic.BaseModel class, or a Python function.\\n        If a dictionary is passed in, it is assumed to already be a valid OpenAI\\n        function.\\n\\nReturns:\\n    A dict version of the passed in function which is compatible with the\\n        OpenAI function-calling API.',\n",
       "   'l': False,\n",
       "   'g': ['This function encrypts or decrypts a given text using a given cipher.\\n    \\n    Args:\\n        greppable (bool): Whether the text should be greppable.\\n        Cipher (str): The name of the cipher to use.\\n        text (str): The text to encrypt or decrypt.\\n        debug (bool): Whether to print debug messages.\\n    \\n    Returns:\\n        str: The decrypted or encrypted text.',\n",
       "    'This function will call the encryption class and return the encrypted text.\\n    \\n    Args:\\n        greppable (bool): Whether or not to grep the text\\n        Cipher (class): The class to use for encryption\\n        text (str): The text to encrypt\\n        debug (bool): Whether or not to print debug information\\n    \\n    Returns:\\n        str: The encrypted text',\n",
       "    'This function will encrypt or decrypt a string\\n    :param text: The text to be encrypted or decrypted\\n    :param Cipher: The cipher to be used for encryption or decryption\\n    :param greppable: Whether or not the text is greppable\\n    :param debug: Whether or not to print debug information\\n    :return: The decrypted or encrypted text',\n",
       "    'A function that encrypts or decrypts a given text.\\n    \\n    Args:\\n        greppable (bool, optional): If True, the text is encrypted with the given Cipher.\\n        Cipher (class, optional): The class that is used to encrypt the text.\\n        text (str, optional): The text to be encrypted or decrypted.\\n        debug (bool, optional): If True, the function prints the text to be encrypted or decrypted.\\n    \\n    Returns:\\n        str: The encrypted or decrypted text.']},\n",
       "  {'c': 'def _chain_type(self) -> str:\\n    return \"vector_db_qa\"',\n",
       "   'd': 'Return the chain type.',\n",
       "   'l': True,\n",
       "   'g': ['函数说明:对输入数据进行分类,返回分类结果\\n    Parameters:\\n        datToClass - 输入数据\\n        classifierArr - 训练好的分类器\\n    Returns:\\n        classEst - 分类结果',\n",
       "    'adaClassify(datToClass,classifierArr)\\n    \\n    Parameters\\n    ----------\\n    datToClass : ndarray\\n        The data to classify.\\n    classifierArr : list\\n        The list of classifiers.\\n    \\n    Returns\\n    -------\\n    list\\n        The list of classifications.',\n",
       "    'Function: adaClassify\\n    \\n    Description:\\n    \\n    Parameters:\\n    \\n    Returns:',\n",
       "    'This function classifies a given data matrix by a given list of classifiers.\\n    \\n    Parameters:\\n    datToClass: a matrix of data to be classified.\\n    classifierArr: a list of classifiers.\\n    \\n    Returns:\\n    classEst: a matrix of class estimates.']},\n",
       "  {'c': '    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        *,\\n        prompt: Optional[PromptTemplate] = None,\\n        criteria: Optional[Union[CRITERIA_TYPE, str]] = None,\\n        **kwargs: Any,\\n    ) -> PairwiseStringEvalChain:\\n        expected_input_vars = {\"prediction\", \"prediction_b\", \"input\", \"criteria\"}\\n        prompt_ = prompt or PROMPT\\n        if expected_input_vars != set(prompt_.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt_.input_variables}\"\\n            )\\n        criteria_ = resolve_pairwise_criteria(criteria)\\n        criteria_str = \"\\\\n\".join(f\"{k}: {v}\" if v else k for k, v in criteria_.items())\\n        return cls(llm=llm, prompt=prompt_.partial(criteria=criteria_str), **kwargs)',\n",
       "   'd': 'Expect input key.\\n\\n:meta private:',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'vocab_size: size of the vocabulary\\n    num_hiddens: size of the hidden state\\n    device: cpu or gpu\\n    get_params: function to get the parameters of the model\\n    init_state: function to initialize the hidden state\\n    forward_fn: function to compute the forward pass of the model',\n",
       "    '',\n",
       "    'Initialize the RNN.\\n    \\n    Arguments:\\n    vocab_size -- integer, size of the vocabulary\\n    num_hiddens -- integer, number of hidden units\\n    device -- torch.device, device to use for computation (cpu or gpu)\\n    get_params -- function, function to get the model parameters\\n    init_state -- function, function to initialize the hidden state\\n    forward_fn -- function, function to perform a forward pass of the model (used in training)']},\n",
       "  {'c': 'def generate(\\n    self,\\n    prompts: List[str],\\n    stop: Optional[List[str]] = None,\\n    callbacks: Callbacks = None,\\n    *,\\n    tags: Optional[List[str]] = None,\\n    metadata: Optional[Dict[str, Any]] = None,\\n    **kwargs: Any,\\n) -> LLMResult:\\n    if not isinstance(prompts, list):\\n        raise ValueError(\\n            \"Argument \\'prompts\\' is expected to be of type List[str], received\"\\n            f\" argument of type {type(prompts)}.\"\\n        )\\n    params = self.dict()\\n    params[\"stop\"] = stop\\n    options = {\"stop\": stop}\\n    (\\n        existing_prompts,\\n        llm_string,\\n        missing_prompt_idxs,\\n        missing_prompts,\\n    ) = get_prompts(params, prompts)\\n    disregard_cache = self.cache is not None and not self.cache\\n    callback_manager = CallbackManager.configure(\\n        callbacks,\\n        self.callbacks,\\n        self.verbose,\\n        tags,\\n        self.tags,\\n        metadata,\\n        self.metadata,\\n    )\\n    new_arg_supported = inspect.signature(self._generate).parameters.get(\\n        \"run_manager\"\\n    )\\n    if langchain.llm_cache is None or disregard_cache:\\n        if self.cache is not None and self.cache:\\n            raise ValueError(\\n                \"Asked to cache, but no cache found at `langchain.cache`.\"\\n            )\\n        run_managers = callback_manager.on_llm_start(\\n            dumpd(self), prompts, invocation_params=params, options=options\\n        )\\n        output = self._generate_helper(\\n            prompts, stop, run_managers, bool(new_arg_supported), **kwargs\\n        )\\n        return output\\n    if len(missing_prompts) > 0:\\n        run_managers = callback_manager.on_llm_start(\\n            dumpd(self), missing_prompts, invocation_params=params, options=options\\n        )\\n        new_results = self._generate_helper(\\n            missing_prompts, stop, run_managers, bool(new_arg_supported), **kwargs\\n        )\\n        llm_output = update_cache(\\n            existing_prompts, llm_string, missing_prompt_idxs, new_results, prompts\\n        )\\n        run_info = (\\n            [RunInfo(run_id=run_manager.run_id) for run_manager in run_managers]\\n            if run_managers\\n            else None\\n        )\\n    else:\\n        llm_output = {}\\n        run_info = None\\n    generations = [existing_prompts[i] for i in range(len(prompts))]\\n    return LLMResult(generations=generations, llm_output=llm_output, run=run_info)',\n",
       "   'd': 'The type of output this runnable produces specified as a type annotation.',\n",
       "   'l': False,\n",
       "   'g': ['Uppercase the string or list of strings. \\n    \\n    Parameters\\n    ----------\\n    v : str or list of str or set of str\\n    \\n    Returns\\n    -------\\n    str or list of str or set of str',\n",
       "    ':param cls: \\n    :param v: \\n    :return:',\n",
       "    '_summary_\\n\\n    Args:\\n        cls (type): _description_\\n        v (Union[str, List[str], Set[str]]): _description_\\n\\n    Returns:\\n        Union[str, List[str], Set[str]]: _description_',\n",
       "    '_summary_\\n\\n    Args:\\n        cls (_type_): _description_\\n        v (Union[str, List[str], Set[str]]): _description_\\n\\n    Returns:\\n        Union[str, List[str], Set[str]]: _description_']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: List[str],\\n        metadatas: Optional[List[dict]] = None,\\n        doc_metadata: Optional[dict] = None,\\n    ) -> None:\\n        self.vectorstore.add_texts(texts, metadatas, doc_metadata or {})',\n",
       "   'd': 'Lazily parse the blob.',\n",
       "   'l': False,\n",
       "   'g': ['Ungroup the expression.',\n",
       "    '::str -> TokenConverter\\n\\n    Convert an expression into a TokenConverter.',\n",
       "    'Ungroup a nested grouping expression.\\n    \\n    >>> ungroup(\"(a b c)\")\\n    \\'abc\\'\\n    >>> ungroup(\"(a (b c) d)\")\\n    \\'abcdc\\'\\n    >>> ungroup(\"(a (b c (d e f)) g)\")\\n    \\'abdcdefg\\'\\n    >>> ungroup(\"(a (b (c (d e (f g)))) h)\")\\n    \\'abcefgdh\\'\\n    >>> ungroup(\"(a (b (c (d (e (f (g (h)))))))))\")\\n    \\'abcefgdh\\'\\n    >>> ungroup(\"(a (b (c (d (e (f (g (h)))))))))\")\\n    \\'abcefgdh\\'\\n    >>> ungroup(\"(a (b (c (d (e (f (g (h)))))))))\")\\n    \\'abcefgdh\\'\\n    >>> ungroup(\"(a (b (c (d (e (f (g (h)))))))))\")\\n    \\'abcefgdh\\'\\n    >>> ungroup(\"(a (b (c (d (e (f (g (h)))))))))\")\\n    \\'abce',\n",
       "    'Convert a string into a list of tokens.']},\n",
       "  {'c': '    def on_tool_error(\\n        self,\\n        error: BaseException,\\n        *,\\n        run_id: UUID,\\n        **kwargs: Any,\\n    ) -> None:\\n        if not run_id:\\n            raise TracerException(\"No run_id provided for on_tool_error callback.\")\\n        tool_run = self.run_map.get(str(run_id))\\n        if tool_run is None or tool_run.run_type != \"tool\":\\n            raise TracerException(f\"No tool Run found to be traced for {run_id}\")\\n\\n        tool_run.error = repr(error)\\n        tool_run.end_time = datetime.utcnow()\\n        tool_run.events.append({\"name\": \"error\", \"time\": tool_run.end_time})\\n        self._end_trace(tool_run)\\n        self._on_tool_error(tool_run)',\n",
       "   'd': 'Handle an error for a tool run.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Dispatches the input to the dispatcher.\\n    Args:\\n      inp: the input to dispatch.\\n    Returns:\\n      the output of the dispatcher.',\n",
       "    '.\\n    Dispatches the input to the registered dispatchers.\\n    Args:\\n      inp: input to be dispatched.\\n    Returns:\\n      The output of the registered dispatchers.',\n",
       "    '.dispatch(self, inp)\\n\\n    Dispatch the inp to the dispatchers.\\n\\n    Args:\\n      inp: A Tensor.\\n\\n    Returns:\\n      A Tensor.',\n",
       "    '.']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\\n\\n        examples = self._get_examples(**kwargs)\\n        examples = [\\n            {k: e[k] for k in self.example_prompt.input_variables} for e in examples\\n        ]\\n\\n        example_strings = [\\n            self.example_prompt.format(**example) for example in examples\\n        ]\\n\\n        pieces = [self.prefix, *example_strings, self.suffix]\\n        template = self.example_separator.join([piece for piece in pieces if piece])\\n\\n\\n        return DEFAULT_FORMATTER_MAPPING[self.template_format](template, **kwargs)',\n",
       "   'd': 'Construct FAISS wrapper from raw documents.\\n\\nThis is a user friendly interface that:\\n    1. Embeds documents.\\n    2. Creates an in memory docstore\\n    3. Initializes the FAISS database\\n\\nThis is intended to be a quick way to get started.\\n\\nExample:\\n    .. code-block:: python\\n\\n        from langchain import FAISS\\n        from langchain.embeddings import OpenAIEmbeddings\\n\\n        embeddings = OpenAIEmbeddings()\\n        text_embeddings = embeddings.embed_documents(texts)\\n        text_embedding_pairs = zip(texts, text_embeddings)\\n        faiss = FAISS.from_embeddings(text_embedding_pairs, embeddings)',\n",
       "   'l': False,\n",
       "   'g': ['_summary_\\n    Build backbone from config.\\n    Args:\\n        cfg (ConfigDict): Config object.\\n        default_args (dict, optional): Default arguments to be used for building the backbone. Defaults to None.\\n    Returns:\\n        nn.Module: The backbone.',\n",
       "    'Build a backbone from a cfg dict.\\n    \\n    Args:\\n        cfg (dict): A dictionary containing the configuration for the backbone.\\n        default_args (dict, optional): A dictionary containing the default arguments for the backbone. Defaults to None.\\n    \\n    Returns:\\n        Backbone: A backbone object.',\n",
       "    'Build a backbone.',\n",
       "    '_summary_\\n    Build backbone\\n    Args:\\n        cfg (ConfigDict): config\\n        default_args (dict, optional): Defaults to None.\\n    Returns:\\n        nn.Module: backbone']},\n",
       "  {'c': '    def lazy_parse(self, blob: Blob) -> Iterator[Document]:\\n        url_parse_result = urlparse(str(blob.path)) if blob.path else None\\n\\n        if (\\n            url_parse_result\\n            and url_parse_result.scheme == \"s3\"\\n            and url_parse_result.netloc\\n        ):\\n            textract_response_json = self.tc.call_textract(\\n                input_document=str(blob.path),\\n                features=self.textract_features,\\n                boto3_textract_client=self.boto3_textract_client,\\n            )\\n        else:\\n            textract_response_json = self.tc.call_textract(\\n                input_document=blob.as_bytes(),\\n                features=self.textract_features,\\n                call_mode=self.tc.Textract_Call_Mode.FORCE_SYNC,\\n                boto3_textract_client=self.boto3_textract_client,\\n            )\\n\\n        current_text = \"\"\\n        current_page = 1\\n        for block in textract_response_json[\"Blocks\"]:\\n            if \"Page\" in block and not (int(block[\"Page\"]) == current_page):\\n                yield Document(\\n                    page_content=current_text,\\n                    metadata={\"source\": blob.source, \"page\": current_page},\\n                )\\n                current_text = \"\"\\n                current_page = int(block[\"Page\"])\\n            if \"Text\" in block:\\n                current_text += block[\"Text\"] + \" \"\\n\\n        yield Document(\\n            page_content=current_text,\\n            metadata={\"source\": blob.source, \"page\": current_page},\\n        )',\n",
       "   'd': 'Iterates over the Blob pages and returns an Iterator with a Document\\nfor each page, like the other parsers If multi-page document, blob.path\\nhas to be set to the S3 URI and for single page docs the blob.data is taken',\n",
       "   'l': True,\n",
       "   'g': ['(self, number=10, le=False, ge=False) -> int:\\n\\n    Return a random number between 10 and 100, if le and ge are False,\\n    else between 100 and 140.\\n\\n    Parameters\\n    ----------\\n    number : int, optional\\n        Number of elements in the list. The default is 10.\\n    le : bool, optional\\n        If True, the random number will be between 100 and 140. The default is False.\\n    ge : bool, optional\\n        If True, the random number will be between 10 and 100. The default is False.\\n\\n    Returns\\n    -------\\n    int\\n        A random number between 10 and 100, if le and ge are False,\\n        else between 100 and 140.',\n",
       "    '.\\n    Returns a random number between 10 and 100.',\n",
       "    \"Fonction qui permet de randomiser le nombre d'éléments d'une liste\",\n",
       "    '.\\n    Returns a random number between 10 and 100.']},\n",
       "  {'c': 'def load_agent(\\n    path: Union[str, Path], **kwargs: Any\\n) -> Union[BaseSingleActionAgent, BaseMultiActionAgent]:\\n    valid_suffixes = {\"json\", \"yaml\"}\\n    if hub_result := try_load_from_hub(\\n        path, _load_agent_from_file, \"agents\", valid_suffixes\\n    ):\\n        return hub_result\\n    else:\\n        return _load_agent_from_file(path, **kwargs)',\n",
       "   'd': 'Create a class from a string template.\\n\\nArgs:\\n    template: a template.\\n    template_format: format of the template.\\n    **kwargs: keyword arguments to pass to the constructor.\\n\\nReturns:\\n    A new instance of this class.',\n",
       "   'l': False,\n",
       "   'g': ['Converts an XML node to a dictionary.\\n    \\n    :param xml: XML node to convert\\n    :return: dictionary',\n",
       "    'Converts an XML node to a Python dictionary.\\n    \\n    Args:\\n        xml (xml.dom.minidom.Element): The XML node to convert.\\n    \\n    Returns:\\n        dict: A Python dictionary representing the XML node.',\n",
       "    '_to_dict_',\n",
       "    'Convert a XML node to a dictionary\\n    \\n    :param xml: A XML node\\n    :type xml: xml.dom.minidom.Element\\n    \\n    :return: A dictionary\\n    :rtype: dict']},\n",
       "  {'c': '    def plan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[AgentAction, AgentFinish]:\\n        inputs = {**kwargs, **{\"intermediate_steps\": intermediate_steps}}\\n        output = self.runnable.invoke(inputs, config={\"callbacks\": callbacks})\\n        return output',\n",
       "   'd': 'Define retry mechanism.',\n",
       "   'l': False,\n",
       "   'g': ['This function prepares the sample text for the model.',\n",
       "    'Prepare sample text for chat completion\\n    \\n    Args:\\n        example: dict\\n\\n    Returns:\\n        text: str',\n",
       "    'Prepare sample text for model. \\n    \\n    Args:\\n        example (dict): Example dictionary with prompt and completion. \\n        \\n    Returns:\\n        str: Prepared text.',\n",
       "    'Takes a single example from the dataset and prepares it for use in a model.\\n    \\n    Parameters\\n    ----------\\n    example: dict\\n        A dictionary containing the following keys:\\n        - prompt: The prompt text for the example.\\n        - completion: The completion text for the example.\\n    \\n    Returns\\n    -------\\n    text: str\\n        The prepared text for use in a model.']},\n",
       "  {'c': 'def OutputType(self) -> Any:\\n    func = getattr(self, \"func\", None) or getattr(self, \"afunc\")\\n    try:\\n        sig = inspect.signature(func)\\n        return (\\n            sig.return_annotation\\n            if sig.return_annotation != inspect.Signature.empty\\n            else Any\\n        )\\n    except ValueError:\\n        return Any',\n",
       "   'd': 'Compute query embeddings using a HuggingFace instruct model.\\n\\nArgs:\\n    text: The text to embed.\\n\\nReturns:\\n    Embeddings for the text.',\n",
       "   'l': False,\n",
       "   'g': ['for all agents in the agents list.',\n",
       "    'Kick off the process of the game.',\n",
       "    'Kick off the game.',\n",
       "    '.\\n    The kickoff method is the entry point of the simulation.\\n    It starts the simulation by initializing the agents,\\n    setting the initial positions, and running the simulation loop.']},\n",
       "  {'c': 'def validate_chains(cls, values: Dict) -> Dict:\\n    for chain in values[\"chains\"]:\\n        if len(chain.input_keys) != 1:\\n            raise ValueError(\\n                \"Chains used in SimplePipeline should all have one input, got \"\\n                f\"{chain} with {len(chain.input_keys)} inputs.\"\\n            )\\n        if len(chain.output_keys) != 1:\\n            raise ValueError(\\n                \"Chains used in SimplePipeline should all have one output, got \"\\n                f\"{chain} with {len(chain.output_keys)} outputs.\"\\n            )\\n    return values',\n",
       "   'd': 'Execute the query, return the results or an error message.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Logs the current status of the server',\n",
       "    'Capture logs from the `up` method and return a list of the events.',\n",
       "    '']},\n",
       "  {'c': '    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        collection_name: str = \"LangChainCollection\",\\n        connection_args: dict[str, Any] = {},\\n        consistency_level: str = \"Session\",\\n        index_params: Optional[dict] = None,\\n        search_params: Optional[dict] = None,\\n        drop_old: bool = False,\\n        **kwargs: Any,\\n    ) -> Zilliz:\\n        vector_db = cls(\\n            embedding_function=embedding,\\n            collection_name=collection_name,\\n            connection_args=connection_args,\\n            consistency_level=consistency_level,\\n            index_params=index_params,\\n            search_params=search_params,\\n            drop_old=drop_old,\\n            **kwargs,\\n        )\\n        vector_db.add_texts(texts=texts, metadatas=metadatas)\\n        return vector_db',\n",
       "   'd': 'Create a Zilliz collection, indexes it with HNSW, and insert data.\\n\\nArgs:\\n    texts (List[str]): Text data.\\n    embedding (Embeddings): Embedding function.\\n    metadatas (Optional[List[dict]]): Metadata for each text if it exists.\\n        Defaults to None.\\n    collection_name (str, optional): Collection name to use. Defaults to\\n        \"LangChainCollection\".\\n    connection_args (dict[str, Any], optional): Connection args to use. Defaults\\n        to DEFAULT_MILVUS_CONNECTION.\\n    consistency_level (str, optional): Which consistency level to use. Defaults\\n        to \"Session\".\\n    index_params (Optional[dict], optional): Which index_params to use.\\n        Defaults to None.\\n    search_params (Optional[dict], optional): Which search params to use.\\n        Defaults to None.\\n    drop_old (Optional[bool], optional): Whether to drop the collection with\\n        that name if it exists. Defaults to False.\\n\\nReturns:\\n    Zilliz: Zilliz Vector Store',\n",
       "   'l': True,\n",
       "   'g': ['Trains an autoencoder for the given problem and returns the model.',\n",
       "    'Trains an autoencoder on the pong dataset.',\n",
       "    '@param problem_name:\\n    @param data_dir:\\n    @param output_dir:\\n    @param hparams:\\n    @param epoch:\\n    @return:',\n",
       "    '@param problem_name:\\n    @param data_dir:\\n    @param output_dir:\\n    @param hparams:\\n    @param epoch:\\n    @return:']},\n",
       "  {'c': '    def on_chain_end(self, outputs: Union[Dict[str, Any], Any], **kwargs: Any) -> None:\\n        handle_event(\\n            self.handlers,\\n            \"on_chain_end\",\\n            \"ignore_chain\",\\n            outputs,\\n            run_id=self.run_id,\\n            parent_run_id=self.parent_run_id,\\n            tags=self.tags,\\n            **kwargs,\\n        )',\n",
       "   'd': 'Run when chain ends running.\\n\\nArgs:\\n    outputs (Union[Dict[str, Any], Any]): The outputs of the chain.',\n",
       "   'l': True,\n",
       "   'g': ['_forward',\n",
       "    'forward pass\\n    \\n    Args:\\n        x: input tensor, shape [batch_size, input_size]\\n    \\n    Returns:\\n        out: output tensor, shape [batch_size, output_size]',\n",
       "    'Forward pass of the model.\\n\\n    Args:\\n        x: [batch_size, input_size]\\n    Returns:\\n        out: [batch_size, output_size]',\n",
       "    'Forward pass of the RNN.\\n\\n    Args:\\n        x (Tensor): Input tensor of shape `[batch_size, input_size]`.\\n\\n    Returns:\\n        Tensor: Output tensor of shape `[batch_size, output_size]`.']},\n",
       "  {'c': 'def __init__(\\n    self,\\n    file_path: str,\\n    password: Optional[Union[str, bytes]] = None,\\n    headers: Optional[Dict] = None,\\n) -> None:\\n    try:\\n        import pypdf\\n    except ImportError:\\n        raise ImportError(\\n            \"pypdf package not found, please install it with \" \"`pip install pypdf`\"\\n        )\\n    self.parser = PyPDFParser(password=password)\\n    super().__init__(file_path, headers=headers)',\n",
       "   'd': 'Validate input variables.\\n\\nIf input_variables is not set, it will be set to the union of\\nall input variables in the messages.\\n\\nArgs:\\n    values: values to validate.\\n\\nReturns:\\n    Validated values.',\n",
       "   'l': False,\n",
       "   'g': ['.',\n",
       "    '.',\n",
       "    '.\\n    Override to save a previous pk to the config.',\n",
       "    'Save the model.']},\n",
       "  {'c': '    def lazy_load(self) -> Iterator[chat_loaders.ChatSession]:\\n        import sqlite3\\n\\n        try:\\n            conn = sqlite3.connect(self.db_path)\\n        except sqlite3.OperationalError as e:\\n            raise ValueError(\\n                f\"Could not open iMessage DB file {self.db_path}.\\\\n\"\\n                \"Make sure your terminal emulator has disk access to this file.\\\\n\"\\n                \"   You can either copy the DB file to an accessible location\"\\n                \" or grant full disk access for your terminal emulator.\"\\n                \"  You can grant full disk access for your terminal emulator\"\\n                \" in System Settings > Security and Privacy > Full Disk Access.\"\\n            ) from e\\n        cursor = conn.cursor()\\n\\n\\n        cursor.execute(\"SELECT ROWID FROM chat\")\\n        chat_ids = [row[0] for row in cursor.fetchall()]\\n\\n        for chat_id in chat_ids:\\n            yield self._load_single_chat_session(cursor, chat_id)\\n\\n        conn.close()',\n",
       "   'd': 'Lazy load the chat sessions from the iMessage chat.db\\nand yield them in the required format.\\n\\nYields:\\n    ChatSession: Loaded chat session.',\n",
       "   'l': True,\n",
       "   'g': ['>>> Color.from_hex(\"#000000\")\\n    Color(0, 0, 0)\\n    >>> Color.from_hex(\"#000\")\\n    Color(0, 0, 0)\\n    >>> Color.from_hex(\"#0000\")\\n    Color(0, 0, 0)\\n    >>> Color.from_hex(\"#00\")\\n    Color(0, 0, 0)\\n    >>> Color.from_hex(\"#0\")\\n    Color(0, 0, 0)\\n    >>> Color.from_hex(\"#00000\")\\n    Color(0, 0, 0)\\n    >>> Color.from_hex(\"#0000000\")\\n    Color(0, 0, 0)\\n    >>> Color.from_hex(\"#00000000\")\\n    Color(0, 0, 0)\\n    >>> Color.from_hex(\"#000000000\")\\n    Color(0, 0, 0)\\n    >>> Color.from_hex(\"#0000',\n",
       "    '.',\n",
       "    '.',\n",
       "    ':param color_hex: Color in hex format (e.g. #000000)\\n    :return: Color instance']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        cls._try_init_vertexai(values)\\n        tuned_model_name = values.get(\"tuned_model_name\")\\n        model_name = values[\"model_name\"]\\n        try:\\n            if not is_codey_model(model_name):\\n                from vertexai.preview.language_models import TextGenerationModel\\n\\n                if tuned_model_name:\\n                    values[\"client\"] = TextGenerationModel.get_tuned_model(\\n                        tuned_model_name\\n                    )\\n                else:\\n                    values[\"client\"] = TextGenerationModel.from_pretrained(model_name)\\n            else:\\n                from vertexai.preview.language_models import CodeGenerationModel\\n\\n                if tuned_model_name:\\n                    values[\"client\"] = CodeGenerationModel.get_tuned_model(\\n                        tuned_model_name\\n                    )\\n                else:\\n                    values[\"client\"] = CodeGenerationModel.from_pretrained(model_name)\\n        except ImportError:\\n            raise_vertex_import_error()\\n        return values',\n",
       "   'd': 'Validate that the python package exists in environment.',\n",
       "   'l': True,\n",
       "   'g': ['Returns the number of items in the cart.',\n",
       "    'Returns the number of items in the list.',\n",
       "    'Return the number of items in the list.',\n",
       "    'returns the number of items in the cart']},\n",
       "  {'c': '    def lazy_load(self) -> Iterator[chat_loaders.ChatSession]:\\n        try:\\n            conn = sqlite3.connect(self.db_path)\\n        except sqlite3.OperationalError as e:\\n            raise ValueError(\\n                f\"Could not open iMessage DB file {self.db_path}.\\\\n\"\\n                \"Make sure your terminal emulator has disk access to this file.\\\\n\"\\n                \"   You can either copy the DB file to an accessible location\"\\n                \" or grant full disk access for your terminal emulator.\"\\n                \"  You can grant full disk access for your terminal emulator\"\\n                \" in System Settings > Security and Privacy > Full Disk Access.\"\\n            ) from e\\n        cursor = conn.cursor()\\n\\n\\n        cursor.execute(\"SELECT ROWID FROM chat\")\\n        chat_ids = [row[0] for row in cursor.fetchall()]\\n\\n        for chat_id in chat_ids:\\n            yield self._load_single_chat_session(cursor, chat_id)\\n\\n        conn.close()',\n",
       "   'd': 'Lazy load the chat sessions from the iMessage chat.db\\nand yield them in the required format.\\n\\nYields:\\n    ChatSession: Loaded chat session.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Gets the cached logic.',\n",
       "    'This function is used to get the cached logic',\n",
       "    '.',\n",
       "    '.']},\n",
       "  {'c': 'def _setup_evaluation(\\n    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,\\n    examples: Iterator[Example],\\n    evaluation: Optional[RunEvalConfig],\\n    data_type: DataType,\\n) -> Tuple[Optional[List[RunEvaluator]], Iterator[Example]]:\\n    if evaluation:\\n        first_example, examples = _first_example(examples)\\n        if isinstance(llm_or_chain_factory, BaseLanguageModel):\\n            run_inputs, run_outputs = None, None\\n            run_type = RunTypeEnum.llm\\n        else:\\n            run_type = RunTypeEnum.chain\\n            if data_type in (DataType.chat, DataType.llm):\\n                raise ValueError(\\n                    \"Cannot evaluate a chain on dataset with \"\\n                    f\"data_type={data_type.value}. \"\\n                    \"Please specify a dataset with the default \\'kv\\' data type.\"\\n                )\\n            chain = llm_or_chain_factory()\\n            run_inputs = chain.input_keys\\n            run_outputs = chain.output_keys\\n        run_evaluators = _load_run_evaluators(\\n            evaluation,\\n            run_type,\\n            data_type,\\n            list(first_example.outputs) if first_example.outputs else None,\\n            run_inputs,\\n            run_outputs,\\n        )\\n    else:\\n\\n        run_evaluators = None\\n    return run_evaluators, examples',\n",
       "   'd': 'Configure the evaluators to run on the results of the chain.',\n",
       "   'l': True,\n",
       "   'g': ['.', '.', 'Return a string representation of this object.', '.']},\n",
       "  {'c': '    def __init__(self, **kwargs: Any):\\n        super().__init__(**kwargs)\\n        try:\\n            from InstructorEmbedding import INSTRUCTOR\\n\\n            self.client = INSTRUCTOR(\\n                self.model_name, cache_folder=self.cache_folder, **self.model_kwargs\\n            )\\n        except ImportError as e:\\n            raise ImportError(\"Dependencies for InstructorEmbedding not found.\") from e',\n",
       "   'd': 'Initialize the sentence_transformer.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Lock the object.\\n    \\n    @return: A lock object.',\n",
       "    ':param args: \\n    :param kwargs: \\n    :return:',\n",
       "    ':param args: \\n    :param kwargs: \\n    :return:']},\n",
       "  {'c': '    def on_chain_end(\\n        self,\\n        outputs: Dict[str, Any],\\n        *,\\n        run_id: UUID,\\n        inputs: Optional[Dict[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        if not run_id:\\n            raise TracerException(\"No run_id provided for on_chain_end callback.\")\\n        chain_run = self.run_map.get(str(run_id))\\n        if chain_run is None:\\n            raise TracerException(f\"No chain Run found to be traced for {run_id}\")\\n\\n        chain_run.outputs = outputs\\n        chain_run.end_time = datetime.utcnow()\\n        chain_run.events.append({\"name\": \"end\", \"time\": chain_run.end_time})\\n        if inputs is not None:\\n            chain_run.inputs = inputs\\n        self._end_trace(chain_run)\\n        self._on_chain_end(chain_run)',\n",
       "   'd': 'Load file.',\n",
       "   'l': False,\n",
       "   'g': ['Deletes a SageMaker model.',\n",
       "    '_summary_',\n",
       "    'Deletes a model from SageMaker.\\n    \\n    Args:\\n        model_name (str): The name of the model to delete.\\n    \\n    Returns:\\n        str: An empty string.',\n",
       "    'Delete the model with the given name\\n    \\n    Parameters\\n    ----------\\n    model_name : str\\n        The name of the model to delete\\n    \\n    Returns\\n    -------\\n    str\\n        An empty string if the model is deleted successfully, otherwise an error message']},\n",
       "  {'c': 'def completion_with_retry(\\n    llm: VertexAI,\\n    *args: Any,\\n    run_manager: Optional[CallbackManagerForLLMRun] = None,\\n    **kwargs: Any,\\n) -> Any:\\n    retry_decorator = _create_retry_decorator(llm, run_manager=run_manager)\\n\\n    @retry_decorator\\n    def _completion_with_retry(*args: Any, **kwargs: Any) -> Any:\\n        return llm.client.predict(*args, **kwargs)\\n\\n    return _completion_with_retry(*args, **kwargs)',\n",
       "   'd': 'Use tenacity to retry the completion call.',\n",
       "   'l': True,\n",
       "   'g': ['Start the thread and wait for it to be stopped.',\n",
       "    '::classmethod::',\n",
       "    '',\n",
       "    'Returns True if the server is running, False otherwise.']},\n",
       "  {'c': '    def evaluate(\\n        self,\\n        examples: Sequence[dict],\\n        predictions: Sequence[dict],\\n        question_key: str = \"query\",\\n        answer_key: str = \"answer\",\\n        prediction_key: str = \"result\",\\n        *,\\n        callbacks: Callbacks = None,\\n    ) -> List[dict]:\\n        inputs = [\\n            {\\n                \"query\": example[question_key],\\n                \"answer\": example[answer_key],\\n                \"result\": predictions[i][prediction_key],\\n            }\\n            for i, example in enumerate(examples)\\n        ]\\n\\n        return self.apply(inputs, callbacks=callbacks)',\n",
       "   'd': 'Load file.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    Returns:\\n        SparseArrayType: The embedding of the model.',\n",
       "    '.',\n",
       "    '.',\n",
       "    '.']},\n",
       "  {'c': 'def __init__(self, database_path: str = \".langchain.db\"):\\n    engine = create_engine(f\"sqlite:///{database_path}\")\\n    super().__init__(engine)',\n",
       "   'd': 'Raise error - saving not supported for Agent Executors.',\n",
       "   'l': False,\n",
       "   'g': ['_test_put()', '', '', '']},\n",
       "  {'c': '    def set(self, results, query):\\n        node = self.lookup[query]\\n        if node is not None:\\n\\n            node.results = results\\n            self.linked_list.move_to_front(node)\\n        else:\\n\\n            if self.size == self.MAX_SIZE:\\n\\n                self.lookup.pop(self.linked_list.tail.query, None)\\n                self.linked_list.remove_from_tail()\\n            else:\\n                self.size += 1\\n\\n            new_node = Node(results)\\n            self.linked_list.append_to_front(new_node)\\n            self.lookup[query] = new_node',\n",
       "   'd': 'Return Elasticsearch documents most similar to query.\\n\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 4.\\n    filter: Array of Elasticsearch filter clauses to apply to the query.\\n\\nReturns:\\n    List of Documents most similar to the query,\\n    in descending order of similarity.',\n",
       "   'l': False,\n",
       "   'g': ['_acquire_token_func_\\n\\n    Args:\\n        sp_directory_id (str): The directory ID of the service principal.\\n        sp_client_id (str): The client ID of the service principal.\\n        sp_client_secret (str): The client secret of the service principal.\\n\\n    Returns:\\n        dict[str, Any]: A dictionary containing the access token and the expiration time.',\n",
       "    'Return a dict containing the access token and expiry time.',\n",
       "    'Gets a token for the Microsoft Graph API.',\n",
       "    'acquire_token_func() -> dict[str, Any]\\n\\n    This function is used to acquire a token for the Microsoft Graph API.\\n\\n    Returns:\\n        dict[str, Any]: A dictionary containing the access token and expiration time.']},\n",
       "  {'c': 'def get_pipeline() -> Any:\\n    model_id = \"facebook/bart-base\"\\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\\n    model = AutoModelForCausalLM.from_pretrained(model_id)\\n    return pipeline(\"feature-extraction\", model=model, tokenizer=tokenizer)',\n",
       "   'd': 'Get pipeline for testing.',\n",
       "   'l': True,\n",
       "   'g': ['Add a new document to Milvus. \\n    \\n    Args:\\n        kb_file (KnowledgeFile): The KnowledgeFile object to add.\\n        \\n    Returns:\\n        bool: True if the document is added successfully, False otherwise.',\n",
       "    '_summary_\\n\\n    Args:\\n        kb_file (KnowledgeFile): _description_\\n\\n    Returns:\\n        _type_: _description_',\n",
       "    'Add a KnowledgeFile to Milvus.\\n    \\n    Args:\\n        kb_file (KnowledgeFile): The KnowledgeFile to be added.\\n    \\n    Returns:\\n        bool: True if the KnowledgeFile was added successfully, False otherwise.',\n",
       "    'Add documents to Milvus.\\n    \\n    Args:\\n        kb_file (KnowledgeFile): KnowledgeFile object.\\n        \\n    Returns:\\n        status (bool): True if the documents are added successfully, False otherwise.']},\n",
       "  {'c': '    def parse(self, text: str) -> T:\\n',\n",
       "   'd': 'Parse a single string model output into some structure.\\n\\nArgs:\\n    text: String output of a language model.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': True,\n",
       "   'g': ['Test the scrape_dai_docs function',\n",
       "    'Test that we can scrape the DAI docs and convert them to outputs.',\n",
       "    'Test that we can scrape all the docs',\n",
       "    'Test scrape_dai_docs_all_pandoc()']},\n",
       "  {'c': '    def index(\\n        self,\\n        dims_length: Union[int, None],\\n        vector_query_field: str,\\n        similarity: Union[DistanceStrategy, None],\\n    ) -> Dict:\\n        if similarity is DistanceStrategy.COSINE:\\n            similarityAlgo = \"cosine\"\\n        elif similarity is DistanceStrategy.EUCLIDEAN_DISTANCE:\\n            similarityAlgo = \"l2_norm\"\\n        elif similarity is DistanceStrategy.DOT_PRODUCT:\\n            similarityAlgo = \"dot_product\"\\n        else:\\n            raise ValueError(f\"Similarity {similarity} not supported.\")\\n\\n        return {\\n            \"mappings\": {\\n                \"properties\": {\\n                    vector_query_field: {\\n                        \"type\": \"dense_vector\",\\n                        \"dims\": dims_length,\\n                        \"index\": True,\\n                        \"similarity\": similarityAlgo,\\n                    },\\n                }\\n            }\\n        }',\n",
       "   'd': 'Create the mapping for the Elasticsearch index.',\n",
       "   'l': True,\n",
       "   'g': ['.', '.', '(self, hook, file_args):\\n        Run a hook.', '.']},\n",
       "  {'c': 'def _type(self) -> str:\\n    return \"default\"',\n",
       "   'd': 'Call out to Clarfai\\'s PostModelOutputs endpoint.\\n\\nArgs:\\n    prompt: The prompt to pass into the model.\\n    stop: Optional list of stop words to use when generating.\\n\\nReturns:\\n    The string generated by the model.\\n\\nExample:\\n    .. code-block:: python\\n\\n        response = clarifai_llm(\"Tell me a joke.\")',\n",
       "   'l': False,\n",
       "   'g': ['.',\n",
       "    '.\\n\\n    Args:\\n        bottomUpInput (numpy.ndarray):\\n        enableLearn (bool):\\n        computeInfOutput (bool):\\n\\n    Returns:\\n        numpy.ndarray:',\n",
       "    '.',\n",
       "    '.']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        huggingface_api_key = get_from_dict_or_env(\\n            values, \"huggingface_api_key\", \"HUGGINGFACE_API_KEY\"\\n        )\\n        try:\\n            from petals import AutoDistributedModelForCausalLM\\n            from transformers import AutoTokenizer\\n\\n            model_name = values[\"model_name\"]\\n            values[\"tokenizer\"] = AutoTokenizer.from_pretrained(model_name)\\n            values[\"client\"] = AutoDistributedModelForCausalLM.from_pretrained(\\n                model_name\\n            )\\n            values[\"huggingface_api_key\"] = huggingface_api_key\\n\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import transformers or petals python package.\"\\n                \"Please install with `pip install -U transformers petals`.\"\\n            )\\n        return values',\n",
       "   'd': 'To a BaseMessage.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    List of BaseMessage.',\n",
       "   'l': False,\n",
       "   'g': ['Selects rows that match the given regex.\\n\\n    Args:\\n        regex (str): The regex to match.\\n        columns (list of pandas.Series): The columns to select.\\n        unselect (bool, optional): If True, unselects the rows that match the regex. Defaults to False.\\n\\n    Returns:\\n        pandas.DataFrame: The selected DataFrame.',\n",
       "    'Selects rows that match the given regex in the given columns.\\n\\n    Parameters\\n    ----------\\n    regex : str\\n        The regex to match.\\n    columns : list of pandas.Series\\n        The columns to match.\\n    unselect : bool, optional\\n        Whether to unselect the rows that match the regex.\\n\\n    Returns\\n    -------\\n    pandas.DataFrame\\n        The selected rows.',\n",
       "    'Selects rows matching the given regex.\\n    \\n    Args:\\n        regex (str): The regex to match against.\\n        columns (list): The columns to select.\\n        unselect (bool): If True, unselect rows that match the regex.\\n    \\n    Returns:\\n        None',\n",
       "    'Select rows based on a regular expression.\\n    \\n    Parameters\\n    ----------\\n    regex : str\\n        The regular expression to match.\\n    columns : list of str\\n        The column names to match.\\n    unselect : bool, optional\\n        Whether to unselect the rows that do not match the regular expression.\\n        Default is False.\\n    \\n    Returns\\n    -------\\n    pd.DataFrame\\n        The selected rows.']},\n",
       "  {'c': '    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:\\n        _handle_event(\\n            self.handlers,\\n            \"on_chain_end\",\\n            \"ignore_chain\",\\n            outputs,\\n            run_id=self.run_id,\\n            parent_run_id=self.parent_run_id,\\n            tags=self.tags,\\n            **kwargs,\\n        )',\n",
       "   'd': 'Run when chain ends running.\\n\\nArgs:\\n    outputs (Dict[str, Any]): The outputs of the chain.',\n",
       "   'l': True,\n",
       "   'g': ['Forward pass of the model.\\n    \\n    Args:\\n        x (torch.Tensor): Input tensor of shape (batch_size, sequence_length).\\n        emb (torch.Tensor): Embedding tensor of shape (vocab_size, embedding_size).\\n\\n    Returns:\\n        torch.Tensor: Output tensor of shape (batch_size, sequence_length, embedding_size).',\n",
       "    'Forward pass of the model.',\n",
       "    'x: [batch_size, seq_len, emb_dim]\\n    emb: [emb_dim, emb_dim]',\n",
       "    'x: [batch_size, seq_len, emb_size]\\n    emb: [emb_size, emb_size]']},\n",
       "  {'c': 'def on_chain_error(\\n    self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\\n) -> None:\\n    self.metrics[\"step\"] += 1\\n    self.metrics[\"errors\"] += 1',\n",
       "   'd': 'Run when chain errors.\\n\\nArgs:\\n    error (Exception or KeyboardInterrupt): The error.',\n",
       "   'l': False,\n",
       "   'g': ['Get the background configuration.',\n",
       "    'Returns a dictionary of backgrounds and their respective options',\n",
       "    'This function returns the background image to be used by the bot.',\n",
       "    'Get the background configuration']},\n",
       "  {'c': '    def load(self) -> List[Document]:\\n        from pdfminer.high_level import extract_text_to_fp\\n        from pdfminer.layout import LAParams\\n        from pdfminer.utils import open_filename\\n\\n        output_string = StringIO()\\n        with open_filename(self.file_path, \"rb\") as fp:\\n            extract_text_to_fp(\\n                fp,\\n                output_string,\\n                codec=\"\",\\n                laparams=LAParams(),\\n                output_type=\"html\",\\n            )\\n        metadata = {\"source\": self.file_path}\\n        return [Document(page_content=output_string.getvalue(), metadata=metadata)]',\n",
       "   'd': 'Clear the *whole* semantic cache.',\n",
       "   'l': False,\n",
       "   'g': [':param paths: \\n    :return:',\n",
       "    'Iterate over all files in a list of directories. \\n    \\n    Parameters\\n    ----------\\n    paths : list\\n        A list of directories to iterate over.\\n\\n    Yields\\n    ------\\n    str\\n        The path to a Python file.',\n",
       "    'Iterate over all files in a list of paths.\\n    \\n    Args:\\n        paths (list): List of paths to search.\\n    \\n    Yields:\\n        str: Path to a file that ends with .py.',\n",
       "    'Yields:\\n        Path to python source code']},\n",
       "  {'c': '    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        text_key: str = \"text\",\\n        index_name: Optional[str] = None,\\n        client: Any = None,\\n        host: List[str] = [\"172.20.31.10:13000\"],\\n        user: str = \"root\",\\n        password: str = \"123123\",\\n        batch_size: int = 500,\\n        **kwargs: Any,\\n    ) -> Dingo:\\n        try:\\n            import dingodb\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import dingo python package. \"\\n                \"Please install it with `pip install dingodb`.\"\\n            )\\n\\n        if client is not None:\\n            dingo_client = client\\n        else:\\n            try:\\n\\n                dingo_client = dingodb.DingoDB(user, password, host)\\n            except ValueError as e:\\n                raise ValueError(f\"Dingo failed to connect: {e}\")\\n        if kwargs is not None and kwargs.get(\"self_id\") is True:\\n            if index_name not in dingo_client.get_index():\\n                dingo_client.create_index(index_name, 1024, auto_id=False)\\n        else:\\n            if index_name not in dingo_client.get_index():\\n                dingo_client.create_index(index_name, 1024)\\n\\n\\n\\n\\n        ids = ids or [str(uuid.uuid1().int)[:13] for _ in texts]\\n        metadatas_list = []\\n        texts = list(texts)\\n        embeds = embedding.embed_documents(texts)\\n        for i, text in enumerate(texts):\\n            metadata = metadatas[i] if metadatas else {}\\n            metadata[text_key] = text\\n            metadatas_list.append(metadata)\\n\\n\\n        for i in range(0, len(list(texts)), batch_size):\\n            j = i + batch_size\\n            add_res = dingo_client.vector_add(\\n                index_name, metadatas_list[i:j], embeds[i:j], ids[i:j]\\n            )\\n            if not add_res:\\n                raise Exception(\"vector add fail\")\\n        return cls(embedding, text_key, client=dingo_client, index_name=index_name)',\n",
       "   'd': 'Construct Dingo wrapper from raw documents.\\n\\n        This is a user friendly interface that:\\n            1. Embeds documents.\\n            2. Adds the documents to a provided Dingo index\\n\\n        This is intended to be a quick way to get started.\\n\\n        Example:\\n            .. code-block:: python\\n\\n                from langchain.vectorstores import Dingo\\n                from langchain.embeddings import OpenAIEmbeddings\\n                import dingodb\\nsss\\n                embeddings = OpenAIEmbeddings()\\n                dingo = Dingo.from_texts(\\n                    texts,\\n                    embeddings,\\n                    index_name=\"langchain-demo\"\\n                )',\n",
       "   'l': True,\n",
       "   'g': [\"(self, init_data_by_df, dtype='stock_day', if_fq='bfq')\\n    \\n    Args:\\n        init_data_by_df (pd.DataFrame):\\n        dtype (str, optional): Defaults to 'stock_day'.\\n        if_fq (str, optional): Defaults to 'bfq'.\\n    \\n    Raises:\\n        TypeError: if init_data_by_df is not kind of DataFrame type !\\n    \\n    Returns:\\n        None:\",\n",
       "    '',\n",
       "    'for stock_day data, the init_data_by_df should be a DataFrame',\n",
       "    \"(self, init_data_by_df, dtype='stock_day', if_fq='bfq')\\n    \\n    初始化函数，初始化类变量\\n    \\n    Parameters\\n    ----------\\n    init_data_by_df : pd.DataFrame\\n        需要初始化的DataFrame，数据格式为：日期、收盘价、开盘价、最高价、最低价、成交量、成交额\\n    dtype : str, optional\\n        数据类型，默认为'stock_day'，支持'stock_day'、'stock_min'、'stock_hour'、'stock_5min'、'stock_15min'、'stock_30min'、'stock_60min'、'stock_1day'、'stock_1week'、'stock_1month'、'stock_1year'、'stock_5year'、'stock_10year'、'stock_15year'、'stock_20year'、'stock_50year'、'stock_100year'、'stock_200year'、'stock_500year'、'stock_100\"]},\n",
       "  {'c': 'def __init__(\\n    self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n) -> None:\\n    super().__init__(**kwargs)\\n    self._separator = separator\\n    self._is_separator_regex = is_separator_regex',\n",
       "   'd': 'Create an LLMChain that uses an OpenAI function to get a structured output.\\n\\nArgs:\\n    output_schema: Either a dictionary or pydantic.BaseModel class. If a dictionary\\n        is passed in, it\\'s assumed to already be a valid JsonSchema.\\n        For best results, pydantic.BaseModels should have docstrings describing what\\n        the schema represents and descriptions for the parameters.\\n    llm: Language model to use, assumed to support the OpenAI function-calling API.\\n    prompt: BasePromptTemplate to pass to the model.\\n    output_key: The key to use when returning the output in LLMChain.__call__.\\n    output_parser: BaseLLMOutputParser to use for parsing model outputs. By default\\n        will be inferred from the function types. If pydantic.BaseModels are passed\\n        in, then the OutputParser will try to parse outputs using those. Otherwise\\n        model outputs will simply be parsed as JSON.\\n\\nReturns:\\n    An LLMChain that will pass the given function to the model.\\n\\nExample:\\n    .. code-block:: python\\n\\n            from typing import Optional\\n\\n            from langchain.chains.openai_functions import create_structured_output_chain\\n            from langchain.chat_models import ChatOpenAI\\n            from langchain.prompts import ChatPromptTemplate\\n\\n            from langchain.pydantic_v1 import BaseModel, Field\\n\\n            class Dog(BaseModel):\\n                \"\"\"Identifying information about a dog.\"\"\"\\n\\n                name: str = Field(..., description=\"The dog\\'s name\")\\n                color: str = Field(..., description=\"The dog\\'s color\")\\n                fav_food: Optional[str] = Field(None, description=\"The dog\\'s favorite food\")\\n\\n            llm = ChatOpenAI(model=\"gpt-3.5-turbo-0613\", temperature=0)\\n            prompt = ChatPromptTemplate.from_messages(\\n                [\\n                    (\"system\", \"You are a world class algorithm for extracting information in structured formats.\"),\\n                    (\"human\", \"Use the given format to extract information from the following input: {input}\"),\\n                    (\"human\", \"Tip: Make sure to answer in the correct format\"),\\n                ]\\n            )\\n            chain = create_structured_output_chain(Dog, llm, prompt)\\n            chain.run(\"Harry was a chubby brown beagle who loved chicken\")\\n            # -> Dog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")',\n",
       "   'l': False,\n",
       "   'g': ['Update the data in the database',\n",
       "    'Update the data of a node.',\n",
       "    'This method updates the data in the database.\\n    \\n    Args:\\n        data (dict): The data to be updated.',\n",
       "    '']},\n",
       "  {'c': 'def similarity_search(\\n    self, query: str, k: int = 4, filter: Optional[dict] = None, **kwargs: Any\\n) -> List[Document]:\\n    results = self.knn_search(query=query, k=k, **kwargs)\\n    return [doc for doc, score in results]',\n",
       "   'd': 'Pass through to `knn_search`',\n",
       "   'l': True,\n",
       "   'g': ['.', '.', '.\\n    Loads games from the given directory.', '.']},\n",
       "  {'c': '    def semantic_hybrid_search_with_score(\\n        self, query: str, k: int = 4, filters: Optional[str] = None\\n    ) -> List[Tuple[Document, float]]:\\n        from azure.search.documents.models import Vector\\n\\n        results = self.client.search(\\n            search_text=query,\\n            vector=Vector(\\n                value=np.array(\\n                    self.embedding_function(query), dtype=np.float32\\n                ).tolist(),\\n                k=50,\\n                fields=FIELDS_CONTENT_VECTOR,\\n            ),\\n            select=[f\"{FIELDS_ID},{FIELDS_CONTENT},{FIELDS_METADATA}\"],\\n            filter=filters,\\n            query_type=\"semantic\",\\n            query_language=self.semantic_query_language,\\n            semantic_configuration_name=self.semantic_configuration_name,\\n            query_caption=\"extractive\",\\n            query_answer=\"extractive\",\\n            top=k,\\n        )\\n\\n        semantic_answers = results.get_answers()\\n        semantic_answers_dict = {}\\n        for semantic_answer in semantic_answers:\\n            semantic_answers_dict[semantic_answer.key] = {\\n                \"text\": semantic_answer.text,\\n                \"highlights\": semantic_answer.highlights,\\n            }\\n\\n        docs = [\\n            (\\n                Document(\\n                    page_content=result[\"content\"],\\n                    metadata={\\n                        **json.loads(result[\"metadata\"]),\\n                        **{\\n                            \"captions\": {\\n                                \"text\": result.get(\"@search.captions\", [{}])[0].text,\\n                                \"highlights\": result.get(\"@search.captions\", [{}])[\\n                                    0\\n                                ].highlights,\\n                            }\\n                            if result.get(\"@search.captions\")\\n                            else {},\\n                            \"answers\": semantic_answers_dict.get(\\n                                json.loads(result[\"metadata\"]).get(\"key\"), \"\"\\n                            ),\\n                        },\\n                    },\\n                ),\\n                float(result[\"@search.score\"]),\\n            )\\n            for result in results\\n        ]\\n        return docs',\n",
       "   'd': 'Return docs most similar to query.\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 4.\\nReturns:\\n    List of Documents most similar to the query.',\n",
       "   'l': False,\n",
       "   'g': ['Returns the content between start and end in a string.',\n",
       "    'Return the content between start and end.\\n    \\n    Parameters\\n    ----------\\n    string : str\\n        The string to search in.\\n    start : str\\n        The start of the content.\\n    end : str, optional\\n        The end of the content. The default is None.\\n\\n    Returns\\n    -------\\n    str\\n        The content between start and end.\\n\\n    Examples\\n    --------\\n    >>> str_between(\"Hello, world!\", \"Hello\", \"!\")\\n    \"world!\"\\n    >>> str_between(\"Hello, world!\", \"Hello\")\\n    \"world!\"\\n    >>> str_between(\"Hello, world!\", \"Hello\", \"!\")\\n    \"world!\"\\n    >>> str_between(\"Hello, world!\", \"Hello\", \"!\")\\n    \"world!\"\\n    >>> str_between(\"Hello, world!\", \"Hello\", \"!\")\\n    \"world!\"\\n    >>> str_between(\"Hello, world!\", \"Hello\", \"!\")\\n    \"world!\"\\n    >>> str_between(\"Hello, world!\", \"Hello\", \"!\")\\n    \"world!\"\\n    >>> str_between(\"Hello, world!\", \"Hello\", \"!\")\\n    \"world!\"',\n",
       "    'Returns the content between start and end in string.',\n",
       "    'Return the content between start and end, if end is not provided,\\n    return everything after start.']},\n",
       "  {'c': '    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        try:\\n            from aleph_alpha_client import (\\n                Prompt,\\n                SemanticEmbeddingRequest,\\n                SemanticRepresentation,\\n            )\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import aleph_alpha_client python package. \"\\n                \"Please install it with `pip install aleph_alpha_client`.\"\\n            )\\n        document_embeddings = []\\n\\n        for text in texts:\\n            document_params = {\\n                \"prompt\": Prompt.from_text(text),\\n                \"representation\": SemanticRepresentation.Document,\\n                \"compress_to_size\": self.compress_to_size,\\n                \"normalize\": self.normalize,\\n                \"contextual_control_threshold\": self.contextual_control_threshold,\\n                \"control_log_additive\": self.control_log_additive,\\n            }\\n\\n            document_request = SemanticEmbeddingRequest(**document_params)\\n            document_response = self.client.semantic_embed(\\n                request=document_request, model=self.model\\n            )\\n\\n            document_embeddings.append(document_response.embedding)\\n\\n        return document_embeddings',\n",
       "   'd': 'Whether the evaluation requires a reference text.',\n",
       "   'l': False,\n",
       "   'g': [\"for now we don't use axolotl\\n    if axolotl:\\n        return YowStackBuilder.getDefaultLayers(axolotl = True, groups = groups, media=media,privacy=privacy, profiles=profiles)\\n    else:\",\n",
       "    '.',\n",
       "    '.',\n",
       "    '.']},\n",
       "  {'c': 'def load(self) -> List[Document]:\\n    return list(self.lazy_load())',\n",
       "   'd': 'Eagerly load the content.',\n",
       "   'l': True,\n",
       "   'g': ['versioning',\n",
       "    '_test_get_version_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0',\n",
       "    'Test the get method',\n",
       "    'Test that the get() method returns the correct status code and items']},\n",
       "  {'c': 'def post(self, url: str, data: Dict[str, Any], **kwargs: Any) -> str:\\n    return self.requests.post(url, data, **kwargs).text',\n",
       "   'd': 'POST to the URL and return the text.',\n",
       "   'l': True,\n",
       "   'g': ['_adb_command\\n\\n    :param command:\\n    :return:',\n",
       "    \"'Run a shell command on the device'\",\n",
       "    \"'Execute an ADB command on the device.\\n\\n    :param command: The command to execute.\\n    :return: The output of the command.\",\n",
       "    '_adb_command(self, command)\\n\\n    Runs a command on the device.\\n\\n    :param command: The command to run.\\n    :type command: str\\n    :return: The output of the command.\\n    :rtype: str']},\n",
       "  {'c': '    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n        for gen in return_val:\\n            if not isinstance(gen, Generation):\\n                raise ValueError(\\n                    \"Momento only supports caching of normal LLM generations, \"\\n                    f\"got {type(gen)}\"\\n                )\\n        key = self.__key(prompt, llm_string)\\n        value = _dump_generations_to_json(return_val)\\n        set_response = self.cache_client.set(self.cache_name, key, value, self.ttl)\\n        from momento.responses import CacheSet\\n\\n        if isinstance(set_response, CacheSet.Success):\\n            pass\\n        elif isinstance(set_response, CacheSet.Error):\\n            raise set_response.inner_exception\\n        else:\\n            raise Exception(f\"Unexpected response: {set_response}\")',\n",
       "   'd': 'Initialize the sentence_transformer.',\n",
       "   'l': False,\n",
       "   'g': ['.',\n",
       "    '(im, label) -> (im, label)',\n",
       "    '.',\n",
       "    '(im, label) -> (im, label)']},\n",
       "  {'c': '    def from_embeddings(\\n        cls,\\n        text_embeddings: List[Tuple[str, List[float]]],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        metric: str = DEFAULT_METRIC,\\n        trees: int = 100,\\n        n_jobs: int = -1,\\n        **kwargs: Any,\\n    ) -> Annoy:\\n        texts = [t[0] for t in text_embeddings]\\n        embeddings = [t[1] for t in text_embeddings]\\n\\n        return cls.__from(\\n            texts, embeddings, embedding, metadatas, metric, trees, n_jobs, **kwargs\\n        )',\n",
       "   'd': 'Construct Annoy wrapper from embeddings.\\n\\nArgs:\\n    text_embeddings: List of tuples of (text, embedding)\\n    embedding: Embedding function to use.\\n    metadatas: List of metadata dictionaries to associate with documents.\\n    metric: Metric to use for indexing. Defaults to \"angular\".\\n    trees: Number of trees to use for indexing. Defaults to 100.\\n    n_jobs: Number of jobs to use for indexing. Defaults to -1\\n\\nThis is a user friendly interface that:\\n    1. Creates an in memory docstore with provided embeddings\\n    2. Initializes the Annoy database\\n\\nThis is intended to be a quick way to get started.\\n\\nExample:\\n    .. code-block:: python\\n\\n        from langchain import Annoy\\n        from langchain.embeddings import OpenAIEmbeddings\\n        embeddings = OpenAIEmbeddings()\\n        text_embeddings = embeddings.embed_documents(texts)\\n        text_embedding_pairs = list(zip(texts, text_embeddings))\\n        db = Annoy.from_embeddings(text_embedding_pairs, embeddings)',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Args:\\n        input_sequence (tf.Tensor): Input sequence to be encoded.\\n        training (bool): Whether to use the training or inference mode.\\n        mask (tf.Tensor, optional): Mask to apply to the input sequence. Defaults to None.\\n    Returns:\\n        dict: Dictionary containing the output of the encoder.',\n",
       "    '.\\n    Run the model on the given input sequence.\\n\\n    :param input_sequence: input sequence to run the model on\\n    :param training: whether to run in training mode\\n    :param mask: mask to use for padding\\n    :return: dictionary of tensors',\n",
       "    '.\\n    Call the model.\\n\\n    Args:\\n        input_sequence: The input sequence.\\n        training: If True, the model is trained.\\n        mask: The mask.\\n\\n    Returns:\\n        The output sequence.',\n",
       "    '(self, input_sequence, training=True, mask=None)\\n\\n    Parameters\\n    ----------\\n    input_sequence : tf.Tensor\\n        The input sequence to the model.\\n    training : bool\\n        Whether to use the model in training mode.\\n    mask : tf.Tensor\\n        The mask to use for the model.\\n\\n    Returns\\n    -------\\n    dict\\n        A dictionary containing the output of the model.']},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n    rows = self._search_rows(prompt, llm_string)\\n    if rows:\\n        return [loads(row[0]) for row in rows]\\n    return None',\n",
       "   'd': \"Call out to Aleph Alpha's asymmetric Document endpoint.\\n\\nArgs:\\n    texts: The list of texts to embed.\\n\\nReturns:\\n    List of embeddings, one for each text.\",\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Parses the data from the file and returns a list of lists',\n",
       "    '',\n",
       "    'This function reads the data from the file and returns the data as a list of lists.\\n    \\n    Parameters:\\n    filename (str): The name of the file to be read.\\n    \\n    Returns:\\n    list: A list of lists containing the data from the file.']},\n",
       "  {'c': '    def _create_search_request(self, query: str) -> SearchRequest:\\n        from google.cloud.discoveryengine_v1beta import SearchRequest\\n\\n        query_expansion_spec = SearchRequest.QueryExpansionSpec(\\n            condition=self.query_expansion_condition,\\n        )\\n\\n        spell_correction_spec = SearchRequest.SpellCorrectionSpec(\\n            mode=self.spell_correction_mode\\n        )\\n\\n        if self.engine_data_type == 0:\\n            if self.get_extractive_answers:\\n                extractive_content_spec = (\\n                    SearchRequest.ContentSearchSpec.ExtractiveContentSpec(\\n                        max_extractive_answer_count=self.max_extractive_answer_count,\\n                    )\\n                )\\n            else:\\n                extractive_content_spec = (\\n                    SearchRequest.ContentSearchSpec.ExtractiveContentSpec(\\n                        max_extractive_segment_count=self.max_extractive_segment_count,\\n                    )\\n                )\\n            content_search_spec = SearchRequest.ContentSearchSpec(\\n                extractive_content_spec=extractive_content_spec\\n            )\\n        elif self.engine_data_type == 1:\\n            content_search_spec = None\\n        elif self.engine_data_type == 2:\\n            content_search_spec = SearchRequest.ContentSearchSpec(\\n                extractive_content_spec=SearchRequest.ContentSearchSpec.ExtractiveContentSpec(\\n                    max_extractive_answer_count=self.max_extractive_answer_count,\\n                )\\n            )\\n        else:\\n            raise NotImplementedError(\\n                \"Only data store type 0 (Unstructured), 1 (Structured),\"\\n                \"or 2 (Website with Advanced Indexing) are supported currently.\"\\n                + f\" Got {self.engine_data_type}\"\\n            )\\n\\n        return SearchRequest(\\n            query=query,\\n            filter=self.filter,\\n            serving_config=self._serving_config,\\n            page_size=self.max_documents,\\n            content_search_spec=content_search_spec,\\n            query_expansion_spec=query_expansion_spec,\\n            spell_correction_spec=spell_correction_spec,\\n        )',\n",
       "   'd': 'Prepares a SearchRequest object.',\n",
       "   'l': True,\n",
       "   'g': ['.',\n",
       "    'to run the function in a new thread.',\n",
       "    '.',\n",
       "    'to be called from the target thread']},\n",
       "  {'c': '    def from_chains(\\n        cls, llm: BaseLanguageModel, chains: List[ChainConfig], **kwargs: Any\\n    ) -> AgentExecutor:\\n        tools = [\\n            Tool(\\n                name=c.action_name,\\n                func=c.action,\\n                description=c.action_description,\\n            )\\n            for c in chains\\n        ]\\n        agent = ZeroShotAgent.from_llm_and_tools(llm, tools)\\n        return cls(agent=agent, tools=tools, **kwargs)',\n",
       "   'd': 'User friendly way to initialize the MRKL chain.\\n\\nThis is intended to be an easy way to get up and running with the\\nMRKL chain.\\n\\nArgs:\\n    llm: The LLM to use as the agent LLM.\\n    chains: The chains the MRKL system has access to.\\n    **kwargs: parameters to be passed to initialization.\\n\\nReturns:\\n    An initialized MRKL chain.\\n\\nExample:\\n    .. code-block:: python\\n\\n        from langchain import LLMMathChain, OpenAI, SerpAPIWrapper, MRKLChain\\n        from langchain.chains.mrkl.base import ChainConfig\\n        llm = OpenAI(temperature=0)\\n        search = SerpAPIWrapper()\\n        llm_math_chain = LLMMathChain(llm=llm)\\n        chains = [\\n            ChainConfig(\\n                action_name = \"Search\",\\n                action=search.search,\\n                action_description=\"useful for searching\"\\n            ),\\n            ChainConfig(\\n                action_name=\"Calculator\",\\n                action=llm_math_chain.run,\\n                action_description=\"useful for doing math\"\\n            )\\n        ]\\n        mrkl = MRKLChain.from_chains(llm, chains)',\n",
       "   'l': True,\n",
       "   'g': [\"Returns today's date.\",\n",
       "    'Return the current date in the format YYYY-MM-DD',\n",
       "    \"Return today's date in YYYY-MM-DD format.\",\n",
       "    'Returns the current date in YYYY-MM-DD format.']},\n",
       "  {'c': '    def split_text(self, text: str) -> List[Document]:\\n        lines = text.split(\"\\\\n\")\\n\\n        lines_with_metadata: List[LineType] = []\\n\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n\\n\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            for sep, name in self.headers_to_split_on:\\n\\n                if stripped_line.startswith(sep) and (\\n\\n\\n                    len(stripped_line) == len(sep)\\n                    or stripped_line[len(sep)] == \" \"\\n                ):\\n\\n                    if name is not None:\\n\\n                        current_header_level = sep.count(\"\\n\\n\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n\\n\\n                            popped_header = header_stack.pop()\\n\\n\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])\\n\\n\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n\\n                        initial_metadata[name] = header[\"data\"]\\n\\n\\n\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n\\n\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
       "   'd': 'Initialize an instance of RedisCache.\\n\\nThis method initializes an object with Redis caching capabilities.\\nIt takes a `redis_` parameter, which should be an instance of a Redis\\nclient class, allowing the object to interact with a Redis\\nserver for caching purposes.\\n\\nParameters:\\n    redis_ (Any): An instance of a Redis client class\\n        (e.g., redis.Redis) used for caching.\\n        This allows the object to communicate with a\\n        Redis server for caching operations.\\n    ttl (int, optional): Time-to-live (TTL) for cached items in seconds.\\n        If provided, it sets the time duration for how long cached\\n        items will remain valid. If not provided, cached items will not\\n        have an automatic expiration.',\n",
       "   'l': False,\n",
       "   'g': ['_input_example_',\n",
       "    'input_example = torch.randn(16, self._feat_in, 256).to(self._device)\\n    return tuple([input_example])',\n",
       "    'This is an example of how to input data into the model.',\n",
       "    'input_example = torch.randn(16, self._feat_in, 256).to(next(self.parameters()).device)\\n    return tuple([input_example])']},\n",
       "  {'c': 'def dict(self, **kwargs: Any) -> Dict:\\n    _dict = super().dict()\\n    del _dict[\"output_parser\"]\\n    return _dict',\n",
       "   'd': 'Return type of llm.',\n",
       "   'l': False,\n",
       "   'g': ['Save the document',\n",
       "    '_summary_\\n    \\n    _description_\\n    \\n    _param_1_: _type_\\n    \\n    _return_: _type_\\n    \\n    _raises_: _type_',\n",
       "    'Save the document.',\n",
       "    'Save the document.']},\n",
       "  {'c': '    def index(\\n        self,\\n        dims_length: Union[int, None],\\n        vector_query_field: str,\\n        similarity: Union[DistanceStrategy, None],\\n    ) -> Dict:\\n        if similarity is DistanceStrategy.COSINE:\\n            similarityAlgo = \"cosine\"\\n        elif similarity is DistanceStrategy.EUCLIDEAN_DISTANCE:\\n            similarityAlgo = \"l2_norm\"\\n        elif similarity is DistanceStrategy.DOT_PRODUCT:\\n            similarityAlgo = \"dot_product\"\\n        else:\\n            raise ValueError(f\"Similarity {similarity} not supported.\")\\n\\n        return {\\n            \"mappings\": {\\n                \"properties\": {\\n                    vector_query_field: {\\n                        \"type\": \"dense_vector\",\\n                        \"dims\": dims_length,\\n                        \"index\": True,\\n                        \"similarity\": similarityAlgo,\\n                    },\\n                }\\n            }\\n        }',\n",
       "   'd': 'Create the mapping for the Elasticsearch index.',\n",
       "   'l': True,\n",
       "   'g': ['Check if data is not empty and not only whitespaces',\n",
       "    'Check if the data is not empty and not only spaces\\n    \\n    :param data: str\\n    :return: bool',\n",
       "    'Checks if a string has data.\\n    \\n    Args:\\n        data (str): The string to check.\\n    \\n    Returns:\\n        bool: True if the string has data, False otherwise.',\n",
       "    'Checks if the given string has any content,\\n    i.e. not just whitespace.']},\n",
       "  {'c': '    def delete(\\n        self,\\n        ids: Optional[List[str]] = None,\\n        refresh_indices: Optional[bool] = True,\\n        **kwargs: Any,\\n    ) -> Optional[bool]:\\n        try:\\n            from elasticsearch.helpers import BulkIndexError, bulk\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import elasticsearch python package. \"\\n                \"Please install it with `pip install elasticsearch`.\"\\n            )\\n\\n        body = []\\n\\n        if ids is None:\\n            raise ValueError(\"ids must be provided.\")\\n\\n        for _id in ids:\\n            body.append({\"_op_type\": \"delete\", \"_index\": self.index_name, \"_id\": _id})\\n\\n        if len(body) > 0:\\n            try:\\n                bulk(self.client, body, refresh=refresh_indices, ignore_status=404)\\n                logger.debug(f\"Deleted {len(body)} texts from index\")\\n\\n                return True\\n            except BulkIndexError as e:\\n                logger.error(f\"Error deleting texts: {e}\")\\n                firstError = e.errors[0].get(\"index\", {}).get(\"error\", {})\\n                logger.error(f\"First error reason: {firstError.get(\\'reason\\')}\")\\n                raise e\\n\\n        else:\\n            logger.debug(\"No texts to delete from index\")\\n            return False',\n",
       "   'd': 'Delete documents from the Elasticsearch index.\\n\\nArgs:\\n    ids: List of ids of documents to delete.\\n    refresh_indices: Whether to refresh the index\\n                    after deleting documents. Defaults to True.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Loads the prompt cache from the given path.',\n",
       "    '.\\n    Load the prompt cache from the given path.\\n\\n    Args:\\n        path (str): The path to the prompt cache file.\\n\\n    Returns:\\n        dict: A dictionary containing the prompt IDs and prompt keys-values.',\n",
       "    '.\\n    Load the prompt cache from disk.',\n",
       "    '.\\n    Load the prompt cache from the given path.']},\n",
       "  {'c': 'def args(self) -> dict:\\n    return self.args_schema.schema()[\"properties\"]',\n",
       "   'd': 'Start a trace for a chain run.',\n",
       "   'l': False,\n",
       "   'g': ['Get the cached response from the cache\\n    \\n    Args:\\n        cached_response (Any): The cached response from the cache\\n    \\n    Returns:\\n        Any: The cached response from the cache',\n",
       "    'Get the cached response if any',\n",
       "    'Get the cached response logic.',\n",
       "    'Get cached response']},\n",
       "  {'c': 'def add_documents(self, documents: List[Document], **kwargs: Any) -> List[str]:\\n    return self.vectorstore.add_documents(documents, **kwargs)',\n",
       "   'd': 'Create a new TextSplitter.',\n",
       "   'l': False,\n",
       "   'g': ['', 'Test seek() with SEEK_CUR', '', '']},\n",
       "  {'c': 'def load_default_session() -> TracerSessionV1:\\n    return TracerSessionV1(\\n        id=TEST_SESSION_ID, name=\"default\", start_time=datetime.utcnow()\\n    )',\n",
       "   'd': 'Load a tracing session.',\n",
       "   'l': True,\n",
       "   'g': ['_summary_\\n\\n    Args:\\n        name (_type_): _description_\\n\\n    Returns:\\n        _type_: _description_',\n",
       "    'Check if a column should be filtered out.\\n    \\n    Args:\\n        name (str): Name of the column.\\n        regex (str): Regular expression to match against column names.\\n        virtual (bool): Whether the column is a virtual column.\\n        strings (bool): Whether the column should be filtered out for strings.\\n        hidden (bool): Whether the column should be filtered out for hidden columns.\\n        \\n    Returns:\\n        bool: True if the column should be filtered out, False otherwise.',\n",
       "    '_summary_\\n\\n    Args:\\n        name (_type_): _description_\\n\\n    Returns:\\n        _type_: _description_',\n",
       "    'Check if column is valid for the given filter.\\n    \\n    :param name: column name\\n    :return: True if column is valid, False otherwise']},\n",
       "  {'c': 'def dict(self, **kwargs: Any) -> Dict:\\n    _dict = super().dict()\\n    try:\\n        _type = self._agent_type\\n    except NotImplementedError:\\n        _type = None\\n    if isinstance(_type, AgentType):\\n        _dict[\"_type\"] = str(_type.value)\\n    elif _type is not None:\\n        _dict[\"_type\"] = _type\\n    return _dict',\n",
       "   'd': 'Return dictionary representation of agent.',\n",
       "   'l': True,\n",
       "   'g': ['(str) -> str\\n    Returns the simulator name.',\n",
       "    '.',\n",
       "    \"Return the simulator's name.\",\n",
       "    '.\\n    Returns:\\n        str: The simulator name.']},\n",
       "  {'c': 'def _run(\\n    self,\\n    query: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> str:\\n    return self.llm_chain.predict(\\n        query=query, callbacks=run_manager.get_child() if run_manager else None\\n    )',\n",
       "   'd': 'Use the LLM to check the query.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    .. versionadded:: 1.11.0',\n",
       "    '.',\n",
       "    '.\\n    Report the exception to Sentry.',\n",
       "    '.']},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n    generations = []\\n\\n    results = self.redis.hgetall(self._key(prompt, llm_string))\\n    if results:\\n        for _, text in results.items():\\n            generations.append(Generation(text=text))\\n    return generations if generations else None',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': ['.', '.', '.', '.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        session: Optional[CassandraSession],\\n        keyspace: Optional[str],\\n        embedding: Embeddings,\\n        table_name: str = CASSANDRA_SEMANTIC_CACHE_DEFAULT_TABLE_NAME,\\n        distance_metric: str = CASSANDRA_SEMANTIC_CACHE_DEFAULT_DISTANCE_METRIC,\\n        score_threshold: float = CASSANDRA_SEMANTIC_CACHE_DEFAULT_SCORE_THRESHOLD,\\n        ttl_seconds: Optional[int] = CASSANDRA_SEMANTIC_CACHE_DEFAULT_TTL_SECONDS,\\n        skip_provisioning: bool = False,\\n    ):\\n        try:\\n            from cassio.table import MetadataVectorCassandraTable\\n        except (ImportError, ModuleNotFoundError):\\n            raise ValueError(\\n                \"Could not import cassio python package. \"\\n                \"Please install it with `pip install cassio`.\"\\n            )\\n        self.session = session\\n        self.keyspace = keyspace\\n        self.embedding = embedding\\n        self.table_name = table_name\\n        self.distance_metric = distance_metric\\n        self.score_threshold = score_threshold\\n        self.ttl_seconds = ttl_seconds\\n\\n\\n\\n\\n\\n\\n        @lru_cache(maxsize=CASSANDRA_SEMANTIC_CACHE_EMBEDDING_CACHE_SIZE)\\n        def _cache_embedding(text: str) -> List[float]:\\n            return self.embedding.embed_query(text=text)\\n\\n        self._get_embedding = _cache_embedding\\n        self.embedding_dimension = self._get_embedding_dimension()\\n\\n        self.table = MetadataVectorCassandraTable(\\n            session=self.session,\\n            keyspace=self.keyspace,\\n            table=self.table_name,\\n            primary_key_type=[\"TEXT\"],\\n            vector_dimension=self.embedding_dimension,\\n            ttl_seconds=self.ttl_seconds,\\n            metadata_indexing=(\"allow\", {\"_llm_string_hash\"}),\\n            skip_provisioning=skip_provisioning,\\n        )',\n",
       "   'd': 'Lazily parse the blob.',\n",
       "   'l': False,\n",
       "   'g': ['.', '.', '.', '.']},\n",
       "  {'c': 'def parse(self, text: str) -> str:\\n    return text',\n",
       "   'd': 'Returns the input text with no changes.',\n",
       "   'l': True,\n",
       "   'g': ['Returns the range of the range.',\n",
       "    'Returns the range of the range object.',\n",
       "    'Returns the range of the given range.',\n",
       "    'Returns the range of the range.']},\n",
       "  {'c': 'def __init__(self, engine: Engine, cache_schema: Type[FullLLMCache] = FullLLMCache):\\n    self.engine = engine\\n    self.cache_schema = cache_schema\\n    self.cache_schema.metadata.create_all(self.engine)',\n",
       "   'd': 'Initialize by creating all tables.',\n",
       "   'l': True,\n",
       "   'g': ['Test that the upper-package is installed with a case mismatch',\n",
       "    'Test that a pip install with the wrong version of the package fails.',\n",
       "    '',\n",
       "    'Tests that the requirement is installed correctly']},\n",
       "  {'c': '    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n        for gen in return_val:\\n            if not isinstance(gen, Generation):\\n                raise ValueError(\\n                    \"UpstashRedisCache supports caching of normal LLM generations, \"\\n                    f\"got {type(gen)}\"\\n                )\\n            if isinstance(gen, ChatGeneration):\\n                warnings.warn(\\n                    \"NOTE: Generation has not been cached. UpstashRedisCache does not\"\\n                    \" support caching ChatModel outputs.\"\\n                )\\n                return\\n\\n        key = self._key(prompt, llm_string)\\n\\n        mapping = {\\n            str(idx): generation.text for idx, generation in enumerate(return_val)\\n        }\\n        self.redis.hset(key=key, values=mapping)\\n\\n        if self.ttl is not None:\\n            self.redis.expire(key, self.ttl)',\n",
       "   'd': 'Update cache based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': ['.', '.', '()', '()\\n    def start(self):']},\n",
       "  {'c': 'def __init__(\\n    self, file_path: str, password: Optional[Union[str, bytes]] = None\\n) -> None:\\n    try:\\n        import pypdf\\n    except ImportError:\\n        raise ImportError(\\n            \"pypdf package not found, please install it with \" \"`pip install pypdf`\"\\n        )\\n    self.parser = PyPDFParser(password=password)\\n    super().__init__(file_path)',\n",
       "   'd': 'Initialize the OpenAI object.',\n",
       "   'l': False,\n",
       "   'g': ['2022-04-14\\n    @author: Administrator\\n    @return:',\n",
       "    '2016-12-27 15:54:44\\n    :param sentence:\\n    :return:',\n",
       "    '2022-02-13 15:28:35.707000 199108089494737832 [INFO]\\n    def _normalized_numbers(self, sentence):',\n",
       "    '2022-02-08 15:05:49.486880\\n    @param sentence:\\n    @return:']},\n",
       "  {'c': 'def __init__(self) -> None:\\n    try:\\n        import pypdfium2\\n    except ImportError:\\n        raise ImportError(\\n            \"pypdfium2 package not found, please install it with\"\\n            \" `pip install pypdfium2`\"\\n        )',\n",
       "   'd': 'Initialize the parser.',\n",
       "   'l': True,\n",
       "   'g': ['Initialize the YOLOv8 model.\\n    \\n    Parameters:\\n    - model (str or Path): Path to the YOLOv8 model file or URL.\\n    - task (str): Task to perform.\\n    - verbose (bool): Whether to print verbose output.\\n    \\n    Returns:\\n    - YOLOWorld: Instance of the YOLOWorld class.',\n",
       "    'Args:\\n        model (str, optional): path to model. Defaults to \"yolov8n.pt\".\\n        task (str, optional): task to perform. Defaults to None.\\n        verbose (bool, optional): verbose mode. Defaults to False.',\n",
       "    'Args:\\n        model (str or Path): Path to model.\\n        task (str or None): Task to use.\\n        verbose (bool): Verbose.',\n",
       "    'Args:\\n        model (str): path to model or model name\\n        task (str): task to perform (default: None)\\n        verbose (bool): verbosity (default: False)']},\n",
       "  {'c': '    def from_template(\\n        cls: Type[MessagePromptTemplateT],\\n        template: str,\\n        template_format: str = \"f-string\",\\n        **kwargs: Any,\\n    ) -> MessagePromptTemplateT:\\n        prompt = PromptTemplate.from_template(template, template_format=template_format)\\n        return cls(prompt=prompt, **kwargs)',\n",
       "   'd': 'Use the tool.',\n",
       "   'l': False,\n",
       "   'g': ['405 Method Not Allowed',\n",
       "    'Creates a method not allowed handler that sets the status to 405 and\\n    sets the Allow header to the allowed methods.\\n\\n    :param allowed_methods: A list of HTTP methods that are allowed.\\n    :type allowed_methods: list\\n    :return: A function that sets the status to 405 and sets the Allow header.\\n    :rtype: function',\n",
       "    'Create a method that responds with a 405 Method Not Allowed.',\n",
       "    'Return a 405 Method Not Allowed response.']},\n",
       "  {'c': '    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[PromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> QAEvalChain:\\n        prompt = prompt or PROMPT\\n        expected_input_vars = {\"query\", \"answer\", \"result\"}\\n        if expected_input_vars != set(prompt.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt.input_variables}\"\\n            )\\n        return cls(llm=llm, prompt=prompt, **kwargs)',\n",
       "   'd': 'Return the output parser type for serialization.',\n",
       "   'l': False,\n",
       "   'g': ['Validates the exchange value.\\n    \\n    Args:\\n        value (str): The exchange value.\\n    \\n    Returns:\\n        Optional[str]: A string indicating the error if the value is invalid, otherwise None.',\n",
       "    'Validate the exchange value.\\n    \\n    :param value: The exchange value to validate.\\n    :return: The validated exchange value or None if the value is invalid.',\n",
       "    'Validates the exchange input.',\n",
       "    'This function validates the exchange value \\n    \\n    :param value: str\\n    :return: Optional[str]']},\n",
       "  {'c': '    def parse(self, text: str) -> T:\\n',\n",
       "   'd': 'Parse a single string model output into some structure.\\n\\nArgs:\\n    text: String output of a language model.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': True,\n",
       "   'g': ['Decrypt the encrypted credhist with the given key.',\n",
       "    'Decrypt the credential history data using the supplied key.\\n    \\n    :param enckey: The encryption key to use.\\n    \\n    :returns: None',\n",
       "    'Decrypt the encrypted credhist.encrypted data with the given key',\n",
       "    'Decrypt the credentials history using the given key.']},\n",
       "  {'c': '    def _call(\\n        self,\\n        prompt: str,\\n        stop: Optional[List[str]] = None,\\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\\n        **kwargs: Any,\\n    ) -> str:\\n        try:\\n            from pipeline import PipelineCloud\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import pipeline-ai python package. \"\\n                \"Please install it with `pip install pipeline-ai`.\"\\n            )\\n        client = PipelineCloud(token=self.pipeline_api_key)\\n        params = self.pipeline_kwargs or {}\\n        params = {**params, **kwargs}\\n\\n        run = client.run_pipeline(self.pipeline_key, [prompt, params])\\n        try:\\n            text = run.result_preview[0][0]\\n        except AttributeError:\\n            raise AttributeError(\\n                f\"A pipeline run should have a `result_preview` attribute.\"\\n                f\"Run was: {run}\"\\n            )\\n        if stop is not None:\\n\\n\\n            text = enforce_stop_tokens(text, stop)\\n        return text',\n",
       "   'd': 'Call to Pipeline Cloud endpoint.',\n",
       "   'l': True,\n",
       "   'g': ['def _repr_html_(self):',\n",
       "    'Return HTML representation of the widget.',\n",
       "    'Return the HTML representation of the widget.',\n",
       "    '']},\n",
       "  {'c': 'def similarity_search_with_score(\\n    self, query: str, k: int = 4, filter: Optional[dict] = None, **kwargs: Any\\n) -> List[Tuple[Document, float]]:\\n    embedding = self.embedding.embed_query(query)\\n    script_query = _default_script_query(embedding, filter)\\n    response = self.client_search(\\n        self.client, self.index_name, script_query, size=k\\n    )\\n    hits = [hit for hit in response[\"hits\"][\"hits\"]]\\n    docs_and_scores = [\\n        (\\n            Document(\\n                page_content=hit[\"_source\"][\"text\"],\\n                metadata=hit[\"_source\"][\"metadata\"],\\n            ),\\n            hit[\"_score\"],\\n        )\\n        for hit in hits\\n    ]\\n    return docs_and_scores',\n",
       "   'd': 'Row-wise cosine similarity with optional top-k and score threshold filtering.\\n\\nArgs:\\n    X: Matrix.\\n    Y: Matrix, same width as X.\\n    top_k: Max number of results to return.\\n    score_threshold: Minimum cosine similarity of results.\\n\\nReturns:\\n    Tuple of two lists. First contains two-tuples of indices (X_idx, Y_idx),\\n        second contains corresponding cosine similarities.',\n",
       "   'l': False,\n",
       "   'g': ['v2.0',\n",
       "    '_get_message_by_status_impl_by_status',\n",
       "    'Gets the message for a given status.\\n    \\n    Args:\\n        cls (ScanStatus): The ScanStatus class.\\n        status (int): The status to get the message for.\\n    \\n    Returns:\\n        str: The message for the given status.\\n    \\n    Raises:\\n        ValueError: If the status is not a valid ScanStatus.',\n",
       "    'Get message by status.\\n    \\n    :param cls: Class\\n    :param status: ScanStatus\\n    :return: message']},\n",
       "  {'c': '    def _on_run_create(self, run: Run) -> None:\\n        if run.parent_run_id is None:\\n            self.send_stream.send_nowait(\\n                RunLogPatch(\\n                    {\\n                        \"op\": \"replace\",\\n                        \"path\": \"\",\\n                        \"value\": RunState(\\n                            id=str(run.id),\\n                            streamed_output=[],\\n                            final_output=None,\\n                            logs={},\\n                        ),\\n                    }\\n                )\\n            )\\n\\n        if not self.include_run(run):\\n            return\\n\\n\\n        with self.lock:\\n            self._counter_map_by_name[run.name] += 1\\n            count = self._counter_map_by_name[run.name]\\n            self._key_map_by_run_id[run.id] = (\\n                run.name if count == 1 else f\"{run.name}:{count}\"\\n            )\\n\\n\\n        self.send_stream.send_nowait(\\n            RunLogPatch(\\n                {\\n                    \"op\": \"add\",\\n                    \"path\": f\"/logs/{self._key_map_by_run_id[run.id]}\",\\n                    \"value\": LogEntry(\\n                        id=str(run.id),\\n                        name=run.name,\\n                        type=run.run_type,\\n                        tags=run.tags or [],\\n                        metadata=(run.extra or {}).get(\"metadata\", {}),\\n                        start_time=run.start_time.isoformat(timespec=\"milliseconds\"),\\n                        streamed_output_str=[],\\n                        final_output=None,\\n                        end_time=None,\\n                    ),\\n                }\\n            )\\n        )',\n",
       "   'd': 'Initialize with web page and whether to load all paths.\\n\\nArgs:\\n    web_page: The web page to load or the starting point from where\\n        relative paths are discovered.\\n    load_all_paths: If set to True, all relative paths in the navbar\\n        are loaded instead of only `web_page`.\\n    base_url: If `load_all_paths` is True, the relative paths are\\n        appended to this base url. Defaults to `web_page`.\\n    content_selector: The CSS selector for the content to load.\\n        Defaults to \"main\".\\n    continue_on_failure: whether to continue loading the sitemap if an error\\n        occurs loading a url, emitting a warning instead of raising an\\n        exception. Setting this to True makes the loader more robust, but also\\n        may result in missing data. Default: False',\n",
       "   'l': False,\n",
       "   'g': ['.',\n",
       "    'Returns a Page object for the heap region containing the address addr.',\n",
       "    '.\\n    Get a page boundary for a given address.\\n\\n    :param addr:\\n    :return:',\n",
       "    '.']},\n",
       "  {'c': '    def parse_result(self, result: List[Generation]) -> T:\\n',\n",
       "   'd': 'Return response when agent has been stopped due to max iterations.',\n",
       "   'l': False,\n",
       "   'g': ['Parameters\\n    ----------\\n    default : float, optional\\n        The default value. The default is 1.0.\\n    label : str, optional\\n        The label. The default is None.',\n",
       "    '_init_',\n",
       "    '_init_',\n",
       "    '_summary_\\n\\n    Args:\\n        default (Optional[float], optional): _description_. Defaults to None.\\n        label (Optional[str], optional): _description_. Defaults to None.\\n        **kwargs: _description_.']},\n",
       "  {'c': '    def clear(self, **kwargs: Any) -> None:\\n        from gptcache import Cache\\n\\n        for gptcache_instance in self.gptcache_dict.values():\\n            gptcache_instance = cast(Cache, gptcache_instance)\\n            gptcache_instance.flush()\\n\\n        self.gptcache_dict.clear()',\n",
       "   'd': 'Lazy load records from FeatureLayer.',\n",
       "   'l': False,\n",
       "   'g': ['_read_addr_range\\n    \\n    Read a range of memory from the given address space.\\n    \\n    Args:\\n        start (int): The start address.\\n        end (int): The end address.\\n        addr_space (int): The address space to read from.\\n    \\n    Returns:\\n        bytes: The read data.',\n",
       "    '.\\n\\n    :param start:\\n    :param end:\\n    :param addr_space:\\n    :return:',\n",
       "    '.\\n    Reads a range of addresses from the specified address space.\\n    :param start: The start address.\\n    :param end: The end address.\\n    :param addr_space: The address space to read from.\\n    :return: A list of addresses.',\n",
       "    '.read_addr_range(self, start, end, addr_space=None)\\n\\n    Read a range of memory.\\n\\n    Parameters\\n    ----------\\n    start : int\\n        The start address of the memory to read.\\n\\n    end : int\\n        The end address of the memory to read.\\n\\n    addr_space : AddressSpace\\n        The address space to read from.\\n\\n    Returns\\n    -------\\n    data : array_like\\n        The memory read.']},\n",
       "  {'c': '    def get_child(self, tag: Optional[str] = None) -> AsyncCallbackManager:\\n        manager = AsyncCallbackManager(handlers=[], parent_run_id=self.run_id)\\n        manager.set_handlers(self.inheritable_handlers)\\n        manager.add_tags(self.inheritable_tags)\\n        manager.add_metadata(self.inheritable_metadata)\\n        if tag is not None:\\n            manager.add_tags([tag], False)\\n        return manager',\n",
       "   'd': 'Update an attribute of a specified task',\n",
       "   'l': False,\n",
       "   'g': ['search the corresponding LM ID',\n",
       "    'Search for the correspond LM ID in the LM set. \\n    \\n    Parameters\\n    ----------\\n    xAug : array\\n        Augmented state.\\n    PAug : array\\n        Augmented covariance matrix.\\n    zi : array\\n        Innovation matrix.\\n\\n    Returns\\n    -------\\n    minid : int\\n        Correspond LM ID.',\n",
       "    'This function finds the index of the minimum distance between the \\n    input zi and the corresponding LM. \\n    \\n    Parameters\\n    ----------\\n    xAug : array_like\\n        The augmented state vector\\n    PAug : array_like\\n        The augmented state covariance matrix\\n    zi : array_like\\n        The input vector\\n    \\n    Returns\\n    -------\\n    minid : int\\n        The index of the minimum distance',\n",
       "    'def calc_n_LM(xAug):']},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n    rows = self._search_rows(prompt, llm_string)\\n    if rows:\\n        return [loads(row[0]) for row in rows]\\n    return None',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': ['Randomly initializes k centroids from X\\n    \\n    Parameters\\n    ----------\\n    X : np.ndarray\\n        Training data, shape (n_samples, n_features)\\n        \\n    Returns\\n    -------\\n    centroids : np.ndarray\\n        Centroids, shape (k, n_features)',\n",
       "    'Initialise the centroids to random points from the dataset.\\n    \\n    Parameters\\n    ----------\\n    X : array-like, shape = [n_samples, n_features]\\n        The training data, where n_samples is the number of samples and\\n        n_features is the number of features.\\n        \\n    Returns\\n    -------\\n    centroids : ndarray, shape = [k, n_features]\\n        The initial centroids.',\n",
       "    'Initialize centroids randomly\\n    \\n    Parameters\\n    ----------\\n    X : numpy.ndarray\\n        Input data matrix.\\n    \\n    Returns\\n    -------\\n    centroids : numpy.ndarray\\n        Centroids.',\n",
       "    'Initialize centroids randomly\\n    \\n    Parameters\\n    ----------\\n    X : np.ndarray\\n        Input data.\\n    \\n    Returns\\n    -------\\n    centroids : np.ndarray\\n        Initial centroids.']},\n",
       "  {'c': '    def OutputType(self) -> Type[Output]:\\n        for cls in self.__class__.__orig_bases__:\\n            type_args = get_args(cls)\\n            if type_args and len(type_args) == 2:\\n                return type_args[1]\\n\\n        raise TypeError(\\n            f\"Runnable {self.__class__.__name__} doesn\\'t have an inferable OutputType. \"\\n            \"Override the OutputType property to specify the output type.\"\\n        )',\n",
       "   'd': 'The type of output this runnable produces specified as a type annotation.',\n",
       "   'l': True,\n",
       "   'g': ['Set the feeds to be used for scraping',\n",
       "    'Sets the feeds for the scraper.',\n",
       "    'Set the feed urls for the feed extractor.',\n",
       "    'Sets the feeds list']},\n",
       "  {'c': 'def __init__(self, **kwargs: Any) -> None:\\n    separators = self.get_separators_for_language(Language.LATEX)\\n    super().__init__(separators=separators, **kwargs)',\n",
       "   'd': 'Initialize a LatexTextSplitter.',\n",
       "   'l': True,\n",
       "   'g': ['.',\n",
       "    '(self, predictor, default_args):\\n\\n        Decorator for wrapping a predictor\\'s forward method to set the\\n        default arguments passed to the predictor\\'s forward method.\\n\\n        Parameters\\n        ----------\\n        predictor : Predictor\\n            The predictor to decorate.\\n\\n        default_args : dict\\n            The default arguments to set on the predictor\\'s forward method.\\n\\n        Returns\\n        -------\\n        Predictor\\n            The decorated predictor.\\n\\n        Example\\n        -------\\n        >>> from pytorch_lightning.core.lightning import LightningModule\\n        >>> from pytorch_lightning.core.decorators import wrap_forward_with_set_fields\\n        >>>\\n        >>> class MyPredictor(LightningModule):\\n        >>>     def __init__(self, *args, **kwargs):\\n        >>>         super().__init__(*args, **kwargs)\\n        >>>\\n        >>>     @wrap_forward_with_set_fields(default_args={\"a\": 1, \"b\": 2})\\n        >>>     def forward(self, x):\\n        >>>         return x + self.a + self.b\\n        >>>\\n        >>> predictor = MyPredictor()\\n        >>> print(predictor',\n",
       "    '.',\n",
       "    '.']},\n",
       "  {'c': '    def __init__(self, **kwargs: Any):\\n        super().__init__(**kwargs)\\n        try:\\n            from InstructorEmbedding import INSTRUCTOR\\n\\n            self.client = INSTRUCTOR(\\n                self.model_name, cache_folder=self.cache_folder, **self.model_kwargs\\n            )\\n        except ImportError as e:\\n            raise ImportError(\"Dependencies for InstructorEmbedding not found.\") from e',\n",
       "   'd': 'Initialize the sentence_transformer.',\n",
       "   'l': True,\n",
       "   'g': ['Recursively get all children of an element.\\n    \\n    :param element: Element to get children of.\\n    :param tag: Tag name to filter children by.\\n    :return: Set of elements.',\n",
       "    '_recursive_children(element, tag=None, _results=None)\\n\\n    Recursively get all children of the element.\\n\\n    :param element: Element\\n    :param tag: str\\n    :param _results: set\\n    :return: list',\n",
       "    '_recursive_children(element, tag: str = None, _results=None)\\n    \\n    Recursively returns a set of all children of element.\\n    \\n    Args:\\n        element: Element to recursively search for children of.\\n        tag: Optional tag name to filter results by.\\n        _results: Optional set to store results in.\\n    \\n    Returns:\\n        Set of all children of element.',\n",
       "    'Recursively find all children of the element\\n    \\n    Args:\\n        element (Element): Element to find children of\\n        tag (str): Tag name to filter children by\\n        _results (set): Set to store results\\n    \\n    Returns:\\n        list: List of children elements']},\n",
       "  {'c': 'def _call_with_config(\\n    self,\\n    func: Union[\\n        Callable[[Input], Output],\\n        Callable[[Input, CallbackManagerForChainRun], Output],\\n        Callable[[Input, CallbackManagerForChainRun, RunnableConfig], Output],\\n    ],\\n    input: Input,\\n    config: Optional[RunnableConfig],\\n    run_type: Optional[str] = None,\\n) -> Output:\\n    config = ensure_config(config)\\n    callback_manager = get_callback_manager_for_config(config)\\n    run_manager = callback_manager.on_chain_start(\\n        dumpd(self),\\n        input,\\n        run_type=run_type,\\n    )\\n    try:\\n        if accepts_run_manager_and_config(func):\\n            output = func(\\n                input,\\n                run_manager=run_manager,\\n                config=config,\\n            )\\n        elif accepts_run_manager(func):\\n            output = func(input, run_manager=run_manager)\\n        else:\\n            output = func(input)\\n    except Exception as e:\\n        run_manager.on_chain_error(e)\\n        raise\\n    else:\\n        run_manager.on_chain_end(dumpd(output))\\n        return output',\n",
       "   'd': 'Initialize by passing in Redis instance.',\n",
       "   'l': False,\n",
       "   'g': ['{\\n        \"type\": \"input\",\\n        \"label\": \"Input Widget\",\\n        \"placeholder\": \"Enter your name\",\\n        \"required\": True,\\n        \"value\": \"William\"\\n    }',\n",
       "    \"'This function creates a widget that allows the user to enter text.\",\n",
       "    'Create the widget for the input.',\n",
       "    \"'Создаем виджет ввода\"]},\n",
       "  {'c': 'def stream(\\n    self, input: Input, config: Optional[RunnableConfig] = None\\n) -> Iterator[Output]:\\n    yield self.invoke(input, config)',\n",
       "   'd': 'Default implementation of stream, which calls invoke.\\nSubclasses should override this method if they support streaming output.',\n",
       "   'l': True,\n",
       "   'g': [':param df_lists:',\n",
       "    '_init__\\n    :param df_lists: list of tuples (df, prob)',\n",
       "    '_init__\\n\\n    Args:\\n        df_lists (list):',\n",
       "    ':param df_lists: list of tuples or list of lists\\n    :return:']},\n",
       "  {'c': 'def test_vectara_add_documents() -> None:\\n    docsearch: Vectara = Vectara()\\n\\n\\n    texts1 = [\"grounded generation\", \"retrieval augmented generation\", \"data privacy\"]\\n    md = [{\"abbr\": get_abbr(t)} for t in texts1]\\n    doc_id1 = docsearch.add_texts(\\n        texts1,\\n        metadatas=md,\\n        doc_metadata={\"test_num\": \"1\"},\\n    )\\n\\n\\n    texts2 = [\"large language model\", \"information retrieval\", \"question answering\"]\\n    doc_id2 = docsearch.add_documents(\\n        [Document(page_content=t, metadata={\"abbr\": get_abbr(t)}) for t in texts2],\\n        doc_metadata={\"test_num\": \"2\"},\\n    )\\n    doc_ids = doc_id1 + doc_id2\\n\\n\\n    output1 = docsearch.similarity_search(\\n        \"large language model\",\\n        k=2,\\n        n_sentence_context=0,\\n    )\\n    assert len(output1) == 2\\n    assert output1[0].page_content == \"large language model\"\\n    assert output1[0].metadata[\"abbr\"] == \"llm\"\\n    assert output1[1].page_content == \"information retrieval\"\\n    assert output1[1].metadata[\"abbr\"] == \"ir\"\\n\\n\\n\\n    output2 = docsearch.similarity_search(\\n        \"large language model\",\\n        k=1,\\n        n_sentence_context=0,\\n        filter=\"doc.test_num = 1\",\\n    )\\n    assert len(output2) == 1\\n    assert output2[0].page_content == \"retrieval augmented generation\"\\n    assert output2[0].metadata[\"abbr\"] == \"rag\"\\n\\n    for doc_id in doc_ids:\\n        docsearch._delete_doc(doc_id)',\n",
       "   'd': 'Test end to end construction and search.',\n",
       "   'l': True,\n",
       "   'g': ['This method is used to get the page layout of the page.',\n",
       "    'This function will return the page layout of the current document.',\n",
       "    '',\n",
       "    '@return: the current page layout.']},\n",
       "  {'c': 'def dict(self, **kwargs: Any) -> Dict:\\n    output_parser_dict = super().dict(**kwargs)\\n    output_parser_dict[\"_type\"] = self._type\\n    return output_parser_dict',\n",
       "   'd': 'Return dictionary representation of output parser.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Forward pass.\\n\\n    Parameters\\n    ----------\\n    x : torch.Tensor\\n        Input tensor of shape (batch_size, num_nodes, num_features).\\n    emb : torch.Tensor\\n        Embedding tensor of shape (num_nodes, num_features).\\n\\n    Returns\\n    -------\\n    torch.Tensor\\n        Output tensor of shape (batch_size, num_nodes, num_features).',\n",
       "    '',\n",
       "    'forward function']},\n",
       "  {'c': 'def __init__(self) -> None:\\n    self._cache: Dict[Tuple[str, str], RETURN_VAL_TYPE] = {}',\n",
       "   'd': 'Initialize with empty cache.',\n",
       "   'l': True,\n",
       "   'g': ['Decorator for registering a resource body class.',\n",
       "    '_register_resource_type\\n\\n    Register a resource type with the resource body class.\\n\\n    :param cls:\\n    :param resource_body_cls:\\n    :return:',\n",
       "    'Decorator for registering a resource body class.\\n    \\n    :param cls: The class that will be registered.\\n    :param resource_body_cls: The class that will be registered.\\n    :return: The registered class.',\n",
       "    'Register a new resource type']},\n",
       "  {'c': 'def return_stopped_response(\\n    self,\\n    early_stopping_method: str,\\n    intermediate_steps: List[Tuple[AgentAction, str]],\\n    **kwargs: Any,\\n) -> AgentFinish:\\n    if early_stopping_method == \"force\":\\n\\n        return AgentFinish(\\n            {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\\n        )\\n    elif early_stopping_method == \"generate\":\\n\\n        thoughts = \"\"\\n        for action, observation in intermediate_steps:\\n            thoughts += action.log\\n            thoughts += (\\n                f\"\\\\n{self.observation_prefix}{observation}\\\\n{self.llm_prefix}\"\\n            )\\n\\n        thoughts += (\\n            \"\\\\n\\\\nI now need to return a final answer based on the previous steps:\"\\n        )\\n        new_inputs = {\"agent_scratchpad\": thoughts, \"stop\": self._stop}\\n        full_inputs = {**kwargs, **new_inputs}\\n        full_output = self.llm_chain.predict(**full_inputs)\\n\\n        parsed_output = self.output_parser.parse(full_output)\\n        if isinstance(parsed_output, AgentFinish):\\n\\n            return parsed_output\\n        else:\\n\\n\\n            return AgentFinish({\"output\": full_output}, full_output)\\n    else:\\n        raise ValueError(\\n            \"early_stopping_method should be one of `force` or `generate`, \"\\n            f\"got {early_stopping_method}\"\\n        )',\n",
       "   'd': 'Return response when agent has been stopped due to max iterations.',\n",
       "   'l': True,\n",
       "   'g': ['for each raw data file, create an indexable document\\n    and store it in the store',\n",
       "    '.',\n",
       "    '(self)',\n",
       "    '.write() method to write the data to the store.']},\n",
       "  {'c': 'def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n    for gen in return_val:\\n        if not isinstance(gen, Generation):\\n            raise ValueError(\\n                \"RedisSemanticCache only supports caching of \"\\n                f\"normal LLM generations, got {type(gen)}\"\\n            )\\n        if isinstance(gen, ChatGeneration):\\n            warnings.warn(\\n                \"NOTE: Generation has not been cached. RedisSentimentCache does not\"\\n                \" support caching ChatModel outputs.\"\\n            )\\n            return\\n    llm_cache = self._get_llm_cache(llm_string)\\n    _dump_generations_to_json([g for g in return_val])\\n    metadata = {\\n        \"llm_string\": llm_string,\\n        \"prompt\": prompt,\\n        \"return_val\": _dump_generations_to_json([g for g in return_val]),\\n    }\\n    llm_cache.add_texts(texts=[prompt], metadatas=[metadata])',\n",
       "   'd': 'Update cache based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': ['Checks that the given string is a valid transaction hash.',\n",
       "    'Check if the given value is a valid transaction hash.',\n",
       "    'Checks if the given value is a valid transaction hash.\\n\\n    :param value: The value to check.\\n    :type value: str\\n\\n    :raises RPCError: If the given value is not a valid transaction hash.',\n",
       "    'Check if value is a valid transaction hash.']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        openai_api_key = get_from_dict_or_env(\\n            values, \"openai_api_key\", \"OPENAI_API_KEY\"\\n        )\\n        openai_api_base = get_from_dict_or_env(\\n            values,\\n            \"openai_api_base\",\\n            \"OPENAI_API_BASE\",\\n            default=\"\",\\n        )\\n        openai_proxy = get_from_dict_or_env(\\n            values,\\n            \"openai_proxy\",\\n            \"OPENAI_PROXY\",\\n            default=\"\",\\n        )\\n        openai_organization = get_from_dict_or_env(\\n            values, \"openai_organization\", \"OPENAI_ORGANIZATION\", default=\"\"\\n        )\\n        try:\\n            import openai\\n\\n            openai.api_key = openai_api_key\\n            if openai_api_base:\\n                openai.api_base = openai_api_base\\n            if openai_organization:\\n                openai.organization = openai_organization\\n            if openai_proxy:\\n                openai.proxy = {\"http\": openai_proxy, \"https\": openai_proxy}\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import openai python package. \"\\n                \"Please install it with `pip install openai`.\"\\n            )\\n        try:\\n            values[\"client\"] = openai.ChatCompletion\\n        except AttributeError:\\n            raise ValueError(\\n                \"`openai` has no `ChatCompletion` attribute, this is likely \"\\n                \"due to an old version of the openai package. Try upgrading it \"\\n                \"with `pip install --upgrade openai`.\"\\n            )\\n        warnings.warn(\\n            \"You are trying to use a chat model. This way of initializing it is \"\\n            \"no longer supported. Instead, please use: \"\\n            \"`from langchain.chat_models import ChatOpenAI`\"\\n        )\\n        return values',\n",
       "   'd': 'Validate that api key and python package exists in environment.',\n",
       "   'l': True,\n",
       "   'g': ['for the given stage name, get the api url',\n",
       "    '.get_api_url()\\n\\n    Returns:\\n        str: api url',\n",
       "    '- Returns the URL of the API Gateway for a given stage.\\n\\n    :param stage_name: The name of the stage.\\n\\n    :returns: The URL of the API Gateway.\\n\\n    :rtype: str',\n",
       "    '-']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        web_path: Union[str, List[str]],\\n        header_template: Optional[dict] = None,\\n        verify_ssl: Optional[bool] = True,\\n        proxies: Optional[dict] = None,\\n        requests_per_second: int = 2,\\n        requests_kwargs: Optional[Dict[str, Any]] = None,\\n        raise_for_status: bool = False,\\n    ):\\n        if isinstance(web_path, str):\\n            self.web_paths = [web_path]\\n        elif isinstance(web_path, List):\\n            self.web_paths = web_path\\n\\n        headers = header_template or default_header_template\\n        if not headers.get(\"User-Agent\"):\\n            try:\\n                from fake_useragent import UserAgent\\n\\n                headers[\"User-Agent\"] = UserAgent().random\\n            except ImportError:\\n                logger.info(\\n                    \"fake_useragent not found, using default user agent.\"\\n                    \"To get a realistic header for requests, \"\\n                    \"`pip install fake_useragent`.\"\\n                )\\n\\n        self.session = requests.Session()\\n        self.session.headers = dict(headers)\\n        self.session.verify = verify_ssl\\n\\n        if proxies:\\n            self.session.proxies.update(proxies)\\n\\n        self.requests_per_second = requests_per_second\\n        self.requests_kwargs = requests_kwargs or {}\\n        self.raise_for_status = raise_for_status',\n",
       "   'd': 'Initialize with a webpage path.',\n",
       "   'l': True,\n",
       "   'g': ['(self, x):',\n",
       "    'Args:\\n        x: [batch_size, num_frames, 1]\\n\\n    Returns:',\n",
       "    '(self, x):',\n",
       "    'Args:\\n        x (Tensor): [batch, 1, 256, 256]\\n\\n    Returns:\\n        sine_merge (Tensor): [batch, 1, 256, 256]\\n        noise (Tensor): [batch, 1, 256, 256]\\n        uv (Tensor): [batch, 1, 256, 256]']},\n",
       "  {'c': '    def _evaluate_in_project(self, run: Run, evaluator: RunEvaluator) -> None:\\n        try:\\n            if self.project_name is None:\\n                self.client.evaluate_run(run, evaluator)\\n            with tracing_v2_enabled(\\n                project_name=self.project_name, tags=[\"eval\"], client=self.client\\n            ):\\n                self.client.evaluate_run(run, evaluator)\\n        except Exception as e:\\n            logger.error(\\n                f\"Error evaluating run {run.id} with \"\\n                f\"{evaluator.__class__.__name__}: {e}\",\\n                exc_info=True,\\n            )\\n            raise e',\n",
       "   'd': 'Evaluate the run in the project.\\n\\nParameters\\n----------\\nrun : Run\\n    The run to be evaluated.\\nevaluator : RunEvaluator\\n    The evaluator to use for evaluating the run.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Sets the ignore users.\\n\\n    :param users: list of users to ignore\\n    :return: self',\n",
       "    '.\\n\\n    Args:\\n        users (list, optional): List of users to ignore. Defaults to [].\\n\\n    Returns:\\n        self: The current instance of the `UserBot` class.',\n",
       "    '.set_ignore_users(self, users: list = [])\\n\\n    Sets the ignore users for the bot.\\n\\n    Args:\\n        users (list): A list of users to ignore.\\n\\n    Returns:\\n        self: The bot instance.',\n",
       "    '.\\n    Sets the users to ignore.\\n\\n    Args:\\n        users (list, optional): List of users to ignore. Defaults to [].\\n\\n    Returns:\\n        self: The current instance.']},\n",
       "  {'c': '    def similarity_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        embedding = self.embedding.embed_query(text=query)\\n        return self.similarity_search_by_vector(\\n            embedding=embedding,\\n            k=k,\\n        )',\n",
       "   'd': 'Validate that the python package exists in the environment.',\n",
       "   'l': False,\n",
       "   'g': ['Extract features from the given arguments and kwargs.',\n",
       "    'Extract features from the model.\\n    \\n    Arguments:\\n    \\n        *args:\\n            \\n        **kwargs:\\n    \\n    Returns:\\n    \\n        extracted features',\n",
       "    'Extract features from the model.\\n    \\n    Args:\\n        args (tuple): Arguments to pass to the model.\\n        kwargs (dict): Keyword arguments to pass to the model.\\n\\n    Returns:\\n        tuple: Extracted features.',\n",
       "    'Extract features from the given arguments']},\n",
       "  {'c': '    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Any = None,\\n        metadatas: Optional[List[dict]] = None,\\n        index_name: str = \"\",\\n        url: str = \"http://localhost:8882\",\\n        api_key: str = \"\",\\n        add_documents_settings: Optional[Dict[str, Any]] = None,\\n        searchable_attributes: Optional[List[str]] = None,\\n        page_content_builder: Optional[Callable[[Dict[str, str]], str]] = None,\\n        index_settings: Optional[Dict[str, Any]] = None,\\n        verbose: bool = True,\\n        **kwargs: Any,\\n    ) -> Marqo:\\n        try:\\n            import marqo\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import marqo python package. \"\\n                \"Please install it with `pip install marqo`.\"\\n            )\\n\\n        if not index_name:\\n            index_name = str(uuid.uuid4())\\n\\n        client = marqo.Client(url=url, api_key=api_key)\\n\\n        try:\\n            client.create_index(index_name, settings_dict=index_settings or {})\\n            if verbose:\\n                print(f\"Created {index_name} successfully.\")\\n        except Exception:\\n            if verbose:\\n                print(f\"Index {index_name} exists.\")\\n\\n        instance: Marqo = cls(\\n            client,\\n            index_name,\\n            searchable_attributes=searchable_attributes,\\n            add_documents_settings=add_documents_settings or {},\\n            page_content_builder=page_content_builder,\\n        )\\n        instance.add_texts(texts, metadatas)\\n        return instance',\n",
       "   'd': 'Return Marqo initialized from texts. Note that Marqo does not need\\nembeddings, we retain the parameter to adhere to the Liskov\\nsubstitution principle.\\n\\nThis is a quick way to get started with marqo - simply provide your texts and\\nmetadatas and this will create an instance of the data store and index the\\nprovided data.\\n\\nTo know the ids of your documents with this approach you will need to include\\nthem in under the key \"_id\" in your metadatas for each text\\n\\nExample:\\n.. code-block:: python\\n\\n        from langchain.vectorstores import Marqo\\n\\n        datastore = Marqo(texts=[\\'text\\'], index_name=\\'my-first-index\\',\\n        url=\\'http://localhost:8882\\')\\n\\nArgs:\\n    texts (List[str]): A list of texts to index into marqo upon creation.\\n    embedding (Any, optional): Embeddings (not required). Defaults to None.\\n    index_name (str, optional): The name of the index to use, if none is\\n    provided then one will be created with a UUID. Defaults to None.\\n    url (str, optional): The URL for Marqo. Defaults to \"http://localhost:8882\".\\n    api_key (str, optional): The API key for Marqo. Defaults to \"\".\\n    metadatas (Optional[List[dict]], optional): A list of metadatas, to\\n    accompany the texts. Defaults to None.\\n    this is only used when a new index is being created. Defaults to \"cpu\". Can\\n    be \"cpu\" or \"cuda\".\\n    add_documents_settings (Optional[Dict[str, Any]], optional): Settings\\n    for adding documents, see\\n    https://docs.marqo.ai/0.0.16/API-Reference/documents/#query-parameters.\\n    Defaults to {}.\\n    index_settings (Optional[Dict[str, Any]], optional): Index settings if\\n    the index doesn\\'t exist, see\\n    https://docs.marqo.ai/0.0.16/API-Reference/indexes/#index-defaults-object.\\n    Defaults to {}.\\n\\nReturns:\\n    Marqo: An instance of the Marqo vector store',\n",
       "   'l': True,\n",
       "   'g': ['Record the result of a query.',\n",
       "    ':param result: \\n    :return:',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special',\n",
       "   'd': 'Create a new TextSplitter.',\n",
       "   'l': True,\n",
       "   'g': ['_from_keypair\\n    \\n    Args:\\n        cls (_type_): _description_\\n        keypair (_type_): _description_\\n    \\n    Returns:\\n        _type_: _description_',\n",
       "    '.from_keypair(cls, keypair)\\n\\n    Create a new instance of this class from a keypair.\\n\\n    :param keypair: a tuple of 32 bytes\\n    :return: a new instance of this class',\n",
       "    '.from_keypair(cls, keypair) -> cls\\n\\n    Create a new instance from a keypair.',\n",
       "    '(cls, keypair) -> cls\\n\\n    Create a new instance from a keypair.']},\n",
       "  {'c': 'def create_openai_fn_chain(\\n    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\\n    llm: BaseLanguageModel,\\n    prompt: BasePromptTemplate,\\n    *,\\n    output_key: str = \"function\",\\n    output_parser: Optional[BaseLLMOutputParser] = None,\\n    **kwargs: Any,\\n) -> LLMChain:\\n    if not functions:\\n        raise ValueError(\"Need to pass in at least one function. Received zero.\")\\n    openai_functions = [convert_to_openai_function(f) for f in functions]\\n    fn_names = [oai_fn[\"name\"] for oai_fn in openai_functions]\\n    output_parser = output_parser or _get_openai_output_parser(functions, fn_names)\\n    llm_kwargs: Dict[str, Any] = {\\n        \"functions\": openai_functions,\\n    }\\n    if len(openai_functions) == 1:\\n        llm_kwargs[\"function_call\"] = {\"name\": openai_functions[0][\"name\"]}\\n    llm_chain = LLMChain(\\n        llm=llm,\\n        prompt=prompt,\\n        output_parser=output_parser,\\n        llm_kwargs=llm_kwargs,\\n        output_key=output_key,\\n        **kwargs,\\n    )\\n    return llm_chain',\n",
       "   'd': 'Eagerly load the content.',\n",
       "   'l': False,\n",
       "   'g': ['for each count, add the count to the data structure.',\n",
       "    '.',\n",
       "    '.',\n",
       "    '.']},\n",
       "  {'c': 'def _call_with_config(\\n    self,\\n    func: Callable[[Any], Output],\\n    input: Any,\\n    config: Optional[RunnableConfig],\\n    run_type: Optional[str] = None,\\n) -> Output:\\n    config = ensure_config(config)\\n    callback_manager = get_callback_manager_for_config(config)\\n    run_manager = callback_manager.on_chain_start(\\n        dumpd(self),\\n        input if isinstance(input, dict) else {\"input\": input},\\n        run_type=run_type,\\n    )\\n    try:\\n        output = func(input)\\n    except Exception as e:\\n        run_manager.on_chain_error(e)\\n        raise\\n    else:\\n        output_for_tracer = dumpd(output)\\n        run_manager.on_chain_end(\\n            output_for_tracer\\n            if isinstance(output_for_tracer, dict)\\n            else {\"output\": output_for_tracer}\\n        )\\n        return output',\n",
       "   'd': 'Return Vectara documents most similar to query, along with scores.\\n\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 5.\\n    lambda_val: lexical match parameter for hybrid search.\\n    filter: Dictionary of argument(s) to filter on metadata. For example a\\n        filter can be \"doc.rating > 3.0 and part.lang = \\'deu\\'\"} see\\n        https://docs.vectara.com/docs/search-apis/sql/filter-overview\\n        for more details.\\n    n_sentence_context: number of sentences before/after the matching segment\\n        to add\\n\\nReturns:\\n    List of Documents most similar to the query and score for each.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    Returns the offset of the key in the chunk.',\n",
       "    '.\\n    Returns the offset of a chunk key.',\n",
       "    '.\\n    Return the offset of the key in the chunk.\\n\\n    :param key: The key to search for.\\n    :type key: str\\n    :rtype: int',\n",
       "    '.\\n    :param key:\\n    :return:']},\n",
       "  {'c': '    def embed_query(self, text: str) -> List[float]:\\n        return self.embed_documents([text])[0]',\n",
       "   'd': \"Call out to Cohere's embedding endpoint.\\n\\nArgs:\\n    text: The text to embed.\\n\\nReturns:\\n    Embeddings for the text.\",\n",
       "   'l': True,\n",
       "   'g': ['(self, element):\\n        Inserts a new element into the list.\\n        \\n        The element is added to the end of the list.',\n",
       "    '(self, element):\\n        Insert an element into the database.\\n\\n        Args:\\n            element: The element to insert.\\n\\n        Returns:\\n            None.',\n",
       "    '(self, element):',\n",
       "    'for each element, update the cache if the query matches']},\n",
       "  {'c': '    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[Dict[Any, Any]]] = None,\\n        **kwargs: Any,\\n    ) -> ElasticKnnSearch:\\n        index_name = kwargs.get(\"index_name\", str(uuid.uuid4()))\\n        es_connection = kwargs.get(\"es_connection\")\\n        es_cloud_id = kwargs.get(\"es_cloud_id\")\\n        es_user = kwargs.get(\"es_user\")\\n        es_password = kwargs.get(\"es_password\")\\n        vector_query_field = kwargs.get(\"vector_query_field\", \"vector\")\\n        query_field = kwargs.get(\"query_field\", \"text\")\\n        model_id = kwargs.get(\"model_id\")\\n        dims = kwargs.get(\"dims\")\\n\\n        if dims is None:\\n            raise ValueError(\"ElasticKnnSearch requires \\'dims\\' parameter\")\\n\\n        optional_args = {}\\n\\n        if vector_query_field is not None:\\n            optional_args[\"vector_query_field\"] = vector_query_field\\n\\n        if query_field is not None:\\n            optional_args[\"query_field\"] = query_field\\n\\n        knnvectorsearch = cls(\\n            index_name=index_name,\\n            embedding=embedding,\\n            es_connection=es_connection,\\n            es_cloud_id=es_cloud_id,\\n            es_user=es_user,\\n            es_password=es_password,\\n            **optional_args,\\n        )\\n\\n        knnvectorsearch.add_texts(texts, model_id=model_id, dims=dims, **optional_args)\\n\\n        return knnvectorsearch',\n",
       "   'd': 'Create a new ElasticKnnSearch instance and add a list of texts to the\\n    Elasticsearch index.\\n\\nArgs:\\n    texts (List[str]): The texts to add to the index.\\n    embedding (Embeddings): The embedding model to use for transforming the\\n        texts into vectors.\\n    metadatas (List[Dict[Any, Any]], optional): A list of metadata dictionaries\\n        to associate with the texts.\\n    **kwargs: Arbitrary keyword arguments.\\n\\nReturns:\\n    A new ElasticKnnSearch instance.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Test default space.',\n",
       "    '.',\n",
       "    '.',\n",
       "    '.\\n    @pytest.mark.parametrize(\"space_data\", [\\n        {\\n            \"name\": \"test\",\\n            \"description\": \"test\",\\n            \"members\": json.dumps([{\"user_id\": 2, \"role\": \"MASTER\"}, {\"user_id\": 3, \"role\": \"DEVELOPER\"}])\\n        }\\n    ])\\n    def test_get_update_default_space(self, user, testapp, client, space_data):\\n        resp = client.put(\\'%s/%d\\' % (self.uri_prefix, 1), data=space_data)\\n\\n        response_success(resp)\\n        self.compare_space_req_resp(space_data, resp)']},\n",
       "  {'c': 'def test_prompt_jinja2_wrong_input_variables() -> None:\\n    template = \"This is a {{ foo }} test.\"\\n    input_variables = [\"bar\"]\\n    with pytest.warns(UserWarning):\\n        PromptTemplate(\\n            input_variables=input_variables,\\n            template=template,\\n            template_format=\"jinja2\",\\n            validate_template=True,\\n        )\\n    assert PromptTemplate(\\n        input_variables=input_variables, template=template, template_format=\"jinja2\"\\n    ).input_variables == [\"foo\"]',\n",
       "   'd': 'Test warn deprecated.',\n",
       "   'l': False,\n",
       "   'g': ['v\\n    Returns:\\n        bool: True if the operation was successful, False otherwise.',\n",
       "    'Returns True if the request was successful, False otherwise.',\n",
       "    'Returns:\\n        bool: True if the request was successful, False otherwise.',\n",
       "    'Return True if the task was successful, False otherwise.']},\n",
       "  {'c': 'def run(\\n    self,\\n    tool_input: Union[str, Dict],\\n    verbose: Optional[bool] = None,\\n    start_color: Optional[str] = \"green\",\\n    color: Optional[str] = \"green\",\\n    callbacks: Callbacks = None,\\n    *,\\n    tags: Optional[List[str]] = None,\\n    metadata: Optional[Dict[str, Any]] = None,\\n    run_id: Optional[UUID] = None,\\n    **kwargs: Any,\\n) -> Any:\\n    parsed_input = self._parse_input(tool_input)\\n    if not self.verbose and verbose is not None:\\n        verbose_ = verbose\\n    else:\\n        verbose_ = self.verbose\\n    callback_manager = CallbackManager.configure(\\n        callbacks,\\n        self.callbacks,\\n        verbose_,\\n        tags,\\n        self.tags,\\n        metadata,\\n        self.metadata,\\n    )\\n\\n    new_arg_supported = signature(self._run).parameters.get(\"run_manager\")\\n    run_manager = callback_manager.on_tool_start(\\n        {\"name\": self.name, \"description\": self.description},\\n        tool_input if isinstance(tool_input, str) else str(tool_input),\\n        color=start_color,\\n        run_id=run_id,\\n        **kwargs,\\n    )\\n    try:\\n        tool_args, tool_kwargs = self._to_args_and_kwargs(parsed_input)\\n        observation = (\\n            self._run(*tool_args, run_manager=run_manager, **tool_kwargs)\\n            if new_arg_supported\\n            else self._run(*tool_args, **tool_kwargs)\\n        )\\n    except ToolException as e:\\n        if not self.handle_tool_error:\\n            run_manager.on_tool_error(e)\\n            raise e\\n        elif isinstance(self.handle_tool_error, bool):\\n            if e.args:\\n                observation = e.args[0]\\n            else:\\n                observation = \"Tool execution error\"\\n        elif isinstance(self.handle_tool_error, str):\\n            observation = self.handle_tool_error\\n        elif callable(self.handle_tool_error):\\n            observation = self.handle_tool_error(e)\\n        else:\\n            raise ValueError(\\n                f\"Got unexpected type of `handle_tool_error`. Expected bool, str \"\\n                f\"or callable. Received: {self.handle_tool_error}\"\\n            )\\n        run_manager.on_tool_end(\\n            str(observation), color=\"red\", name=self.name, **kwargs\\n        )\\n        return observation\\n    except (Exception, KeyboardInterrupt) as e:\\n        run_manager.on_tool_error(e)\\n        raise e\\n    else:\\n        run_manager.on_tool_end(\\n            str(observation), color=color, name=self.name, **kwargs\\n        )\\n        return observation',\n",
       "   'd': 'Run the tool.',\n",
       "   'l': True,\n",
       "   'g': ['.', '', '.\\n    Initialize the DecodeCache.', '.']},\n",
       "  {'c': '    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n        for gen in return_val:\\n            if not isinstance(gen, Generation):\\n                raise ValueError(\\n                    \"GPTCache only supports caching of normal LLM generations, \"\\n                    f\"got {type(gen)}\"\\n                )\\n        from gptcache.adapter.api import put\\n\\n        _gptcache = self._get_gptcache(llm_string)\\n        handled_data = json.dumps([generation.dict() for generation in return_val])\\n        put(prompt, handled_data, cache_obj=_gptcache)\\n        return None',\n",
       "   'd': 'Load a tracing session.',\n",
       "   'l': False,\n",
       "   'g': ['Decorator for making a poutine queue.\\n    \\n    :param fn: function to decorate\\n    :param queue: queue to use for the poutine\\n    :param max_tries: maximum number of times to retry the poutine\\n    :return: decorated function',\n",
       "    '>>> @queue\\n    >>> def fn(x):\\n    >>>     return x*2\\n    >>> fn(1)\\n    2\\n    >>> fn(2)\\n    4\\n    >>> fn(3)\\n    6\\n    >>> fn(4)\\n    8\\n    >>> fn(5)\\n    10\\n    >>> fn(6)\\n    12\\n    >>> fn(7)\\n    14\\n    >>> fn(8)\\n    16\\n    >>> fn(9)\\n    18\\n    >>> fn(10)\\n    20\\n    >>> fn(11)\\n    22\\n    >>> fn(12)\\n    24\\n    >>> fn(13)\\n    26\\n    >>> fn(14)\\n    28\\n    >>> fn(15)\\n    30\\n    >>> fn(16)\\n    32\\n    >>> fn(17)\\n    34\\n    >>> fn(18)\\n    36\\n    >>> fn(19)\\n    38\\n    >>> fn(20)\\n    40\\n    >>> fn(21',\n",
       "    '>>> queue = queue(lambda x: x*2)\\n    >>> queue(1)\\n    2\\n    >>> queue(2)\\n    4\\n    >>> queue(3)\\n    6\\n    >>> queue(4)\\n    8\\n    >>> queue(5)\\n    10\\n    >>> queue(6)\\n    12\\n    >>> queue(7)\\n    14\\n    >>> queue(8)\\n    16\\n    >>> queue(9)\\n    18\\n    >>> queue(10)\\n    20\\n    >>> queue(11)\\n    22\\n    >>> queue(12)\\n    24\\n    >>> queue(13)\\n    26\\n    >>> queue(14)\\n    28\\n    >>> queue(15)\\n    30\\n    >>> queue(16)\\n    32\\n    >>> queue(17)\\n    34\\n    >>> queue(18)\\n    36\\n    >>> queue(19)\\n    38\\n    >>> queue(20)\\n    40\\n    >>> queue(21)\\n    42',\n",
       "    'Queue a function to be executed in a thread pool\\n    \\n    :param fn: function to execute\\n    :param queue: queue to use for executing the function\\n    :param max_tries: max number of times to retry the function\\n    \\n    :return: function that wraps the function to be executed in a thread pool']},\n",
       "  {'c': '    def check_dependencies(cls, values: Dict) -> Dict:\\n        try:\\n            from nltk.translate.bleu_score import (\\n                SmoothingFunction,\\n                sentence_bleu,\\n            )\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Not all the correct dependencies for this ExampleSelect exist.\"\\n                \"Please install nltk with `pip install nltk`.\"\\n            ) from e\\n\\n        return values',\n",
       "   'd': 'Load a list of URLs using Playwright.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    ':param env: \\n    :param skip:',\n",
       "    ':param env: \\n    :param skip:']},\n",
       "  {'c': '    def get_token_ids(self, text: str) -> List[int]:\\n        if sys.version_info[1] < 8:\\n            return super().get_token_ids(text)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate get_num_tokens. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        enc = tiktoken.encoding_for_model(self.model_name)\\n        return enc.encode(\\n            text,\\n            allowed_special=self.allowed_special,\\n            disallowed_special=self.disallowed_special,\\n        )',\n",
       "   'd': 'Get the token IDs using the tiktoken package.',\n",
       "   'l': True,\n",
       "   'g': ['for log probability of a given parameters',\n",
       "    '.',\n",
       "    '.',\n",
       "    \"(th.Tensor, th.Tensor) -> Tuple[th.Tensor, th.Tensor]\\n\\n    Computes the log-probability of a model's parameters.\\n\\n    Parameters\\n    ----------\\n    *args, **kwargs\\n        Arguments to the model's `log_prob` method.\\n\\n    Returns\\n    -------\\n    log_prob : th.Tensor\\n        The log-probability of the parameters.\\n    log_prob_sum : th.Tensor\\n        The sum of the log-probabilities of the parameters.\"]},\n",
       "  {'c': 'def _type(self) -> str:\\n    return \"default\"',\n",
       "   'd': 'Return the output parser type for serialization.',\n",
       "   'l': True,\n",
       "   'g': ['Compute the beta values for a linear schedule\\n    \\n    Parameters\\n    ----------\\n    num_diffusion_timesteps: int\\n        number of betas to compute\\n    alpha_bar: function\\n        schedule parameter controls the sharpness of the schedule\\n    max_beta: float\\n        the maximum beta to use; use values lower than 1 to\\n        prevent singularities.\\n        \\n    Returns\\n    -------\\n    betas: np.ndarray\\n        the beta values for each timestep',\n",
       "    'Generate a sequence of betas that corresponds to the training schedule\\n    :param num_diffusion_timesteps: the number of betas to generate, these are the training timesteps\\n    :param alpha_bar: the function that estimates the rate at which the exponential decay of the beta will be\\n    :param max_beta: the maximum beta to use; use values between 0.999 and 1.0 to avoid overshooting',\n",
       "    'Calculate the beta values for a given alpha_bar.\\n\\n    Args:\\n        num_diffusion_timesteps (int): Number of diffusion timesteps.\\n        alpha_bar (function): Function that returns the alpha_bar value for a given t.\\n        max_beta (float, optional): Maximum beta value to use. Defaults to 0.999.\\n\\n    Returns:\\n        np.ndarray: Array of beta values.',\n",
       "    'Calculate the beta values for a given alpha_bar function.\\n    \\n    Args:\\n        num_diffusion_timesteps (int): The number of diffusion timesteps.\\n        alpha_bar (function): A function that returns the value of alpha_bar at a given time.\\n        max_beta (float, optional): The maximum beta value to use. Defaults to 0.999.\\n        \\n    Returns:\\n        np.ndarray: An array of beta values.']},\n",
       "  {'c': 'def dict(self, **kwargs: Any) -> Dict:\\n    _dict = super().dict()\\n    del _dict[\"output_parser\"]\\n    return _dict',\n",
       "   'd': 'Return dictionary representation of agent.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Gets the configuration reference for the class.\\n\\n    Returns:\\n        str: The configuration reference for the class.',\n",
       "    '.\\n\\n    Returns:\\n\\n        str:',\n",
       "    '.\\n\\n    Returns:\\n\\n        str: The config reference.',\n",
       "    '_get_config_ref\\n    Returns the config reference']},\n",
       "  {'c': 'def lazy_import_playwright_browsers() -> Tuple[Type[AsyncBrowser], Type[SyncBrowser]]:\\n    try:\\n        from playwright.async_api import Browser as AsyncBrowser\\n        from playwright.sync_api import Browser as SyncBrowser\\n    except ImportError:\\n        raise ImportError(\\n            \"The \\'playwright\\' package is required to use the playwright tools.\"\\n            \" Please install it with \\'pip install playwright\\'.\"\\n        )\\n    return AsyncBrowser, SyncBrowser',\n",
       "   'd': 'Split incoming text and return chunks.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'def __init__(self, exclude=[], include=[], invalid=False):',\n",
       "    '',\n",
       "    'Initialize the class.\\n\\n    @param exclude: A list of strings to exclude from the signature.\\n    @param include: A list of strings to include in the signature.\\n    @param invalid: Whether or not to display invalid signatures.']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[Dict[Any, Any]]] = None,\\n        model_id: Optional[str] = None,\\n        refresh_indices: bool = False,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        if not self.client.indices.exists(index=self.index_name):\\n            dims = kwargs.get(\"dims\")\\n\\n            if dims is None:\\n                raise ValueError(\"ElasticKnnSearch requires \\'dims\\' parameter\")\\n\\n            similarity = kwargs.get(\"similarity\")\\n            optional_args = {}\\n\\n            if similarity is not None:\\n                optional_args[\"similarity\"] = similarity\\n\\n            mapping = self._default_knn_mapping(dims=dims, **optional_args)\\n            self.create_knn_index(mapping)\\n\\n        embeddings = self.embedding.embed_documents(list(texts))\\n\\n\\n        body: List[Mapping[str, Any]] = []\\n        for text, vector in zip(texts, embeddings):\\n            body.extend(\\n                [\\n                    {\"index\": {\"_index\": self.index_name}},\\n                    {\"text\": text, \"vector\": vector},\\n                ]\\n            )\\n\\n        responses = self.client.bulk(operations=body)\\n\\n        ids = [\\n            item[\"index\"][\"_id\"]\\n            for item in responses[\"items\"]\\n            if item[\"index\"][\"result\"] == \"created\"\\n        ]\\n\\n        if refresh_indices:\\n            self.client.indices.refresh(index=self.index_name)\\n\\n        return ids',\n",
       "   'd': 'Return the input keys.\\n\\nReturns:\\n    List of input keys.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    Fits the model to the data.\\n    :param X: The data.\\n    :param Y: The labels.\\n    :return: None.',\n",
       "    '.\\n    Fits the model to the given data.',\n",
       "    '.\\n    Fits the model to the data.\\n\\n    Parameters\\n    ----------\\n    X : np.ndarray\\n        Data points.\\n    Y : np.ndarray\\n        Data labels.\\n\\n    Returns\\n    -------\\n    None',\n",
       "    '.\\n    Fit the model to the data.\\n    Parameters\\n    ----------\\n    X : np.ndarray\\n        The data to fit the model to.\\n    Y : np.ndarray\\n        The labels to fit the model to.\\n    Returns\\n    -------\\n    None']},\n",
       "  {'c': 'def test_octoai_endpoint_call_error() -> None:\\n    llm = OctoAIEndpoint(\\n        endpoint_url=\"https://mpt-7b-demo-kk0powt97tmb.octoai.cloud/generate\",\\n        model_kwargs={\"max_new_tokens\": -1},\\n    )\\n    with pytest.raises(ValueError):\\n        llm(\"Which state is Los Angeles in?\")',\n",
       "   'd': 'Run the LLM on the given prompts.',\n",
       "   'l': False,\n",
       "   'g': ['Sets the model type to \"yolov3\"',\n",
       "    'Sets the model type to YOLOv3',\n",
       "    'Sets the model type to YOLOv3.',\n",
       "    'Sets the model type to YOLOv3.']},\n",
       "  {'c': '    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        text_key: str = \"text\",\\n        index_name: Optional[str] = None,\\n        client: Any = None,\\n        host: List[str] = [\"172.20.31.10:13000\"],\\n        user: str = \"root\",\\n        password: str = \"123123\",\\n        batch_size: int = 500,\\n        **kwargs: Any,\\n    ) -> Dingo:\\n        try:\\n            import dingodb\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import dingo python package. \"\\n                \"Please install it with `pip install dingodb`.\"\\n            )\\n\\n        if client is not None:\\n            dingo_client = client\\n        else:\\n            try:\\n\\n                dingo_client = dingodb.DingoDB(user, password, host)\\n            except ValueError as e:\\n                raise ValueError(f\"Dingo failed to connect: {e}\")\\n        if kwargs is not None and kwargs.get(\"self_id\") is True:\\n            if index_name not in dingo_client.get_index():\\n                dingo_client.create_index(index_name, 1024, auto_id=False)\\n        else:\\n            if index_name not in dingo_client.get_index():\\n                dingo_client.create_index(index_name, 1024)\\n\\n\\n\\n\\n        ids = ids or [str(uuid.uuid1().int)[:13] for _ in texts]\\n        metadatas_list = []\\n        texts = list(texts)\\n        embeds = embedding.embed_documents(texts)\\n        for i, text in enumerate(texts):\\n            metadata = metadatas[i] if metadatas else {}\\n            metadata[text_key] = text\\n            metadatas_list.append(metadata)\\n\\n\\n        for i in range(0, len(list(texts)), batch_size):\\n            j = i + batch_size\\n            add_res = dingo_client.vector_add(\\n                index_name, metadatas_list[i:j], embeds[i:j], ids[i:j]\\n            )\\n            if not add_res:\\n                raise Exception(\"vector add fail\")\\n        return cls(embedding, text_key, client=dingo_client, index_name=index_name)',\n",
       "   'd': 'Run the tool.',\n",
       "   'l': False,\n",
       "   'g': ['-*- coding: utf-8 -*-',\n",
       "    '',\n",
       "    'Returns the encoding of the file.',\n",
       "    'Returns the encoding used to encode the raw data.']},\n",
       "  {'c': '    def format_messages(self, **kwargs: Any) -> List[BaseMessage]:\\n        return [self.format(**kwargs)]',\n",
       "   'd': 'Initialize with Chroma client.',\n",
       "   'l': False,\n",
       "   'g': ['_get_item\\n\\n    Get the object at the given entry.\\n\\n    @param entry: The entry to get the object from.\\n    @param handle_value: If True, return the handle value.\\n    @return: The object at the given entry.',\n",
       "    'Get an object from the object list.',\n",
       "    '.',\n",
       "    '_get_item']},\n",
       "  {'c': '    def __init__(self, **data: Any) -> None:\\n        try:\\n            from google.cloud.discoveryengine_v1beta import SearchServiceClient\\n        except ImportError as exc:\\n            raise ImportError(\\n                \"google.cloud.discoveryengine is not installed.\"\\n                \"Please install it with pip install google-cloud-discoveryengine\"\\n            ) from exc\\n        try:\\n            from google.api_core.client_options import ClientOptions\\n        except ImportError as exc:\\n            raise ImportError(\\n                \"google.api_core.client_options is not installed.\"\\n                \"Please install it with pip install google-api-core\"\\n            ) from exc\\n\\n        super().__init__(**data)\\n\\n\\n\\n        api_endpoint = (\\n            \"discoveryengine.googleapis.com\"\\n            if self.location_id == \"global\"\\n            else f\"{self.location_id}-discoveryengine.googleapis.com\"\\n        )\\n\\n        self._client = SearchServiceClient(\\n            credentials=self.credentials,\\n            client_options=ClientOptions(api_endpoint=api_endpoint),\\n        )\\n\\n        self._serving_config = self._client.serving_config_path(\\n            project=self.project_id,\\n            location=self.location_id,\\n            data_store=self.data_store_id,\\n            serving_config=self.serving_config_id,\\n        )',\n",
       "   'd': 'Initializes private fields.',\n",
       "   'l': True,\n",
       "   'g': ['to iterate over the buffers',\n",
       "    'for each buffer in self._buffers',\n",
       "    '.\\n\\n    Return an iterator over the buffers.',\n",
       "    '.\\n    Return an iterator for the buffers.']},\n",
       "  {'c': 'def _run(\\n    self,\\n    query: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> Union[List[Dict], str]:\\n    try:\\n        return self.api_wrapper.results(\\n            query,\\n            self.max_results,\\n        )\\n    except Exception as e:\\n        return repr(e)',\n",
       "   'd': 'Use the tool.',\n",
       "   'l': True,\n",
       "   'g': ['', '', 'This is a docstring.', '']},\n",
       "  {'c': '    def similarity_search_with_score(\\n        self,\\n        query: str,\\n        k: int = 5,\\n        lambda_val: float = 0.025,\\n        filter: Optional[str] = None,\\n        n_sentence_context: int = 2,\\n        **kwargs: Any,\\n    ) -> List[Tuple[Document, float]]:\\n        data = json.dumps(\\n            {\\n                \"query\": [\\n                    {\\n                        \"query\": query,\\n                        \"start\": 0,\\n                        \"num_results\": k,\\n                        \"context_config\": {\\n                            \"sentences_before\": n_sentence_context,\\n                            \"sentences_after\": n_sentence_context,\\n                        },\\n                        \"corpus_key\": [\\n                            {\\n                                \"customer_id\": self._vectara_customer_id,\\n                                \"corpus_id\": self._vectara_corpus_id,\\n                                \"metadataFilter\": filter,\\n                                \"lexical_interpolation_config\": {\"lambda\": lambda_val},\\n                            }\\n                        ],\\n                    }\\n                ]\\n            }\\n        )\\n\\n        response = self._session.post(\\n            headers=self._get_post_headers(),\\n            url=\"https://api.vectara.io/v1/query\",\\n            data=data,\\n            timeout=self.vectara_api_timeout,\\n        )\\n\\n        if response.status_code != 200:\\n            logger.error(\\n                \"Query failed %s\",\\n                f\"(code {response.status_code}, reason {response.reason}, details \"\\n                f\"{response.text})\",\\n            )\\n            return []\\n\\n        result = response.json()\\n\\n        responses = result[\"responseSet\"][0][\"response\"]\\n        vectara_default_metadata = [\"lang\", \"len\", \"offset\"]\\n        docs = [\\n            (\\n                Document(\\n                    page_content=x[\"text\"],\\n                    metadata={\\n                        m[\"name\"]: m[\"value\"]\\n                        for m in x[\"metadata\"]\\n                        if m[\"name\"] not in vectara_default_metadata\\n                    },\\n                ),\\n                x[\"score\"],\\n            )\\n            for x in responses\\n        ]\\n        return docs',\n",
       "   'd': 'Return Vectara documents most similar to query, along with scores.\\n\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 5.\\n    lambda_val: lexical match parameter for hybrid search.\\n    filter: Dictionary of argument(s) to filter on metadata. For example a\\n        filter can be \"doc.rating > 3.0 and part.lang = \\'deu\\'\"} see\\n        https://docs.vectara.com/docs/search-apis/sql/filter-overview\\n        for more details.\\n    n_sentence_context: number of sentences before/after the matching segment\\n        to add, defaults to 2\\n\\nReturns:\\n    List of Documents most similar to the query and score for each.',\n",
       "   'l': True,\n",
       "   'g': ['for the driver to be set to the current instance.',\n",
       "    '(self, driver):\\n\\n        Sets the driver to use for the test.\\n\\n        Args:\\n            driver (webdriver.WebDriver): The driver to use for the test.\\n\\n        Raises:\\n            ValueError: If `driver` is `None`.',\n",
       "    '(self, driver):',\n",
       "    '.\\n    Sets the driver.\\n\\n    :param driver: The driver to use.\\n    :type driver: :class:`selenium.webdriver.remote.webdriver.WebDriver`\\n    :raises ValueError: If the driver is `None']},\n",
       "  {'c': 'def return_values(self) -> List[str]:\\n    return []',\n",
       "   'd': 'Update cache based on prompt and llm_string.',\n",
       "   'l': False,\n",
       "   'g': ['Randomly flip the image horizontally or vertically.\\n    \\n    Parameters\\n    ----------\\n    \\n    img : torch.Tensor\\n        The image tensor to be flipped.\\n    \\n    d : int\\n        The dimension to be flipped.\\n    \\n    Returns\\n    -------\\n    \\n    torch.Tensor\\n        The flipped image tensor.',\n",
       "    'Randomly flips the image horizontally.\\n\\n    Args:\\n        img (PIL.Image.Image): Image to be flipped.\\n        d (int, optional): The axis to be flipped. Defaults to 0.\\n\\n    Returns:\\n        PIL.Image.Image: Flipped image.',\n",
       "    'Apply a random horizontal flip to the image.\\n    \\n    Parameters\\n    ----------\\n    img : torch.Tensor\\n        Input image.\\n    \\n    d : int, optional\\n        The dimension of the image to flip. Default is 0.\\n    \\n    Returns\\n    -------\\n    torch.Tensor\\n        The flipped image.',\n",
       "    'Flip the image horizontally or vertically.\\n    \\n    Args:\\n        img (Tensor): Input image tensor.\\n        d (int, optional): 0 for horizontal flip, 1 for vertical flip. Defaults to 0.\\n    \\n    Returns:\\n        Tensor: Flipped image tensor.']},\n",
       "  {'c': 'def requires_input(self) -> bool:\\n    return True',\n",
       "   'd': 'Validate that api key and python package exists in environment',\n",
       "   'l': False,\n",
       "   'g': ['(init)',\n",
       "    '.\\n    Args:\\n        source (str): Path to source image.\\n        results (list): List of results.\\n        device (str): Device to use.',\n",
       "    '(Clip) Class for CLIP image retrieval.\\n    Args:\\n        source (str): Path to source image.\\n        results (int): Number of results to return.\\n        device (str): Device to use.',\n",
       "    '.\\n    Args:\\n        source (str): Source image path or URL.\\n        results (list): List of results.\\n        device (str): Device to run the model on.']},\n",
       "  {'c': '    def input_keys(self) -> List[str]:\\n        return list(set(self.llm_chain.input_keys) - {\"intermediate_steps\"})',\n",
       "   'd': 'Return the input keys.\\n\\nReturns:\\n    List of input keys.',\n",
       "   'l': True,\n",
       "   'g': ['This is the handler for the F10 key.', '.', '.', '.']},\n",
       "  {'c': 'def serialize_chat_messages(self, messages: List[Dict]) -> str:\\n    if isinstance(messages, list) and messages:\\n        if isinstance(messages[0], dict):\\n            chat_messages = _get_messages_from_run_dict(messages)\\n        elif isinstance(messages[0], list):\\n\\n            chat_messages = _get_messages_from_run_dict(messages[0])\\n        else:\\n            raise ValueError(f\"Could not extract messages to evaluate {messages}\")\\n        return get_buffer_string(chat_messages)\\n    raise ValueError(f\"Could not extract messages to evaluate {messages}\")',\n",
       "   'd': 'Extract the input messages from the run.',\n",
       "   'l': True,\n",
       "   'g': ['(self, regex):\\n\\n    Returns a query that matches the given regular expression.',\n",
       "    'for the given regex, return a QueryRegex object',\n",
       "    '.\\n    Returns a QueryRegex object for this key.',\n",
       "    'for each character in the regex,\\n    if it is a literal, add it to the query\\n    if it is a group, add it to the query\\n    if it is a quantifier, add it to the query\\n    if it is a quantifier, add it to the query\\n    if it is a quantifier, add it to the query\\n    if it is a quantifier, add it to the query\\n    if it is a quantifier, add it to the query\\n    if it is a quantifier, add it to the query\\n    if it is a quantifier, add it to the query\\n    if it is a quantifier, add it to the query\\n    if it is a quantifier, add it to the query\\n    if it is a quantifier, add it to the query\\n    if it is a quantifier, add it to the query\\n    if it is a quantifier, add it to the query\\n    if it is a quantifier, add it to the query\\n    if it is a quantifier, add it to the query\\n    if it is a quantifier, add it to the query\\n    if it is a quantifier, add it to the']},\n",
       "  {'c': '    def on_chain_start(\\n        self,\\n        serialized: Dict[str, Any],\\n        inputs: Dict[str, Any],\\n        run_id: Optional[UUID] = None,\\n        **kwargs: Any,\\n    ) -> CallbackManagerForChainRun:\\n        if run_id is None:\\n            run_id = uuid4()\\n\\n        _handle_event(\\n            self.handlers,\\n            \"on_chain_start\",\\n            \"ignore_chain\",\\n            serialized,\\n            inputs,\\n            run_id=run_id,\\n            parent_run_id=self.parent_run_id,\\n            tags=self.tags,\\n            metadata=self.metadata,\\n            **kwargs,\\n        )\\n\\n        return CallbackManagerForChainRun(\\n            run_id=run_id,\\n            handlers=self.handlers,\\n            inheritable_handlers=self.inheritable_handlers,\\n            parent_run_id=self.parent_run_id,\\n            tags=self.tags,\\n            inheritable_tags=self.inheritable_tags,\\n            metadata=self.metadata,\\n            inheritable_metadata=self.inheritable_metadata,\\n        )',\n",
       "   'd': 'Run when chain starts running.\\n\\nArgs:\\n    serialized (Dict[str, Any]): The serialized chain.\\n    inputs (Dict[str, Any]): The inputs to the chain.\\n    run_id (UUID, optional): The ID of the run. Defaults to None.\\n\\nReturns:\\n    CallbackManagerForChainRun: The callback manager for the chain run.',\n",
       "   'l': True,\n",
       "   'g': ['Delete a tensor by name',\n",
       "    'Delete a tensor from the graph. \\n    \\n    Args:\\n        name (str): The name of the tensor to delete.',\n",
       "    'Delete a tensor by name.\\n    \\n    Args:\\n        name (str): The name of the tensor to delete.',\n",
       "    'Delete a tensor by name.\\n    \\n    Parameters\\n    ----------\\n    name : str\\n        The name of the tensor to delete.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        init_func: Union[\\n            Callable[[Any, str], None], Callable[[Any], None], None\\n        ] = None,\\n    ):\\n        try:\\n            import gptcache\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import gptcache python package. \"\\n                \"Please install it with `pip install gptcache`.\"\\n            )\\n\\n        self.init_gptcache_func: Union[\\n            Callable[[Any, str], None], Callable[[Any], None], None\\n        ] = init_func\\n        self.gptcache_dict: Dict[str, Any] = {}',\n",
       "   'd': 'Initialize by passing in init function (default: `None`).\\n\\nArgs:\\n    init_func (Optional[Callable[[Any], None]]): init `GPTCache` function\\n    (default: `None`)\\n\\nExample:\\n.. code-block:: python\\n\\n    # Initialize GPTCache with a custom init function\\n    import gptcache\\n    from gptcache.processor.pre import get_prompt\\n    from gptcache.manager.factory import get_data_manager\\n\\n    # Avoid multiple caches using the same file,\\n    causing different llm model caches to affect each other\\n\\n    def init_gptcache(cache_obj: gptcache.Cache, llm str):\\n        cache_obj.init(\\n            pre_embedding_func=get_prompt,\\n            data_manager=manager_factory(\\n                manager=\"map\",\\n                data_dir=f\"map_cache_{llm}\"\\n            ),\\n        )\\n\\n    langchain.llm_cache = GPTCache(init_gptcache)',\n",
       "   'l': True,\n",
       "   'g': ['(float)',\n",
       "    '(dict) -> dict',\n",
       "    '_parse_result(self, d)\\n\\n    Parse a result from the server.\\n\\n    :param dict d: the result dict\\n    :return: the parsed result\\n    :rtype: dict',\n",
       "    '(float) -> float\\n\\n    Return the standard deviation of the data in d.']},\n",
       "  {'c': 'def _batch_with_config(\\n    self,\\n    func: Union[\\n        Callable[[List[Input]], List[Union[Exception, Output]]],\\n        Callable[\\n            [List[Input], List[CallbackManagerForChainRun]],\\n            List[Union[Exception, Output]],\\n        ],\\n        Callable[\\n            [List[Input], List[CallbackManagerForChainRun], List[RunnableConfig]],\\n            List[Union[Exception, Output]],\\n        ],\\n    ],\\n    input: List[Input],\\n    config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,\\n    *,\\n    return_exceptions: bool = False,\\n    run_type: Optional[str] = None,\\n) -> List[Output]:\\n    configs = get_config_list(config, len(input))\\n    callback_managers = [get_callback_manager_for_config(c) for c in configs]\\n    run_managers = [\\n        callback_manager.on_chain_start(\\n            dumpd(self),\\n            input,\\n            run_type=run_type,\\n            name=config.get(\"run_name\"),\\n        )\\n        for callback_manager, input, config in zip(\\n            callback_managers, input, configs\\n        )\\n    ]\\n    try:\\n        if accepts_run_manager_and_config(func):\\n            output = func(\\n                input,\\n                run_manager=run_managers,\\n                config=configs,\\n            )\\n        elif accepts_run_manager(func):\\n            output = func(input, run_manager=run_managers)\\n        else:\\n            output = func(input)\\n    except Exception as e:\\n        for run_manager in run_managers:\\n            run_manager.on_chain_error(e)\\n        if return_exceptions:\\n            return cast(List[Output], [e for _ in input])\\n        else:\\n            raise\\n    else:\\n        first_exception: Optional[Exception] = None\\n        for run_manager, out in zip(run_managers, output):\\n            if isinstance(out, Exception):\\n                first_exception = first_exception or out\\n                run_manager.on_chain_error(out)\\n            else:\\n                run_manager.on_chain_end(dumpd(out))\\n        if return_exceptions or first_exception is None:\\n            return cast(List[Output], output)\\n        else:\\n            raise first_exception',\n",
       "   'd': 'Initialize the loader.\\n\\nArgs:\\n    file_path: A file, url or s3 path for input file\\n    textract_features: Features to be used for extraction, each feature\\n                       should be passed as a str that conforms to the enum\\n                       `Textract_Features`, see `amazon-textract-caller` pkg\\n    client: boto3 textract client (Optional)\\n    credentials_profile_name: AWS profile name, if not default (Optional)\\n    region_name: AWS region, eg us-east-1 (Optional)\\n    endpoint_url: endpoint url for the textract service (Optional)',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Parse level content from text',\n",
       "    'Parse level content from text\\n    \\n    :param text: Text to parse\\n    :param pattern: Pattern for searching\\n    :return: List of tree objects',\n",
       "    ':param text: \\n    :param pattern: \\n    :return:']},\n",
       "  {'c': '    def from_run_and_data_type(\\n        cls,\\n        evaluator: StringEvaluator,\\n        run_type: str,\\n        data_type: DataType,\\n        input_key: Optional[str] = None,\\n        prediction_key: Optional[str] = None,\\n        reference_key: Optional[str] = None,\\n        tags: Optional[List[str]] = None,\\n    ) -> StringRunEvaluatorChain:\\n        if run_type == \"llm\":\\n            run_mapper: StringRunMapper = LLMStringRunMapper()\\n        elif run_type == \"chain\":\\n            run_mapper = ChainStringRunMapper(\\n                input_key=input_key, prediction_key=prediction_key\\n            )\\n        else:\\n            raise ValueError(\\n                f\"Unsupported run type {run_type}. Expected one of \\'llm\\' or \\'chain\\'.\"\\n            )\\n\\n\\n        if reference_key is not None or data_type in (DataType.llm, DataType.chat):\\n            example_mapper = StringExampleMapper(reference_key=reference_key)\\n        elif evaluator.requires_reference:\\n            raise ValueError(\\n                f\"Evaluator {evaluator.evaluation_name} requires a reference\"\\n                \" example from the dataset. Please specify the reference key from\"\\n                \" amongst the dataset outputs keys.\"\\n            )\\n        else:\\n            example_mapper = None\\n        return cls(\\n            name=evaluator.evaluation_name,\\n            run_mapper=run_mapper,\\n            example_mapper=example_mapper,\\n            string_evaluator=evaluator,\\n            tags=tags,\\n        )',\n",
       "   'd': 'Create a StringRunEvaluatorChain from an evaluator and the run and dataset types.\\n\\nThis method provides an easy way to instantiate a StringRunEvaluatorChain, by\\ntaking an evaluator and information about the type of run and the data.\\nThe method supports LLM and chain runs.\\n\\nArgs:\\n    evaluator (StringEvaluator): The string evaluator to use.\\n    run_type (str): The type of run being evaluated.\\n        Supported types are LLM and Chain.\\n    data_type (DataType): The type of dataset used in the run.\\n    input_key (str, optional): The key used to map the input from the run.\\n    prediction_key (str, optional): The key used to map the prediction from the run.\\n    reference_key (str, optional): The key used to map the reference from the dataset.\\n    tags (List[str], optional): List of tags to attach to the evaluation chain.\\n\\nReturns:\\n    StringRunEvaluatorChain: The instantiated evaluation chain.\\n\\nRaises:\\n    ValueError: If the run type is not supported, or if the evaluator requires a\\n        reference from the dataset but the reference key is not provided.',\n",
       "   'l': True,\n",
       "   'g': ['Clear the queue.',\n",
       "    'Clear the jobs list.\\n\\n    :param tag: the tag to clear\\n    :type tag: str or None',\n",
       "    'Clear all jobs.',\n",
       "    'Clear all jobs or jobs with a specific tag.']},\n",
       "  {'c': 'def step(\\n    self, inputs: dict, callbacks: Callbacks = None, **kwargs: Any\\n) -> StepResponse:\\n',\n",
       "   'd': 'Take step.',\n",
       "   'l': True,\n",
       "   'g': ['(Optional) Initialize the checkpoint manager.\\n\\n    Args:\\n        checkpoint (str): Path to the checkpoint.\\n        directory (str): Directory to store the checkpoints.\\n        device (str): Device to use for training.\\n        max_to_keep (int): Maximum number of checkpoints to keep.',\n",
       "    '.',\n",
       "    '.',\n",
       "    '.']},\n",
       "  {'c': 'def load(self) -> List[Document]:\\n    try:\\n        import boto3\\n    except ImportError:\\n        raise ImportError(\\n            \"Could not import boto3 python package. \"\\n            \"Please install it with `pip install boto3`.\"\\n        )\\n    s3 = boto3.resource(\\n        \"s3\",\\n        region_name=self.region_name,\\n        api_version=self.api_version,\\n        use_ssl=self.use_ssl,\\n        verify=self.verify,\\n        endpoint_url=self.endpoint_url,\\n        aws_access_key_id=self.aws_access_key_id,\\n        aws_secret_access_key=self.aws_secret_access_key,\\n        aws_session_token=self.aws_session_token,\\n        boto_config=self.boto_config,\\n    )\\n    bucket = s3.Bucket(self.bucket)\\n    docs = []\\n    for obj in bucket.objects.filter(Prefix=self.prefix):\\n        loader = S3FileLoader(\\n            self.bucket,\\n            obj.key,\\n            region_name=self.region_name,\\n            api_version=self.api_version,\\n            use_ssl=self.use_ssl,\\n            verify=self.verify,\\n            endpoint_url=self.endpoint_url,\\n            aws_access_key_id=self.aws_access_key_id,\\n            aws_secret_access_key=self.aws_secret_access_key,\\n            aws_session_token=self.aws_session_token,\\n            boto_config=self.boto_config,\\n        )\\n        docs.extend(loader.load())\\n    return docs',\n",
       "   'd': 'Configure the callback manager.\\n\\nArgs:\\n    inheritable_callbacks (Optional[Callbacks], optional): The inheritable\\n        callbacks. Defaults to None.\\n    local_callbacks (Optional[Callbacks], optional): The local callbacks.\\n        Defaults to None.\\n    verbose (bool, optional): Whether to enable verbose mode. Defaults to False.\\n    inheritable_tags (Optional[List[str]], optional): The inheritable tags.\\n        Defaults to None.\\n    local_tags (Optional[List[str]], optional): The local tags.\\n        Defaults to None.\\n    inheritable_metadata (Optional[Dict[str, Any]], optional): The inheritable\\n        metadata. Defaults to None.\\n    local_metadata (Optional[Dict[str, Any]], optional): The local metadata.\\n        Defaults to None.\\n\\nReturns:\\n    CallbackManager: The configured callback manager.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    ':param text: Text to be highlighted',\n",
       "    'Highlights the text in the given Text widget.\\n    \\n    Args:\\n        text (Text): The Text widget to highlight.\\n    \\n    Returns:\\n        None',\n",
       "    'Highlights text in the given Text widget.']},\n",
       "  {'c': '    def index(\\n        self,\\n        dims_length: Union[int, None],\\n        vector_query_field: str,\\n        similarity: Union[DistanceStrategy, None],\\n    ) -> Dict:\\n        return {\\n            \"mappings\": {\\n                \"properties\": {\\n                    vector_query_field: {\\n                        \"type\": \"dense_vector\",\\n                        \"dims\": dims_length,\\n                        \"index\": False,\\n                    },\\n                }\\n            }\\n        }',\n",
       "   'd': 'Create the mapping for the Elasticsearch index.',\n",
       "   'l': True,\n",
       "   'g': ['_summary_\\n    Remove the desktop shortcut for the current game.',\n",
       "    'Removes the desktop shortcut for the currently selected game.',\n",
       "    'Remove the desktop shortcut for the selected game.',\n",
       "    '_summary_\\n\\n    Remove the launcher shortcut for the current game.\\n\\n    Args:\\n        _args (tuple): _description_']},\n",
       "  {'c': 'def _llm_type(self) -> str:\\n    return \"fireworks-chat\"',\n",
       "   'd': 'Return type of llm.',\n",
       "   'l': True,\n",
       "   'g': ['(dLdy, X) -> (dX, dW, dB)\\n\\n    dLdy is the derivative of the loss with respect to the output of the\\n    network.\\n    X is the input to the network.',\n",
       "    '.\\n\\n    Parameters\\n    ----------\\n    dLdy : np.ndarray\\n        The derivative of the loss with respect to the output of the last layer.\\n    X : np.ndarray\\n        The input to the last layer.\\n\\n    Returns\\n    -------\\n    dX : np.ndarray\\n        The derivative of the loss with respect to the input to the last layer.\\n    dW : np.ndarray\\n        The derivative of the loss with respect to the weights of the last layer.\\n    dB : np.ndarray\\n        The derivative of the loss with respect to the biases of the last layer.',\n",
       "    '.\\n    Backward pass.',\n",
       "    '.\\n    Backward pass.\\n    Args:\\n        dLdy: gradient of loss with respect to the output of the last layer.\\n        X: input to the last layer.\\n    Returns:\\n        dX, dW, dB: gradients with respect to the input to the first layer.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        init_func: Union[\\n            Callable[[Any, str], None], Callable[[Any], None], None\\n        ] = None,\\n    ):\\n        try:\\n            import gptcache\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import gptcache python package. \"\\n                \"Please install it with `pip install gptcache`.\"\\n            )\\n\\n        self.init_gptcache_func: Union[\\n            Callable[[Any, str], None], Callable[[Any], None], None\\n        ] = init_func\\n        self.gptcache_dict: Dict[str, Any] = {}',\n",
       "   'd': 'Initialize with file path.',\n",
       "   'l': False,\n",
       "   'g': ['_expand_tile__\\n\\n    Args:\\n        value (_type_): _description_\\n        newdim (_type_): _description_\\n\\n    Returns:\\n        _type_: _description_',\n",
       "    'Expand a tensor to a new dimension.\\n    \\n    Args:\\n        value: Tensor to expand.\\n        newdim: New dimension to expand to.\\n    \\n    Returns:\\n        Expanded tensor.',\n",
       "    'Expand a tensor to a new shape.\\n    \\n    Parameters\\n    ----------\\n    value : tensor\\n        The tensor to expand.\\n    newdim : int\\n        The new dimension to expand to.\\n    \\n    Returns\\n    -------\\n    tensor\\n        The expanded tensor.',\n",
       "    '_expand_tile__\\n\\n    Expands a tile of a given value into a new dimension.\\n\\n    Parameters\\n    ----------\\n    value : tensor\\n        The value to be expanded.\\n    newdim : int\\n        The dimension to expand the value to.\\n\\n    Returns\\n    -------\\n    tensor\\n        The expanded tile.']},\n",
       "  {'c': '    def similarity_search_with_score(\\n        self,\\n        query: str,\\n        k: int = 5,\\n        lambda_val: float = 0.025,\\n        filter: Optional[str] = None,\\n        n_sentence_context: int = 2,\\n        **kwargs: Any,\\n    ) -> List[Tuple[Document, float]]:\\n        data = json.dumps(\\n            {\\n                \"query\": [\\n                    {\\n                        \"query\": query,\\n                        \"start\": 0,\\n                        \"num_results\": k,\\n                        \"context_config\": {\\n                            \"sentences_before\": n_sentence_context,\\n                            \"sentences_after\": n_sentence_context,\\n                        },\\n                        \"corpus_key\": [\\n                            {\\n                                \"customer_id\": self._vectara_customer_id,\\n                                \"corpus_id\": self._vectara_corpus_id,\\n                                \"metadataFilter\": filter,\\n                                \"lexical_interpolation_config\": {\"lambda\": lambda_val},\\n                            }\\n                        ],\\n                    }\\n                ]\\n            }\\n        )\\n\\n        response = self._session.post(\\n            headers=self._get_post_headers(),\\n            url=\"https://api.vectara.io/v1/query\",\\n            data=data,\\n            timeout=self.vectara_api_timeout,\\n        )\\n\\n        if response.status_code != 200:\\n            logger.error(\\n                \"Query failed %s\",\\n                f\"(code {response.status_code}, reason {response.reason}, details \"\\n                f\"{response.text})\",\\n            )\\n            return []\\n\\n        result = response.json()\\n\\n        responses = result[\"responseSet\"][0][\"response\"]\\n        vectara_default_metadata = [\"lang\", \"len\", \"offset\"]\\n        docs = [\\n            (\\n                Document(\\n                    page_content=x[\"text\"],\\n                    metadata={\\n                        m[\"name\"]: m[\"value\"]\\n                        for m in x[\"metadata\"]\\n                        if m[\"name\"] not in vectara_default_metadata\\n                    },\\n                ),\\n                x[\"score\"],\\n            )\\n            for x in responses\\n        ]\\n        return docs',\n",
       "   'd': 'Return Vectara documents most similar to query, along with scores.\\n\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 5.\\n    lambda_val: lexical match parameter for hybrid search.\\n    filter: Dictionary of argument(s) to filter on metadata. For example a\\n        filter can be \"doc.rating > 3.0 and part.lang = \\'deu\\'\"} see\\n        https://docs.vectara.com/docs/search-apis/sql/filter-overview\\n        for more details.\\n    n_sentence_context: number of sentences before/after the matching segment\\n        to add, defaults to 2\\n\\nReturns:\\n    List of Documents most similar to the query and score for each.',\n",
       "   'l': True,\n",
       "   'g': ['Returns a string representation of the object.',\n",
       "    'Returns:\\n        str: String representation of the PdfInfo object.',\n",
       "    'Return a string representation of the PdfInfo object.',\n",
       "    'Return a string representation of the PdfInfo object.']},\n",
       "  {'c': 'def _type(self) -> str:\\n    return \"default\"',\n",
       "   'd': 'Return the output parser type for serialization.',\n",
       "   'l': True,\n",
       "   'g': ['timeout decorator',\n",
       "    \"'timeout' decorator for asynchronous functions.\\n\\n    :param timeout: a tuple of (seconds, microseconds)\\n    :return: a timeout object\",\n",
       "    'Timeout decorator',\n",
       "    \"Sets the timeout for the current thread. \\n    \\n    Args:\\n        timeout (tuple): A tuple containing the timeout duration and unit (e.g., (1, 'second')). \\n        If None, no timeout is set. \\n        \\n    Returns:\\n        Timeout: An instance of the Timeout class.\"]},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        embeddings = None\\n        if self.embedding_func is not None:\\n            embeddings = self.embedding_func.embed_documents(list(texts))\\n        if embeddings is None:\\n            raise ValueError(\"embeddings is None\")\\n        if self.flag:\\n            dbs_list = self.vearch.list_dbs()\\n            if self.using_db_name not in dbs_list:\\n                create_db_code = self.vearch.create_db(self.using_db_name)\\n                if not create_db_code:\\n                    raise ValueError(\"create db failed!!!\")\\n            space_list = self.vearch.list_spaces(self.using_db_name)\\n            if self.using_table_name not in space_list:\\n                create_space_code = self._create_space(len(embeddings[0]))\\n                if not create_space_code:\\n                    raise ValueError(\"create space failed!!!\")\\n            docid = []\\n            if embeddings is not None and metadatas is not None:\\n                for text, metadata, embed in zip(texts, metadatas, embeddings):\\n                    profiles: dict[str, Any] = {}\\n                    profiles[\"text\"] = text\\n                    profiles[\"metadata\"] = metadata[\"source\"]\\n                    embed_np = np.array(embed)\\n                    profiles[\"text_embedding\"] = {\\n                        \"feature\": (embed_np / np.linalg.norm(embed_np)).tolist()\\n                    }\\n                    insert_res = self.vearch.insert_one(\\n                        self.using_db_name, self.using_table_name, profiles\\n                    )\\n                    if insert_res[\"status\"] == 200:\\n                        docid.append(insert_res[\"_id\"])\\n                        continue\\n                    else:\\n                        retry_insert = self.vearch.insert_one(\\n                            self.using_db_name, self.using_table_name, profiles\\n                        )\\n                        docid.append(retry_insert[\"_id\"])\\n                        continue\\n        else:\\n            table_path = os.path.join(\\n                self.using_metapath, self.using_table_name + \".schema\"\\n            )\\n            if not os.path.exists(table_path):\\n                dim = len(embeddings[0])\\n                response_code = self._create_table(dim)\\n                if response_code:\\n                    raise ValueError(\"create table failed!!!\")\\n            if embeddings is not None and metadatas is not None:\\n                doc_items = []\\n                for text, metadata, embed in zip(texts, metadatas, embeddings):\\n                    profiles_v: dict[str, Any] = {}\\n                    profiles_v[\"text\"] = text\\n                    profiles_v[\"metadata\"] = metadata[\"source\"]\\n                    embed_np = np.array(embed)\\n                    profiles_v[\"text_embedding\"] = embed_np / np.linalg.norm(embed_np)\\n                    doc_items.append(profiles_v)\\n\\n                docid = self.vearch.add(doc_items)\\n                t_time = 0\\n                while len(docid) != len(embeddings):\\n                    time.sleep(0.5)\\n                    if t_time > 6:\\n                        break\\n                    t_time += 1\\n                self.vearch.dump()\\n        return docid',\n",
       "   'd': 'Returns:\\n    List of ids from adding the texts into the vectorstore.',\n",
       "   'l': True,\n",
       "   'g': ['This function is called when the user clicks the middle button on the mouse.\\n    It checks if the tab at the clicked position is the active tab, and if so,\\n    it closes the tab.',\n",
       "    '',\n",
       "    'Close the current tab when the middle mouse button is pressed.',\n",
       "    '']},\n",
       "  {'c': '    def evaluate(\\n        self,\\n        examples: Sequence[dict],\\n        predictions: Sequence[dict],\\n        question_key: str = \"query\",\\n        answer_key: str = \"answer\",\\n        prediction_key: str = \"result\",\\n        *,\\n        callbacks: Callbacks = None,\\n    ) -> List[dict]:\\n        inputs = [\\n            {\\n                \"query\": example[question_key],\\n                \"answer\": example[answer_key],\\n                \"result\": predictions[i][prediction_key],\\n            }\\n            for i, example in enumerate(examples)\\n        ]\\n\\n        return self.apply(inputs, callbacks=callbacks)',\n",
       "   'd': 'Evaluate question answering examples and predictions.',\n",
       "   'l': True,\n",
       "   'g': ['Returns:\\n        Dict[str, Any]:',\n",
       "    'Serialize the event into a dictionary.',\n",
       "    'Returns:\\n        Dict[str, Any]:',\n",
       "    'Returns the object data in an object that can be serialized into JSON']},\n",
       "  {'c': '    def _get_elements(self) -> List:\\n        from unstructured.partition.auto import partition\\n\\n        try:\\n            import boto3\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import `boto3` python package. \"\\n                \"Please install it with `pip install boto3`.\"\\n            )\\n        s3 = boto3.client(\"s3\")\\n        with tempfile.TemporaryDirectory() as temp_dir:\\n            file_path = f\"{temp_dir}/{self.key}\"\\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n            s3.download_file(self.bucket, self.key, file_path)\\n            return partition(filename=file_path)',\n",
       "   'd': 'Get elements.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Parse a file and return its contents as a string.\\n\\n    Args:\\n        file: The file to parse.\\n        errors: The errors to ignore.\\n\\n    Returns:\\n        The contents of the file as a string.\\n\\n    Raises:\\n        ValueError: If docx2txt is not installed.',\n",
       "    '.\\n    Parses the given file and returns its contents as a string.\\n\\n    Args:\\n        file (Path): The path to the file to be parsed.\\n        errors (str): The error handling strategy to use.\\n\\n    Returns:\\n        str: The contents of the file as a string.\\n\\n    Raises:\\n        ValueError: If the docx2txt library is not installed.',\n",
       "    '.',\n",
       "    'for parsing Microsoft Word files.']},\n",
       "  {'c': '    def input_keys(self) -> List[str]:\\n',\n",
       "   'd': 'Return the input keys.\\n\\n:meta private:',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Args:\\n      random: An instance of the Random class.\\n      cells: A list of cells.\\n      connections: An instance of the Connections class.\\n    Returns:\\n      A cell from the list of cells.',\n",
       "    '.\\n    Args:\\n      random: A Random instance.\\n      cells: A list of Cell instances.\\n      connections: A Connections instance.\\n    Returns:\\n      A Cell instance.',\n",
       "    '.\\n    @param random: the random number generator\\n    @param cells: the cells\\n    @param connections: the connections\\n    @return: the least used cell',\n",
       "    '.\\n\\n    :param random:\\n    :param cells:\\n    :param connections:\\n    :return:']},\n",
       "  {'c': 'def search_api(query: str) -> str:\\n    assert isinstance(query, str)\\n    return f\"API result - {query}\"',\n",
       "   'd': 'Run when chain errors.\\n\\nArgs:\\n    error (Exception or KeyboardInterrupt): The error.',\n",
       "   'l': False,\n",
       "   'g': ['start_project(self, idea, role=\"BOSS\", cause_by=BossRequirement)',\n",
       "    'start_project',\n",
       "    '_start_project\\n\\n    Starts a project with the given idea.\\n\\n    Args:\\n        idea (str): The idea to start the project with.\\n        role (str, optional): The role of the person starting the project. Defaults to \"BOSS\".\\n        cause_by (BossRequirement, optional): The requirement that caused the project to start. Defaults to BossRequirement.\\n\\n    Returns:\\n        None',\n",
       "    '']},\n",
       "  {'c': 'def load(self) -> List[Document]:\\n    return list(self.lazy_load())',\n",
       "   'd': 'Return type of llm.',\n",
       "   'l': False,\n",
       "   'g': ['Test that the get_crypto_info_csv_path function returns a pathlib.Path object.',\n",
       "    'Test that the function returns a pathlib.Path object.',\n",
       "    'Test that the function returns a pathlib.Path object.',\n",
       "    '{type: str}']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\\n        return DEFAULT_FORMATTER_MAPPING[self.template_format](self.template, **kwargs)',\n",
       "   'd': 'Add a list of texts to the Elasticsearch index.\\n\\nArgs:\\n    texts (Iterable[str]): The texts to add to the index.\\n    metadatas (List[Dict[Any, Any]], optional): A list of metadata dictionaries\\n        to associate with the texts.\\n    model_id (str, optional): The ID of the model to use for transforming the\\n        texts into vectors.\\n    refresh_indices (bool, optional): Whether to refresh the Elasticsearch\\n        indices after adding the texts.\\n    **kwargs: Arbitrary keyword arguments.\\n\\nReturns:\\n    A list of IDs for the added texts.',\n",
       "   'l': False,\n",
       "   'g': ['Patch the environment with the values passed in the kwargs.\\n    \\n    Parameters\\n    ----------\\n    kwargs : dict\\n        A dictionary of key-value pairs to be added to the environment.\\n    \\n    Yields\\n    ------\\n    None\\n        Yields None.\\n    \\n    Example\\n    -------\\n    >>> patch_environment(\\n    ...     PYTHONPATH=\"/path/to/python\",\\n    ...     PATH=\"/path/to/bin:/path/to/other/bin\"\\n    ... )',\n",
       "    \"'Patches' environment variables.\",\n",
       "    \"'patch_environment' is a generator function that patches the environment\\n    variables with the values passed in the `kwargs` dictionary.\\n\\n    Args:\\n        **kwargs (dict): A dictionary containing the environment variables to be\\n        patched.\\n\\n    Yields:\\n        None\\n\\n    Example:\\n        >>> patch_environment(FOO=1, BAR=2)\\n        >>> os.environ['FOO'] == '1'\\n        True\\n        >>> os.environ['BAR'] == '2'\\n        True\",\n",
       "    'Patch environment variables.\\n    \\n    :param kwargs: Environment variables to patch.\\n    :return: Generator']},\n",
       "  {'c': '    def parse(self, text: str) -> T:\\n',\n",
       "   'd': 'Parse a single string model output into some structure.\\n\\nArgs:\\n    text: String output of a language model.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': True,\n",
       "   'g': ['Save an event to Sentry.\\n\\n    :param cache_key: Optional cache key.\\n    :param data: Event data.\\n    :param kwargs: Additional kwargs.',\n",
       "    'Saves the event to Sentry.\\n    \\n    :param cache_key: The cache key to use.\\n    :param data: The event data.',\n",
       "    'Save an event to the database.\\n    \\n    :param cache_key: The cache key to use for the event.\\n    :param data: The event data to save.',\n",
       "    'Save event to sentry. \\n    \\n    :param cache_key: (optional) cache key to store event data\\n    :param data: (optional) event data\\n    :param kwargs: (optional) event data']},\n",
       "  {'c': 'def _call(\\n    self,\\n    messages: List[BaseMessage],\\n    stop: Optional[List[str]] = None,\\n    run_manager: Optional[CallbackManagerForLLMRun] = None,\\n    **kwargs: Any,\\n) -> str:\\n    response = self.responses[self.i]\\n    if self.i < len(self.responses) - 1:\\n        self.i += 1\\n    else:\\n        self.i = 0\\n    return response',\n",
       "   'd': 'Create a new TextSplitter.',\n",
       "   'l': False,\n",
       "   'g': ['Calculates the factorial of a number.\\n    \\n    Parameters:\\n    num (int): The number for which to calculate the factorial.\\n    \\n    Returns:\\n    int: The factorial of the given number.',\n",
       "    'calculate factorial of a number',\n",
       "    'This function calculates the factorial of a number. \\n    \\n    Args:\\n        num (int): The number for which the factorial is to be calculated.\\n        \\n    Returns:\\n        int: The factorial of the number.',\n",
       "    'Calculate factorial of a number']},\n",
       "  {'c': '    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n        for gen in return_val:\\n            if not isinstance(gen, Generation):\\n                raise ValueError(\\n                    \"Momento only supports caching of normal LLM generations, \"\\n                    f\"got {type(gen)}\"\\n                )\\n        key = self.__key(prompt, llm_string)\\n        value = _dump_generations_to_json(return_val)\\n        set_response = self.cache_client.set(self.cache_name, key, value, self.ttl)\\n        from momento.responses import CacheSet\\n\\n        if isinstance(set_response, CacheSet.Success):\\n            pass\\n        elif isinstance(set_response, CacheSet.Error):\\n            raise set_response.inner_exception\\n        else:\\n            raise Exception(f\"Unexpected response: {set_response}\")',\n",
       "   'd': 'Store llm generations in cache.\\n\\nArgs:\\n    prompt (str): The prompt run through the language model.\\n    llm_string (str): The language model string.\\n    return_val (RETURN_VAL_TYPE): A list of language model generations.\\n\\nRaises:\\n    SdkException: Momento service or network error\\n    Exception: Unexpected response',\n",
       "   'l': True,\n",
       "   'g': ['Backward step of a single layer\\n    :param input_tensor: Input tensor\\n    :param output_tensor: Output tensor\\n    :param output_tensor_grad: Output tensor gradient\\n    :return: Input tensor gradient',\n",
       "    'Backward step.\\n\\n    :param input_tensor:\\n    :param output_tensor:\\n    :param output_tensor_grad:\\n    :return:',\n",
       "    'Backward step',\n",
       "    'Backward step of a neural network.\\n\\n    Args:\\n        input_tensor (torch.Tensor): The input tensor.\\n        output_tensor (torch.Tensor): The output tensor.\\n        output_tensor_grad (torch.Tensor): The gradient of the output tensor.\\n\\n    Returns:\\n        Optional[torch.Tensor]: The gradient of the input tensor.']},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n    item = self.kv_cache.get(\\n        llm_string=_hash(llm_string),\\n        prompt=_hash(prompt),\\n    )\\n    if item:\\n        return _load_generations_from_json(item[\"body_blob\"])\\n    else:\\n        return None',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': ['Parameters\\n    ----------\\n    ax : matplotlib.axes.Axes\\n    data : array_like\\n    center : float',\n",
       "    'for each data point, draw a point and label it with the value.',\n",
       "    ', draw_points(self, ax, data, center)',\n",
       "    '.\\n    Draw points in a scatter plot.\\n    Parameters\\n    ----------\\n    ax : matplotlib axes object\\n        Axes to draw points on.\\n    data : 1d array\\n        Array of data points.\\n    center : float\\n        Center of the scatter plot.']},\n",
       "  {'c': '    def from_template(cls, template: str, **kwargs: Any) -> ChatPromptTemplate:\\n        prompt_template = PromptTemplate.from_template(template, **kwargs)\\n        message = HumanMessagePromptTemplate(prompt=prompt_template)\\n        return cls.from_messages([message])',\n",
       "   'd': 'Get the default parameters for calling OpenAI API.',\n",
       "   'l': False,\n",
       "   'g': ['', 'Creates the session object.', '', '']},\n",
       "  {'c': 'def _identifying_params(self) -> Mapping[str, Any]:\\n    return {\\n        \"anyscale_service_url\": self.anyscale_service_url,\\n        \"anyscale_service_route\": self.anyscale_service_route,\\n    }',\n",
       "   'd': 'Get the identifying parameters.',\n",
       "   'l': True,\n",
       "   'g': ['.', '.', '.', '.']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        refresh_indices: bool = True,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        try:\\n            from elasticsearch.exceptions import NotFoundError\\n            from elasticsearch.helpers import bulk\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import elasticsearch python package. \"\\n                \"Please install it with `pip install elasticsearch`.\"\\n            )\\n        requests = []\\n        ids = ids or [str(uuid.uuid4()) for _ in texts]\\n        embeddings = self.embedding.embed_documents(list(texts))\\n        dim = len(embeddings[0])\\n        mapping = _default_text_mapping(dim)\\n\\n\\n        try:\\n            self.client.indices.get(index=self.index_name)\\n        except NotFoundError:\\n\\n\\n            self.create_index(self.client, self.index_name, mapping)\\n\\n        for i, text in enumerate(texts):\\n            metadata = metadatas[i] if metadatas else {}\\n            request = {\\n                \"_op_type\": \"index\",\\n                \"_index\": self.index_name,\\n                \"vector\": embeddings[i],\\n                \"text\": text,\\n                \"metadata\": metadata,\\n                \"_id\": ids[i],\\n            }\\n            requests.append(request)\\n        bulk(self.client, requests)\\n\\n        if refresh_indices:\\n            self.client.indices.refresh(index=self.index_name)\\n        return ids',\n",
       "   'd': 'Call function that may optionally accept a run_manager and/or config.',\n",
       "   'l': False,\n",
       "   'g': ['for value < 0, return -inf',\n",
       "    'for value < 0, return -inf',\n",
       "    '(float, float, float) -> float\\n\\n    Returns the log-cumulative distribution function of the\\n    PolyaGamma distribution at a given value.\\n\\n    Arguments:\\n        value (float): the value at which the CDF is evaluated\\n        h (float): the shape parameter\\n        z (float): the scale parameter\\n\\n    Returns:\\n        float: the log-cumulative distribution function of the\\n        PolyaGamma distribution at the given value\\n\\n    Raises:\\n        ValueError: if the value is less than 0 or if h <= 0\\n\\n    Examples:\\n        >>> logcdf(0.5, 2.0, 3.0)\\n        -1.2328174798463305\\n        >>> logcdf(1.0, 2.0, 3.0)\\n        -1.2328174798463305\\n        >>> logcdf(1.5, 2.0, 3.0)\\n        -1.2328174798463305\\n        >>> logcdf(2.0, 2.0',\n",
       "    ', value, h, z):']},\n",
       "  {'c': '    def knn_search(\\n        self,\\n        query: Optional[str] = None,\\n        k: Optional[int] = 10,\\n        query_vector: Optional[List[float]] = None,\\n        model_id: Optional[str] = None,\\n        size: Optional[int] = 10,\\n        source: Optional[bool] = True,\\n        fields: Optional[\\n            Union[List[Mapping[str, Any]], Tuple[Mapping[str, Any], ...], None]\\n        ] = None,\\n        page_content: Optional[str] = \"text\",\\n    ) -> List[Tuple[Document, float]]:\\n        if not source and (\\n            fields is None or not any(page_content in field for field in fields)\\n        ):\\n            raise ValueError(\"If source=False `page_content` field must be in `fields`\")\\n\\n        knn_query_body = self._default_knn_query(\\n            query_vector=query_vector, query=query, model_id=model_id, k=k\\n        )\\n\\n\\n        response = self.client.search(\\n            index=self.index_name,\\n            knn=knn_query_body,\\n            size=size,\\n            source=source,\\n            fields=fields,\\n        )\\n\\n        hits = [hit for hit in response[\"hits\"][\"hits\"]]\\n        docs_and_scores = [\\n            (\\n                Document(\\n                    page_content=hit[\"_source\"][page_content]\\n                    if source\\n                    else hit[\"fields\"][page_content][0],\\n                    metadata=hit[\"fields\"] if fields else {},\\n                ),\\n                hit[\"_score\"],\\n            )\\n            for hit in hits\\n        ]\\n\\n        return docs_and_scores',\n",
       "   'd': 'Will always return list of memory variables.\\n:meta private:',\n",
       "   'l': False,\n",
       "   'g': ['Get a list of documents from the KB.\\n\\n    Args:\\n        file_name (str, optional): The file name to search for. Defaults to None.\\n        metadata (Dict, optional): The metadata to search for. Defaults to {}.\\n\\n    Returns:\\n        List[Document]: A list of documents.',\n",
       "    'Return list of Document objects from the database.\\n    \\n    Parameters\\n    ----------\\n    file_name : str, optional\\n        File name of the document. If None, return all documents.\\n    metadata : Dict, optional\\n        Metadata of the document. If None, return all documents.\\n    \\n    Returns\\n    -------\\n    List[Document]\\n        List of Document objects.',\n",
       "    'Get all documents in a KB.\\n\\n    Args:\\n        file_name (str): File name to search.\\n        metadata (Dict): Metadata to filter documents.\\n\\n    Returns:\\n        List[Document]: List of documents.',\n",
       "    '_list_docs_from_db_\\n\\n    Args:\\n        file_name (str, optional): _description_. Defaults to None.\\n        metadata (Dict, optional): _description_. Defaults to {}.\\n\\n    Returns:\\n        List[Document]: _description_']},\n",
       "  {'c': '    def load(self) -> List[Document]:\\n        docs = []\\n        with open(self.file_path, newline=\"\", encoding=self.encoding) as csvfile:\\n            csv_reader = csv.DictReader(csvfile, **self.csv_args)\\n            for i, row in enumerate(csv_reader):\\n                content = \"\\\\n\".join(f\"{k.strip()}: {v.strip()}\" for k, v in row.items())\\n                try:\\n                    source = (\\n                        row[self.source_column]\\n                        if self.source_column is not None\\n                        else self.file_path\\n                    )\\n                except KeyError:\\n                    raise ValueError(\\n                        f\"Source column \\'{self.source_column}\\' not found in CSV file.\"\\n                    )\\n                metadata = {\"source\": source, \"row\": i}\\n                doc = Document(page_content=content, metadata=metadata)\\n                docs.append(doc)\\n\\n        return docs',\n",
       "   'd': 'For Painless Scripting Search, this is the default query.',\n",
       "   'l': False,\n",
       "   'g': ['(w0)',\n",
       "    '(self, w0=30)\\n\\n    Initializes a ``Keras.Model.Model`` with the following attributes:\\n\\n    - ``w0``: ``float``\\n      The frequency of the sinusoid.\\n\\n    - ``fan_mode``: ``str``\\n      The fan mode of the sinusoid.\\n\\n    - ``power``: ``float``\\n      The power of the sinusoid.\\n\\n    - ``gain``: ``float``\\n      The gain of the sinusoid.',\n",
       "    '',\n",
       "    '.\\n    Initialize the network.']},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n    return self._cache.get((prompt, llm_string), None)',\n",
       "   'd': 'Return the input keys.\\n\\n:meta private:',\n",
       "   'l': False,\n",
       "   'g': ['Return string representation of PdfInfo.',\n",
       "    '',\n",
       "    'Return a string representation of the PdfInfo object.',\n",
       "    '']},\n",
       "  {'c': 'def from_function(\\n    cls,\\n    func: Callable,\\n    name: str,\\n    description: str,\\n    return_direct: bool = False,\\n    args_schema: Optional[Type[BaseModel]] = None,\\n    **kwargs: Any,\\n) -> Tool:\\n    return cls(\\n        name=name,\\n        func=func,\\n        description=description,\\n        return_direct=return_direct,\\n        args_schema=args_schema,\\n        **kwargs,\\n    )',\n",
       "   'd': 'Get default mime-type based parser.',\n",
       "   'l': False,\n",
       "   'g': ['Deletes a game from the database.',\n",
       "    'Deletes a game from the database. \\n    \\n    Parameters\\n    ----------\\n    slug : str\\n        The slug of the game to delete. \\n    \\n    Returns\\n    -------\\n    None',\n",
       "    'Deletes a game from the database',\n",
       "    'Deletes a game from the database.']},\n",
       "  {'c': 'def split_text(self, text: str) -> List[str]:\\n',\n",
       "   'd': 'Return the output parser type for serialization.',\n",
       "   'l': False,\n",
       "   'g': ['Unzip a file entry into the target directory.',\n",
       "    'Unzip file entry to target directory.\\n    \\n    Args:\\n        zip_ref (zipfile.ZipFile): zipfile object.\\n        file_entry (zipfile.ZipInfo): zipfile entry object.\\n        target_dir (str): target directory.',\n",
       "    'Unzip a file entry from a zip file.\\n    \\n    :param zip_ref: A ZipFile object.\\n    :param file_entry: A ZipFile.ZipInfo object.\\n    :param target_dir: The directory to extract the file to.\\n    :return: The path to the extracted file.',\n",
       "    'Unzip a single file entry from a zipfile.\\n    \\n    Args:\\n        zip_ref (zipfile.ZipFile): zipfile object.\\n        file_entry (zipfile.ZipInfo): zip entry object.\\n        target_dir (str): directory to extract to.\\n    \\n    Returns:\\n        str: path to extracted file.']},\n",
       "  {'c': '    def _parse(self, content: str, docs: List[Document]) -> None:\\n        data = self._jq_schema.input(json.loads(content))\\n\\n\\n\\n\\n        if self._content_key is not None:\\n            self._validate_content_key(data)\\n        if self._metadata_func is not None:\\n            self._validate_metadata_func(data)\\n\\n        for i, sample in enumerate(data, len(docs) + 1):\\n            text = self._get_text(sample=sample)\\n            metadata = self._get_metadata(\\n                sample=sample, source=str(self.file_path), seq_num=i\\n            )\\n            docs.append(Document(page_content=text, metadata=metadata))',\n",
       "   'd': 'Convert given content to documents.',\n",
       "   'l': True,\n",
       "   'g': ['Apply rotation to image\\n    \\n    Parameters\\n    ----------\\n    img : numpy.ndarray\\n        Image to rotate.\\n    factor : int, optional\\n        Number of times to rotate the image. The default is 0.\\n    \\n    Returns\\n    -------\\n    numpy.ndarray\\n        Rotated image.',\n",
       "    'Applies a rotation factor to the image.\\n    \\n    Parameters\\n    ----------\\n    img : ndarray\\n        The image to rotate.\\n    factor : int, optional\\n        The rotation factor.\\n    \\n    Returns\\n    -------\\n    ndarray\\n        The rotated image.',\n",
       "    'Apply the filter to the image\\n    \\n    Parameters\\n    ----------\\n    img : ndarray\\n        Input image\\n    factor : int, optional\\n        Rotation factor. The default is 0.\\n    \\n    Returns\\n    -------\\n    ndarray\\n        Filtered image',\n",
       "    'Apply the filter to the image. \\n    \\n    Parameters\\n    ----------\\n    img : array-like\\n        The image to be filtered.\\n    factor : int, optional\\n        The factor to rotate the image by. Defaults to 0.\\n    \\n    Returns\\n    -------\\n    array-like\\n        The filtered image.']},\n",
       "  {'c': '    def similarity_search_with_score(\\n        self, query: str, k: int = 4, filter: Optional[dict] = None\\n    ) -> List[Tuple[Document, float]]:\\n        embedding = self.embedding.embed_query(query)\\n        conn = self.connection_pool.connect()\\n        result = []\\n        where_clause: str = \"\"\\n        where_clause_values: List[Any] = []\\n        if filter:\\n            where_clause = \"WHERE \"\\n            arguments = []\\n\\n            def build_where_clause(\\n                where_clause_values: List[Any],\\n                sub_filter: dict,\\n                prefix_args: List[str] = [],\\n            ) -> None:\\n                for key in sub_filter.keys():\\n                    if isinstance(sub_filter[key], dict):\\n                        build_where_clause(\\n                            where_clause_values, sub_filter[key], prefix_args + [key]\\n                        )\\n                    else:\\n                        arguments.append(\\n                            \"JSON_EXTRACT_JSON({}, {}) = %s\".format(\\n                                self.metadata_field,\\n                                \", \".join([\"%s\"] * (len(prefix_args) + 1)),\\n                            )\\n                        )\\n                        where_clause_values += prefix_args + [key]\\n                        where_clause_values.append(json.dumps(sub_filter[key]))\\n\\n            build_where_clause(where_clause_values, filter)\\n            where_clause += \" AND \".join(arguments)\\n\\n        try:\\n            cur = conn.cursor()\\n            try:\\n                cur.execute(\\n                    \"\"\"SELECT {}, {}, {}({}, JSON_ARRAY_PACK(%s)) as __score\\n                    FROM {} {} ORDER BY __score {} LIMIT %s\"\"\".format(\\n                        self.content_field,\\n                        self.metadata_field,\\n                        self.distance_strategy,\\n                        self.vector_field,\\n                        self.table_name,\\n                        where_clause,\\n                        ORDERING_DIRECTIVE[self.distance_strategy],\\n                    ),\\n                    (\"[{}]\".format(\",\".join(map(str, embedding))),)\\n                    + tuple(where_clause_values)\\n                    + (k,),\\n                )\\n\\n                for row in cur.fetchall():\\n                    doc = Document(page_content=row[0], metadata=row[1])\\n                    result.append((doc, float(row[2])))\\n            finally:\\n                cur.close()\\n        finally:\\n            conn.close()\\n        return result',\n",
       "   'd': 'Return docs most similar to query. Uses cosine similarity.\\n\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 4.\\n    filter: A dictionary of metadata fields and values to filter by.\\n            Defaults to None.\\n\\nReturns:\\n    List of Documents most similar to the query and score for each',\n",
       "   'l': True,\n",
       "   'g': ['.', '.', '.', '.']},\n",
       "  {'c': 'def _type(self) -> str:\\n    raise NotImplementedError(\\n        f\"_type property is not implemented in class {self.__class__.__name__}.\"\\n        \" This is required for serialization.\"\\n    )',\n",
       "   'd': 'Compute doc embeddings using a HuggingFace instruct model.\\n\\nArgs:\\n    texts: The list of texts to embed.\\n\\nReturns:\\n    List of embeddings, one for each text.',\n",
       "   'l': False,\n",
       "   'g': ['Safely convert v to a string.',\n",
       "    '',\n",
       "    'Convert a Python object to a string safely. \\n    \\n    This function is useful for converting objects to a string, \\n    and also for converting objects to a string, and then \\n    passing the result to a function that expects a string.',\n",
       "    'Convert a value to a string, escaping any unicode characters.']},\n",
       "  {'c': 'def on_tool_error(\\n    self,\\n    error: Union[Exception, KeyboardInterrupt],\\n    *,\\n    run_id: UUID,\\n    parent_run_id: Optional[UUID] = None,\\n    **kwargs: Any,\\n) -> Any:\\n',\n",
       "   'd': 'Run when tool errors.',\n",
       "   'l': True,\n",
       "   'g': ['.',\n",
       "    '.\\n    Constructor for the class.',\n",
       "    '(self, result=None, verbose=False, print_all=False)',\n",
       "    '.']},\n",
       "  {'c': 'def dict(self, **kwargs: Any) -> Dict:\\n    _dict = super().dict()\\n    del _dict[\"output_parser\"]\\n    return _dict',\n",
       "   'd': 'Return dictionary representation of agent.',\n",
       "   'l': True,\n",
       "   'g': ['find xpath attribute',\n",
       "    'Find the first node matching xpath with key and val. \\n    \\n    Parameters\\n    ----------\\n    node : Node\\n        The node to search.\\n    xpath : str\\n        The xpath expression to search for.\\n    key : str\\n        The key to search for.\\n    val : str\\n        The value to search for.\\n    \\n    Returns\\n    -------\\n    Node\\n        The first node matching xpath with key and val.',\n",
       "    'Finds an xpath attribute in a given node.\\n    \\n    @param node: a BeautifulSoup node\\n    @param xpath: xpath expression\\n    @param key: attribute name\\n    @param val: attribute value\\n    @return: a BeautifulSoup node',\n",
       "    'Find xpath attribute in node.\\n    \\n    :param node: Node\\n    :param xpath: XPath\\n    :param key: Attribute name\\n    :param val: Attribute value\\n    :return: Node']},\n",
       "  {'c': 'def on_retriever_start(\\n    self,\\n    serialized: Dict[str, Any],\\n    query: str,\\n    *,\\n    run_id: UUID,\\n    parent_run_id: Optional[UUID] = None,\\n    tags: Optional[List[str]] = None,\\n    metadata: Optional[Dict[str, Any]] = None,\\n    **kwargs: Any,\\n) -> None:\\n    parent_run_id_ = str(parent_run_id) if parent_run_id else None\\n    execution_order = self._get_execution_order(parent_run_id_)\\n    start_time = datetime.utcnow()\\n    if metadata:\\n        kwargs.update({\"metadata\": metadata})\\n    retrieval_run = Run(\\n        id=run_id,\\n        name=\"Retriever\",\\n        parent_run_id=parent_run_id,\\n        serialized=serialized,\\n        inputs={\"query\": query},\\n        extra=kwargs,\\n        events=[{\"name\": \"start\", \"time\": start_time}],\\n        start_time=start_time,\\n        execution_order=execution_order,\\n        child_execution_order=execution_order,\\n        tags=tags,\\n        child_runs=[],\\n        run_type=\"retriever\",\\n    )\\n    self._start_trace(retrieval_run)\\n    self._on_retriever_start(retrieval_run)',\n",
       "   'd': 'Run when Retriever starts running.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'def reduce_CreateInheritableAnnotation(self, *kids):',\n",
       "    '_reduce_CreateInheritableAnnotation']},\n",
       "  {'c': 'def search_api(query: str) -> str:\\n    assert isinstance(query, str)\\n    return f\"API result - {query}\"',\n",
       "   'd': 'Run when chain errors.',\n",
       "   'l': False,\n",
       "   'g': ['Returns a list of watched extensions.',\n",
       "    'Returns a list of watched extensions.',\n",
       "    '_watchedExtensions',\n",
       "    'Get a list of watched extensions.\\n    \\n    :return: List of watched extensions.\\n    :rtype: list']},\n",
       "  {'c': '    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Any = None,\\n        metadatas: Optional[List[dict]] = None,\\n        index_name: str = \"\",\\n        url: str = \"http://localhost:8882\",\\n        api_key: str = \"\",\\n        add_documents_settings: Optional[Dict[str, Any]] = None,\\n        searchable_attributes: Optional[List[str]] = None,\\n        page_content_builder: Optional[Callable[[Dict[str, str]], str]] = None,\\n        index_settings: Optional[Dict[str, Any]] = None,\\n        verbose: bool = True,\\n        **kwargs: Any,\\n    ) -> Marqo:\\n        try:\\n            import marqo\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import marqo python package. \"\\n                \"Please install it with `pip install marqo`.\"\\n            )\\n\\n        if not index_name:\\n            index_name = str(uuid.uuid4())\\n\\n        client = marqo.Client(url=url, api_key=api_key)\\n\\n        try:\\n            client.create_index(index_name, settings_dict=index_settings or {})\\n            if verbose:\\n                print(f\"Created {index_name} successfully.\")\\n        except Exception:\\n            if verbose:\\n                print(f\"Index {index_name} exists.\")\\n\\n        instance: Marqo = cls(\\n            client,\\n            index_name,\\n            searchable_attributes=searchable_attributes,\\n            add_documents_settings=add_documents_settings or {},\\n            page_content_builder=page_content_builder,\\n        )\\n        instance.add_texts(texts, metadatas)\\n        return instance',\n",
       "   'd': 'Return Marqo initialized from texts. Note that Marqo does not need\\nembeddings, we retain the parameter to adhere to the Liskov\\nsubstitution principle.\\n\\nThis is a quick way to get started with marqo - simply provide your texts and\\nmetadatas and this will create an instance of the data store and index the\\nprovided data.\\n\\nTo know the ids of your documents with this approach you will need to include\\nthem in under the key \"_id\" in your metadatas for each text\\n\\nExample:\\n.. code-block:: python\\n\\n        from langchain.vectorstores import Marqo\\n\\n        datastore = Marqo(texts=[\\'text\\'], index_name=\\'my-first-index\\',\\n        url=\\'http://localhost:8882\\')\\n\\nArgs:\\n    texts (List[str]): A list of texts to index into marqo upon creation.\\n    embedding (Any, optional): Embeddings (not required). Defaults to None.\\n    index_name (str, optional): The name of the index to use, if none is\\n    provided then one will be created with a UUID. Defaults to None.\\n    url (str, optional): The URL for Marqo. Defaults to \"http://localhost:8882\".\\n    api_key (str, optional): The API key for Marqo. Defaults to \"\".\\n    metadatas (Optional[List[dict]], optional): A list of metadatas, to\\n    accompany the texts. Defaults to None.\\n    this is only used when a new index is being created. Defaults to \"cpu\". Can\\n    be \"cpu\" or \"cuda\".\\n    add_documents_settings (Optional[Dict[str, Any]], optional): Settings\\n    for adding documents, see\\n    https://docs.marqo.ai/0.0.16/API-Reference/documents/#query-parameters.\\n    Defaults to {}.\\n    index_settings (Optional[Dict[str, Any]], optional): Index settings if\\n    the index doesn\\'t exist, see\\n    https://docs.marqo.ai/0.0.16/API-Reference/indexes/#index-defaults-object.\\n    Defaults to {}.\\n\\nReturns:\\n    Marqo: An instance of the Marqo vector store',\n",
       "   'l': True,\n",
       "   'g': ['Check if toolbar should be displayed.',\n",
       "    'Show the admin toolbar only for internal IPs.',\n",
       "    'Show the toolbar only for the internal IP addresses.',\n",
       "    'Show toolbar on debug mode']},\n",
       "  {'c': '    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        document_embeddings = []\\n\\n        for text in texts:\\n            document_embeddings.append(self._embed(text))\\n        return document_embeddings',\n",
       "   'd': \"Call out to Aleph Alpha's Document endpoint.\\n\\nArgs:\\n    texts: The list of texts to embed.\\n\\nReturns:\\n    List of embeddings, one for each text.\",\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Returns a sorter for sorting groups.\\n    \\n    Parameters\\n    ----------\\n    group_index : array_like\\n        Index of groups.\\n    ngroups : int\\n        Number of groups.\\n    \\n    Returns\\n    -------\\n    sorter : array_like\\n        Index of groups.',\n",
       "    '',\n",
       "    '']},\n",
       "  {'c': 'def test_deeplakewith_persistence() -> None:\\n    dataset_path = \"./tests/persist_dir\"\\n    if deeplake.exists(dataset_path):\\n        deeplake.delete(dataset_path)\\n\\n    texts = [\"foo\", \"bar\", \"baz\"]\\n    docsearch = DeepLake.from_texts(\\n        dataset_path=dataset_path,\\n        texts=texts,\\n        embedding=FakeEmbeddings(),\\n    )\\n\\n    output = docsearch.similarity_search(\"foo\", k=1)\\n    assert output == [Document(page_content=\"foo\")]\\n\\n\\n    docsearch = DeepLake(\\n        dataset_path=dataset_path,\\n        embedding_function=FakeEmbeddings(),\\n    )\\n    output = docsearch.similarity_search(\"foo\", k=1)\\n\\n\\n    docsearch.delete_dataset()',\n",
       "   'd': 'Test error is raised when input variables are not provided.',\n",
       "   'l': False,\n",
       "   'g': ['', 'new_objfile_handler:', '', '']},\n",
       "  {'c': 'def test_deduplication(\\n    record_manager: SQLRecordManager, vector_store: VectorStore\\n) -> None:\\n    docs = [\\n        Document(\\n            page_content=\"This is a test document.\",\\n            metadata={\"source\": \"1\"},\\n        ),\\n        Document(\\n            page_content=\"This is a test document.\",\\n            metadata={\"source\": \"1\"},\\n        ),\\n    ]\\n\\n\\n    assert index(docs, record_manager, vector_store, cleanup=\"full\") == {\\n        \"num_added\": 1,\\n        \"num_deleted\": 0,\\n        \"num_skipped\": 0,\\n        \"num_updated\": 0,\\n    }',\n",
       "   'd': 'Append the message to the record in db',\n",
       "   'l': False,\n",
       "   'g': ['.',\n",
       "    '.\\n    Returns:\\n        str: The title of the game.',\n",
       "    'for the title of the game',\n",
       "    '.\\n    Returns\\n    -------\\n    str\\n        The title of the game.']},\n",
       "  {'c': '    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n        for gen in return_val:\\n            if not isinstance(gen, Generation):\\n                raise ValueError(\\n                    \"Momento only supports caching of normal LLM generations, \"\\n                    f\"got {type(gen)}\"\\n                )\\n        key = self.__key(prompt, llm_string)\\n        value = _dump_generations_to_json(return_val)\\n        set_response = self.cache_client.set(self.cache_name, key, value, self.ttl)\\n        from momento.responses import CacheSet\\n\\n        if isinstance(set_response, CacheSet.Success):\\n            pass\\n        elif isinstance(set_response, CacheSet.Error):\\n            raise set_response.inner_exception\\n        else:\\n            raise Exception(f\"Unexpected response: {set_response}\")',\n",
       "   'd': 'Store llm generations in cache.\\n\\nArgs:\\n    prompt (str): The prompt run through the language model.\\n    llm_string (str): The language model string.\\n    return_val (RETURN_VAL_TYPE): A list of language model generations.\\n\\nRaises:\\n    SdkException: Momento service or network error\\n    Exception: Unexpected response',\n",
       "   'l': True,\n",
       "   'g': ['Return the first match of the given regex in the given string.',\n",
       "    'Extract text from the document using a regex.',\n",
       "    'Extract all text from the regex in the current document.',\n",
       "    'Extracts text from the HTML document using a regex pattern.\\n    \\n    Args:\\n        regex (str): The regex pattern to use for extraction.\\n    \\n    Returns:\\n        str: The extracted text.']},\n",
       "  {'c': '    def from_function(\\n        cls,\\n        func: Callable,\\n        name: Optional[str] = None,\\n        description: Optional[str] = None,\\n        return_direct: bool = False,\\n        args_schema: Optional[Type[BaseModel]] = None,\\n        infer_schema: bool = True,\\n        **kwargs: Any,\\n    ) -> StructuredTool:\\n        name = name or func.__name__\\n        description = description or func.__doc__\\n        assert (\\n            description is not None\\n        ), \"Function must have a docstring if description not provided.\"\\n\\n\\n\\n        description = f\"{name}{signature(func)} - {description.strip()}\"\\n        _args_schema = args_schema\\n        if _args_schema is None and infer_schema:\\n            _args_schema = create_schema_from_function(f\"{name}Schema\", func)\\n        return cls(\\n            name=name,\\n            func=func,\\n            args_schema=_args_schema,\\n            description=description,\\n            return_direct=return_direct,\\n            **kwargs,\\n        )',\n",
       "   'd': 'Create tool from a given function.\\n\\nA classmethod that helps to create a tool from a function.\\n\\nArgs:\\n    func: The function from which to create a tool\\n    name: The name of the tool. Defaults to the function name\\n    description: The description of the tool. Defaults to the function docstring\\n    return_direct: Whether to return the result directly or as a callback\\n    args_schema: The schema of the tool\\'s input arguments\\n    infer_schema: Whether to infer the schema from the function\\'s signature\\n    **kwargs: Additional arguments to pass to the tool\\n\\nReturns:\\n    The tool\\n\\nExamples:\\n    ... code-block:: python\\n        def add(a: int, b: int) -> int:\\n            \"\"\"Add two numbers\"\"\"\\n            return a + b\\n        tool = StructuredTool.from_function(add)\\n        tool.run(1, 2) # 3',\n",
       "   'l': True,\n",
       "   'g': ['Sorts the results based on the score of the query.\\n\\n    Args:\\n        rank_results (bool): Whether to rank the results based on the score of the query.\\n        hits (List[dict]): The list of hits.\\n\\n    Returns:\\n        List[dict]: The sorted list of hits.',\n",
       "    'Sort the results by score or cross_score.\\n    \\n    Args:\\n        rank_results (bool): Whether to rank results by score or cross_score.\\n        hits (List[dict]): The list of hits.\\n    \\n    Returns:\\n        List[dict]: The sorted list of hits.',\n",
       "    'Sort the hits by score.',\n",
       "    '_summary_\\n\\n    Args:\\n        rank_results (bool):\\n        hits (List[dict]):\\n\\n    Returns:\\n        List[dict]:']},\n",
       "  {'c': 'def load_prompt_from_config(config: dict) -> BasePromptTemplate:\\n    if \"_type\" not in config:\\n        logger.warning(\"No `_type` key found, defaulting to `prompt`.\")\\n    config_type = config.pop(\"_type\", \"prompt\")\\n\\n    if config_type not in type_to_loader_dict:\\n        raise ValueError(f\"Loading {config_type} prompt not supported\")\\n\\n    prompt_loader = type_to_loader_dict[config_type]\\n\\n\\n\\n    return prompt_loader(config)',\n",
       "   'd': 'Load prompt from Config Dict.',\n",
       "   'l': True,\n",
       "   'g': ['Import a symbol from the current module.\\n\\n    :param attr: The symbol to import.\\n    :return: The imported symbol.',\n",
       "    \"Import a symbol from the app's models module.\\n    \\n    For example, if the app's models module is named `app.models`, this\\n    method will import the symbol `app.models.SomeModel`.\\n    \\n    This method works in the same way as `import_relative_to_app` in\\n    `flask_sqlalchemy.model`, except it works in the app's models module.\\n    \\n    Args:\\n        attr (str): The symbol to import.\\n    \\n    Returns:\\n        Any: The symbol.\\n    \\n    Raises:\\n        ImportError: If the symbol could not be imported.\",\n",
       "    '_get_symbol_by_name_from_app_module_',\n",
       "    \"Import a symbol from the app's models module. \\n    \\n    This is a wrapper around the `symbol_by_name` function in the `flask.app` module. \\n    \\n    This is used to import models from the app's models module. \\n    \\n    This is used to import models from the app's models module. \\n    \\n    This is used to import models from the app's models module. \\n    \\n    This is used to import models from the app's models module. \\n    \\n    This is used to import models from the app's models module. \\n    \\n    This is used to import models from the app's models module. \\n    \\n    This is used to import models from the app's models module. \\n    \\n    This is used to import models from the app's models module. \\n    \\n    This is used to import models from the app's models module. \\n    \\n    This is used to import models from the app's models module. \\n    \\n    This is used to import models from the app's models module.\"]},\n",
       "  {'c': 'def _run(\\n    self,\\n    query: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> str:\\n    return self.llm_chain.predict(\\n        query=query, callbacks=run_manager.get_child() if run_manager else None\\n    )',\n",
       "   'd': 'Use the LLM to check the query.',\n",
       "   'l': True,\n",
       "   'g': ['for deterministic = False\\n        \\n        Returns:\\n            th.Tensor: action distribution.',\n",
       "    '.\\n    Args:\\n        observation: Observation to predict actions for.\\n        deterministic: Whether to return deterministic actions.\\n    Returns:\\n        Actions predicted by the model.',\n",
       "    '.\\n    Args:\\n        observation: Observation to predict actions for.\\n        deterministic: Whether to return the deterministic actions or the\\n            actions sampled from the distribution.\\n    Returns:\\n        Actions predicted by the model.',\n",
       "    '(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        bulk_size: int = 500,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        embeddings = self.embedding_function.embed_documents(list(texts))\\n        _validate_embeddings_and_bulk_size(len(embeddings), bulk_size)\\n        index_name = _get_kwargs_value(kwargs, \"index_name\", self.index_name)\\n        text_field = _get_kwargs_value(kwargs, \"text_field\", \"text\")\\n        dim = len(embeddings[0])\\n        engine = _get_kwargs_value(kwargs, \"engine\", \"nmslib\")\\n        space_type = _get_kwargs_value(kwargs, \"space_type\", \"l2\")\\n        ef_search = _get_kwargs_value(kwargs, \"ef_search\", 512)\\n        ef_construction = _get_kwargs_value(kwargs, \"ef_construction\", 512)\\n        m = _get_kwargs_value(kwargs, \"m\", 16)\\n        vector_field = _get_kwargs_value(kwargs, \"vector_field\", \"vector_field\")\\n        max_chunk_bytes = _get_kwargs_value(kwargs, \"max_chunk_bytes\", 1 * 1024 * 1024)\\n\\n        _validate_aoss_with_engines(self.is_aoss, engine)\\n\\n        mapping = _default_text_mapping(\\n            dim, engine, space_type, ef_search, ef_construction, m, vector_field\\n        )\\n\\n        return _bulk_ingest_embeddings(\\n            self.client,\\n            index_name,\\n            embeddings,\\n            texts,\\n            metadatas=metadatas,\\n            ids=ids,\\n            vector_field=vector_field,\\n            text_field=text_field,\\n            mapping=mapping,\\n            max_chunk_bytes=max_chunk_bytes,\\n            is_aoss=self.is_aoss,\\n        )',\n",
       "   'd': 'Run more texts through the embeddings and add to the vectorstore.\\n\\nArgs:\\n    texts: Iterable of strings to add to the vectorstore.\\n    metadatas: Optional list of metadatas associated with the texts.\\n    ids: Optional list of ids to associate with the texts.\\n    bulk_size: Bulk API request count; Default: 500\\n\\nReturns:\\n    List of ids from adding the texts into the vectorstore.\\n\\nOptional Args:\\n    vector_field: Document field embeddings are stored in. Defaults to\\n    \"vector_field\".\\n\\n    text_field: Document field the text of the document is stored in. Defaults\\n    to \"text\".',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Preconvert an item into a serializable form. \\n    \\n    Parameters\\n    ----------\\n    item : object\\n        The item to preconvert.\\n\\n    Returns\\n    -------\\n    object\\n        The preconverted item.',\n",
       "    'Preconvert items to appropriate types.',\n",
       "    'Convert a value to a list.']},\n",
       "  {'c': 'def on_retriever_error(\\n    self,\\n    error: BaseException,\\n    **kwargs: Any,\\n) -> None:\\n    _handle_event(\\n        self.handlers,\\n        \"on_retriever_error\",\\n        \"ignore_retriever\",\\n        error,\\n        run_id=self.run_id,\\n        parent_run_id=self.parent_run_id,\\n        tags=self.tags,\\n        **kwargs,\\n    )',\n",
       "   'd': 'Run when retriever errors.',\n",
       "   'l': True,\n",
       "   'g': ['.',\n",
       "    '(int)\\n    Returns:\\n        (int)',\n",
       "    '(self, cfg):',\n",
       "    '(self, cfg) -> int:']},\n",
       "  {'c': 'def search_api(query: str) -> str:\\n    return \"API result\"',\n",
       "   'd': 'Search the API for the query.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Returns a dictionary with environment variables.',\n",
       "    'Returns a dict of environment variables.',\n",
       "    'Get the environment variables for the game.\\n    \\n    :param prefer_system_libs: Whether to prefer system libraries over wine libraries.\\n    :param wine_path: Path to the wine installation.\\n    :return: The environment variables.']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: List[str],\\n        metadatas: Optional[List[dict]] = None,\\n        doc_metadata: Optional[dict] = None,\\n    ) -> None:\\n        self.vectorstore.add_texts(texts, metadatas, doc_metadata or {})',\n",
       "   'd': 'Return type of llm.',\n",
       "   'l': False,\n",
       "   'g': ['Test clone method',\n",
       "    'Test clone method.',\n",
       "    'Test clone method.',\n",
       "    '_test_clone(self, estimator_instance)\\n\\n    Test clone function.\\n\\n    Parameters\\n    ----------\\n    estimator_instance : estimator\\n        estimator instance.\\n\\n    Returns\\n    -------\\n    None.']},\n",
       "  {'c': '    def __init__(self, file_path: str):\\n        try:\\n            from pdfminer.high_level import extract_text_to_fp\\n        except ImportError:\\n            raise ImportError(\\n                \"`pdfminer` package not found, please install it with \"\\n                \"`pip install pdfminer.six`\"\\n            )\\n\\n        super().__init__(file_path)',\n",
       "   'd': 'Initialize with a file path.',\n",
       "   'l': True,\n",
       "   'g': ['Initializes input for the process.\\n\\n    Parameters\\n    ----------\\n    img1_path : str\\n        Path to the first image.\\n    img2_path : str, optional\\n        Path to the second image. The default is None.\\n\\n    Returns\\n    -------\\n    img_list : list\\n        List of images.\\n    bulkProcess : bool\\n        True if the input is a list of images, False otherwise.',\n",
       "    'img1_path: str or list of str\\n    img2_path: str or list of str',\n",
       "    'img1_path : str\\n        path of image to be processed\\n    img2_path : str\\n        path of image to be processed',\n",
       "    'Initialize the input data for the model. \\n    \\n    Parameters\\n    ----------\\n    img1_path : str or list\\n        The path of the first image.\\n    img2_path : str or None\\n        The path of the second image. If None, then img1_path will be used as the second image.\\n\\n    Returns\\n    -------\\n    img_list : list\\n        A list of the image paths.\\n    bulkProcess : bool\\n        A boolean indicating whether the input is a list of image paths.']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        bulk_size: int = 500,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        embeddings = self.embedding_function.embed_documents(list(texts))\\n        _validate_embeddings_and_bulk_size(len(embeddings), bulk_size)\\n        text_field = _get_kwargs_value(kwargs, \"text_field\", \"text\")\\n        dim = len(embeddings[0])\\n        engine = _get_kwargs_value(kwargs, \"engine\", \"nmslib\")\\n        space_type = _get_kwargs_value(kwargs, \"space_type\", \"l2\")\\n        ef_search = _get_kwargs_value(kwargs, \"ef_search\", 512)\\n        ef_construction = _get_kwargs_value(kwargs, \"ef_construction\", 512)\\n        m = _get_kwargs_value(kwargs, \"m\", 16)\\n        vector_field = _get_kwargs_value(kwargs, \"vector_field\", \"vector_field\")\\n        max_chunk_bytes = _get_kwargs_value(kwargs, \"max_chunk_bytes\", 1 * 1024 * 1024)\\n\\n        _validate_aoss_with_engines(self.is_aoss, engine)\\n\\n        mapping = _default_text_mapping(\\n            dim, engine, space_type, ef_search, ef_construction, m, vector_field\\n        )\\n\\n        return _bulk_ingest_embeddings(\\n            self.client,\\n            self.index_name,\\n            embeddings,\\n            texts,\\n            metadatas=metadatas,\\n            ids=ids,\\n            vector_field=vector_field,\\n            text_field=text_field,\\n            mapping=mapping,\\n            max_chunk_bytes=max_chunk_bytes,\\n            is_aoss=self.is_aoss,\\n        )',\n",
       "   'd': 'Run more texts through the embeddings and add to the vectorstore.\\n\\nArgs:\\n    texts: Iterable of strings to add to the vectorstore.\\n    metadatas: Optional list of metadatas associated with the texts.\\n    ids: Optional list of ids to associate with the texts.\\n    bulk_size: Bulk API request count; Default: 500\\n\\nReturns:\\n    List of ids from adding the texts into the vectorstore.\\n\\nOptional Args:\\n    vector_field: Document field embeddings are stored in. Defaults to\\n    \"vector_field\".\\n\\n    text_field: Document field the text of the document is stored in. Defaults\\n    to \"text\".',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def index(\\n        self,\\n        dims_length: Union[int, None],\\n        vector_query_field: str,\\n        similarity: Union[DistanceStrategy, None],\\n    ) -> Dict:\\n',\n",
       "   'd': 'Executes when the index is created.\\n\\nArgs:\\n    dims_length: Numeric length of the embedding vectors,\\n                or None if not using vector-based query.\\n    vector_query_field: The field containing the vector\\n                        representations in the index.\\n    similarity: The similarity strategy to use,\\n                or None if not using one.\\n\\nReturns:\\n    Dict: The Elasticsearch settings and mappings for the strategy.',\n",
       "   'l': True,\n",
       "   'g': [\",\\n    Args:\\n        kernel_size (int or tuple): The size of the convolving kernel.\\n        stride (int or tuple): The stride of the convolution.\\n        padding (int or tuple): The amount of padding added to the input.\\n        data_format (str): The data format of the input.\\n        device (str, optional): The device to allocate the module's resources on.\\n        v (None or Tensor, optional): The input tensor.\\n        dtype (None or Tensor, optional): The data type of the input tensor.\",\n",
       "    ',\\n    Parameters\\n    ----------\\n    kernel_size : int or tuple of ints\\n        The size of the convolving kernel.\\n    stride : int or tuple of ints\\n        The stride of the convolution.\\n    padding : int or tuple of ints\\n        The number of \"pixels\" to pad the input along each dimension.\\n    data_format : {\\'NHWC\\', \\'NCHW\\'}\\n        Whether the data is in channel-major (NHWC) or channel-minor (NCHW) format.\\n    device : str or None\\n        The device to place the module\\'s parameters on.\\n    dtype : str or None\\n        The data type of the module\\'s parameters.',\n",
       "    ',\\n    Args:\\n        kernel_size (int or tuple): Size of the convolving kernel.\\n        stride (int or tuple): Stride of the convolution.\\n        padding (int or tuple): Padding added to both sides of the input.\\n        data_format (str): Data layout format of the input.\\n        device (str or None): Device on which the layer will be created.\\n        dtype (str or None): Data type of the input.',\n",
       "    ',\\n    Arguments:\\n        kernel_size {int} -- The size of the kernel\\n        stride {int} -- The stride of the convolution\\n        padding {int} -- The padding of the convolution\\n        data_format {str} -- The data format of the input data\\n        device {str} -- The device to place the module on\\n        v {None} -- The version of the module\\n        dtype {None} -- The data type of the module']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[Dict[Any, Any]]] = None,\\n        model_id: Optional[str] = None,\\n        refresh_indices: bool = False,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        if not self.client.indices.exists(index=self.index_name):\\n            dims = kwargs.get(\"dims\")\\n\\n            if dims is None:\\n                raise ValueError(\"ElasticKnnSearch requires \\'dims\\' parameter\")\\n\\n            similarity = kwargs.get(\"similarity\")\\n            optional_args = {}\\n\\n            if similarity is not None:\\n                optional_args[\"similarity\"] = similarity\\n\\n            mapping = self._default_knn_mapping(dims=dims, **optional_args)\\n            self.create_knn_index(mapping)\\n\\n        embeddings = self.embedding.embed_documents(list(texts))\\n\\n\\n        body: List[Mapping[str, Any]] = []\\n        for text, vector in zip(texts, embeddings):\\n            body.extend(\\n                [\\n                    {\"index\": {\"_index\": self.index_name}},\\n                    {\"text\": text, \"vector\": vector},\\n                ]\\n            )\\n\\n        responses = self.client.bulk(operations=body)\\n\\n        ids = [\\n            item[\"index\"][\"_id\"]\\n            for item in responses[\"items\"]\\n            if item[\"index\"][\"result\"] == \"created\"\\n        ]\\n\\n        if refresh_indices:\\n            self.client.indices.refresh(index=self.index_name)\\n\\n        return ids',\n",
       "   'd': 'Add a list of texts to the Elasticsearch index.\\n\\nArgs:\\n    texts (Iterable[str]): The texts to add to the index.\\n    metadatas (List[Dict[Any, Any]], optional): A list of metadata dictionaries\\n        to associate with the texts.\\n    model_id (str, optional): The ID of the model to use for transforming the\\n        texts into vectors.\\n    refresh_indices (bool, optional): Whether to refresh the Elasticsearch\\n        indices after adding the texts.\\n    **kwargs: Arbitrary keyword arguments.\\n\\nReturns:\\n    A list of IDs for the added texts.',\n",
       "   'l': True,\n",
       "   'g': ['Return a list of tokens, the number of tokens, and a list of tuples\\n    (start_idx, end_idx) where the tokens appear in the row.',\n",
       "    '',\n",
       "    'Given a row, return the tokens, the number of tokens, and the token ranges',\n",
       "    'Given a row, return the text and the number of tokens in the row.']},\n",
       "  {'c': '    def input_keys(self) -> List[str]:\\n',\n",
       "   'd': 'Create k-shot example selector using example list and embeddings.\\n\\nReshuffles examples dynamically based on query similarity.\\n\\nArgs:\\n    examples: List of examples to use in the prompt.\\n    embeddings: An iniialized embedding API interface, e.g. OpenAIEmbeddings().\\n    vectorstore_cls: A vector store DB interface class, e.g. FAISS.\\n    k: Number of examples to select\\n    input_keys: If provided, the search is based on the input variables\\n        instead of all variables.\\n    vectorstore_cls_kwargs: optional kwargs containing url for vector store\\n\\nReturns:\\n    The ExampleSelector instantiated, backed by a vector store.',\n",
       "   'l': False,\n",
       "   'g': ['Returns a string that can be used as the ms_payload parameter in the msfvenom command.\\n\\n    Args:\\n        payload (str): The payload to use.\\n\\n    Returns:\\n        str: The ms_payload parameter.',\n",
       "    'Returns a ms payload based on the given payload.',\n",
       "    'This function takes a string as input and returns the corresponding payload name.',\n",
       "    'Returns the ms payload for the given payload number.']},\n",
       "  {'c': 'def plan(self, inputs: dict, callbacks: Callbacks = None, **kwargs: Any) -> Plan:\\n    inputs[\"tools\"] = [\\n        f\"{tool.name}: {tool.description}\" for tool in inputs[\"hf_tools\"]\\n    ]\\n    llm_response = self.llm_chain.run(**inputs, stop=self.stop, callbacks=callbacks)\\n    return self.output_parser.parse(llm_response, inputs[\"hf_tools\"])',\n",
       "   'd': 'Compute the score for a prediction and reference.\\n\\nArgs:\\n    inputs (Dict[str, Any]): The input data.\\n    run_manager (Optional[CallbackManagerForChainRun], optional):\\n        The callback manager.\\n\\nReturns:\\n    Dict[str, Any]: The computed score.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    :param parent:\\n    :param params:\\n    :param response:\\n    :return:',\n",
       "    '.\\n    :param parent:\\n    :param params:\\n    :param response:\\n    :return:',\n",
       "    '.\\n    Calls the API and returns the result.',\n",
       "    '.\\n\\n    :param parent:\\n    :param params:\\n    :param response:\\n    :return:']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        messages = self.format_messages(**kwargs)\\n        return get_buffer_string(messages)',\n",
       "   'd': 'Lazily parse the blob.',\n",
       "   'l': False,\n",
       "   'g': ['Align a mobj on a border of a bounding box.\\n\\n    Parameters\\n    ----------\\n    direction : np.array\\n        Direction to align the mobj on the border.\\n    buff : int\\n        Buffer to add to the direction.\\n\\n    Returns\\n    -------\\n    mobj : mobj\\n        The mobj that was aligned.',\n",
       "    'Align the current mobj on the border of the frame. \\n    \\n    Parameters\\n    ----------\\n    direction : tuple\\n        The direction to align the current mobj.\\n    buff : int\\n        The buffer to use when aligning the mobj on the border of the frame.\\n        \\n    Returns\\n    -------\\n    mobj : mobj\\n        The current mobj with the aligned on the border.',\n",
       "    'Align the current mobject on the border of the frame\\n    \\n    Parameters\\n    ----------\\n    direction : tuple\\n        Direction to align the mobject on the border\\n    buff : int\\n        Buffer around the mobject to align\\n    \\n    Returns\\n    -------\\n    mobject\\n        The aligned mobject',\n",
       "    \"_Align the object to the border of the frame.\\n\\n    Parameters\\n    ----------\\n    direction : np.array\\n        The direction vector of the object.\\n    buff : float\\n        The buffer to add to the object's position.\"]},\n",
       "  {'c': '    def __init__(self, **kwargs: Any):\\n        super().__init__(**kwargs)\\n        try:\\n            import sentence_transformers\\n\\n        except ImportError as exc:\\n            raise ImportError(\\n                \"Could not import sentence_transformers python package. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            ) from exc\\n\\n        self.client = sentence_transformers.SentenceTransformer(\\n            self.model_name, cache_folder=self.cache_folder, **self.model_kwargs\\n        )',\n",
       "   'd': 'Initialize the sentence_transformer.',\n",
       "   'l': True,\n",
       "   'g': ['Returns the names of the slots used by the optimizer.',\n",
       "    'Returns all the names of the slots used by the optimizer. \\n    \\n    :return: list of strings',\n",
       "    'Return a list of strings representing the names of the slots used by this \\n    optimizer.',\n",
       "    'Returns a list of strings that are the names of the slots used by this \\n    optimizer.']},\n",
       "  {'c': '    def similarity_search(\\n        self, query: str, k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        docs_with_scores = self.similarity_search_with_score(query, k, **kwargs)\\n        return [doc[0] for doc in docs_with_scores]',\n",
       "   'd': 'Test the APIOperation class.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '@param logins: a dictionary of logins to User objects\\n    @param ins: input stream\\n    @param out: output stream',\n",
       "    'Initialize the UserIO object.\\n\\n    Parameters\\n    ----------\\n    logins : dict\\n        A dictionary of logins to users.\\n    ins : file-like object\\n        The input stream.\\n    out : file-like object\\n        The output stream.',\n",
       "    '_init__']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        huggingface_api_key = get_from_dict_or_env(\\n            values, \"huggingface_api_key\", \"HUGGINGFACE_API_KEY\"\\n        )\\n        try:\\n            from petals import DistributedBloomForCausalLM\\n            from transformers import BloomTokenizerFast\\n\\n            model_name = values[\"model_name\"]\\n            values[\"tokenizer\"] = BloomTokenizerFast.from_pretrained(model_name)\\n            values[\"client\"] = DistributedBloomForCausalLM.from_pretrained(model_name)\\n            values[\"huggingface_api_key\"] = huggingface_api_key\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers or petals python package.\"\\n                \"Please install with `pip install -U transformers petals`.\"\\n            )\\n        return values',\n",
       "   'd': 'Validate that api key and python package exists in environment.',\n",
       "   'l': True,\n",
       "   'g': [\"Parameters\\n    ----------\\n    kernel_type : str, optional\\n        The type of kernel to use. Defaults to 'primal'.\\n    dim : int, optional\\n        The dimension of the input data. Defaults to 30.\\n    lamb : float, optional\\n        The regularization parameter. Defaults to 1.\\n    gamma : float, optional\\n        The parameter for the gamma kernel. Defaults to 1.\",\n",
       "    \"Parameters\\n    ----------\\n    kernel_type : str, optional\\n        The kernel type. The default is 'primal'.\\n    dim : int, optional\\n        The dimension of the input data. The default is 30.\\n    lamb : float, optional\\n        The regularization parameter. The default is 1.\\n    gamma : float, optional\\n        The gamma parameter. The default is 1.\",\n",
       "    'Parameters\\n    ----------\\n    kernel_type : str\\n        kernel type\\n    dim : int\\n        dimension of the data\\n    lamb : float\\n        regularization parameter\\n    gamma : float\\n        parameter for the kernel',\n",
       "    'Constructor for the class.']},\n",
       "  {'c': 'def create_extraction_chain_pydantic(\\n    pydantic_schema: Any,\\n    llm: BaseLanguageModel,\\n    prompt: Optional[BasePromptTemplate] = None,\\n    verbose: bool = False,\\n) -> Chain:\\n    class PydanticSchema(BaseModel):\\n        info: List[pydantic_schema]\\n\\n    openai_schema = pydantic_schema.schema()\\n    openai_schema = _resolve_schema_references(\\n        openai_schema, openai_schema.get(\"definitions\", {})\\n    )\\n\\n    function = _get_extraction_function(openai_schema)\\n    extraction_prompt = prompt or ChatPromptTemplate.from_template(_EXTRACTION_TEMPLATE)\\n    output_parser = PydanticAttrOutputFunctionsParser(\\n        pydantic_schema=PydanticSchema, attr_name=\"info\"\\n    )\\n    llm_kwargs = get_llm_kwargs(function)\\n    chain = LLMChain(\\n        llm=llm,\\n        prompt=extraction_prompt,\\n        llm_kwargs=llm_kwargs,\\n        output_parser=output_parser,\\n        verbose=verbose,\\n    )\\n    return chain',\n",
       "   'd': 'Get docs to run questioning over.',\n",
       "   'l': False,\n",
       "   'g': ['Test for the bad ratio input',\n",
       "    '_test_smote_bad_ratio_raises_value_error',\n",
       "    'Test for bad ratio.',\n",
       "    'test that ratio is a float between 0 and 1']},\n",
       "  {'c': '    def on_tool_error(\\n        self,\\n        error: BaseException,\\n        *,\\n        run_id: UUID,\\n        **kwargs: Any,\\n    ) -> None:\\n        if not run_id:\\n            raise TracerException(\"No run_id provided for on_tool_error callback.\")\\n        tool_run = self.run_map.get(str(run_id))\\n        if tool_run is None or tool_run.run_type != \"tool\":\\n            raise TracerException(f\"No tool Run found to be traced for {run_id}\")\\n\\n        tool_run.error = repr(error)\\n        tool_run.end_time = datetime.utcnow()\\n        tool_run.events.append({\"name\": \"error\", \"time\": tool_run.end_time})\\n        self._end_trace(tool_run)\\n        self._on_tool_error(tool_run)',\n",
       "   'd': 'Get the token IDs using the tiktoken package.',\n",
       "   'l': False,\n",
       "   'g': ['Depth First Search',\n",
       "    ':param source: \\n    :param visited: \\n    :return:',\n",
       "    'dfs(source,visited)',\n",
       "    ':param source: \\n    :param visited: \\n    :return:']},\n",
       "  {'c': '    def load(self) -> List[Document]:\\n        account = self._auth()\\n        storage = account.storage()\\n        drive = storage.get_drive(self.drive_id)\\n        docs: List[Document] = []\\n        if not drive:\\n            raise ValueError(f\"There isn\\'t a drive with id {self.drive_id}.\")\\n        if self.folder_path:\\n            folder = self._get_folder_from_path(drive=drive)\\n            docs.extend(self._load_from_folder(folder=folder))\\n        elif self.object_ids:\\n            docs.extend(self._load_from_object_ids(drive=drive))\\n        return docs',\n",
       "   'd': 'Loads all supported document files from the specified OneDrive drive\\nand return a list of Document objects.\\n\\nReturns:\\n    List[Document]: A list of Document objects\\n    representing the loaded documents.\\n\\nRaises:\\n    ValueError: If the specified drive ID\\n    does not correspond to a drive in the OneDrive storage.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Handle an event.\\n\\n    :param event: The event to handle.\\n    :param params: Optional parameters to pass to the event handler.\\n    :param kwparams: Optional keyword parameters to pass to the event handler.\\n    :return: The return value of the event handler.',\n",
       "    '.',\n",
       "    '.',\n",
       "    '.']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        cls._try_init_vertexai(values)\\n        tuned_model_name = values.get(\"tuned_model_name\")\\n        model_name = values[\"model_name\"]\\n        try:\\n            if not is_codey_model(model_name):\\n                from vertexai.preview.language_models import TextGenerationModel\\n\\n                if tuned_model_name:\\n                    values[\"client\"] = TextGenerationModel.get_tuned_model(\\n                        tuned_model_name\\n                    )\\n                else:\\n                    values[\"client\"] = TextGenerationModel.from_pretrained(model_name)\\n            else:\\n                from vertexai.preview.language_models import CodeGenerationModel\\n\\n                if tuned_model_name:\\n                    values[\"client\"] = CodeGenerationModel.get_tuned_model(\\n                        tuned_model_name\\n                    )\\n                else:\\n                    values[\"client\"] = CodeGenerationModel.from_pretrained(model_name)\\n        except ImportError:\\n            raise_vertex_import_error()\\n        return values',\n",
       "   'd': 'Get docs to run questioning over.',\n",
       "   'l': False,\n",
       "   'g': ['Given a query, a list of choices, and an optional processor and scorer,\\n    return the most likely choice.',\n",
       "    'Extract the best one from a list of choices. \\n    \\n    Parameters\\n    ----------\\n    query : str\\n        The query.\\n    choices : list of str\\n        The list of choices.\\n    processor : callable, optional\\n        The processor to apply to the query.\\n    scorer : callable, optional\\n        The scorer to apply to the query.\\n    score_cutoff : float, optional\\n        The score cutoff to apply to the query.\\n\\n    Returns\\n    -------\\n    str or None\\n        The best choice or None if there is no best choice.',\n",
       "    'Extract the best match from a list of choices,\\n    with the given query and processor.\\n    \\n    Args:\\n        query: the query to be matched\\n        choices: a list of choices\\n        processor: a function to process the query\\n        scorer: a function to score the choices\\n        score_cutoff: the cutoff score to consider a choice a match\\n    \\n    Returns:\\n        The best match from the list of choices, or None if no match is found.',\n",
       "    'Extract one of the choices that best matches the query.\\n    \\n    Args:\\n        query (str): The query to extract one of the choices from.\\n        choices (list): The list of choices to extract from.\\n        processor (callable): A function that processes the query and choices.\\n        scorer (callable): A function that scores the choices.\\n        score_cutoff (int): The minimum score to consider a choice.\\n        \\n    Returns:\\n        tuple: A tuple containing the best choice and its score.']},\n",
       "  {'c': '    def __init__(self, **kwargs: Any):\\n        super().__init__(**kwargs)\\n        try:\\n            import sentence_transformers\\n\\n        except ImportError as exc:\\n            raise ImportError(\\n                \"Could not import sentence_transformers python package. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            ) from exc\\n\\n        self.client = sentence_transformers.SentenceTransformer(\\n            self.model_name, cache_folder=self.cache_folder, **self.model_kwargs\\n        )',\n",
       "   'd': 'Use the LLM to check the query.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    Backward pass.',\n",
       "    '.\\n\\n    Parameters\\n    ----------\\n    dLdy : float\\n        Gradient of the loss with respect to the output of the last layer.\\n    X : array-like\\n        The input to the last layer.\\n\\n    Returns\\n    -------\\n    dX : array-like\\n        Gradient of the loss with respect to the input of the last layer.\\n    dW : array-like\\n        Gradient of the loss with respect to the weights of the last layer.\\n    dB : array-like\\n        Gradient of the loss with respect to the bias of the last layer.',\n",
       "    '.\\n    Backward pass.',\n",
       "    '.\\n    Backward pass.\\n    Parameters\\n    ----------\\n    dLdy : torch.Tensor\\n        The gradient of the loss with respect to the output of the model.\\n    X : torch.Tensor\\n        The input to the model.\\n    Returns\\n    -------\\n    dX : torch.Tensor\\n        The gradient of the loss with respect to the input to the model.\\n    dW : torch.Tensor\\n        The gradient of the loss with respect to the weights of the model.\\n    dB : torch.Tensor\\n        The gradient of the loss with respect to the bias of the model.']},\n",
       "  {'c': 'def type(self) -> str:\\n',\n",
       "   'd': 'Validate that api key and python package exists in environment.',\n",
       "   'l': False,\n",
       "   'g': ['Prune heads of the model.',\n",
       "    'Prunes heads of the model.',\n",
       "    'Prunes heads of the model.',\n",
       "    \"Prune heads of the model.\\n    \\n    :param heads_to_prune: dict of {layer_num: list of heads to prune e.g. {'layer_4': [3]}};\\n    See base class AttentionModule for more details.\"]},\n",
       "  {'c': '    def parse_result(self, result: List[Generation]) -> T:\\n        return self.parse(result[0].text)',\n",
       "   'd': 'Parse a list of candidate model Generations into a specific format.\\n\\nThe return value is parsed from only the first Generation in the result, which\\n    is assumed to be the highest-likelihood Generation.\\n\\nArgs:\\n    result: A list of Generations to be parsed. The Generations are assumed\\n        to be different candidate outputs for a single model input.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    '_save_m2m\\n\\n    Saves the many to many relationships.',\n",
       "    '',\n",
       "    'Save the object.\\n\\n    :param commit:\\n    :return:']},\n",
       "  {'c': '    def update(\\n        self,\\n        keys: Sequence[str],\\n        *,\\n        group_ids: Optional[Sequence[Optional[str]]] = None,\\n        time_at_least: Optional[float] = None,\\n    ) -> None:\\n        if group_ids is None:\\n            group_ids = [None] * len(keys)\\n\\n        if len(keys) != len(group_ids):\\n            raise ValueError(\\n                f\"Number of keys ({len(keys)}) does not match number of \"\\n                f\"group_ids ({len(group_ids)})\"\\n            )\\n\\n\\n\\n\\n\\n\\n\\n\\n        update_time = self.get_time()\\n\\n        if time_at_least and update_time < time_at_least:\\n\\n            raise AssertionError(f\"Time sync issue: {update_time} < {time_at_least}\")\\n\\n        records_to_upsert = [\\n            {\\n                \"key\": key,\\n                \"namespace\": self.namespace,\\n                \"updated_at\": update_time,\\n                \"group_id\": group_id,\\n            }\\n            for key, group_id in zip(keys, group_ids)\\n        ]\\n\\n        with self._make_session() as session:\\n            if self.dialect == \"sqlite\":\\n                from sqlalchemy.dialects.sqlite import insert\\n\\n\\n\\n                insert_stmt = insert(UpsertionRecord).values(records_to_upsert)\\n                stmt = insert_stmt.on_conflict_do_update(\\n                    [UpsertionRecord.key, UpsertionRecord.namespace],\\n                    set_=dict(\\n\\n                        updated_at=insert_stmt.excluded.updated_at,\\n                        group_id=insert_stmt.excluded.group_id,\\n                    ),\\n                )\\n            elif self.dialect == \"postgresql\":\\n                from sqlalchemy.dialects.postgresql import insert\\n\\n\\n\\n                insert_stmt = insert(UpsertionRecord).values(records_to_upsert)\\n                stmt = insert_stmt.on_conflict_do_update(\\n                    \"uix_key_namespace\",\\n                    set_=dict(\\n\\n                        updated_at=insert_stmt.excluded.updated_at,\\n                        group_id=insert_stmt.excluded.group_id,\\n                    ),\\n                )\\n            else:\\n                raise NotImplementedError(f\"Unsupported dialect {self.dialect}\")\\n\\n            session.execute(stmt)\\n            session.commit()',\n",
       "   'd': 'Upsert records into the SQLite database.',\n",
       "   'l': True,\n",
       "   'g': ['for a function object',\n",
       "    '.',\n",
       "    '.\\n    Decorator for function.',\n",
       "    '.\\n    Decorator that sets the decorated function as the function to be called\\n    when the decorator is called.']},\n",
       "  {'c': '    def lazy_load(self) -> Iterator[Document]:\\n        try:\\n            from qcloud_cos import CosS3Client\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import cos-python-sdk-v5 python package. \"\\n                \"Please install it with `pip install cos-python-sdk-v5`.\"\\n            )\\n\\n\\n        client = CosS3Client(self.conf)\\n        with tempfile.TemporaryDirectory() as temp_dir:\\n            file_path = f\"{temp_dir}/{self.bucket}/{self.key}\"\\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n\\n            client.download_file(\\n                Bucket=self.bucket, Key=self.key, DestFilePath=file_path\\n            )\\n            loader = UnstructuredFileLoader(file_path)\\n\\n            return iter(loader.load())',\n",
       "   'd': 'Load QA Eval Chain from LLM.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    ':param json:\\n    :return:',\n",
       "    'Convert a JSON object to an object of the correct type.',\n",
       "    'Get an object from the JSON data.\\n    \\n    Parameters\\n    ----------\\n    json : dict\\n        The JSON data.\\n    \\n    Returns\\n    -------\\n    Object\\n        The object.']},\n",
       "  {'c': 'def type(self) -> str:\\n    return \"function\"',\n",
       "   'd': 'Type of the message, used for serialization.',\n",
       "   'l': True,\n",
       "   'g': ['* @return {boolean}',\n",
       "    '.',\n",
       "    '.',\n",
       "    'This function will raise an error if the counter is greater than the count.']},\n",
       "  {'c': 'def requires_reference(self) -> bool:\\n    return True',\n",
       "   'd': 'Whether the evaluation requires a reference text.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Raise an exception if the given exception type is not supported.',\n",
       "    '.\\n    Raises a given exception type.',\n",
       "    '.\\n    Raises an exception of the given type.\\n    :param exception_type: The exception type to raise.\\n    :return: True.',\n",
       "    'for testing exceptions']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: List[str],\\n        metadatas: Optional[List[dict]] = None,\\n        doc_metadata: Optional[dict] = {},\\n    ) -> None:\\n        self.vectorstore.add_texts(texts, metadatas, doc_metadata)',\n",
       "   'd': 'Add text to the Vectara vectorstore.\\n\\nArgs:\\n    texts (List[str]): The text\\n    metadatas (List[dict]): Metadata dicts, must line up with existing store',\n",
       "   'l': True,\n",
       "   'g': ['_summary_\\n\\n    Args:\\n        store_type (str): _description_\\n\\n    Returns:\\n        str: _description_',\n",
       "    '_summary_\\n\\n    Args:\\n        store_type (str): _description_\\n\\n    Returns:\\n        str: _description_',\n",
       "    '_summary_\\n\\n    Args:\\n        store_type (str): _description_\\n\\n    Returns:\\n        str: _description_',\n",
       "    '_summary_\\n\\n    Args:\\n        store_type (str): _description_\\n\\n    Returns:\\n        str: _description_']},\n",
       "  {'c': 'def _split_sources(self, answer: str) -> Tuple[str, str]:\\n    if re.search(r\"SOURCES?[:\\\\s]\", answer, re.IGNORECASE):\\n        answer, sources = re.split(\\n            r\"SOURCES?[:]|QUESTION:\\\\s\", answer, flags=re.IGNORECASE\\n        )[:2]\\n        sources = re.split(r\"\\\\n\", sources)[0].strip()\\n    else:\\n        sources = \"\"\\n    return answer, sources',\n",
       "   'd': 'Split sources from answer.',\n",
       "   'l': True,\n",
       "   'g': ['Return an iterator for the list.',\n",
       "    'Returns:\\n        \\n        iter:',\n",
       "    \"Return an iterator over the list's elements.\",\n",
       "    'Return an iterator for the list.']},\n",
       "  {'c': '    def from_documents(\\n        cls,\\n        documents: List[Document],\\n        embedding: Optional[Embeddings] = None,\\n        **kwargs: Any,\\n    ) -> \"ElasticsearchStore\":\\n        elasticsearchStore = ElasticsearchStore._create_cls_from_kwargs(\\n            embedding=embedding, **kwargs\\n        )\\n\\n        elasticsearchStore.add_documents(documents)\\n\\n        return elasticsearchStore',\n",
       "   'd': 'Lookup llm generations in cache by prompt and associated model and settings.\\n\\nArgs:\\n    prompt (str): The prompt run through the language model.\\n    llm_string (str): The language model version and settings.\\n\\nRaises:\\n    SdkException: Momento service or network error\\n\\nReturns:\\n    Optional[RETURN_VAL_TYPE]: A list of language model generations.',\n",
       "   'l': False,\n",
       "   'g': ['.',\n",
       "    '.\\n    Args:\\n        router (Optional[APIRouter], optional):\\n            The router to use for the endpoint. Defaults to None.\\n        router_prefix (str, optional):\\n            The prefix for the router. Defaults to \"/api/v1/awel/trigger\".',\n",
       "    '.',\n",
       "    '.\\n    Args:\\n        router (Optional[APIRouter]): The router to use. Defaults to None.\\n        router_prefix (str, optional): The prefix to use for the router. Defaults to \"/api/v1/awel/trigger\".']},\n",
       "  {'c': 'def _chain_type(self) -> str:\\n    return \"vector_db_qa\"',\n",
       "   'd': 'Return the chain type.',\n",
       "   'l': True,\n",
       "   'g': ['Get proof of address.\\n    \\n    Args:\\n        address (str): Address.\\n        \\n    Returns:\\n        list: List of transactions.',\n",
       "    'Returns a list of all the proof blocks for the given address.',\n",
       "    'Gets a proof for a given address.\\n    \\n    Args:\\n        address (str): Address to get proof for.\\n    \\n    Returns:\\n        list: List of transactions.',\n",
       "    'Get the proof of a given address.\\n    \\n    :param address: The address to get the proof for.\\n    :return: The proof.']},\n",
       "  {'c': '    def configure(\\n        cls,\\n        inheritable_callbacks: Callbacks = None,\\n        local_callbacks: Callbacks = None,\\n        verbose: bool = False,\\n        inheritable_tags: Optional[List[str]] = None,\\n        local_tags: Optional[List[str]] = None,\\n        inheritable_metadata: Optional[Dict[str, Any]] = None,\\n        local_metadata: Optional[Dict[str, Any]] = None,\\n    ) -> AsyncCallbackManager:\\n        return _configure(\\n            cls,\\n            inheritable_callbacks,\\n            local_callbacks,\\n            verbose,\\n            inheritable_tags,\\n            local_tags,\\n            inheritable_metadata,\\n            local_metadata,\\n        )',\n",
       "   'd': 'Configure the async callback manager.\\n\\nArgs:\\n    inheritable_callbacks (Optional[Callbacks], optional): The inheritable\\n        callbacks. Defaults to None.\\n    local_callbacks (Optional[Callbacks], optional): The local callbacks.\\n        Defaults to None.\\n    verbose (bool, optional): Whether to enable verbose mode. Defaults to False.\\n    inheritable_tags (Optional[List[str]], optional): The inheritable tags.\\n        Defaults to None.\\n    local_tags (Optional[List[str]], optional): The local tags.\\n        Defaults to None.\\n    inheritable_metadata (Optional[Dict[str, Any]], optional): The inheritable\\n        metadata. Defaults to None.\\n    local_metadata (Optional[Dict[str, Any]], optional): The local metadata.\\n        Defaults to None.\\n\\nReturns:\\n    AsyncCallbackManager: The configured async callback manager.',\n",
       "   'l': True,\n",
       "   'g': ['XOR the string s with the key k',\n",
       "    '',\n",
       "    ':type s: str\\n    :type k: str\\n    :rtype: str',\n",
       "    'Given a string s and a string k, perform an XOR operation on s with k.\\n    Return the result of the XOR operation.']},\n",
       "  {'c': 'def get_pipeline() -> Any:\\n    model_id = \"facebook/bart-base\"\\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\\n    model = AutoModelForCausalLM.from_pretrained(model_id)\\n    return pipeline(\"feature-extraction\", model=model, tokenizer=tokenizer)',\n",
       "   'd': 'Create a chat prompt template from a template string.\\n\\nCreates a chat template consisting of a single message assumed to be from\\nthe human.\\n\\nArgs:\\n    template: template string\\n    **kwargs: keyword arguments to pass to the constructor.\\n\\nReturns:\\n    A new instance of this class.',\n",
       "   'l': False,\n",
       "   'g': ['@return: Fleet number',\n",
       "    'Gets the fleet number of the OpSi that is currently selected.',\n",
       "    'Returns the OpSi fleet number.',\n",
       "    'Return the fleet number selected by the player.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\\n        embedding_function: Optional[Embeddings] = None,\\n        persist_directory: Optional[str] = None,\\n        client_settings: Optional[chromadb.config.Settings] = None,\\n        collection_metadata: Optional[Dict] = None,\\n        client: Optional[chromadb.Client] = None,\\n        relevance_score_fn: Optional[Callable[[float], float]] = None,\\n    ) -> None:\\n        try:\\n            import chromadb\\n            import chromadb.config\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import chromadb python package. \"\\n                \"Please install it with `pip install chromadb`.\"\\n            )\\n\\n        if client is not None:\\n            self._client_settings = client_settings\\n            self._client = client\\n            self._persist_directory = persist_directory\\n        else:\\n            if client_settings:\\n                _client_settings = client_settings\\n            elif persist_directory:\\n\\n                major, minor, _ = chromadb.__version__.split(\".\")\\n                if int(major) == 0 and int(minor) < 4:\\n                    _client_settings = chromadb.config.Settings(\\n                        chroma_db_impl=\"duckdb+parquet\",\\n                    )\\n                else:\\n                    _client_settings = chromadb.config.Settings(is_persistent=True)\\n                _client_settings.persist_directory = persist_directory\\n            else:\\n                _client_settings = chromadb.config.Settings()\\n            self._client_settings = _client_settings\\n            self._client = chromadb.Client(_client_settings)\\n            self._persist_directory = (\\n                _client_settings.persist_directory or persist_directory\\n            )\\n\\n        self._embedding_function = embedding_function\\n        self._collection = self._client.get_or_create_collection(\\n            name=collection_name,\\n            embedding_function=self._embedding_function.embed_documents\\n            if self._embedding_function is not None\\n            else None,\\n            metadata=collection_metadata,\\n        )\\n        self.override_relevance_score_fn = relevance_score_fn',\n",
       "   'd': 'Lazily load documents.',\n",
       "   'l': False,\n",
       "   'g': ['(str, Path, bool) -> None:',\n",
       "    '(str, DisplayablePath, bool) -> None',\n",
       "    '(self, path: Union[str, Path], parent_path: \"DisplayablePath\", is_last: bool)',\n",
       "    'Parameters\\n    ----------\\n    path : str or Path\\n        The path to the file or directory.\\n    parent_path : DisplayablePath\\n        The parent path.\\n    is_last : bool\\n        Whether this is the last item in the path.']},\n",
       "  {'c': 'def _get_builtin_translator(vectorstore: VectorStore) -> Visitor:\\n    vectorstore_cls = vectorstore.__class__\\n    BUILTIN_TRANSLATORS: Dict[Type[VectorStore], Type[Visitor]] = {\\n        Pinecone: PineconeTranslator,\\n        Chroma: ChromaTranslator,\\n        DashVector: DashvectorTranslator,\\n        Weaviate: WeaviateTranslator,\\n        Qdrant: QdrantTranslator,\\n        MyScale: MyScaleTranslator,\\n        DeepLake: DeepLakeTranslator,\\n        ElasticsearchStore: ElasticsearchTranslator,\\n        Milvus: MilvusTranslator,\\n    }\\n    if vectorstore_cls not in BUILTIN_TRANSLATORS:\\n        raise ValueError(\\n            f\"Self query retriever with Vector Store type {vectorstore_cls}\"\\n            f\" not supported.\"\\n        )\\n    if isinstance(vectorstore, Qdrant):\\n        return QdrantTranslator(metadata_key=vectorstore.metadata_payload_key)\\n    elif isinstance(vectorstore, MyScale):\\n        return MyScaleTranslator(metadata_key=vectorstore.metadata_column)\\n    return BUILTIN_TRANSLATORS[vectorstore_cls]()',\n",
       "   'd': 'Get the translator class corresponding to the vector store class.',\n",
       "   'l': True,\n",
       "   'g': ['Writes data to the buffer.',\n",
       "    'Writes data to the buffer.',\n",
       "    'Write data to the output.\\n    \\n    Args:\\n        data (str): The data to write.\\n    \\n    Returns:\\n        None',\n",
       "    'Write data to the stream.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace',\n",
       "   'd': 'Process a list of pages into a list of documents.',\n",
       "   'l': False,\n",
       "   'g': ['Returns the url for the emoji.',\n",
       "    'Returns the URL of the emoji.',\n",
       "    'Returns the url of the emoji',\n",
       "    'Returns the url of the emoji.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        web_path: Union[str, List[str]],\\n        header_template: Optional[dict] = None,\\n        verify_ssl: Optional[bool] = True,\\n        proxies: Optional[dict] = None,\\n        requests_per_second: int = 2,\\n        requests_kwargs: Dict[str, Any] = {},\\n        raise_for_status: bool = False,\\n    ):\\n        if isinstance(web_path, str):\\n            self.web_paths = [web_path]\\n        elif isinstance(web_path, List):\\n            self.web_paths = web_path\\n\\n        headers = header_template or default_header_template\\n        if not headers.get(\"User-Agent\"):\\n            try:\\n                from fake_useragent import UserAgent\\n\\n                headers[\"User-Agent\"] = UserAgent().random\\n            except ImportError:\\n                logger.info(\\n                    \"fake_useragent not found, using default user agent.\"\\n                    \"To get a realistic header for requests, \"\\n                    \"`pip install fake_useragent`.\"\\n                )\\n\\n        self.session = requests.Session()\\n        self.session.headers = dict(headers)\\n        self.session.verify = verify_ssl\\n\\n        if proxies:\\n            self.session.proxies.update(proxies)\\n\\n        self.requests_per_second = requests_per_second\\n        self.requests_kwargs = requests_kwargs\\n        self.raise_for_status = raise_for_status',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Prompts the user with a question and returns True or False \\n    based on their answer.',\n",
       "    'Prompts the user for confirmation.\\n    \\n    :param question: The question to ask the user.\\n    :type question: str\\n    :return: True if the user confirmed, False otherwise.\\n    :rtype: bool',\n",
       "    'Prompts the user with a question and returns True if they say yes,\\n    and False if they say no.']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        messages = self.format_messages(**kwargs)\\n        return get_buffer_string(messages)',\n",
       "   'd': 'Format the prompt with inputs generating a string.\\n\\nUse this method to generate a string representation of a prompt consisting\\nof chat messages.\\n\\nUseful for feeding into a string based completion language model or debugging.\\n\\nArgs:\\n    **kwargs: keyword arguments to use for formatting.\\n\\nReturns:\\n    A string representation of the prompt',\n",
       "   'l': True,\n",
       "   'g': ['.', '.', '.', '.']},\n",
       "  {'c': '    def _anonymize(self, text: str, language: Optional[str] = None) -> str:\\n        if language is None:\\n            language = self.supported_languages[0]\\n\\n        if language not in self.supported_languages:\\n            raise ValueError(\\n                f\"Language \\'{language}\\' is not supported. \"\\n                f\"Supported languages are: {self.supported_languages}. \"\\n                \"Change your language configuration file to add more languages.\"\\n            )\\n\\n        analyzer_results = self._analyzer.analyze(\\n            text,\\n            entities=self.analyzed_fields,\\n            language=language,\\n        )\\n\\n        filtered_analyzer_results = (\\n            self._anonymizer._remove_conflicts_and_get_text_manipulation_data(\\n                analyzer_results\\n            )\\n        )\\n\\n        anonymizer_results = self._anonymizer.anonymize(\\n            text,\\n            analyzer_results=analyzer_results,\\n            operators=self.operators,\\n        )\\n\\n        self._update_deanonymizer_mapping(\\n            text, filtered_analyzer_results, anonymizer_results\\n        )\\n\\n        return anonymizer_results.text',\n",
       "   'd': \"Test intervention chain correctly transforms\\nthe LLM's text completion into a setting-like object.\",\n",
       "   'l': False,\n",
       "   'g': ['Multiplication of two rigid transformations.\\n\\n    Args:\\n        right (torch.Tensor): The right multiplicand.\\n\\n    Returns:\\n        Rigid: The product of the two rigid transformations.',\n",
       "    '.\\n    Multiply two Rigid objects.',\n",
       "    'for multiplication with a right-hand Tensor',\n",
       "    'Multiply two Rigid objects.']},\n",
       "  {'c': '    def max_marginal_relevance_search_by_vector(\\n        self,\\n        embedding: List[float],\\n        k: int = 4,\\n        fetch_k: int = 20,\\n        lambda_mult: float = 0.5,\\n        search_params: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        results = self._client.vector_search(\\n            self._index_name, [embedding], search_params, k\\n        )\\n\\n        mmr_selected = maximal_marginal_relevance(\\n            np.array([embedding], dtype=np.float32),\\n            [item[\"floatValues\"] for item in results[0][\"vectorWithDistances\"]],\\n            k=k,\\n            lambda_mult=lambda_mult,\\n        )\\n        selected = [\\n            results[0][\"vectorWithDistances\"][i][\"metaData\"] for i in mmr_selected\\n        ]\\n        return [\\n            Document(page_content=metadata.pop((self._text_key)), metadata=metadata)\\n            for metadata in selected\\n        ]',\n",
       "   'd': 'Return docs selected using the maximal marginal relevance.\\n\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\n\\nArgs:\\n    embedding: Embedding to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 4.\\n    fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n    lambda_mult: Number between 0 and 1 that determines the degree\\n                of diversity among the results with 0 corresponding\\n                to maximum diversity and 1 to minimum diversity.\\n                Defaults to 0.5.\\nReturns:\\n    List of Documents selected by maximal marginal relevance.',\n",
       "   'l': True,\n",
       "   'g': ['获取微博内容 \\n    :param info: \\n    :param is_original: \\n    :return:',\n",
       "    '获取微博内容',\n",
       "    '获取微博内容\\n    :param info:\\n    :param is_original: 是否是原创微博\\n    :return:',\n",
       "    '获取微博内容']},\n",
       "  {'c': 'def _generate(\\n    self,\\n    prompts: List[str],\\n    stop: Optional[List[str]] = None,\\n    run_manager: Optional[CallbackManagerForLLMRun] = None,\\n    **kwargs: Any,\\n) -> LLMResult:\\n',\n",
       "   'd': 'Run the LLM on the given prompts.',\n",
       "   'l': True,\n",
       "   'g': ['Safely saves the model for HF Trainer.\\n    \\n    Args:\\n        trainer: The trainer object.\\n        output_dir: The output directory to save the model.',\n",
       "    '_summary_\\n    Args:\\n        trainer: transformers.Trainer\\n        output_dir: str',\n",
       "    '_summary_\\n    Save model for HF Trainer\\n    Args:\\n        trainer (transformers.Trainer): The HF Trainer instance\\n        output_dir (str): The output directory to save the model',\n",
       "    'Saves the model, if needed.\\n    \\n    Args:\\n        trainer: A transformers.Trainer object.\\n        output_dir: The output directory.']},\n",
       "  {'c': 'def cosine_similarity_top_k(\\n    X: Matrix,\\n    Y: Matrix,\\n    top_k: Optional[int] = 5,\\n    score_threshold: Optional[float] = None,\\n) -> Tuple[List[Tuple[int, int]], List[float]]:\\n    if len(X) == 0 or len(Y) == 0:\\n        return [], []\\n    score_array = cosine_similarity(X, Y)\\n    sorted_idxs = score_array.flatten().argsort()[::-1]\\n    top_k = top_k or len(sorted_idxs)\\n    top_idxs = sorted_idxs[:top_k]\\n    score_threshold = score_threshold or -1.0\\n    top_idxs = top_idxs[score_array.flatten()[top_idxs] > score_threshold]\\n    ret_idxs = [(x // score_array.shape[1], x % score_array.shape[1]) for x in top_idxs]\\n    scores = score_array.flatten()[top_idxs].tolist()\\n    return ret_idxs, scores',\n",
       "   'd': 'Row-wise cosine similarity with optional top-k and score threshold filtering.\\n\\nArgs:\\n    X: Matrix.\\n    Y: Matrix, same width as X.\\n    top_k: Max number of results to return.\\n    score_threshold: Minimum cosine similarity of results.\\n\\nReturns:\\n    Tuple of two lists. First contains two-tuples of indices (X_idx, Y_idx),\\n        second contains corresponding cosine similarities.',\n",
       "   'l': True,\n",
       "   'g': ['Return the next item.',\n",
       "    'Return the next item in the sequence.\\n    \\n    :return:',\n",
       "    'Returns the next item in the sequence.',\n",
       "    'Returns the next element in the sequence.']},\n",
       "  {'c': '    def from_function(\\n        cls,\\n        func: Callable,\\n        name: Optional[str] = None,\\n        description: Optional[str] = None,\\n        return_direct: bool = False,\\n        args_schema: Optional[Type[BaseModel]] = None,\\n        infer_schema: bool = True,\\n        **kwargs: Any,\\n    ) -> StructuredTool:\\n        name = name or func.__name__\\n        description = description or func.__doc__\\n        assert (\\n            description is not None\\n        ), \"Function must have a docstring if description not provided.\"\\n\\n\\n\\n        description = f\"{name}{signature(func)} - {description.strip()}\"\\n        _args_schema = args_schema\\n        if _args_schema is None and infer_schema:\\n            _args_schema = create_schema_from_function(f\"{name}Schema\", func)\\n        return cls(\\n            name=name,\\n            func=func,\\n            args_schema=_args_schema,\\n            description=description,\\n            return_direct=return_direct,\\n            **kwargs,\\n        )',\n",
       "   'd': 'Create tool from a given function.\\n\\nA classmethod that helps to create a tool from a function.\\n\\nArgs:\\n    func: The function from which to create a tool\\n    name: The name of the tool. Defaults to the function name\\n    description: The description of the tool. Defaults to the function docstring\\n    return_direct: Whether to return the result directly or as a callback\\n    args_schema: The schema of the tool\\'s input arguments\\n    infer_schema: Whether to infer the schema from the function\\'s signature\\n    **kwargs: Additional arguments to pass to the tool\\n\\nReturns:\\n    The tool\\n\\nExamples:\\n    ... code-block:: python\\n        def add(a: int, b: int) -> int:\\n            \"\"\"Add two numbers\"\"\"\\n            return a + b\\n        tool = StructuredTool.from_function(add)\\n        tool.run(1, 2) # 3',\n",
       "   'l': True,\n",
       "   'g': ['Test that the list_courses() method returns the expected output',\n",
       "    'Test that list_courses returns the expected output',\n",
       "    'Test the list_courses method.',\n",
       "    'This test is for the list_courses method. \\n    \\n    The test should:\\n    - Call get_page_json with the correct url\\n    - Return the json from the fixture\\n    - Assert that the returned json matches the expected json']},\n",
       "  {'c': '    def index(\\n        self,\\n        dims_length: Union[int, None],\\n        vector_query_field: str,\\n        similarity: Union[DistanceStrategy, None],\\n    ) -> Dict:\\n        if similarity is DistanceStrategy.COSINE:\\n            similarityAlgo = \"cosine\"\\n        elif similarity is DistanceStrategy.EUCLIDEAN_DISTANCE:\\n            similarityAlgo = \"l2_norm\"\\n        elif similarity is DistanceStrategy.DOT_PRODUCT:\\n            similarityAlgo = \"dot_product\"\\n        else:\\n            raise ValueError(f\"Similarity {similarity} not supported.\")\\n\\n        return {\\n            \"mappings\": {\\n                \"properties\": {\\n                    vector_query_field: {\\n                        \"type\": \"dense_vector\",\\n                        \"dims\": dims_length,\\n                        \"index\": True,\\n                        \"similarity\": similarityAlgo,\\n                    },\\n                }\\n            }\\n        }',\n",
       "   'd': 'Formats response',\n",
       "   'l': False,\n",
       "   'g': ['Initialize the space with random points.\\n\\n    Parameters\\n    ----------\\n    init_points : int\\n        The number of points to initialize the space with.\\n\\n    Returns\\n    -------\\n    None',\n",
       "    'Initialize the points\\n    :param init_points: number of points to initialize\\n    :return:',\n",
       "    'Initialize the simulation.\\n\\n    Parameters\\n    ----------\\n    init_points : int\\n        Number of points to initialize the simulation.',\n",
       "    'Initialize the environment with random points.']},\n",
       "  {'c': '    def get_task_attribute(self, query: str) -> Dict:\\n        task = self.get_task(query, fault_tolerant=True)\\n        params, error = load_query(query, fault_tolerant=True)\\n        if not isinstance(params, dict):\\n            return {\"Error\": error}\\n\\n        if params[\"attribute_name\"] not in task:\\n            return {\\n                \"Error\": f\"\"\"attribute_name = {params[\\'attribute_name\\']} was not\\nfound in task keys {task.keys()}. Please call again with one of the key names.\"\"\"\\n            }\\n\\n        return {params[\"attribute_name\"]: task[params[\"attribute_name\"]]}',\n",
       "   'd': 'Pass through to `knn_search`',\n",
       "   'l': False,\n",
       "   'g': ['Prevent the use of dash events',\n",
       "    'Raises a NonExistentEventException if the dash events are not\\n    supported by dash.',\n",
       "    'This function raises an exception if dash events are used.',\n",
       "    'Raise NonExistentEventException if dash events are used.']},\n",
       "  {'c': '    def clear(self, **kwargs: Any) -> None:\\n        from gptcache import Cache\\n\\n        for gptcache_instance in self.gptcache_dict.values():\\n            gptcache_instance = cast(Cache, gptcache_instance)\\n            gptcache_instance.flush()\\n\\n        self.gptcache_dict.clear()',\n",
       "   'd': 'Clear cache.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    :param environment_dict:\\n    :return:', '.', '.', '.']},\n",
       "  {'c': 'def validate_environment(cls, values: Dict) -> Dict:\\n    values[\"fireworks_api_key\"] = get_from_dict_or_env(\\n        values, \"fireworks_api_key\", \"FIREWORKS_API_KEY\"\\n    )\\n    return values',\n",
       "   'd': 'Initialize the PairwiseStringEvalChain from an LLM.\\n\\nArgs:\\n    llm (BaseLanguageModel): The LLM to use.\\n    prompt (PromptTemplate, optional): The prompt to use.\\n    **kwargs (Any): Additional keyword arguments.\\n\\nReturns:\\n    PairwiseStringEvalChain: The initialized PairwiseStringEvalChain.\\n\\nRaises:\\n    ValueError: If the input variables are not as expected.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    Extract code from the response.\\n\\n    Args:\\n        response (str): The response to extract the code from.\\n        separator (str, optional): The separator to use for splitting the code. Defaults to \"```\".\\n\\n    Raises:\\n        NoCodeFoundError: If no code is found in the response.\\n\\n    Returns:\\n        str: The extracted code.',\n",
       "    '.\\n    Extract the code from the response.\\n\\n    Args:\\n        response (str): The response from the server.\\n        separator (str, optional): The separator used to split the code. Defaults to \"```\".\\n\\n    Raises:\\n        NoCodeFoundError: If no code is found in the response.\\n\\n    Returns:\\n        str: The code extracted from the response.',\n",
       "    '_extract_code(self, response: str, separator: str = \"```\") -> str:\\n\\n    Extract the code from the response.\\n\\n    Args:\\n        response (str): The response from the API.\\n        separator (str, optional): The separator used to separate the code. Defaults to \"```\".\\n\\n    Returns:\\n        str: The code extracted from the response.\\n\\n    Raises:\\n        NoCodeFoundError: If no code is found in the response.\\n\\n    Example:\\n        >>> response = \"```python\\\\nprint(\\'Hello, World!\\')\\\\n```\"\\n        >>> code = _extract_code(response)\\n        >>> print(code)\\n        print(\\'Hello, World!\\')',\n",
       "    '.\\n    Args:\\n        response (str): The response to extract the code from.\\n        separator (str, optional): The separator to use to split the code. Defaults to \"```\".\\n\\n    Raises:\\n        NoCodeFoundError: If no code is found in the response.\\n\\n    Returns:\\n        str: The code extracted from the response.']},\n",
       "  {'c': 'def map(self, run: Run) -> Dict[str, str]:\\n    if run.run_type != \"llm\":\\n        raise ValueError(\"LLM RunMapper only supports LLM runs.\")\\n    elif not run.outputs:\\n        if run.error:\\n            raise ValueError(\\n                f\"Cannot evaluate errored LLM run {run.id}: {run.error}\"\\n            )\\n        else:\\n            raise ValueError(\\n                f\"Run {run.id} has no outputs. Cannot evaluate this run.\"\\n            )\\n    else:\\n        try:\\n            inputs = self.serialize_inputs(run.inputs)\\n        except Exception as e:\\n            raise ValueError(\\n                f\"Could not parse LM input from run inputs {run.inputs}\"\\n            ) from e\\n        try:\\n            output_ = self.serialize_outputs(run.outputs)\\n        except Exception as e:\\n            raise ValueError(\\n                f\"Could not parse LM prediction from run outputs {run.outputs}\"\\n            ) from e\\n        return {\"input\": inputs, \"prediction\": output_}',\n",
       "   'd': 'Maps the Run to a dictionary.',\n",
       "   'l': True,\n",
       "   'g': ['Initializes a new instance of the Ability class.\\n\\n    Args:\\n      data: A dict containing the data for the ability.',\n",
       "    ':param data: A data object containing the ability data.',\n",
       "    '.',\n",
       "    '.']},\n",
       "  {'c': 'def _load_agent_from_file(\\n    file: Union[str, Path], **kwargs: Any\\n) -> Union[BaseSingleActionAgent, BaseMultiActionAgent]:\\n    if isinstance(file, str):\\n        file_path = Path(file)\\n    else:\\n        file_path = file\\n\\n    if file_path.suffix == \".json\":\\n        with open(file_path) as f:\\n            config = json.load(f)\\n    elif file_path.suffix == \".yaml\":\\n        with open(file_path, \"r\") as f:\\n            config = yaml.safe_load(f)\\n    else:\\n        raise ValueError(\"File type must be json or yaml\")\\n\\n    return load_agent_from_config(config, **kwargs)',\n",
       "   'd': 'Format the prompt template.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    Formatted message.',\n",
       "   'l': False,\n",
       "   'g': ['Returns the path to the output directory.',\n",
       "    '_summary_\\n\\n    Returns:\\n        str: _description_',\n",
       "    'Return the path to the output directory',\n",
       "    '_summary_\\n\\n    Returns:\\n        str: _description_']},\n",
       "  {'c': '    def load(\\n        self,\\n        space_key: Optional[str] = None,\\n        page_ids: Optional[List[str]] = None,\\n        label: Optional[str] = None,\\n        cql: Optional[str] = None,\\n        include_restricted_content: bool = False,\\n        include_archived_content: bool = False,\\n        include_attachments: bool = False,\\n        include_comments: bool = False,\\n        content_format: ContentFormat = ContentFormat.STORAGE,\\n        limit: Optional[int] = 50,\\n        max_pages: Optional[int] = 1000,\\n        ocr_languages: Optional[str] = None,\\n    ) -> List[Document]:\\n        if not space_key and not page_ids and not label and not cql:\\n            raise ValueError(\\n                \"Must specify at least one among `space_key`, `page_ids`, \"\\n                \"`label`, `cql` parameters.\"\\n            )\\n\\n        docs = []\\n\\n        if space_key:\\n            pages = self.paginate_request(\\n                self.confluence.get_all_pages_from_space,\\n                space=space_key,\\n                limit=limit,\\n                max_pages=max_pages,\\n                status=\"any\" if include_archived_content else \"current\",\\n                expand=content_format.value,\\n            )\\n            docs += self.process_pages(\\n                pages,\\n                include_restricted_content,\\n                include_attachments,\\n                include_comments,\\n                content_format,\\n                ocr_languages,\\n            )\\n\\n        if label:\\n            pages = self.paginate_request(\\n                self.confluence.get_all_pages_by_label,\\n                label=label,\\n                limit=limit,\\n                max_pages=max_pages,\\n            )\\n            ids_by_label = [page[\"id\"] for page in pages]\\n            if page_ids:\\n                page_ids = list(set(page_ids + ids_by_label))\\n            else:\\n                page_ids = list(set(ids_by_label))\\n\\n        if cql:\\n            pages = self.paginate_request(\\n                self._search_content_by_cql,\\n                cql=cql,\\n                limit=limit,\\n                max_pages=max_pages,\\n                include_archived_spaces=include_archived_content,\\n                expand=content_format.value,\\n            )\\n            docs += self.process_pages(\\n                pages,\\n                include_restricted_content,\\n                include_attachments,\\n                include_comments,\\n                content_format,\\n                ocr_languages,\\n            )\\n\\n        if page_ids:\\n            for page_id in page_ids:\\n                get_page = retry(\\n                    reraise=True,\\n                    stop=stop_after_attempt(\\n                        self.number_of_retries\\n                    ),\\n                    wait=wait_exponential(\\n                        multiplier=1,\\n                        min=self.min_retry_seconds,\\n                        max=self.max_retry_seconds,\\n                    ),\\n                    before_sleep=before_sleep_log(logger, logging.WARNING),\\n                )(self.confluence.get_page_by_id)\\n                page = get_page(page_id=page_id, expand=content_format.value)\\n                if not include_restricted_content and not self.is_public_page(page):\\n                    continue\\n                doc = self.process_page(\\n                    page,\\n                    include_attachments,\\n                    include_comments,\\n                    content_format,\\n                    ocr_languages,\\n                )\\n                docs.append(doc)\\n\\n        return docs',\n",
       "   'd': 'Initialize with necessary components.',\n",
       "   'l': False,\n",
       "   'g': ['.', ':return:', '.', '.']},\n",
       "  {'c': '    def input_keys(self) -> List[str]:\\n        return [self.input_key]',\n",
       "   'd': 'Expect input key.\\n\\n:meta private:',\n",
       "   'l': True,\n",
       "   'g': ['Sends a payload to the device.\\n    \\n    Args:\\n        payload (dict): The payload to send to the device.\\n    \\n    Returns:\\n        dict: The response from the device.',\n",
       "    'Sends a message to the client.',\n",
       "    'Send the payload to the server.\\n    \\n    @param payload: The payload to send.\\n    @type payload: dict\\n    @return: The response from the server.\\n    @rtype: dict',\n",
       "    'Send a message to the server.']},\n",
       "  {'c': 'def _load_run_evaluators(\\n    config: smith_eval.RunEvalConfig,\\n    run_type: str,\\n    data_type: DataType,\\n    example_outputs: Optional[List[str]],\\n    run_inputs: Optional[List[str]],\\n    run_outputs: Optional[List[str]],\\n) -> List[RunEvaluator]:\\n    run_evaluators = []\\n    input_key, prediction_key, reference_key = None, None, None\\n    if (\\n        config.evaluators\\n        or any([isinstance(e, EvaluatorType) for e in config.evaluators])\\n        or (\\n            config.custom_evaluators\\n            and any([isinstance(e, StringEvaluator) for e in config.custom_evaluators])\\n        )\\n    ):\\n        input_key, prediction_key, reference_key = _get_keys(\\n            config, run_inputs, run_outputs, example_outputs\\n        )\\n    for eval_config in config.evaluators:\\n        run_evaluator = _construct_run_evaluator(\\n            eval_config,\\n            config.eval_llm,\\n            run_type,\\n            data_type,\\n            example_outputs,\\n            reference_key,\\n            input_key,\\n            prediction_key,\\n        )\\n        run_evaluators.append(run_evaluator)\\n    custom_evaluators = config.custom_evaluators or []\\n    for custom_evaluator in custom_evaluators:\\n        if isinstance(custom_evaluator, RunEvaluator):\\n            run_evaluators.append(custom_evaluator)\\n        elif isinstance(custom_evaluator, StringEvaluator):\\n            run_evaluators.append(\\n                smith_eval.StringRunEvaluatorChain.from_run_and_data_type(\\n                    custom_evaluator,\\n                    run_type,\\n                    data_type,\\n                    input_key=input_key,\\n                    prediction_key=prediction_key,\\n                    reference_key=reference_key,\\n                )\\n            )\\n        else:\\n            raise ValueError(\\n                f\"Unsupported custom evaluator: {custom_evaluator}.\"\\n                f\" Expected RunEvaluator or StringEvaluator.\"\\n            )\\n\\n    return run_evaluators',\n",
       "   'd': 'Load run evaluators from a configuration.\\n\\nArgs:\\n    config: Configuration for the run evaluators.\\n\\nReturns:\\n    A list of run evaluators.',\n",
       "   'l': True,\n",
       "   'g': ['(object):',\n",
       "    'for clip',\n",
       "    '(init) Initialize the class',\n",
       "    '.\\n    Initialize CLIP model.\\n    Args:\\n        source (str): Path to CLIP model.\\n        results (str): Path to save results.\\n        device (str): Device to use for inference.']},\n",
       "  {'c': '    def refresh_schema(self) -> None:\\n        db_schema = self.query(SCHEMA_QUERY)[0].get(\"schema\")\\n        assert db_schema is not None\\n        self.schema = db_schema',\n",
       "   'd': 'Refreshes the Memgraph graph schema information.',\n",
       "   'l': True,\n",
       "   'g': ['_Print a message to stdout and stderr.\\n    \\n    :param message: The message to print.\\n    :param ostream: The stream to print to.\\n    :type message: str\\n    :type ostream: IO\\n    :return: None',\n",
       "    '_i_\\n\\n    Print a message to stderr.\\n\\n    :param message: The message to print.\\n    :type message: str\\n    :param ostream: The output stream to print to.\\n    :type ostream: str\\n    :return: None\\n    :rtype: None',\n",
       "    'Print a message to the standard output stream.',\n",
       "    'Prints a message to the standard error stream.']},\n",
       "  {'c': '    def lazy_load(self) -> Iterator[Document]:\\n        try:\\n            from qcloud_cos import CosS3Client\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import cos-python-sdk-v5 python package. \"\\n                \"Please install it with `pip install cos-python-sdk-v5`.\"\\n            )\\n\\n\\n        client = CosS3Client(self.conf)\\n        with tempfile.TemporaryDirectory() as temp_dir:\\n            file_path = f\"{temp_dir}/{self.bucket}/{self.key}\"\\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n\\n            client.download_file(\\n                Bucket=self.bucket, Key=self.key, DestFilePath=file_path\\n            )\\n            loader = UnstructuredFileLoader(file_path)\\n\\n            return iter(loader.load())',\n",
       "   'd': 'PUT the URL and return the text.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'This class is used to set hyperparameters of a given variable',\n",
       "    '',\n",
       "    'Class to set hyperparameters for a human']},\n",
       "  {'c': 'def parse(self, text: str) -> List[str]:\\n',\n",
       "   'd': 'Parse the output of an LLM call.',\n",
       "   'l': True,\n",
       "   'g': ['Convert a string representation of a continuation align style to the\\n    appropriate constant.',\n",
       "    'Convert a string to a continuation align style.',\n",
       "    'Converts a string to a continuation align style. \\n    \\n    Parameters\\n    ----------\\n    s : str, optional\\n        The string to convert. If ``None``, the default style is used.\\n    \\n    Returns\\n    -------\\n    str\\n        The continuation align style.\\n    \\n    Raises\\n    ------\\n    ValueError\\n        If the input string is not a valid continuation align style.',\n",
       "    'Convert a string to a continuation align style.\\n    \\n    :param s: the string to convert\\n    :returns: the converted continuation align style\\n    :raises ValueError: if the string is not a valid continuation align style']},\n",
       "  {'c': '    def _run(\\n        self,\\n        *args: Any,\\n        **kwargs: Any,\\n    ) -> Any:\\n',\n",
       "   'd': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing,',\n",
       "   'l': True,\n",
       "   'g': ['Cancel a job.\\n    \\n    :param job: Job to cancel.\\n    :type job: :class:`~celery.worker.control.AsyncResult`',\n",
       "    'Cancel a job.',\n",
       "    \"'Cancel a job.\",\n",
       "    'Cancel a job.\\n\\n    :param job: job to cancel\\n    :type job: :class:`~scrapy.scheduler.Scheduler.job`']},\n",
       "  {'c': 'def try_load_from_hub(\\n    path: Union[str, Path],\\n    loader: Callable[[str], T],\\n    valid_prefix: str,\\n    valid_suffixes: Set[str],\\n    **kwargs: Any,\\n) -> Optional[T]:\\n    if not isinstance(path, str) or not (match := HUB_PATH_RE.match(path)):\\n        return None\\n    ref, remote_path_str = match.groups()\\n    ref = ref[1:] if ref else DEFAULT_REF\\n    remote_path = Path(remote_path_str)\\n    if remote_path.parts[0] != valid_prefix:\\n        return None\\n    if remote_path.suffix[1:] not in valid_suffixes:\\n        raise ValueError(f\"Unsupported file type, must be one of {valid_suffixes}.\")\\n\\n\\n\\n\\n\\n\\n    full_url = urljoin(URL_BASE.format(ref=ref), PurePosixPath(remote_path).__str__())\\n\\n    r = requests.get(full_url, timeout=5)\\n    if r.status_code != 200:\\n        raise ValueError(f\"Could not find file at {full_url}\")\\n    with tempfile.TemporaryDirectory() as tmpdirname:\\n        file = Path(tmpdirname) / remote_path.name\\n        with open(file, \"wb\") as f:\\n            f.write(r.content)\\n        return loader(str(file), **kwargs)',\n",
       "   'd': 'Split markdown file\\nArgs:\\n    text: Markdown file',\n",
       "   'l': False,\n",
       "   'g': ['for a given layer, update the key-value cache for that layer.\\n\\n    Args:\\n        kv (torch.FloatTensor): The key-value cache for the layer.\\n        inference_params (InferenceParams): The inference parameters.',\n",
       "    'for a given kv, update the kv cache for the given layer_idx.',\n",
       "    '.\\n\\n    Args:\\n        kv (torch.FloatTensor): The key-value pairs to update.\\n        inference_params (InferenceParams): The inference parameters.\\n\\n    Returns:\\n        None',\n",
       "    '.']},\n",
       "  {'c': '    def _run(\\n        self,\\n        query: str,\\n        run_manager: Optional[CallbackManagerForToolRun] = None,\\n    ) -> str:\\n        try:\\n            import yfinance\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import yfinance python package. \"\\n                \"Please install it with `pip install yfinance`.\"\\n            )\\n        company = yfinance.Ticker(query)\\n        try:\\n            if company.isin is None:\\n                return f\"Company ticker {query} not found.\"\\n        except (HTTPError, ReadTimeout, ConnectionError):\\n            return f\"Company ticker {query} not found.\"\\n\\n        links = []\\n        try:\\n            links = [n[\"link\"] for n in company.news if n[\"type\"] == \"STORY\"]\\n        except (HTTPError, ReadTimeout, ConnectionError):\\n            if not links:\\n                return f\"No news found for company that searched with {query} ticker.\"\\n        if not links:\\n            return f\"No news found for company that searched with {query} ticker.\"\\n        loader = WebBaseLoader(links)\\n        docs = loader.load()\\n        result = self._format_results(docs, query)\\n        if not result:\\n            return f\"No news found for company that searched with {query} ticker.\"\\n        return result',\n",
       "   'd': 'Use the Yahoo Finance News tool.',\n",
       "   'l': True,\n",
       "   'g': ['(self, r)', '.', '(self, r):', '(self, r):']},\n",
       "  {'c': 'def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\\n    if self.input_keys:\\n        input_variables = {key: input_variables[key] for key in self.input_keys}\\n    query = \" \".join(sorted_values(input_variables))\\n    example_docs = self.vectorstore.similarity_search(query, k=self.k)\\n\\n\\n    examples = [dict(e.metadata) for e in example_docs]\\n\\n    if self.example_keys:\\n        examples = [{k: eg[k] for k in self.example_keys} for eg in examples]\\n    return examples',\n",
       "   'd': 'Parse the output of an LLM call.',\n",
       "   'l': False,\n",
       "   'g': ['.',\n",
       "    '.',\n",
       "    '.',\n",
       "    'Args:\\n        pred: [N, H, W]\\n        gt: [N, H, W]\\n        mask: [N, H, W]\\n        weights: [N, H, W]']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        try:\\n            from bs4 import BeautifulSoup\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import bs4 python package. \"\\n                \"Please install it with `pip install bs4`.\"\\n            )\\n        return values',\n",
       "   'd': 'Upsert records into the SQLite database.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '',\n",
       "    'Return the size of the block that is currently being written to.',\n",
       "    '']},\n",
       "  {'c': '    def validate_input_variables(cls, values: dict) -> dict:\\n        messages = values[\"messages\"]\\n        input_vars = set()\\n        input_types: Dict[str, Any] = values.get(\"input_types\", {})\\n        for message in messages:\\n            if isinstance(message, (BaseMessagePromptTemplate, BaseChatPromptTemplate)):\\n                input_vars.update(message.input_variables)\\n            if isinstance(message, MessagesPlaceholder):\\n                if message.variable_name not in input_types:\\n                    input_types[message.variable_name] = List[AnyMessage]\\n        if \"partial_variables\" in values:\\n            input_vars = input_vars - set(values[\"partial_variables\"])\\n        if \"input_variables\" in values:\\n            if input_vars != set(values[\"input_variables\"]):\\n                raise ValueError(\\n                    \"Got mismatched input_variables. \"\\n                    f\"Expected: {input_vars}. \"\\n                    f\"Got: {values[\\'input_variables\\']}\"\\n                )\\n        else:\\n            values[\"input_variables\"] = sorted(input_vars)\\n        values[\"input_types\"] = input_types\\n        return values',\n",
       "   'd': 'Format the chat template into a string.\\n\\nArgs:\\n    **kwargs: keyword arguments to use for filling in template variables\\n              in all the template messages in this chat template.\\n\\nReturns:\\n    formatted string',\n",
       "   'l': False,\n",
       "   'g': ['Tab key handler.',\n",
       "    'Detect <Tab> key and complete the current buffer.',\n",
       "    '@brief: Handles the <Tab> key\\n    @param event: The event object',\n",
       "    'Detect Tab key.']},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n    stmt = (\\n        select(self.cache_schema.response)\\n        .where(self.cache_schema.prompt == prompt)\\n        .where(self.cache_schema.llm == llm_string)\\n        .order_by(self.cache_schema.idx)\\n    )\\n    with Session(self.engine) as session:\\n        rows = session.execute(stmt).fetchall()\\n        if rows:\\n            try:\\n                return [loads(row[0]) for row in rows]\\n            except Exception:\\n                logger.warning(\\n                    \"Retrieving a cache value that could not be deserialized \"\\n                    \"properly. This is likely due to the cache being in an \"\\n                    \"older format. Please recreate your cache to avoid this \"\\n                    \"error.\"\\n                )\\n\\n\\n                return [Generation(text=row[0]) for row in rows]\\n    return None',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Initialize the generator object.\\n    \\n    Args:\\n        generator (object): The generator object to be initialized.',\n",
       "    'Generator class for generating names',\n",
       "    'Args:\\n        generator (str):']},\n",
       "  {'c': '    def __gt__(self, other: int) -> \"RedisFilterExpression\":\\n        self._set_value(other, int, RedisFilterOperator.GT)\\n        return RedisFilterExpression(str(self))',\n",
       "   'd': 'Create a RedisNumeric greater than filter expression\\n\\nArgs:\\n    other (int): The value to filter on.\\n\\nExample:\\n    >>> from langchain.vectorstores.redis import RedisNum\\n    >>> filter = RedisNum(\"age\") > 18',\n",
       "   'l': True,\n",
       "   'g': ['Returns the number of pages of the document.',\n",
       "    'Return the number of pages in the object.',\n",
       "    'Return the number of pages in the document.',\n",
       "    'Returns the number of pages in the document.']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        github_repository = get_from_dict_or_env(\\n            values, \"github_repository\", \"GITHUB_REPOSITORY\"\\n        )\\n\\n        github_app_id = get_from_dict_or_env(values, \"github_app_id\", \"GITHUB_APP_ID\")\\n\\n        github_app_private_key = get_from_dict_or_env(\\n            values, \"github_app_private_key\", \"GITHUB_APP_PRIVATE_KEY\"\\n        )\\n\\n        github_branch = get_from_dict_or_env(\\n            values, \"github_branch\", \"GITHUB_BRANCH\", default=\"master\"\\n        )\\n        github_base_branch = get_from_dict_or_env(\\n            values, \"github_base_branch\", \"GITHUB_BASE_BRANCH\", default=\"master\"\\n        )\\n\\n        try:\\n            from github import Auth, GithubIntegration\\n\\n        except ImportError:\\n            raise ImportError(\\n                \"PyGithub is not installed. \"\\n                \"Please install it with `pip install PyGithub`\"\\n            )\\n\\n        with open(github_app_private_key, \"r\") as f:\\n            private_key = f.read()\\n\\n        auth = Auth.AppAuth(\\n            github_app_id,\\n            private_key,\\n        )\\n        gi = GithubIntegration(auth=auth)\\n        installation = gi.get_installations()[0]\\n\\n\\n        g = installation.get_github_for_installation()\\n\\n        values[\"github\"] = g\\n        values[\"github_repo_instance\"] = g.get_repo(github_repository)\\n        values[\"github_repository\"] = github_repository\\n        values[\"github_app_id\"] = github_app_id\\n        values[\"github_app_private_key\"] = github_app_private_key\\n        values[\"github_branch\"] = github_branch\\n        values[\"github_base_branch\"] = github_base_branch\\n\\n        return values',\n",
       "   'd': 'Validate that api key and python package exists in environment.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Add a new column to the dataframe.\\n    :param name: name of the new column\\n    :param data: data to fill the new column with\\n    :return:',\n",
       "    'to add a column to the DataFrame',\n",
       "    '.add_column(self, name, data)\\n\\n    Adds a new column to the DataFrame.\\n\\n    Parameters\\n    ----------\\n    name : str\\n        Name of the column to add.\\n    data : ndarray\\n        Data to add to the column.',\n",
       "    '.add_column(self, name, data)\\n\\n    Add a column to the DataFrame.\\n\\n    Parameters\\n    ----------\\n    name : str\\n        The name of the column.\\n    data : ndarray\\n        The data for the column.']},\n",
       "  {'c': '    def get_input_schema(\\n        self, config: Optional[RunnableConfig] = None\\n    ) -> Type[BaseModel]:\\n        func = getattr(self, \"func\", None) or getattr(self, \"afunc\")\\n\\n        if isinstance(func, itemgetter):\\n\\n\\n            items = str(func).replace(\"operator.itemgetter(\", \"\")[:-1].split(\", \")\\n            if all(\\n                item[0] == \"\\'\" and item[-1] == \"\\'\" and len(item) > 2 for item in items\\n            ):\\n\\n                return create_model(\\n                    \"RunnableLambdaInput\",\\n                    **{item[1:-1]: (Any, None) for item in items},\\n                )\\n            else:\\n                return create_model(\"RunnableLambdaInput\", __root__=(List[Any], None))\\n\\n        if self.InputType != Any:\\n            return super().get_input_schema(config)\\n\\n        if dict_keys := get_function_first_arg_dict_keys(func):\\n            return create_model(\\n                \"RunnableLambdaInput\",\\n                **{key: (Any, None) for key in dict_keys},\\n            )\\n\\n        return super().get_input_schema(config)',\n",
       "   'd': 'The pydantic schema for the input to this runnable.',\n",
       "   'l': True,\n",
       "   'g': ['Calculate the F1 score for the predictions and gold labels.\\n    \\n    Args:\\n        predictions (torch.Tensor): Predictions for the data.\\n        gold_labels (torch.Tensor): Gold labels for the data.\\n        mask (Optional[torch.Tensor]): Mask for the data.\\n    \\n    Returns:\\n        torch.Tensor: F1 score for the predictions and gold labels.',\n",
       "    'Computes the F1 score for the predictions and gold labels.',\n",
       "    'Args:\\n        predictions (torch.Tensor): Predictions of the model\\n        gold_labels (torch.Tensor): Gold labels of the data\\n        mask (Optional[torch.Tensor], optional): Mask of the data. Defaults to None.\\n\\n    Returns:\\n        torch.Tensor: Accuracy of the model',\n",
       "    'Computes the loss for the model.\\n\\n    Parameters\\n    ----------\\n    predictions : torch.Tensor\\n        The predictions of the model.\\n    gold_labels : torch.Tensor\\n        The gold labels of the model.\\n    mask : Optional[torch.Tensor], optional\\n        The mask of the model. Defaults to None.\\n\\n    Returns\\n    -------\\n    torch.Tensor\\n        The loss of the model.']},\n",
       "  {'c': '    def from_examples(\\n        cls,\\n        examples: List[dict],\\n        embeddings: Embeddings,\\n        vectorstore_cls: Type[VectorStore],\\n        k: int = 4,\\n        input_keys: Optional[List[str]] = None,\\n        **vectorstore_cls_kwargs: Any,\\n    ) -> SemanticSimilarityExampleSelector:\\n        if input_keys:\\n            string_examples = [\\n                \" \".join(sorted_values({k: eg[k] for k in input_keys}))\\n                for eg in examples\\n            ]\\n        else:\\n            string_examples = [\" \".join(sorted_values(eg)) for eg in examples]\\n        vectorstore = vectorstore_cls.from_texts(\\n            string_examples, embeddings, metadatas=examples, **vectorstore_cls_kwargs\\n        )\\n        return cls(vectorstore=vectorstore, k=k, input_keys=input_keys)',\n",
       "   'd': 'Returns the input text with no changes.',\n",
       "   'l': False,\n",
       "   'g': ['_create_vision_transformer',\n",
       "    '_create_vision_transformer',\n",
       "    '_create_vision_transformer',\n",
       "    '_create_vision_transformer']},\n",
       "  {'c': 'def _call(\\n    self,\\n    messages: List[BaseMessage],\\n    stop: Optional[List[str]] = None,\\n    run_manager: Optional[CallbackManagerForLLMRun] = None,\\n    **kwargs: Any,\\n) -> str:\\n    response = self.responses[self.i]\\n    if self.i < len(self.responses) - 1:\\n        self.i += 1\\n    else:\\n        self.i = 0\\n    return response',\n",
       "   'd': \"First try to lookup in queries, else return 'foo' or 'bar'.\",\n",
       "   'l': True,\n",
       "   'g': ['True loss function',\n",
       "    'True loss function for the model. \\n    \\n    Args:\\n        cls (torch.nn.Module): The model.\\n        result (torch.Tensor): The result tensor.\\n        argd (torch.Tensor): The argument tensor.\\n    \\n    Returns:\\n        torch.Tensor: The loss tensor.',\n",
       "    'Computes the loss for the given result and argd.\\n    \\n    Args:\\n        cls: The loss function to use.\\n        result: The result to compute the loss for.\\n        argd: The argd to compute the loss for.\\n    \\n    Returns:\\n        The loss for the given result and argd.',\n",
       "    'Args:\\n        cls:\\n        result:\\n        argd:\\n    \\n    Returns:\\n        loss:']},\n",
       "  {'c': 'def _run(\\n    self, text: str, run_manager: Optional[CallbackManagerForToolRun] = None\\n) -> str:\\n    try:\\n        data = _parse_input(text)\\n        return self.requests_wrapper.patch(_clean_url(data[\"url\"]), data[\"data\"])\\n    except Exception as e:\\n        return repr(e)',\n",
       "   'd': 'Test end to end construction and search.',\n",
       "   'l': False,\n",
       "   'g': ['Return the number of units in the network.',\n",
       "    'Returns the number of units in the output layer.',\n",
       "    'Returns the number of units in the output layer of the network.',\n",
       "    'Returns the number of units in the network.']},\n",
       "  {'c': 'def __new__(cls, **data: Any) -> Union[OpenAIChat, BaseOpenAI]:\\n    model_name = data.get(\"model_name\", \"\")\\n    if model_name.startswith(\"gpt-3.5-turbo\") or model_name.startswith(\"gpt-4\"):\\n        warnings.warn(\\n            \"You are trying to use a chat model. This way of initializing it is \"\\n            \"no longer supported. Instead, please use: \"\\n            \"`from langchain.chat_models import ChatOpenAI`\"\\n        )\\n        return OpenAIChat(**data)\\n    return super().__new__(cls)',\n",
       "   'd': 'Initialize the OpenAI object.',\n",
       "   'l': True,\n",
       "   'g': ['Return all child prefixes of this prefix.',\n",
       "    'Returns all prefixes that are a child of this prefix.',\n",
       "    'Returns a list of child prefixes for the given prefix',\n",
       "    'Return all child prefixes of this prefix.']},\n",
       "  {'c': 'def similarity_search_with_score(\\n    self, query: str, k: int = 4, filter: Optional[dict] = None, **kwargs: Any\\n) -> List[Tuple[Document, float]]:\\n    embedding = self.embedding.embed_query(query)\\n    script_query = _default_script_query(embedding, filter)\\n    response = self.client_search(\\n        self.client, self.index_name, script_query, size=k\\n    )\\n    hits = [hit for hit in response[\"hits\"][\"hits\"]]\\n    docs_and_scores = [\\n        (\\n            Document(\\n                page_content=hit[\"_source\"][\"text\"],\\n                metadata=hit[\"_source\"][\"metadata\"],\\n            ),\\n            hit[\"_score\"],\\n        )\\n        for hit in hits\\n    ]\\n    return docs_and_scores',\n",
       "   'd': 'Return docs most similar to query.\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 4.\\nReturns:\\n    List of Documents most similar to the query.',\n",
       "   'l': True,\n",
       "   'g': ['.',\n",
       "    '.',\n",
       "    'Bans the given IP address.',\n",
       "    '.\\n    Bans a client from the server.']},\n",
       "  {'c': '    def from_template(\\n        cls: Type[MessagePromptTemplateT],\\n        template: str,\\n        template_format: str = \"f-string\",\\n        **kwargs: Any,\\n    ) -> MessagePromptTemplateT:\\n        prompt = PromptTemplate.from_template(template, template_format=template_format)\\n        return cls(prompt=prompt, **kwargs)',\n",
       "   'd': 'Return type of llm.',\n",
       "   'l': False,\n",
       "   'g': ['Args:\\n        im (np.ndarray): Input image.\\n        im_info (dict): Image info dict.\\n\\n    Returns:\\n        np.ndarray: Preprocessed image.',\n",
       "    '.',\n",
       "    'Args:\\n        im (np.ndarray): Image to be normalized\\n        im_info (dict): Metadata of the image\\n    Returns:\\n        np.ndarray: Normalized image',\n",
       "    'Args:\\n        im (np.ndarray): Input image.\\n        im_info (dict): Image information.']},\n",
       "  {'c': 'def InputType(self) -> Any:\\n    func = getattr(self, \"func\", None) or getattr(self, \"afunc\")\\n    try:\\n        params = inspect.signature(func).parameters\\n        first_param = next(iter(params.values()), None)\\n        if first_param and first_param.annotation != inspect.Parameter.empty:\\n            return first_param.annotation\\n        else:\\n            return Any\\n    except ValueError:\\n        return Any',\n",
       "   'd': \"Adds documents to the docstore and vectorstores.\\n\\nArgs:\\n    documents: List of documents to add\\n    ids: Optional list of ids for documents. If provided should be the same\\n        length as the list of documents. Can provided if parent documents\\n        are already in the document store and you don't want to re-add\\n        to the docstore. If not provided, random UUIDs will be used as\\n        ids.\\n    add_to_docstore: Boolean of whether to add documents to docstore.\\n        This can be false if and only if `ids` are provided. You may want\\n        to set this to False if the documents are already in the docstore\\n        and you don't want to re-add them.\",\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def index(\\n        self,\\n        dims_length: Union[int, None],\\n        vector_query_field: str,\\n        similarity: Union[DistanceStrategy, None],\\n    ) -> Dict:\\n',\n",
       "   'd': 'Executes when the index is created.\\n\\nArgs:\\n    dims_length: Numeric length of the embedding vectors,\\n                or None if not using vector-based query.\\n    vector_query_field: The field containing the vector\\n                        representations in the index.\\n    similarity: The similarity strategy to use,\\n                or None if not using one.\\n\\nReturns:\\n    Dict: The Elasticsearch settings and mappings for the strategy.',\n",
       "   'l': True,\n",
       "   'g': ['.',\n",
       "    '.',\n",
       "    '.',\n",
       "    '.\\n    \\n    :param index:\\n    :param data:\\n    :param modify_index:\\n    :return:']},\n",
       "  {'c': '    def save(self, file_path: Union[Path, str]) -> None:\\n        if isinstance(file_path, str):\\n            save_path = Path(file_path)\\n        else:\\n            save_path = file_path\\n\\n        directory_path = save_path.parent\\n        directory_path.mkdir(parents=True, exist_ok=True)\\n\\n\\n        agent_dict = self.dict()\\n\\n        if save_path.suffix == \".json\":\\n            with open(file_path, \"w\") as f:\\n                json.dump(agent_dict, f, indent=4)\\n        elif save_path.suffix == \".yaml\":\\n            with open(file_path, \"w\") as f:\\n                yaml.dump(agent_dict, f, default_flow_style=False)\\n        else:\\n            raise ValueError(f\"{save_path} must be json or yaml\")',\n",
       "   'd': 'Get docs.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    Checks if the image is partly within the self.image.\\n\\n    :param image: The image to check.\\n    :return: True if the image is partly within the self.image, False otherwise.',\n",
       "    ',\\n    Checks whether the image is partly within the image bounds.',\n",
       "    '.\\n    Checks if a given image is partly within the image.\\n\\n    Args:\\n        image (np.ndarray): The image to check.\\n\\n    Returns:\\n        bool: True if the image is partly within the image, False otherwise.',\n",
       "    '.\\n    Checks if the given image is partially or fully within the image.\\n    :param image: Image to check\\n    :return: True if the image is partially or fully within the image, False otherwise']},\n",
       "  {'c': 'def test_deprecated_property() -> None:\\n    with warnings.catch_warnings(record=True) as warning_list:\\n        warnings.simplefilter(\"always\")\\n\\n        obj = ClassWithDeprecatedMethods()\\n        assert obj.deprecated_property == \"This is a deprecated property.\"\\n\\n        assert len(warning_list) == 1\\n        warning = warning_list[0].message\\n\\n        assert str(warning) == (\\n            \"The function `deprecated_property` was deprecated in \"\\n            \"LangChain 2.0.0 and will be removed in 3.0.0\"\\n        )\\n        doc = ClassWithDeprecatedMethods.deprecated_property.__doc__\\n        assert isinstance(doc, str)\\n        assert doc.startswith(\"[*Deprecated*]  original doc\")',\n",
       "   'd': 'Test deprecated staticmethod.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Parses a string in the format \"device_type:device_id\" and returns a Device object.',\n",
       "    '.\\n    Parses a string in the format \"<device_type>:<device_id>\" and returns a Device object.',\n",
       "    '.\\n    >>> from_str(\"1234567890\")\\n    Device(DeviceType.PHONE, 1234567890)',\n",
       "    '.\\n    Parses a string in the format \"device_type:device_id\" and returns a Device object.\\n\\n    Args:\\n        string (str): The string to parse.\\n\\n    Returns:\\n        Device: A Device object with the parsed device type and ID.']},\n",
       "  {'c': '    def with_listeners(\\n        self,\\n        *,\\n        on_start: Optional[Listener] = None,\\n        on_end: Optional[Listener] = None,\\n        on_error: Optional[Listener] = None,\\n    ) -> Runnable[Input, Output]:\\n        from langchain.callbacks.tracers.root_listeners import RootListenersTracer\\n\\n        return self.__class__(\\n            bound=self.bound,\\n            kwargs=self.kwargs,\\n            config=self.config,\\n            config_factories=[\\n                lambda config: {\\n                    \"callbacks\": [\\n                        RootListenersTracer(\\n                            config=config,\\n                            on_start=on_start,\\n                            on_end=on_end,\\n                            on_error=on_error,\\n                        )\\n                    ],\\n                }\\n            ],\\n            custom_input_type=self.custom_input_type,\\n            custom_output_type=self.custom_output_type,\\n        )',\n",
       "   'd': 'Bind lifecycle listeners to a Runnable, returning a new Runnable.\\n\\non_start: Called before the runnable starts running, with the Run object.\\non_end: Called after the runnable finishes running, with the Run object.\\non_error: Called if the runnable throws an error, with the Run object.\\n\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Returns session info.',\n",
       "    '.',\n",
       "    '.',\n",
       "    '.\\n    Get the session info from the akinator website.']},\n",
       "  {'c': 'def _run(\\n    self,\\n    query: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> str:\\n    chain = RetrievalQAWithSourcesChain.from_chain_type(\\n        self.llm, retriever=self.vectorstore.as_retriever()\\n    )\\n    return json.dumps(\\n        chain(\\n            {chain.question_key: query},\\n            return_only_outputs=True,\\n            callbacks=run_manager.get_child() if run_manager else None,\\n        )\\n    )',\n",
       "   'd': 'Use the tool.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n\\n    :param dtype:\\n    :param size:\\n    :return:',\n",
       "    '.\\n    Return a tensor of the given type and shape.',\n",
       "    'for numpy arrays',\n",
       "    '.\\n    Create a new instance of the tensor.']},\n",
       "  {'c': 'def get_tools(self) -> List[BaseTool]:\\n    description = VectorStoreQATool.get_description(\\n        self.vectorstore_info.name, self.vectorstore_info.description\\n    )\\n    qa_tool = VectorStoreQATool(\\n        name=self.vectorstore_info.name,\\n        description=description,\\n        vectorstore=self.vectorstore_info.vectorstore,\\n        llm=self.llm,\\n    )\\n    description = VectorStoreQAWithSourcesTool.get_description(\\n        self.vectorstore_info.name, self.vectorstore_info.description\\n    )\\n    qa_with_sources_tool = VectorStoreQAWithSourcesTool(\\n        name=f\"{self.vectorstore_info.name}_with_sources\",\\n        description=description,\\n        vectorstore=self.vectorstore_info.vectorstore,\\n        llm=self.llm,\\n    )\\n    return [qa_tool, qa_with_sources_tool]',\n",
       "   'd': 'Return type of llm.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Create a regular link.',\n",
       "    'Creates a regular link.',\n",
       "    'Creates a regular link']},\n",
       "  {'c': '    def lazy_parse(self, blob: Blob) -> Iterator[Document]:\\n        import fitz\\n\\n        with blob.as_bytes_io() as file_path:\\n            doc = fitz.open(file_path)\\n\\n            yield from [\\n                Document(\\n                    page_content=page.get_text(**self.text_kwargs),\\n                    metadata=dict(\\n                        {\\n                            \"source\": blob.source,\\n                            \"file_path\": blob.source,\\n                            \"page\": page.number,\\n                            \"total_pages\": len(doc),\\n                        },\\n                        **{\\n                            k: doc.metadata[k]\\n                            for k in doc.metadata\\n                            if type(doc.metadata[k]) in [str, int]\\n                        },\\n                    ),\\n                )\\n                for page in doc\\n            ]',\n",
       "   'd': 'Lazily parse the blob.',\n",
       "   'l': True,\n",
       "   'g': ['.predict_on_image(self, img)\\n\\n    Args:\\n        img: Image to be predicted on.\\n\\n    Returns:\\n        Prediction of the image.',\n",
       "    '.\\n    Args:\\n        img (np.ndarray): Image to predict on.\\n    Returns:\\n        np.ndarray: Predicted label.',\n",
       "    '.\\n    Predict on a single image.\\n    Args:\\n        img (np.ndarray): Image to predict on.\\n    Returns:\\n        np.ndarray: Predicted label.',\n",
       "    '.\\n    Args:\\n        img (np.ndarray): Input image of shape (3, H, W)\\n\\n    Returns:\\n        np.ndarray: Predicted class label of shape (1,)']},\n",
       "  {'c': 'def return_stopped_response(\\n    self,\\n    early_stopping_method: str,\\n    intermediate_steps: List[Tuple[AgentAction, str]],\\n    **kwargs: Any,\\n) -> AgentFinish:\\n    if early_stopping_method == \"force\":\\n\\n        return AgentFinish({\"output\": \"Agent stopped due to max iterations.\"}, \"\")\\n    else:\\n        raise ValueError(\\n            f\"Got unsupported early_stopping_method `{early_stopping_method}`\"\\n        )',\n",
       "   'd': 'Return response when agent has been stopped due to max iterations.',\n",
       "   'l': True,\n",
       "   'g': ['for each row in X, fit a linear model\\n    to predict y.',\n",
       "    '(self, X, y) -> self\\n    \\n    Fit the model to data X and labels y.\\n    \\n    Returns\\n    -------\\n    self : object\\n        Returns self.',\n",
       "    ',X,y) -> self\\n\\n    Fits the model to the data X and returns self.\\n\\n    Parameters\\n    ----------\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training data.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    Returns\\n    -------\\n    self : object',\n",
       "    '(n_samples, n_features) -> (n_samples, n_features)']},\n",
       "  {'c': 'def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n    all_required_field_names = get_pydantic_field_names(cls)\\n    extra = values.get(\"model_kwargs\", {})\\n    values[\"model_kwargs\"] = build_extra_kwargs(\\n        extra, values, all_required_field_names\\n    )\\n    return values',\n",
       "   'd': 'Build extra kwargs from additional params that were passed in.',\n",
       "   'l': True,\n",
       "   'g': ['Save the model checkpoint.',\n",
       "    'Saves the model checkpoint with the given iteration number.',\n",
       "    'Save the model checkpoint.',\n",
       "    'Saves the model checkpoint.\\n    \\n    Parameters\\n    ----------\\n    iteration : int\\n        The current iteration.\\n    model : torch.nn.Module\\n        The model to be saved.\\n    args : argparse.Namespace\\n        The arguments.']},\n",
       "  {'c': 'def save(self, file_path: Union[Path, str]) -> None:\\n    raise ValueError(\\n        \"Saving not supported for agent executors. \"\\n        \"If you are trying to save the agent, please use the \"\\n        \"`.save_agent(...)`\"\\n    )',\n",
       "   'd': 'Raise error - saving not supported for Agent Executors.',\n",
       "   'l': True,\n",
       "   'g': ['_summary_\\n    Removes SQL from the question\\n    \\n    Args:\\n        question (str): The question to remove SQL from\\n    \\n    Returns:\\n        bool: True if the question was removed, False otherwise\\n    \\n    Raises:\\n        SQLRemoveError: If the question could not be removed',\n",
       "    'Removes SQL from the question',\n",
       "    'Remove SQL from the question.',\n",
       "    '_summary_\\n    \\n    Remove SQL from the database.\\n    \\n    Args:\\n        question (str): The question to remove SQL from.\\n    \\n    Returns:\\n        bool: True if the SQL was removed successfully, False otherwise.\\n    \\n    Raises:\\n        SQLRemoveError: If there was an error removing the SQL.']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> BaseMessage:\\n        text = self.prompt.format(**kwargs)\\n        return AIMessage(content=text, additional_kwargs=self.additional_kwargs)',\n",
       "   'd': 'Get default mime-type based parser.',\n",
       "   'l': False,\n",
       "   'g': ['(int) -> None\\n\\n    Sets the failure threshold for the client.\\n\\n    Args:\\n        failure_threshold (int): The new failure threshold for the client.\\n\\n    Returns:\\n        None',\n",
       "    '.\\n    Sets the failure threshold.',\n",
       "    'for the number of failures to trigger a failure.',\n",
       "    'for failure_threshold']},\n",
       "  {'c': 'def similarity_search_with_score(\\n    self, query: str, k: int = 10, **kwargs: Any\\n) -> List[Tuple[Document, float]]:\\n    return self.knn_search(query=query, k=k, **kwargs)',\n",
       "   'd': 'Pass through to `knn_search including score`',\n",
       "   'l': True,\n",
       "   'g': ['获取视频id', 'get video id from url', '获取视频的id', '从url中获取视频id']},\n",
       "  {'c': 'def map(self, run: Run) -> Dict[str, str]:\\n',\n",
       "   'd': 'Maps the Run to a dictionary.',\n",
       "   'l': True,\n",
       "   'g': ['Calculate the distance between each item and the album info',\n",
       "    'Calculate the distance between the album information and the items.',\n",
       "    'Calculates the distance between the album and the mapping.',\n",
       "    'Returns the distance of the album to the given mapping.']},\n",
       "  {'c': 'def __init__(self) -> None:\\n    self.store: Dict[str, Document] = {}',\n",
       "   'd': 'Run when tool errors.',\n",
       "   'l': False,\n",
       "   'g': ['for multiclass classification',\n",
       "    '.\\n    Args:\\n        y_true: A 1-D array-like with shape `(batch_size,)`.\\n        y_pred: A 1-D array-like with shape `(batch_size,)`.\\n        sample_weight: A 1-D array-like with shape `(batch_size,)`.',\n",
       "    '(y_true, y_pred, sample_weight) -> float',\n",
       "    'for example, if we have a binary classifier,\\n    and the score function is the sigmoid of the logits,\\n    then we can use this to implement binary crossentropy.']},\n",
       "  {'c': '    def execute(self, query: str, params: Optional[dict] = None, retry: int = 0) -> Any:\\n        from nebula3.Exception import IOErrorException, NoValidSessionException\\n        from nebula3.fbthrift.transport.TTransport import TTransportException\\n\\n        params = params or {}\\n        try:\\n            result = self.session_pool.execute_parameter(query, params)\\n            if not result.is_succeeded():\\n                logger.warning(\\n                    f\"Error executing query to NebulaGraph. \"\\n                    f\"Error: {result.error_msg()}\\\\n\"\\n                    f\"Query: {query} \\\\n\"\\n                )\\n            return result\\n\\n        except NoValidSessionException:\\n            logger.warning(\\n                f\"No valid session found in session pool. \"\\n                f\"Please consider increasing the session pool size. \"\\n                f\"Current size: {self.session_pool_size}\"\\n            )\\n            raise ValueError(\\n                f\"No valid session found in session pool. \"\\n                f\"Please consider increasing the session pool size. \"\\n                f\"Current size: {self.session_pool_size}\"\\n            )\\n\\n        except RuntimeError as e:\\n            if retry < RETRY_TIMES:\\n                retry += 1\\n                logger.warning(\\n                    f\"Error executing query to NebulaGraph. \"\\n                    f\"Retrying ({retry}/{RETRY_TIMES})...\\\\n\"\\n                    f\"query: {query} \\\\n\"\\n                    f\"Error: {e}\"\\n                )\\n                return self.execute(query, params, retry)\\n            else:\\n                raise ValueError(f\"Error executing query to NebulaGraph. Error: {e}\")\\n\\n        except (TTransportException, IOErrorException):\\n\\n            if retry < RETRY_TIMES:\\n                retry += 1\\n                logger.warning(\\n                    f\"Connection issue with NebulaGraph. \"\\n                    f\"Retrying ({retry}/{RETRY_TIMES})...\\\\n to recreate session pool\"\\n                )\\n                self.session_pool = self._get_session_pool()\\n                return self.execute(query, params, retry)',\n",
       "   'd': 'Add operators to the anonymizer\\n\\nArgs:\\n    operators: Operators to add to the anonymizer.',\n",
       "   'l': False,\n",
       "   'g': ['.',\n",
       "    '.get_item(self, entry, handle_value = 0)\\n\\n    .get_item(self, entry, handle_value = 0)\\n\\n    .get_item(self, entry, handle_value = 0)\\n\\n    .get_item(self, entry, handle_value = 0)\\n\\n    .get_item(self, entry, handle_value = 0)\\n\\n    .get_item(self, entry, handle_value = 0)\\n\\n    .get_item(self, entry, handle_value = 0)\\n\\n    .get_item(self, entry, handle_value = 0)\\n\\n    .get_item(self, entry, handle_value = 0)\\n\\n    .get_item(self, entry, handle_value = 0)\\n\\n    .get_item(self, entry, handle_value = 0)\\n\\n    .get_item(self, entry, handle_value = 0)\\n\\n    .get_item(self, entry, handle_value = 0)\\n\\n    .get_item(self, entry, handle_value = 0)\\n\\n    .get_item',\n",
       "    'Gets an object from the item.',\n",
       "    '_get_item\\n    \\n    Return the object that is associated with the given entry.\\n    \\n    Parameters:\\n    entry -- the entry to get the object for\\n    handle_value -- if non-zero, the handle value of the entry\\n    \\n    Returns:\\n    the object associated with the entry']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> BaseMessage:\\n        text = self.prompt.format(**kwargs)\\n        return AIMessage(content=text, additional_kwargs=self.additional_kwargs)',\n",
       "   'd': 'Format the prompt template.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    Formatted message.',\n",
       "   'l': True,\n",
       "   'g': [':param x: input tensor, shape [B, T, C]\\n    :param x_mask: mask of input tensor, shape [B, T]\\n    :param offset: offset of input tensor, int\\n    :return: output tensor, shape [B, T, C]',\n",
       "    ':param x: (batch_size, seq_len, emb_dim)\\n    :param x_mask: (batch_size, seq_len)\\n    :param offset: (batch_size, seq_len)\\n    :return:',\n",
       "    'Forward function for the Transformer model.\\n    \\n    Parameters:\\n    x (paddle.Tensor): Input tensor.\\n    x_mask (paddle.Tensor): Mask tensor for input tensor.\\n    offset (int): Offset for positional encoding.\\n    \\n    Returns:\\n    x (paddle.Tensor): Output tensor.\\n    pos_emb (paddle.Tensor): Positional encoding tensor.\\n    x_mask (paddle.Tensor): Mask tensor for output tensor.',\n",
       "    'Args:\\n        x (paddle.Tensor): [batch_size, seq_len, hidden_size]\\n        x_mask (paddle.Tensor): [batch_size, seq_len]\\n    \\n    Returns:\\n        paddle.Tensor: [batch_size, seq_len, hidden_size]']},\n",
       "  {'c': 'def _generate(\\n    self,\\n    prompts: List[str],\\n    stop: Optional[List[str]] = None,\\n    run_manager: Optional[CallbackManagerForLLMRun] = None,\\n    **kwargs: Any,\\n) -> LLMResult:\\n',\n",
       "   'd': 'Run the LLM on the given prompts.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Check if the given URL is on the same host as this URL.',\n",
       "    '.\\n    Returns whether the URL is on the same host as this server.',\n",
       "    '_is_same_host_impl\\n\\n    :param url:\\n    :return:',\n",
       "    '.\\n    Returns:\\n        bool: True if the host is the same as the current host.']},\n",
       "  {'c': 'def args(self) -> dict:\\n    return self.args_schema.schema()[\"properties\"]',\n",
       "   'd': \"The tool's input arguments.\",\n",
       "   'l': True,\n",
       "   'g': ['_raw_to_proto',\n",
       "    'Converts raw data to protobuf\\n    \\n    Args:\\n        raw: raw data\\n    \\n    Returns:\\n        proto',\n",
       "    '',\n",
       "    'Converts a raw feature vector to a protobuf Feature message.\\n    \\n    Args:\\n        raw (bytes): The raw feature vector.\\n    \\n    Returns:\\n        feature_pb2.Feature: The protobuf Feature message.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace',\n",
       "   'd': 'Use the tool.',\n",
       "   'l': False,\n",
       "   'g': ['Get the path for the given environment variable or default.\\n    \\n    Args:\\n        key (str): The name of the environment variable.\\n        default (Optional[Union[PathLike, str]]): The default path to use if the environment variable is not set.\\n\\n    Returns:\\n        Optional[Path]: The path for the given environment variable or default.',\n",
       "    'Get the value of an environment variable.\\n\\n    Parameters\\n    ----------\\n    key : str\\n        The name of the environment variable.\\n    default : Optional[Union[PathLike, str]], optional\\n        The default value to return if the environment variable is not set.\\n\\n    Returns\\n    -------\\n    Optional[Path]\\n        The value of the environment variable.',\n",
       "    'Returns a Path object for the given environment variable key.\\n    If the key is not found in the environment, and default is not provided,\\n    then None is returned.',\n",
       "    'Get the value of an environment variable, or None if not found. \\n    \\n    Args:\\n        key (str): The name of the environment variable.\\n        default (Optional[Union[PathLike, str]]): The default value to return if the environment variable is not found.\\n    \\n    Returns:\\n        Optional[Path]: The value of the environment variable, or None if not found.']},\n",
       "  {'c': 'def __init__(\\n    self,\\n    pl_id_callback: Optional[Callable[..., Any]] = None,\\n    pl_tags: Optional[List[str]] = None,\\n) -> None:\\n    _lazy_import_promptlayer()\\n    self.pl_id_callback = pl_id_callback\\n    self.pl_tags = pl_tags or []\\n    self.runs: Dict[UUID, Dict[str, Any]] = {}',\n",
       "   'd': 'Initialize the PromptLayerCallbackHandler.',\n",
       "   'l': True,\n",
       "   'g': ['@param flatIdx: the index of the flat this neuron is in',\n",
       "    '(init)',\n",
       "    '.\\n    Args:\\n        flatIdx (int): Flat index of the neuron.',\n",
       "    '.']},\n",
       "  {'c': 'def on_tool_error(\\n    self,\\n    error: Union[Exception, KeyboardInterrupt],\\n    *,\\n    run_id: UUID,\\n    parent_run_id: Optional[UUID] = None,\\n    **kwargs: Any,\\n) -> Any:\\n',\n",
       "   'd': 'Run when chain errors.\\n\\nArgs:\\n    error (Exception or KeyboardInterrupt): The error.',\n",
       "   'l': False,\n",
       "   'g': ['Add a flow to the store.',\n",
       "    '_add(self, f: mitmproxy.flow.Flow) -> None:\\n\\n    Adds a flow to the store.\\n\\n    Args:\\n        f (mitmproxy.flow.Flow): The flow to add.',\n",
       "    'Add a flow to the store and notify any listeners.',\n",
       "    'Add a flow to the store.']},\n",
       "  {'c': '    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        criteria: Optional[CRITERIA_TYPE] = None,\\n        *,\\n        prompt: Optional[BasePromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> CriteriaEvalChain:\\n        prompt_ = cls._resolve_prompt(prompt)\\n        if criteria == Criteria.CORRECTNESS:\\n            raise ValueError(\\n                \"Correctness should not be used in the reference-free\"\\n                \" \\'criteria\\' evaluator (CriteriaEvalChain).\"\\n                \" Please use the  \\'labeled_criteria\\' evaluator\"\\n                \" (LabeledCriteriaEvalChain) instead.\"\\n            )\\n        criteria_ = cls.resolve_criteria(criteria)\\n        criteria_str = \"\\\\n\".join(f\"{k}: {v}\" for k, v in criteria_.items())\\n        prompt_ = prompt_.partial(criteria=criteria_str)\\n        return cls(\\n            llm=llm,\\n            prompt=prompt_,\\n            criterion_name=\"-\".join(criteria_),\\n            **kwargs,\\n        )',\n",
       "   'd': 'Create a `CriteriaEvalChain` instance from an llm and criteria.\\n\\nParameters\\n----------\\nllm : BaseLanguageModel\\n    The language model to use for evaluation.\\ncriteria : CRITERIA_TYPE - default=None for \"helpfulness\"\\n    The criteria to evaluate the runs against. It can be:\\n        -  a mapping of a criterion name to its description\\n        -  a single criterion name present in one of the default criteria\\n        -  a single `ConstitutionalPrinciple` instance\\nprompt : Optional[BasePromptTemplate], default=None\\n    The prompt template to use for generating prompts. If not provided,\\n    a default prompt template will be used.\\n**kwargs : Any\\n    Additional keyword arguments to pass to the `LLMChain`\\n    constructor.\\n\\nReturns\\n-------\\nCriteriaEvalChain\\n    An instance of the `CriteriaEvalChain` class.\\n\\nExamples\\n--------\\n>>> from langchain.llms import OpenAI\\n>>> from langchain.evaluation.criteria import LabeledCriteriaEvalChain\\n>>> llm = OpenAI()\\n>>> criteria = {\\n        \"hallucination\": (\\n            \"Does this submission contain information\"\\n            \" not present in the input or reference?\"\\n        ),\\n    }\\n>>> chain = LabeledCriteriaEvalChain.from_llm(\\n        llm=llm,\\n        criteria=criteria,\\n    )',\n",
       "   'l': True,\n",
       "   'g': ['Train the model.\\n\\n    :param args:\\n    :param kwargs:\\n    :return:',\n",
       "    'Override this method to implement your own training logic.',\n",
       "    '',\n",
       "    'Train the model.']},\n",
       "  {'c': '    def __init__(self, file_path: str, *, headers: Optional[Dict] = None):\\n        try:\\n            from pdfminer.high_level import extract_text_to_fp\\n        except ImportError:\\n            raise ImportError(\\n                \"`pdfminer` package not found, please install it with \"\\n                \"`pip install pdfminer.six`\"\\n            )\\n\\n        super().__init__(file_path, headers=headers)',\n",
       "   'd': 'Initialize with a file path.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Send attachments to telegram.\\n    :param chat_id: Chat ID.\\n    :param notify_type: Notification type.\\n    :param attach: Attachments.\\n    :return: True if all attachments have been sent successfully.',\n",
       "    '.\\n\\n    :param chat_id:\\n    :param notify_type:\\n    :param attach:\\n    :return:',\n",
       "    '.\\n\\n    Send attachment to Telegram.\\n\\n    :param chat_id: chat_id.\\n    :param notify_type: notify type.\\n    :param attach: attachment.\\n    :return: True if success, False otherwise.',\n",
       "    '.\\n    Send media to Telegram.\\n\\n    :param chat_id: Chat ID to send to.\\n    :param notify_type: Type of notification.\\n    :param attach: Attachment to send.\\n    :return: True if successfully sent, False otherwise.']},\n",
       "  {'c': '    def parse(self, text: str) -> List[str]:\\n        pattern = r\"\\\\d+\\\\.\\\\s([^\\\\n]+)\"\\n\\n\\n        matches = re.findall(pattern, text)\\n        return matches',\n",
       "   'd': 'Format the prompt with the inputs.\\n\\nArgs:\\n    kwargs: Any arguments to be passed to the prompt template.\\n\\nReturns:\\n    A formatted string.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    prompt.format(variable1=\"foo\")',\n",
       "   'l': False,\n",
       "   'g': [\"_summary_\\n\\n    Args:\\n        path (str, optional): _description_. Defaults to '.'.\\n        commit (bool, optional): _description_. Defaults to True.\\n\\n    Yields:\\n        _type_: _description_\",\n",
       "    '修改配置文件',\n",
       "    'Modify config file.\\n    \\n    Args:\\n        path (str): Path to config file.\\n        commit (bool): Commit changes to git repository.',\n",
       "    'Modify config file in the given path. \\n    \\n    Args:\\n        path (str): The path to the config file.\\n        commit (bool): Whether to commit the changes.\\n    \\n    Yields:\\n        dict: The modified config dictionary.\\n    \\n    Raises:\\n        IOError: If the config file cannot be read or written.\\n    \\n    Returns:\\n        dict: The modified config dictionary.']},\n",
       "  {'c': 'def load(self) -> List[Document]:\\n    loader = UnstructuredPDFLoader(str(self.file_path))\\n    return loader.load()',\n",
       "   'd': 'Construct ElasticVectorSearch wrapper from raw documents.\\n\\nThis is a user-friendly interface that:\\n    1. Embeds documents.\\n    2. Creates a new index for the embeddings in the Elasticsearch instance.\\n    3. Adds the documents to the newly created Elasticsearch index.\\n\\nThis is intended to be a quick way to get started.\\n\\nExample:\\n    .. code-block:: python\\n\\n        from langchain import ElasticVectorSearch\\n        from langchain.embeddings import OpenAIEmbeddings\\n        embeddings = OpenAIEmbeddings()\\n        elastic_vector_search = ElasticVectorSearch.from_texts(\\n            texts,\\n            embeddings,\\n            elasticsearch_url=\"http://localhost:9200\"\\n        )',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'Registers the public key for the client.',\n",
       "    \"Register the public key for the server's ephemeral key exchange.\",\n",
       "    'Register the public key of the remote party.']},\n",
       "  {'c': '    def __init__(\\n        self, file_path: str, mode: str = \"single\", **unstructured_kwargs: Any\\n    ):\\n        validate_unstructured_version(min_unstructured_version=\"0.6.8\")\\n        super().__init__(file_path=file_path, mode=mode, **unstructured_kwargs)',\n",
       "   'd': 'Handle an error for a chain run.',\n",
       "   'l': False,\n",
       "   'g': ['A function that modifies the output of the state machine.',\n",
       "    'This function is a decorator that adds the \"output\" modifier to the string.\\n    \\n    Args:\\n        string (str): The string to be modified.\\n        state (str): The state of the machine.\\n        \\n    Returns:\\n        str: The modified string.',\n",
       "    'This is a function that takes a string and a state and returns a string.',\n",
       "    'This function is used to output the string to the console.\\n    \\n    Parameters:\\n        string (string): The string to be outputted to the console.\\n        state (bool): The state of the program.\\n    \\n    Returns:\\n        string (string): The string to be outputted to the console.']},\n",
       "  {'c': '    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        prompt = inputs[self.input_key]\\n\\n        _intent = self.sparql_intent_chain.run({\"prompt\": prompt}, callbacks=callbacks)\\n        intent = _intent.strip()\\n\\n        if \"SELECT\" in intent and \"UPDATE\" not in intent:\\n            sparql_generation_chain = self.sparql_generation_select_chain\\n            intent = \"SELECT\"\\n        elif \"UPDATE\" in intent and \"SELECT\" not in intent:\\n            sparql_generation_chain = self.sparql_generation_update_chain\\n            intent = \"UPDATE\"\\n        else:\\n            raise ValueError(\\n                \"I am sorry, but this prompt seems to fit none of the currently \"\\n                \"supported SPARQL query types, i.e., SELECT and UPDATE.\"\\n            )\\n\\n        _run_manager.on_text(\"Identified intent:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(intent, color=\"green\", end=\"\\\\n\", verbose=self.verbose)\\n\\n        generated_sparql = sparql_generation_chain.run(\\n            {\"prompt\": prompt, \"schema\": self.graph.get_schema}, callbacks=callbacks\\n        )\\n\\n        _run_manager.on_text(\"Generated SPARQL:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(\\n            generated_sparql, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n\\n        if intent == \"SELECT\":\\n            context = self.graph.query(generated_sparql)\\n\\n            _run_manager.on_text(\"Full Context:\", end=\"\\\\n\", verbose=self.verbose)\\n            _run_manager.on_text(\\n                str(context), color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n            )\\n            result = self.qa_chain(\\n                {\"prompt\": prompt, \"context\": context},\\n                callbacks=callbacks,\\n            )\\n            res = result[self.qa_chain.output_key]\\n        elif intent == \"UPDATE\":\\n            self.graph.update(generated_sparql)\\n            res = \"Successfully inserted triples into the graph.\"\\n        else:\\n            raise ValueError(\"Unsupported SPARQL query type.\")\\n        return {self.output_key: res}',\n",
       "   'd': 'Generate SPARQL query, use it to retrieve a response from the gdb and answer\\nthe question.',\n",
       "   'l': True,\n",
       "   'g': ['判断项目是否需要更新\\n    :param project_name: 项目名称\\n    :param updatetime: 更新时间\\n    :return: True 需要更新 False 不需要更新',\n",
       "    '判断项目是否需要更新\\n    :param project_name: 项目名称\\n    :param updatetime: 项目更新时间\\n    :return: bool',\n",
       "    '检查项目是否需要更新\\n    \\n    :param project_name: 项目名称\\n    :param updatetime: 项目更新时间\\n    :return: 是否需要更新',\n",
       "    '判断项目是否需要更新\\n    :param project_name: 项目名称\\n    :param updatetime: 更新时间\\n    :return:']},\n",
       "  {'c': 'def search_api(query: str) -> str:\\n    assert isinstance(query, str)\\n    return \"API result\"',\n",
       "   'd': 'Search the API for the query.',\n",
       "   'l': True,\n",
       "   'g': ['Initialize the encoder',\n",
       "    'Initialize the encoder CNN.\\n    \\n    Parameters:\\n    - embed_size (int): Dimensionality of the embedding space.\\n    \\n    Returns:\\n    - None',\n",
       "    'Initialize the encoder CNN model.\\n    \\n    Args:\\n    embed_size (int): The dimension of the embedding vector.\\n    \\n    Returns:\\n    None',\n",
       "    'Args:\\n        embed_size (int): Size of the embedding vector']},\n",
       "  {'c': 'def __init__(self, engine: Engine, cache_schema: Type[FullLLMCache] = FullLLMCache):\\n    self.engine = engine\\n    self.cache_schema = cache_schema\\n    self.cache_schema.metadata.create_all(self.engine)',\n",
       "   'd': 'Initialize by creating all tables.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Returns cost of a given state',\n",
       "    '.\\n    Finds the cost of going from start to goal.',\n",
       "    ', returns the cost of a path from start to goal.',\n",
       "    '.\\n    Calculate cost of a path\\n    :param s_start: start state\\n    :param s_goal: goal state\\n    :return: cost of path']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        config: Mapping[str, Any],\\n        stream_name: str,\\n        record_handler: Optional[RecordHandler] = None,\\n        state: Optional[Any] = None,\\n    ) -> None:\\n        source_class = guard_import(\\n            \"source_stripe\", pip_name=\"airbyte-source-stripe\"\\n        ).SourceStripe\\n        super().__init__(\\n            config=config,\\n            source_class=source_class,\\n            stream_name=stream_name,\\n            record_handler=record_handler,\\n            state=state,\\n        )',\n",
       "   'd': 'Initializes the loader.\\n\\nArgs:\\n    config: The config to pass to the source connector.\\n    stream_name: The name of the stream to load.\\n    record_handler: A function that takes in a record and an optional id and\\n        returns a Document. If None, the record will be used as the document.\\n        Defaults to None.\\n    state: The state to pass to the source connector. Defaults to None.',\n",
       "   'l': True,\n",
       "   'g': ['_summary_\\n\\n    Args:\\n        message (str): _description_',\n",
       "    '_summary_\\n\\n    Args:\\n        message (str): _description_',\n",
       "    'Display a message in markdown format.',\n",
       "    '_summary_\\n\\n    Args:\\n        message (str): _description_']},\n",
       "  {'c': 'def return_stopped_response(\\n    self,\\n    early_stopping_method: str,\\n    intermediate_steps: List[Tuple[AgentAction, str]],\\n    **kwargs: Any,\\n) -> AgentFinish:\\n    if early_stopping_method == \"force\":\\n\\n        return AgentFinish(\\n            {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\\n        )\\n    elif early_stopping_method == \"generate\":\\n\\n        thoughts = \"\"\\n        for action, observation in intermediate_steps:\\n            thoughts += action.log\\n            thoughts += (\\n                f\"\\\\n{self.observation_prefix}{observation}\\\\n{self.llm_prefix}\"\\n            )\\n\\n        thoughts += (\\n            \"\\\\n\\\\nI now need to return a final answer based on the previous steps:\"\\n        )\\n        new_inputs = {\"agent_scratchpad\": thoughts, \"stop\": self._stop}\\n        full_inputs = {**kwargs, **new_inputs}\\n        full_output = self.llm_chain.predict(**full_inputs)\\n\\n        parsed_output = self.output_parser.parse(full_output)\\n        if isinstance(parsed_output, AgentFinish):\\n\\n            return parsed_output\\n        else:\\n\\n\\n            return AgentFinish({\"output\": full_output}, full_output)\\n    else:\\n        raise ValueError(\\n            \"early_stopping_method should be one of `force` or `generate`, \"\\n            f\"got {early_stopping_method}\"\\n        )',\n",
       "   'd': 'Clear cache.',\n",
       "   'l': False,\n",
       "   'g': ['for each piece on the board,',\n",
       "    'for the state of the game',\n",
       "    '.',\n",
       "    'for now return an empty state.']},\n",
       "  {'c': 'def _llm_type(self) -> str:\\n    return \"fireworks-chat\"',\n",
       "   'd': 'Validate that the python package exists in environment.',\n",
       "   'l': False,\n",
       "   'g': ['Build DataParallel or DistributedDataParallel.\\n\\n    Args:\\n        model (nn.Module): The model to be built.\\n        device (str): The device to be used.\\n        dim (int): The dimension of the data.\\n        *args: Additional arguments for DataParallel.\\n        **kwargs: Additional keyword arguments for DataParallel.\\n\\n    Returns:\\n        nn.DataParallel or DistributedDataParallel.',\n",
       "    'Build DataParallel or DistributedDataParallel',\n",
       "    'Build DataParallel or DistributedDataParallel.\\n    \\n    Args:\\n        model (nn.Module): The model to be built.\\n        device (str): The device to be used.\\n        dim (int): The dimension of the data.\\n        *args: Additional arguments.\\n        **kwargs: Additional keyword arguments.\\n    \\n    Returns:\\n        nn.DataParallel or DistributedDataParallel.',\n",
       "    \"Build DataParallel for model. \\n    \\n    Args:\\n        model (nn.Module): Model to be built.\\n        device (str): Device type, 'cuda' or 'mlu'.\\n        dim (int): DataParallel dim.\\n        *args: Additional arguments of DataParallel.\\n        **kwargs: Additional keyword arguments of DataParallel.\\n    \\n    Returns:\\n        nn.DataParallel: DataParallel model.\"]},\n",
       "  {'c': 'def split_text(self, text: str) -> List[str]:\\n    splits = self._tokenizer(text)\\n    return self._merge_splits(splits, self._separator)',\n",
       "   'd': 'Initialize with necessary components.',\n",
       "   'l': False,\n",
       "   'g': ['_unsortedbin_\\n\\n    Returns:\\n        _type_: _description_',\n",
       "    '_summary_\\n\\n    Args:\\n        addr (int, optional): _description_. Defaults to None.\\n        verbose (bool, optional): _description_. Defaults to True.\\n\\n    Returns:\\n        _type_: _description_',\n",
       "    '_summary_\\n\\n    Args:\\n        addr (_type_, optional): _description_. Defaults to None.\\n        verbose (_type_, optional): _description_. Defaults to True.\\n\\n    Returns:\\n        _type_: _description_',\n",
       "    '_summary_\\n    \\n    _description_\\n    \\n    Args:\\n        addr (int, optional): _description_. Defaults to None.\\n        verbose (bool, optional): _description_. Defaults to True.\\n    \\n    Returns:\\n        _type_: _description_']},\n",
       "  {'c': 'def _run(\\n    self,\\n    *args: Any,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n    **kwargs: Any,\\n) -> Any:\\n    new_argument_supported = signature(self.func).parameters.get(\"callbacks\")\\n    return (\\n        self.func(\\n            *args,\\n            callbacks=run_manager.get_child() if run_manager else None,\\n            **kwargs,\\n        )\\n        if new_argument_supported\\n        else self.func(*args, **kwargs)\\n    )',\n",
       "   'd': 'Use the tool.',\n",
       "   'l': True,\n",
       "   'g': ['Forward function.',\n",
       "    'Forward pass.\\n\\n    Parameters\\n    ----------\\n    x : torch.Tensor\\n        Input tensor.\\n    \\n    Returns\\n    -------\\n    torch.Tensor\\n        Output tensor.',\n",
       "    'forward function',\n",
       "    'Forward pass of the module.\\n    \\n    Args:\\n        x (Tensor): Input tensor.\\n    \\n    Returns:\\n        Tensor: Output tensor.']},\n",
       "  {'c': \"def check_if_link_is_working(link: str) -> Tuple[bool, str]:\\n    has_error = False\\n    error_message = ''\\n\\n    try:\\n        resp = requests.get(link + '/', timeout=25, headers={\\n            'User-Agent': fake_user_agent(),\\n            'host': get_host_from_link(link)\\n        })\\n\\n        code = resp.status_code\\n\\n        if code >= 400 and not has_cloudflare_protection(resp):\\n            has_error = True\\n            error_message = f'ERR:CLT: {code} : {link}'\\n\\n    except requests.exceptions.SSLError as error:\\n        has_error = True\\n        error_message = f'ERR:SSL: {error} : {link}'\\n\\n    except requests.exceptions.ConnectionError as error:\\n        has_error = True\\n        error_message = f'ERR:CNT: {error} : {link}'\\n\\n    except (TimeoutError, requests.exceptions.ConnectTimeout):\\n        has_error = True\\n        error_message = f'ERR:TMO: {link}'\\n\\n    except requests.exceptions.TooManyRedirects as error:\\n        has_error = True\\n        error_message = f'ERR:TMR: {error} : {link}'\\n\\n    except (Exception, requests.exceptions.RequestException) as error:\\n        has_error = True\\n        error_message = f'ERR:UKN: {error} : {link}'\\n\\n    return (has_error, error_message)\",\n",
       "   'd': 'Handle an error for a tool run.',\n",
       "   'l': False,\n",
       "   'g': ['.', '.', 'for study_id', 'for study_id']},\n",
       "  {'c': '    def embed_query(self, text: str) -> List[float]:\\n        return self._embed(text)',\n",
       "   'd': 'Return the output parser type for serialization.',\n",
       "   'l': False,\n",
       "   'g': ['Delete item from the dictionary. \\n    \\n    Args:\\n        key (str): The key to delete.',\n",
       "    'Delete the element with the specified key.',\n",
       "    'Deletes an item from the dictionary. \\n    \\n    Args:\\n        key (str): The key of the item to delete.',\n",
       "    'Delete the item with the given key.']},\n",
       "  {'c': '    def similarity_search(\\n        self, query: str, k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        docs_with_scores = self.similarity_search_with_score(query, k, **kwargs)\\n        return [doc[0] for doc in docs_with_scores]',\n",
       "   'd': 'Make tools out of functions, can be used with or without arguments.\\n\\nArgs:\\n    *args: The arguments to the tool.\\n    return_direct: Whether to return directly from the tool rather\\n        than continuing the agent loop.\\n    args_schema: optional argument schema for user to specify\\n    infer_schema: Whether to infer the schema of the arguments from\\n        the function\\'s signature. This also makes the resultant tool\\n        accept a dictionary input to its `run()` function.\\n\\nRequires:\\n    - Function must be of type (str) -> str\\n    - Function must have a docstring\\n\\nExamples:\\n    .. code-block:: python\\n\\n        @tool\\n        def search_api(query: str) -> str:\\n            # Searches the API for the query.\\n            return\\n\\n        @tool(\"search\", return_direct=True)\\n        def search_api(query: str) -> str:\\n            # Searches the API for the query.\\n            return',\n",
       "   'l': False,\n",
       "   'g': ['Returns the value at the given offset, in the given format.',\n",
       "    'Gets the value of the given offset.\\n    \\n    @param fmt: The format of the value.\\n    @param offset: The offset to get the value from.\\n    \\n    @return: The value of the given offset.',\n",
       "    '',\n",
       "    'Get the value of the offset from the fmt.']},\n",
       "  {'c': 'def clear(self, **kwargs: Any) -> None:\\n    self._cache = {}',\n",
       "   'd': 'Lazily parse the blob.',\n",
       "   'l': False,\n",
       "   'g': ['Returns:\\n        int: The maximum length of a sequence.',\n",
       "    'Returns the maximum length of sequences used in the dataset.',\n",
       "    'Returns the maximum sequence length in the dataset.',\n",
       "    'Returns:\\n        int: The maximum sequence length.']},\n",
       "  {'c': '    def get_token_ids(self, text: str) -> List[int]:\\n        if sys.version_info[1] < 8:\\n            return super().get_token_ids(text)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate get_num_tokens. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        enc = tiktoken.encoding_for_model(self.model_name)\\n        return enc.encode(\\n            text,\\n            allowed_special=self.allowed_special,\\n            disallowed_special=self.disallowed_special,\\n        )',\n",
       "   'd': 'Get the token IDs using the tiktoken package.',\n",
       "   'l': True,\n",
       "   'g': ['(self, buffers)\\n\\n    Initializes the buffers for the class.\\n\\n    :param buffers: A list of buffers.',\n",
       "    '(class)',\n",
       "    '(self, buffers)\\n\\n    Initialize a new buffer object.\\n\\n    :param buffers: a list of buffers',\n",
       "    '(self, buffers)\\n\\n    Initialize the object.']},\n",
       "  {'c': 'def __init__(\\n    self,\\n    opensearch_url: str,\\n    index_name: str,\\n    embedding_function: Embeddings,\\n    **kwargs: Any,\\n):\\n    self.embedding_function = embedding_function\\n    self.index_name = index_name\\n    http_auth = _get_kwargs_value(kwargs, \"http_auth\", None)\\n    self.is_aoss = _is_aoss_enabled(http_auth=http_auth)\\n    self.client = _get_opensearch_client(opensearch_url, **kwargs)',\n",
       "   'd': 'Initialize with necessary components.',\n",
       "   'l': True,\n",
       "   'g': [':param year:\\n    :param quarter:\\n    :return:',\n",
       "    'get growth data',\n",
       "    '_summary_\\n    \\n    _description_\\n    \\n    Args:\\n        year (int): _description_\\n        quarter (int): _description_\\n    \\n    Returns:\\n        _type_: _description_',\n",
       "    'Returns the growth data for a given year and quarter.\\n    \\n    Parameters\\n    ----------\\n    year : int\\n        The year for which the growth data is requested.\\n    quarter : int\\n        The quarter for which the growth data is requested.\\n        \\n    Returns\\n    -------\\n    pandas.DataFrame\\n        The growth data for the given year and quarter.']},\n",
       "  {'c': 'def lazy_import_playwright_browsers() -> Tuple[Type[AsyncBrowser], Type[SyncBrowser]]:\\n    try:\\n        from playwright.async_api import Browser as AsyncBrowser\\n        from playwright.sync_api import Browser as SyncBrowser\\n    except ImportError:\\n        raise ImportError(\\n            \"The \\'playwright\\' package is required to use the playwright tools.\"\\n            \" Please install it with \\'pip install playwright\\'.\"\\n        )\\n    return AsyncBrowser, SyncBrowser',\n",
       "   'd': 'Initialize with a file path.',\n",
       "   'l': False,\n",
       "   'g': ['this function is used to check if a number is even or odd\\n    \\n    Args:\\n        number (int): number to check\\n    \\n    Returns:\\n        bool: True if number is even, False if number is odd',\n",
       "    'This function is used to check if the number is even or not.',\n",
       "    'this function check if number is even or odd\\n    \\n    :param number: int\\n    :return: bool',\n",
       "    'This function checks if a number is even or not.']},\n",
       "  {'c': 'def _run(\\n    self,\\n    query: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> str:\\n    return self.llm_chain.predict(\\n        query=query,\\n        dialect=self.db.dialect,\\n        callbacks=run_manager.get_child() if run_manager else None,\\n    )',\n",
       "   'd': 'Use the LLM to check the query.',\n",
       "   'l': True,\n",
       "   'g': ['Test that static proxy is working',\n",
       "    'Test static proxy',\n",
       "    '测试静态代理',\n",
       "    'Test static proxy with static proxy']},\n",
       "  {'c': 'def tool(\\n    *args: Union[str, Callable],\\n    return_direct: bool = False,\\n    args_schema: Optional[Type[BaseModel]] = None,\\n    infer_schema: bool = True,\\n) -> Callable:\\n    def _make_with_name(tool_name: str) -> Callable:\\n        def _make_tool(dec_func: Callable) -> BaseTool:\\n            if inspect.iscoroutinefunction(dec_func):\\n                coroutine = dec_func\\n                func = None\\n            else:\\n                coroutine = None\\n                func = dec_func\\n\\n            if infer_schema or args_schema is not None:\\n                return StructuredTool.from_function(\\n                    func,\\n                    coroutine,\\n                    name=tool_name,\\n                    return_direct=return_direct,\\n                    args_schema=args_schema,\\n                    infer_schema=infer_schema,\\n                )\\n\\n\\n            if func.__doc__ is None:\\n                raise ValueError(\\n                    \"Function must have a docstring if \"\\n                    \"description not provided and infer_schema is False.\"\\n                )\\n            return Tool(\\n                name=tool_name,\\n                func=func,\\n                description=f\"{tool_name} tool\",\\n                return_direct=return_direct,\\n                coroutine=coroutine,\\n            )\\n\\n        return _make_tool\\n\\n    if len(args) == 1 and isinstance(args[0], str):\\n\\n\\n        return _make_with_name(args[0])\\n    elif len(args) == 1 and callable(args[0]):\\n\\n\\n        return _make_with_name(args[0].__name__)(args[0])\\n    elif len(args) == 0:\\n\\n\\n        def _partial(func: Callable[[str], str]) -> BaseTool:\\n            return _make_with_name(func.__name__)(func)\\n\\n        return _partial\\n    else:\\n        raise ValueError(\"Too many arguments for tool decorator\")',\n",
       "   'd': 'Test instantiating APIRequestBody from RequestBody with a schema.',\n",
       "   'l': False,\n",
       "   'g': ['Set the model name to use in the environment variables',\n",
       "    'Set the model name in the environment\\n    \\n    Args:\\n        model (str): The name of the model to use\\n    \\n    Raises:\\n        ValidationError: If the model name is not set',\n",
       "    'This function sets the model for the environment.\\n    \\n    Args:\\n        model (str): The name of the model.\\n        \\n    Raises:\\n        ValidationError: If the model is not set correctly.',\n",
       "    'Set the model name to the environment variable.\\n\\n    Args:\\n        model (str): The name of the model.']},\n",
       "  {'c': '    def configure(\\n        cls,\\n        inheritable_callbacks: Callbacks = None,\\n        local_callbacks: Callbacks = None,\\n        verbose: bool = False,\\n        inheritable_tags: Optional[List[str]] = None,\\n        local_tags: Optional[List[str]] = None,\\n        inheritable_metadata: Optional[Dict[str, Any]] = None,\\n        local_metadata: Optional[Dict[str, Any]] = None,\\n    ) -> AsyncCallbackManager:\\n        return _configure(\\n            cls,\\n            inheritable_callbacks,\\n            local_callbacks,\\n            verbose,\\n            inheritable_tags,\\n            local_tags,\\n            inheritable_metadata,\\n            local_metadata,\\n        )',\n",
       "   'd': 'Configure the async callback manager.\\n\\nArgs:\\n    inheritable_callbacks (Optional[Callbacks], optional): The inheritable\\n        callbacks. Defaults to None.\\n    local_callbacks (Optional[Callbacks], optional): The local callbacks.\\n        Defaults to None.\\n    verbose (bool, optional): Whether to enable verbose mode. Defaults to False.\\n    inheritable_tags (Optional[List[str]], optional): The inheritable tags.\\n        Defaults to None.\\n    local_tags (Optional[List[str]], optional): The local tags.\\n        Defaults to None.\\n    inheritable_metadata (Optional[Dict[str, Any]], optional): The inheritable\\n        metadata. Defaults to None.\\n    local_metadata (Optional[Dict[str, Any]], optional): The local metadata.\\n        Defaults to None.\\n\\nReturns:\\n    AsyncCallbackManager: The configured async callback manager.',\n",
       "   'l': True,\n",
       "   'g': ['Returns a list of all the trackers for the current frame.',\n",
       "    '.',\n",
       "    '@return: a list of trackers',\n",
       "    ',\\n    Returns a list of all the trackers.']},\n",
       "  {'c': '    def input_variables(self) -> List[str]:\\n        return self.prompt.input_variables',\n",
       "   'd': 'Input variables for this prompt template.\\n\\nReturns:\\n    List of input variable names.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Test that cleanup removes expired jobs.',\n",
       "    '.cleanup() should remove expired jobs from the registry.',\n",
       "    '.test_cleanup(self):',\n",
       "    '.test_cleanup()']},\n",
       "  {'c': '    def generate(\\n        self,\\n        prompts: List[str],\\n        stop: Optional[List[str]] = None,\\n        callbacks: Optional[Union[Callbacks, List[Callbacks]]] = None,\\n        *,\\n        tags: Optional[Union[List[str], List[List[str]]]] = None,\\n        metadata: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\\n        **kwargs: Any,\\n    ) -> LLMResult:\\n        if not isinstance(prompts, list):\\n            raise ValueError(\\n                \"Argument \\'prompts\\' is expected to be of type List[str], received\"\\n                f\" argument of type {type(prompts)}.\"\\n            )\\n\\n        if (\\n            isinstance(callbacks, list)\\n            and callbacks\\n            and (\\n                isinstance(callbacks[0], (list, BaseCallbackManager))\\n                or callbacks[0] is None\\n            )\\n        ):\\n\\n            assert len(callbacks) == len(prompts)\\n            assert tags is None or (\\n                isinstance(tags, list) and len(tags) == len(prompts)\\n            )\\n            assert metadata is None or (\\n                isinstance(metadata, list) and len(metadata) == len(prompts)\\n            )\\n            callbacks = cast(List[Callbacks], callbacks)\\n            tags_list = cast(List[Optional[List[str]]], tags or ([None] * len(prompts)))\\n            metadata_list = cast(\\n                List[Optional[Dict[str, Any]]], metadata or ([{}] * len(prompts))\\n            )\\n            callback_managers = [\\n                CallbackManager.configure(\\n                    callback,\\n                    self.callbacks,\\n                    self.verbose,\\n                    tag,\\n                    self.tags,\\n                    meta,\\n                    self.metadata,\\n                )\\n                for callback, tag, meta in zip(callbacks, tags_list, metadata_list)\\n            ]\\n        else:\\n\\n            callback_managers = [\\n                CallbackManager.configure(\\n                    cast(Callbacks, callbacks),\\n                    self.callbacks,\\n                    self.verbose,\\n                    cast(List[str], tags),\\n                    self.tags,\\n                    cast(Dict[str, Any], metadata),\\n                    self.metadata,\\n                )\\n            ] * len(prompts)\\n\\n        params = self.dict()\\n        params[\"stop\"] = stop\\n        options = {\"stop\": stop}\\n        (\\n            existing_prompts,\\n            llm_string,\\n            missing_prompt_idxs,\\n            missing_prompts,\\n        ) = get_prompts(params, prompts)\\n        disregard_cache = self.cache is not None and not self.cache\\n        new_arg_supported = inspect.signature(self._generate).parameters.get(\\n            \"run_manager\"\\n        )\\n        if langchain.llm_cache is None or disregard_cache:\\n            if self.cache is not None and self.cache:\\n                raise ValueError(\\n                    \"Asked to cache, but no cache found at `langchain.cache`.\"\\n                )\\n            run_managers = [\\n                callback_manager.on_llm_start(\\n                    dumpd(self), [prompt], invocation_params=params, options=options\\n                )[0]\\n                for callback_manager, prompt in zip(callback_managers, prompts)\\n            ]\\n            output = self._generate_helper(\\n                prompts, stop, run_managers, bool(new_arg_supported), **kwargs\\n            )\\n            return output\\n        if len(missing_prompts) > 0:\\n            run_managers = [\\n                callback_managers[idx].on_llm_start(\\n                    dumpd(self),\\n                    [prompts[idx]],\\n                    invocation_params=params,\\n                    options=options,\\n                )[0]\\n                for idx in missing_prompt_idxs\\n            ]\\n            new_results = self._generate_helper(\\n                missing_prompts, stop, run_managers, bool(new_arg_supported), **kwargs\\n            )\\n            llm_output = update_cache(\\n                existing_prompts, llm_string, missing_prompt_idxs, new_results, prompts\\n            )\\n            run_info = (\\n                [RunInfo(run_id=run_manager.run_id) for run_manager in run_managers]\\n                if run_managers\\n                else None\\n            )\\n        else:\\n            llm_output = {}\\n            run_info = None\\n        generations = [existing_prompts[i] for i in range(len(prompts))]\\n        return LLMResult(generations=generations, llm_output=llm_output, run=run_info)',\n",
       "   'd': 'Clear the cache.\\n\\nRaises:\\n    SdkException: Momento service or network error',\n",
       "   'l': False,\n",
       "   'g': ['(tf.Tensor) -> tf.Tensor\\n      Returns the logits for each token in the body.',\n",
       "    ':param body_output:\\n    :param _:\\n    :return:',\n",
       "    'for each time step, calculate the top-k predictions',\n",
       "    'Args:\\n      body_output: The output of the body.\\n      _ : unused\\n    Returns:\\n      A tensor representing the logits of the top words.']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        cohere_api_key = get_from_dict_or_env(\\n            values, \"cohere_api_key\", \"COHERE_API_KEY\"\\n        )\\n        try:\\n            import cohere\\n\\n            values[\"client\"] = cohere.Client(cohere_api_key)\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import cohere python package. \"\\n                \"Please install it with `pip install cohere`.\"\\n            )\\n        return values',\n",
       "   'd': 'Initialize with Marqo client.',\n",
       "   'l': False,\n",
       "   'g': ['Args:\\n        bboxes: tensor, shape = (num_bboxes, 4)\\n        img_shape: tuple, (height, width)\\n        scale_factor: tuple, (scale_x, scale_y)\\n        flip: bool, whether to flip the image',\n",
       "    'bboxes: (N, 4)\\n    img_shape: (H, W)\\n    scale_factor: (H, W)\\n    flip: bool',\n",
       "    'Args:\\n        bboxes (Tensor): [x1, y1, x2, y2]\\n        img_shape (Tuple): (height, width)\\n        scale_factor (float): scale factor\\n        flip (bool): whether flip the bbox\\n    \\n    Returns:\\n        Tensor: [x1, y1, x2, y2]',\n",
       "    'bboxes: tensor of shape [N, 4]\\n    img_shape: tuple of (height, width)\\n    scale_factor: tuple of (scale_x, scale_y)\\n    flip: bool\\n    \\n    return: tensor of shape [N, 4]']},\n",
       "  {'c': 'def load_llm_from_config(config: dict) -> BaseLLM:\\n    if \"_type\" not in config:\\n        raise ValueError(\"Must specify an LLM Type in config\")\\n    config_type = config.pop(\"_type\")\\n\\n    if config_type not in type_to_cls_dict:\\n        raise ValueError(f\"Loading {config_type} LLM not supported\")\\n\\n    llm_cls = type_to_cls_dict[config_type]\\n    return llm_cls(**config)',\n",
       "   'd': 'Lazy load the chat sessions from the iMessage chat.db\\nand yield them in the required format.\\n\\nYields:\\n    ChatSession: Loaded chat session.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    Called when the app goes to the background.',\n",
       "    '.\\n    Override the default pause method',\n",
       "    '.\\n\\n    Return True to pause the game.',\n",
       "    '.\\n\\n    Return True to pause the game.']},\n",
       "  {'c': 'def _run(\\n    self,\\n    *args: Any,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n    **kwargs: Any,\\n) -> Any:\\n    new_argument_supported = signature(self.func).parameters.get(\"callbacks\")\\n    return (\\n        self.func(\\n            *args,\\n            callbacks=run_manager.get_child() if run_manager else None,\\n            **kwargs,\\n        )\\n        if new_argument_supported\\n        else self.func(*args, **kwargs)\\n    )',\n",
       "   'd': 'Use the tool.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Convert the cell to text.',\n",
       "    'for markdown cells',\n",
       "    '.\\n    Returns the text of the cell.',\n",
       "    '.\\n    Returns the text of the cell.']},\n",
       "  {'c': '    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[Dict[Any, Any]]] = None,\\n        **kwargs: Any,\\n    ) -> ElasticKnnSearch:\\n        index_name = kwargs.get(\"index_name\", str(uuid.uuid4()))\\n        es_connection = kwargs.get(\"es_connection\")\\n        es_cloud_id = kwargs.get(\"es_cloud_id\")\\n        es_user = kwargs.get(\"es_user\")\\n        es_password = kwargs.get(\"es_password\")\\n        vector_query_field = kwargs.get(\"vector_query_field\", \"vector\")\\n        query_field = kwargs.get(\"query_field\", \"text\")\\n        model_id = kwargs.get(\"model_id\")\\n        dims = kwargs.get(\"dims\")\\n\\n        if dims is None:\\n            raise ValueError(\"ElasticKnnSearch requires \\'dims\\' parameter\")\\n\\n        optional_args = {}\\n\\n        if vector_query_field is not None:\\n            optional_args[\"vector_query_field\"] = vector_query_field\\n\\n        if query_field is not None:\\n            optional_args[\"query_field\"] = query_field\\n\\n        knnvectorsearch = cls(\\n            index_name=index_name,\\n            embedding=embedding,\\n            es_connection=es_connection,\\n            es_cloud_id=es_cloud_id,\\n            es_user=es_user,\\n            es_password=es_password,\\n            **optional_args,\\n        )\\n\\n        knnvectorsearch.add_texts(texts, model_id=model_id, dims=dims, **optional_args)\\n\\n        return knnvectorsearch',\n",
       "   'd': 'Get a child callback manager.\\n\\nArgs:\\n    tag (str, optional): The tag for the child callback manager.\\n        Defaults to None.\\n\\nReturns:\\n    AsyncCallbackManager: The child callback manager.',\n",
       "   'l': False,\n",
       "   'g': ['(float)\\n    Computes the reward for the given output sequence.',\n",
       "    '(float)\\n    Computes the reward for a given output sequence.\\n\\n    Args:\\n        output_sequence (torch.Tensor): The output sequence to compute the reward for.\\n        output_sequence_mask (torch.Tensor): The mask to apply to the output sequence.\\n\\n    Returns:\\n        torch.Tensor: The reward for the output sequence.',\n",
       "    '(float)\\n    Computes the reward for the current sequence.\\n\\n    Args:\\n        output_sequence (torch.Tensor): The output sequence of the model.\\n        output_sequence_mask (torch.Tensor): The mask for the output sequence.\\n\\n    Returns:\\n        torch.Tensor: The reward for the current sequence.',\n",
       "    '(batch_size, 1)']},\n",
       "  {'c': 'def from_llm(\\n    cls,\\n    llm: BaseLanguageModel,\\n    db: SQLDatabase,\\n    query_prompt: BasePromptTemplate = PROMPT,\\n    decider_prompt: BasePromptTemplate = DECIDER_PROMPT,\\n    **kwargs: Any,\\n) -> SQLDatabaseSequentialChain:\\n    sql_chain = SQLDatabaseChain.from_llm(llm, db, prompt=query_prompt, **kwargs)\\n    decider_chain = LLMChain(\\n        llm=llm, prompt=decider_prompt, output_key=\"table_names\"\\n    )\\n    return cls(sql_chain=sql_chain, decider_chain=decider_chain, **kwargs)',\n",
       "   'd': 'Load the necessary chains.',\n",
       "   'l': True,\n",
       "   'g': ['(str) -> str\\n\\n    Generates a new text based on the text provided.\\n\\n    Parameters\\n    ----------\\n    text : str\\n        The text to generate from.\\n\\n    Returns\\n    -------\\n    str\\n        The generated text.',\n",
       "    '(self, text):',\n",
       "    '(str) -> str\\n\\n    Generate a new password.',\n",
       "    '(str) -> str\\n    Returns the text generated by the model.']},\n",
       "  {'c': 'def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n    for gen in return_val:\\n        if not isinstance(gen, Generation):\\n            raise ValueError(\\n                \"RedisSemanticCache only supports caching of \"\\n                f\"normal LLM generations, got {type(gen)}\"\\n            )\\n        if isinstance(gen, ChatGeneration):\\n            warnings.warn(\\n                \"NOTE: Generation has not been cached. RedisSentimentCache does not\"\\n                \" support caching ChatModel outputs.\"\\n            )\\n            return\\n    llm_cache = self._get_llm_cache(llm_string)\\n    _dump_generations_to_json([g for g in return_val])\\n    metadata = {\\n        \"llm_string\": llm_string,\\n        \"prompt\": prompt,\\n        \"return_val\": _dump_generations_to_json([g for g in return_val]),\\n    }\\n    llm_cache.add_texts(texts=[prompt], metadatas=[metadata])',\n",
       "   'd': 'Query NebulaGraph database.',\n",
       "   'l': False,\n",
       "   'g': [\"Ask a question.\\n\\n    :param args:\\n        Arguments to pass to Prompt.ask.\\n    :param prompt_type:\\n        Type of prompt to use.\\n    :param kwargs:\\n        Keyword arguments to pass to Prompt.ask.\\n    :return:\\n        The user's response.\",\n",
       "    '_summary_\\n\\n    Args:\\n        *args (str): _description_\\n        prompt_type (type[str] | type[int] | None, optional): _description_. Defaults to None.\\n        **kwargs: _description_\\n\\n    Returns:\\n        str: _description_',\n",
       "    '_summary_\\n\\n    Args:\\n        *args (str): _description_\\n        prompt_type (type[str] | type[int] | None, optional): _description_. Defaults to None.\\n        **kwargs: _description_\\n\\n    Returns:\\n        str: _description_',\n",
       "    \"Prompt the user for input.\\n\\n    Args:\\n        *args: The arguments to pass to Prompt.ask.\\n        prompt_type: The type of prompt to use.\\n        **kwargs: The keyword arguments to pass to Prompt.ask.\\n\\n    Returns:\\n        The user's input.\"]},\n",
       "  {'c': 'def _handle_event(\\n    handlers: List[BaseCallbackHandler],\\n    event_name: str,\\n    ignore_condition_name: Optional[str],\\n    *args: Any,\\n    **kwargs: Any,\\n) -> None:\\n    message_strings: Optional[List[str]] = None\\n    for handler in handlers:\\n        try:\\n            if ignore_condition_name is None or not getattr(\\n                handler, ignore_condition_name\\n            ):\\n                getattr(handler, event_name)(*args, **kwargs)\\n        except NotImplementedError as e:\\n            if event_name == \"on_chat_model_start\":\\n                if message_strings is None:\\n                    message_strings = [get_buffer_string(m) for m in args[1]]\\n                _handle_event(\\n                    [handler],\\n                    \"on_llm_start\",\\n                    \"ignore_llm\",\\n                    args[0],\\n                    message_strings,\\n                    *args[2:],\\n                    **kwargs,\\n                )\\n            else:\\n                logger.warning(\\n                    f\"NotImplementedError in {handler.__class__.__name__}.{event_name}\"\\n                    f\" callback: {e}\"\\n                )\\n        except Exception as e:\\n            logger.warning(\\n                f\"Error in {handler.__class__.__name__}.{event_name} callback: {e}\"\\n            )\\n            if handler.raise_error:\\n                raise e',\n",
       "   'd': 'Generic event handler for CallbackManager.',\n",
       "   'l': True,\n",
       "   'g': ['.',\n",
       "    '(name, value, minval=None, maxval=None, check_fn=None)\\n\\n    Initializes a new instance of the ``Config`` class.\\n\\n    Args:\\n\\n        name (str): The name of the config variable.\\n\\n        value (object): The value of the config variable.\\n\\n        minval (object, optional): The minimum value of the config variable.\\n\\n        maxval (object, optional): The maximum value of the config variable.\\n\\n        check_fn (callable, optional): A callable function that is called to validate the config variable.\\n\\n    Returns:\\n\\n        Config: A new instance of the ``Config`` class.\\n\\n    Example:\\n\\n        config = Config(\"example_config\", 10, minval=0, maxval=100, check_fn=lambda x: x >= 0 and x <= 100)',\n",
       "    '.',\n",
       "    '.']},\n",
       "  {'c': 'def test_query_chain(self) -> None:\\n    llm = OpenAI(temperature=0, max_tokens=512)\\n    query_chain = QueryChain.from_univariate_prompt(llm)\\n    narrative_question = \"How many pets will Marcia end up with? \"\\n    data = query_chain(narrative_question)[Constant.chain_data.value]\\n    self.assertEqual(type(data), QueryModel)',\n",
       "   'd': 'Test end to end construction and search.',\n",
       "   'l': False,\n",
       "   'g': ['Match the given record against the component matchers.\\n    \\n    @param rec: The record to match against.\\n    @type rec: Record\\n    \\n    @return: True if the record matches the component matchers, False otherwise.\\n    @rtype: bool',\n",
       "    'Match a record against the current matcher.',\n",
       "    'Returns True if the given record matches the components of this\\n    matcher.',\n",
       "    'Checks if a record matches a pattern.']},\n",
       "  {'c': '    def plan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[List[AgentAction], AgentFinish]:\\n',\n",
       "   'd': 'Given input, decided what to do.\\n\\nArgs:\\n    intermediate_steps: Steps the LLM has taken to date,\\n        along with the observations.\\n    callbacks: Callbacks to run.\\n    **kwargs: User inputs.\\n\\nReturns:\\n    Actions specifying what tool to use.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    \\n    This function will use the OpenAI API to generate a review draft based on the draft text.\\n    \\n    Args:\\n        draft (str): The draft text to generate a review draft from.\\n    \\n    Returns:\\n        str: The generated review draft.\\n    \\n    Example:\\n        >>> draft = \"This is a sample draft text.\"\\n        >>> review = review_draft(draft)\\n        >>> print(review)\\n        \"This is a sample draft text.\"',\n",
       "    'Review the draft and return a list of review suggestions.\\n\\n    Args:\\n        draft (str): The draft to review.\\n\\n    Returns:\\n        list: A list of review suggestions.',\n",
       "    '.',\n",
       "    'Review a draft of a proposal.']},\n",
       "  {'c': '    def similarity_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        pre_filter: Optional[dict] = None,\\n        post_filter_pipeline: Optional[List[Dict]] = None,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        docs_and_scores = self.similarity_search_with_score(\\n            query,\\n            k=k,\\n            pre_filter=pre_filter,\\n            post_filter_pipeline=post_filter_pipeline,\\n        )\\n        return [doc for doc, _ in docs_and_scores]',\n",
       "   'd': 'Return MongoDB documents most similar to query.\\n\\nUse the knnBeta Operator available in MongoDB Atlas Search\\nThis feature is in early access and available only for evaluation purposes, to\\nvalidate functionality, and to gather feedback from a small closed group of\\nearly access users. It is not recommended for production deployments as we may\\nintroduce breaking changes.\\nFor more: https://www.mongodb.com/docs/atlas/atlas-search/knn-beta\\n\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Optional Number of Documents to return. Defaults to 4.\\n    pre_filter: Optional Dictionary of argument(s) to prefilter on document\\n        fields.\\n    post_filter_pipeline: Optional Pipeline of MongoDB aggregation stages\\n        following the knnBeta search.\\n\\nReturns:\\n    List of Documents most similar to the query and score for each',\n",
       "   'l': True,\n",
       "   'g': ['Return a dictionary with the hyperparameters of the model.',\n",
       "    'Returns hyperparameters of the model.',\n",
       "    'Returns the hyperparameters for the model.',\n",
       "    'Returns the hyperparameters of the model.']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        doc_metadata: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        doc_hash = md5()\\n        for t in texts:\\n            doc_hash.update(t.encode())\\n        doc_id = doc_hash.hexdigest()\\n        if metadatas is None:\\n            metadatas = [{} for _ in texts]\\n        if doc_metadata:\\n            doc_metadata[\"source\"] = \"langchain\"\\n        else:\\n            doc_metadata = {\"source\": \"langchain\"}\\n        doc = {\\n            \"document_id\": doc_id,\\n            \"metadataJson\": json.dumps(doc_metadata),\\n            \"section\": [\\n                {\"text\": text, \"metadataJson\": json.dumps(md)}\\n                for text, md in zip(texts, metadatas)\\n            ],\\n        }\\n\\n        success_str = self._index_doc(doc)\\n        if success_str == \"E_ALREADY_EXISTS\":\\n            self._delete_doc(doc_id)\\n            self._index_doc(doc)\\n        elif success_str == \"E_NO_PERMISSIONS\":\\n            print(\\n                \"\"\"No permissions to add document to Vectara.\\n                Check your corpus ID, customer ID and API key\"\"\"\\n            )\\n        return [doc_id]',\n",
       "   'd': 'Run the LLM on the given prompt and input.',\n",
       "   'l': False,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': 'def return_stopped_response(\\n    self,\\n    early_stopping_method: str,\\n    intermediate_steps: List[Tuple[AgentAction, str]],\\n    **kwargs: Any,\\n) -> AgentFinish:\\n    if early_stopping_method == \"force\":\\n\\n        return AgentFinish(\\n            {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\\n        )\\n    elif early_stopping_method == \"generate\":\\n\\n        thoughts = \"\"\\n        for action, observation in intermediate_steps:\\n            thoughts += action.log\\n            thoughts += (\\n                f\"\\\\n{self.observation_prefix}{observation}\\\\n{self.llm_prefix}\"\\n            )\\n\\n        thoughts += (\\n            \"\\\\n\\\\nI now need to return a final answer based on the previous steps:\"\\n        )\\n        new_inputs = {\"agent_scratchpad\": thoughts, \"stop\": self._stop}\\n        full_inputs = {**kwargs, **new_inputs}\\n        full_output = self.llm_chain.predict(**full_inputs)\\n\\n        parsed_output = self.output_parser.parse(full_output)\\n        if isinstance(parsed_output, AgentFinish):\\n\\n            return parsed_output\\n        else:\\n\\n\\n            return AgentFinish({\"output\": full_output}, full_output)\\n    else:\\n        raise ValueError(\\n            \"early_stopping_method should be one of `force` or `generate`, \"\\n            f\"got {early_stopping_method}\"\\n        )',\n",
       "   'd': 'Validate that either folder_id or document_ids is set, but not both.',\n",
       "   'l': False,\n",
       "   'g': ['Update the latest price for all Boms in the database.\\n        \\n        This function updates the latest price for all Boms in the database.\\n        \\n        It checks if the \"update_bom_costs_automatically\" setting is enabled in the \"Manufacturing Settings\"\\n        module. If the setting is enabled, the function will update the latest price for all Boms in the\\n        database.',\n",
       "    'Update latest price in all BOMs.',\n",
       "    'This function will update the latest price of all the BOMs',\n",
       "    'This function will update the latest price in all BOMs']},\n",
       "  {'c': '    def __init__(self, **kwargs: Any):\\n        super().__init__(**kwargs)\\n        try:\\n            import sentence_transformers\\n\\n        except ImportError as exc:\\n            raise ImportError(\\n                \"Could not import sentence_transformers python package. \"\\n                \"Please install it with `pip install sentence_transformers`.\"\\n            ) from exc\\n\\n        self.client = sentence_transformers.SentenceTransformer(\\n            self.model_name, cache_folder=self.cache_folder, **self.model_kwargs\\n        )',\n",
       "   'd': 'Initialize the sentence_transformer.',\n",
       "   'l': True,\n",
       "   'g': ['Set all gradients to zero.',\n",
       "    'Sets the gradients of all tensors to zero.\\n    \\n    Parameters\\n    ----------\\n    set_to_none : bool\\n        If True, sets the gradients of all tensors to None.\\n        If False, sets the gradients of all tensors to zero.',\n",
       "    'Zeroes the gradients for all parameters in this module.',\n",
       "    'Zeroes out the gradients of all the parameters in this module.']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> BaseMessage:\\n        text = self.prompt.format(**kwargs)\\n        return ChatMessage(\\n            content=text, role=self.role, additional_kwargs=self.additional_kwargs\\n        )',\n",
       "   'd': 'Whether this evaluator requires an input string.',\n",
       "   'l': False,\n",
       "   'g': ['_uname_info function',\n",
       "    '.',\n",
       "    '.\\n    Returns:\\n        str: username',\n",
       "    '_uname_info_docstring_']},\n",
       "  {'c': 'def on_retriever_start(\\n    self,\\n    serialized: Dict[str, Any],\\n    query: str,\\n    *,\\n    run_id: UUID,\\n    parent_run_id: Optional[UUID] = None,\\n    tags: Optional[List[str]] = None,\\n    metadata: Optional[Dict[str, Any]] = None,\\n    **kwargs: Any,\\n) -> None:\\n    parent_run_id_ = str(parent_run_id) if parent_run_id else None\\n    execution_order = self._get_execution_order(parent_run_id_)\\n    start_time = datetime.utcnow()\\n    if metadata:\\n        kwargs.update({\"metadata\": metadata})\\n    retrieval_run = Run(\\n        id=run_id,\\n        name=\"Retriever\",\\n        parent_run_id=parent_run_id,\\n        serialized=serialized,\\n        inputs={\"query\": query},\\n        extra=kwargs,\\n        events=[{\"name\": \"start\", \"time\": start_time}],\\n        start_time=start_time,\\n        execution_order=execution_order,\\n        child_execution_order=execution_order,\\n        tags=tags,\\n        child_runs=[],\\n        run_type=\"retriever\",\\n    )\\n    self._start_trace(retrieval_run)\\n    self._on_retriever_start(retrieval_run)',\n",
       "   'd': 'Run when chain ends running.\\n\\nArgs:\\n    outputs (Union[Dict[str, Any], Any]): The outputs of the chain.',\n",
       "   'l': False,\n",
       "   'g': ['Update the forward references of a class.',\n",
       "    'Update forward references to the cls module.\\n\\n    .. versionadded:: 3.8\\n\\n    .. versionchanged:: 4.0\\n\\n        The ``localns`` argument was added.\\n\\n    :param cls: The class to update forward references for.\\n    :param localns: A dictionary of local names to use for forward references.\\n    :return: None',\n",
       "    'Update forward references in a class.\\n\\n    .. versionadded:: 3.8\\n\\n    .. versionchanged:: 4.0\\n\\n        This function now takes a local namespace as an optional\\n        argument.\\n\\n    .. versionchanged:: 4.1\\n\\n        This function now takes a local namespace as an optional\\n        argument.\\n\\n    .. versionchanged:: 4.2\\n\\n        This function now takes a local namespace as an optional\\n        argument.\\n\\n    .. versionchanged:: 4.3\\n\\n        This function now takes a local namespace as an optional\\n        argument.\\n\\n    .. versionchanged:: 4.4\\n\\n        This function now takes a local namespace as an optional\\n        argument.\\n\\n    .. versionchanged:: 4.5\\n\\n        This function now takes a local namespace as an optional\\n        argument.\\n\\n    .. versionchanged:: 4.6\\n\\n        This function now takes a local namespace as an optional\\n        argument.\\n\\n    .. versionchanged:: 4.7\\n\\n        This function now takes a local namespace as an optional\\n        argument.\\n\\n    .. versionchanged:: 4.8\\n\\n        This function now takes a local namespace as an optional\\n        argument.\\n\\n    .. version',\n",
       "    'Update forward references in the class.\\n    \\n    Parameters\\n    ----------\\n    cls : class\\n        The class to update forward references in.\\n    \\n    localns : dict, optional\\n        The local namespace to use for forward references.\\n        Defaults to None.']},\n",
       "  {'c': '    def compute_metric(self, a: str, b: str) -> float:\\n        score = self.metric(a, b)\\n        if self.normalize_score and self.distance in (\\n            StringDistance.DAMERAU_LEVENSHTEIN,\\n            StringDistance.LEVENSHTEIN,\\n        ):\\n            score = score / max(len(a), len(b))\\n        return score',\n",
       "   'd': 'Compute the distance between two strings.\\n\\nArgs:\\n    a (str): The first string.\\n    b (str): The second string.\\n\\nReturns:\\n    float: The distance between the two strings.',\n",
       "   'l': True,\n",
       "   'g': ['Train the neural network using the given data.',\n",
       "    'Train the neural network.\\n    \\n    Parameters\\n    ----------\\n    data : list\\n        List of tuples (input, output) representing the training data.\\n    iterations : int\\n        Number of training iterations.\\n    rate : float\\n        Learning rate.\\n    momentum : float\\n        Momentum factor.\\n    \\n    Returns\\n    -------\\n    None',\n",
       "    'Train the neural network using backpropagation.\\n\\n    Arguments:\\n    data -- a list of tuples (x, y) to train the neural network with\\n    iterations -- number of times to train the neural network\\n    rate -- learning rate of the neural network\\n    momentum -- momentum of the neural network',\n",
       "    'Trains the network on the given data.\\n    \\n    Parameters\\n    ----------\\n    data : list\\n        List of tuples (input, output).\\n    iterations : int\\n        Number of iterations to train the network.\\n    rate : float\\n        Learning rate.\\n    momentum : float\\n        Momentum factor.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)',\n",
       "   'd': 'The type of the input to this runnable.',\n",
       "   'l': False,\n",
       "   'g': ['Return the URL for this resource.',\n",
       "    'Return the URL of the request.',\n",
       "    'Return the url of the current page.',\n",
       "    'Returns the url for the page.']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        messages = self.format_messages(**kwargs)\\n        return get_buffer_string(messages)',\n",
       "   'd': 'String buffer of memory.',\n",
       "   'l': False,\n",
       "   'g': ['Convert the given value to a Python object.\\n    \\n    :param value: The value to convert.\\n    :return: The converted Python object.',\n",
       "    'Convert a value to a Python object.',\n",
       "    'Convert a value to the Python equivalent.',\n",
       "    'Override to_python method to handle time values']},\n",
       "  {'c': '    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        vectors = np.array(\\n            self.embeddings.embed_documents([inputs[\"prediction\"], inputs[\"reference\"]])\\n        )\\n        score = self._compute_score(vectors)\\n        return {\"score\": score}',\n",
       "   'd': 'Get a child callback manager.\\n\\nArgs:\\n    tag (str, optional): The tag for the child callback manager.\\n        Defaults to None.\\n\\nReturns:\\n    CallbackManager: The child callback manager.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    Stop the scheduler.', '.', '.', '.']},\n",
       "  {'c': 'def requires_reference(self) -> bool:\\n    return True',\n",
       "   'd': 'Combine two prompt templates.\\n\\nArgs:\\n    other: Another prompt template.\\n\\nReturns:\\n    Combined prompt template.',\n",
       "   'l': False,\n",
       "   'g': ['Update budget',\n",
       "    '_summary_\\n\\n    Args:\\n        budget_id (int): _description_\\n        budget (BudgetIn): _description_\\n        Authorize (AuthJWT, optional): _description_. Defaults to Depends(check_auth).\\n\\n    Returns:\\n        _type_: _description_',\n",
       "    'Update budget',\n",
       "    '_summary_\\n\\n    Args:\\n        budget_id (int): _description_\\n        budget (BudgetIn): _description_\\n        Authorize (AuthJWT, optional): _description_. Defaults to Depends(check_auth).\\n\\n    Raises:\\n        HTTPException: _description_\\n\\n    Returns:\\n        _type_: _description_']},\n",
       "  {'c': 'def embed_query(self, text: str) -> List[float]:\\n    try:\\n        angle = float(text)\\n        return [math.cos(angle * math.pi), math.sin(angle * math.pi)]\\n    except ValueError:\\n\\n        return [0.0, 0.0]',\n",
       "   'd': 'Parse a single string model output into some structure.\\n\\nArgs:\\n    text: String output of a language model.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': False,\n",
       "   'g': ['This method is called when the parser encounters a tag.',\n",
       "    'Called when the parser encounters a start tag.',\n",
       "    'for start tag',\n",
       "    'for each tag, check if it is a header tag']},\n",
       "  {'c': 'def __init__(\\n    self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n) -> None:\\n    super().__init__(**kwargs)\\n    self._separator = separator\\n    self._is_separator_regex = is_separator_regex',\n",
       "   'd': \"Call out to Aleph Alpha's Document endpoint.\\n\\nArgs:\\n    texts: The list of texts to embed.\\n\\nReturns:\\n    List of embeddings, one for each text.\",\n",
       "   'l': False,\n",
       "   'g': ['(str, str, str) -> Optional[ModuleSpec]',\n",
       "    '(str, list[str], str) -> Optional[ModuleSpec]',\n",
       "    '(self, fullname, path=None, target=None) -> ModuleSpec | None:',\n",
       "    '.\\n    Find a module by its name.\\n\\n    :param fullname: The name of the module to find.\\n    :param path: The path to search.\\n    :param target: The target to search for.\\n    :return: A :class:`ModuleSpec` instance or None if not found.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        client: Any,\\n        embeddings: Embeddings,\\n        collection_name: str,\\n        text_key: str,\\n        embedding_key: str,\\n        workspace: str = \"commons\",\\n    ):\\n        try:\\n            from rockset import RocksetClient\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import rockset client python package. \"\\n                \"Please install it with `pip install rockset`.\"\\n            )\\n\\n        if not isinstance(client, RocksetClient):\\n            raise ValueError(\\n                f\"client should be an instance of rockset.RocksetClient, \"\\n                f\"got {type(client)}\"\\n            )\\n\\n        self._client = client\\n        self._collection_name = collection_name\\n        self._embeddings = embeddings\\n        self._text_key = text_key\\n        self._embedding_key = embedding_key\\n        self._workspace = workspace\\n\\n        try:\\n            self._client.set_application(\"langchain\")\\n        except AttributeError:\\n\\n            pass',\n",
       "   'd': 'Initialize with Rockset client.\\nArgs:\\n    client: Rockset client object\\n    collection: Rockset collection to insert docs / query\\n    embeddings: Langchain Embeddings object to use to generate\\n                embedding for given text.\\n    text_key: column in Rockset collection to use to store the text\\n    embedding_key: column in Rockset collection to use to store the embedding.\\n                   Note: We must apply `VECTOR_ENFORCE()` on this column via\\n                   Rockset ingest transformation.',\n",
       "   'l': True,\n",
       "   'g': [':type n: int\\n    :rtype: int',\n",
       "    ':type n: int\\n    :rtype: int',\n",
       "    ':type n: int\\n    :rtype: int',\n",
       "    ':type n: int\\n    :rtype: int']},\n",
       "  {'c': 'def _load_run_evaluators(\\n    config: RunEvalConfig,\\n    run_type: str,\\n    data_type: DataType,\\n    example_outputs: Optional[List[str]],\\n    run_inputs: Optional[List[str]],\\n    run_outputs: Optional[List[str]],\\n) -> List[RunEvaluator]:\\n    eval_llm = config.eval_llm or ChatOpenAI(model=\"gpt-4\", temperature=0.0)\\n    run_evaluators = []\\n    input_key = _determine_input_key(config, run_inputs)\\n    prediction_key = _determine_prediction_key(config, run_outputs)\\n    reference_key = _determine_reference_key(config, example_outputs)\\n    for eval_config in config.evaluators:\\n        run_evaluator = _construct_run_evaluator(\\n            eval_config,\\n            eval_llm,\\n            run_type,\\n            data_type,\\n            example_outputs,\\n            reference_key,\\n            input_key,\\n            prediction_key,\\n        )\\n        run_evaluators.append(run_evaluator)\\n    custom_evaluators = config.custom_evaluators or []\\n    for custom_evaluator in custom_evaluators:\\n        if isinstance(custom_evaluator, RunEvaluator):\\n            run_evaluators.append(custom_evaluator)\\n        elif isinstance(custom_evaluator, StringEvaluator):\\n            run_evaluators.append(\\n                StringRunEvaluatorChain.from_run_and_data_type(\\n                    custom_evaluator,\\n                    run_type,\\n                    data_type,\\n                    input_key=input_key,\\n                    prediction_key=prediction_key,\\n                    reference_key=reference_key,\\n                )\\n            )\\n        else:\\n            raise ValueError(\\n                f\"Unsupported custom evaluator: {custom_evaluator}.\"\\n                f\" Expected RunEvaluator or StringEvaluator.\"\\n            )\\n\\n    return run_evaluators',\n",
       "   'd': 'Load run evaluators from a configuration.\\n\\nArgs:\\n    config: Configuration for the run evaluators.\\n\\nReturns:\\n    A list of run evaluators.',\n",
       "   'l': True,\n",
       "   'g': ['Returns the maximum number of processes that can be run simultaneously.',\n",
       "    'Return the maximum number of processes that can be run concurrently.',\n",
       "    'Return the maximum number of processes that can be run in parallel',\n",
       "    'Return the maximum number of processes that can be run in parallel']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> str:\\n        return self.format_prompt(**kwargs).to_string()',\n",
       "   'd': 'Evaluate question answering examples and predictions.',\n",
       "   'l': False,\n",
       "   'g': ['Updates the status of the future object.\\n    \\n    Parameters:\\n    ft (Future): The future object to update the status of.\\n    \\n    Returns:\\n    None',\n",
       "    'Updates the status of the job.\\n    \\n    :param ft: The future object representing the job execution.\\n    :type ft: Future\\n    :return: None',\n",
       "    'Update status of the job',\n",
       "    'Updates the status of the job\\n    \\n    Args:\\n        ft (Future): Future object returned by the job']},\n",
       "  {'c': '    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        vectors = np.array(\\n            self.embeddings.embed_documents([inputs[\"prediction\"], inputs[\"reference\"]])\\n        )\\n        score = self._compute_score(vectors)\\n        return {\"score\": score}',\n",
       "   'd': 'Get the default parameters for calling text generation inference API.',\n",
       "   'l': False,\n",
       "   'g': ['_test_cache_categories',\n",
       "    '_test_cache_categories\\n    \\n    Test that cache is used to store category urls',\n",
       "    '_test_cache_categories',\n",
       "    '.\\n    Test that categories are set correctly.']},\n",
       "  {'c': 'def tool(\\n    *args: Union[str, Callable, Runnable],\\n    return_direct: bool = False,\\n    args_schema: Optional[Type[BaseModel]] = None,\\n    infer_schema: bool = True,\\n) -> Callable:\\n    def _make_with_name(tool_name: str) -> Callable:\\n        def _make_tool(dec_func: Union[Callable, Runnable]) -> BaseTool:\\n            if isinstance(dec_func, Runnable):\\n                if dec_func.input_schema.schema().get(\"type\") != \"object\":\\n                    raise ValueError(\"Runnable must have an object schema.\")\\n\\n                async def ainvoke_wrapper(\\n                    callbacks: Optional[Callbacks] = None, **kwargs: Any\\n                ) -> Any:\\n                    return await dec_func.ainvoke(kwargs, {\"callbacks\": callbacks})\\n\\n                def invoke_wrapper(\\n                    callbacks: Optional[Callbacks] = None, **kwargs: Any\\n                ) -> Any:\\n                    return dec_func.invoke(kwargs, {\"callbacks\": callbacks})\\n\\n                coroutine = ainvoke_wrapper\\n                func = invoke_wrapper\\n                schema = dec_func.input_schema\\n                description = repr(dec_func)\\n            elif inspect.iscoroutinefunction(dec_func):\\n                coroutine = dec_func\\n                func = None\\n                schema = args_schema\\n                description = None\\n            else:\\n                coroutine = None\\n                func = dec_func\\n                schema = args_schema\\n                description = None\\n\\n            if infer_schema or args_schema is not None:\\n                return StructuredTool.from_function(\\n                    func,\\n                    coroutine,\\n                    name=tool_name,\\n                    description=description,\\n                    return_direct=return_direct,\\n                    args_schema=schema,\\n                    infer_schema=infer_schema,\\n                )\\n\\n\\n            if func.__doc__ is None:\\n                raise ValueError(\\n                    \"Function must have a docstring if \"\\n                    \"description not provided and infer_schema is False.\"\\n                )\\n            return Tool(\\n                name=tool_name,\\n                func=func,\\n                description=f\"{tool_name} tool\",\\n                return_direct=return_direct,\\n                coroutine=coroutine,\\n            )\\n\\n        return _make_tool\\n\\n    if len(args) == 2 and isinstance(args[0], str) and isinstance(args[1], Runnable):\\n        return _make_with_name(args[0])(args[1])\\n    elif len(args) == 1 and isinstance(args[0], str):\\n\\n\\n        return _make_with_name(args[0])\\n    elif len(args) == 1 and callable(args[0]):\\n\\n\\n        return _make_with_name(args[0].__name__)(args[0])\\n    elif len(args) == 0:\\n\\n\\n        def _partial(func: Callable[[str], str]) -> BaseTool:\\n            return _make_with_name(func.__name__)(func)\\n\\n        return _partial\\n    else:\\n        raise ValueError(\"Too many arguments for tool decorator\")',\n",
       "   'd': 'Make tools out of functions, can be used with or without arguments.\\n\\nArgs:\\n    *args: The arguments to the tool.\\n    return_direct: Whether to return directly from the tool rather\\n        than continuing the agent loop.\\n    args_schema: optional argument schema for user to specify\\n    infer_schema: Whether to infer the schema of the arguments from\\n        the function\\'s signature. This also makes the resultant tool\\n        accept a dictionary input to its `run()` function.\\n\\nRequires:\\n    - Function must be of type (str) -> str\\n    - Function must have a docstring\\n\\nExamples:\\n    .. code-block:: python\\n\\n        @tool\\n        def search_api(query: str) -> str:\\n            # Searches the API for the query.\\n            return\\n\\n        @tool(\"search\", return_direct=True)\\n        def search_api(query: str) -> str:\\n            # Searches the API for the query.\\n            return',\n",
       "   'l': True,\n",
       "   'g': ['fromfile(*args, **kwargs)\\n\\n    Read data from a file.\\n\\n    Parameters\\n    ----------\\n    *args\\n        The arguments to `numpy.fromfile`.\\n    **kwargs\\n        The keyword arguments to `numpy.fromfile`.\\n\\n    Returns\\n    -------\\n    array\\n        The array read from the file.',\n",
       "    'fromfile(*args, **kwargs)\\n    \\n    Read data from a file.\\n    \\n    Parameters\\n    ----------\\n    args :\\n        The arguments to be passed to numpy.fromfile.\\n    \\n    kwargs :\\n        The keyword arguments to be passed to numpy.fromfile.\\n    \\n    Returns\\n    -------\\n    array :\\n        The data read from the file.',\n",
       "    'Read a numpy array from a file.',\n",
       "    \"fromfile(*args, **kwargs)\\n    \\n    fromfile(filename, dtype=float, sep='\\\\n', skip_header=0, skip_footer=0)\\n    \\n    fromfile(filename, dtype=float, sep='\\\\n', skip_header=0, skip_footer=0)\\n    \\n    fromfile(filename, dtype=float, sep='\\\\n', skip_header=0, skip_footer=0)\\n    \\n    fromfile(filename, dtype=float, sep='\\\\n', skip_header=0, skip_footer=0)\\n    \\n    fromfile(filename, dtype=float, sep='\\\\n', skip_header=0, skip_footer=0)\\n    \\n    fromfile(filename, dtype=float, sep='\\\\n', skip_header=0, skip_footer=0)\\n    \\n    fromfile(filename, dtype=float, sep='\\\\n', skip_header=0, skip_footer=0)\\n    \\n    fromfile(filename, dtype=float, sep='\\\\n', skip_header=0, skip_footer=0)\\n    \\n    fromfile(filename, dtype=float,\"]},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n    llm_cache = self._get_llm_cache(llm_string)\\n    generations: List = []\\n\\n    results = llm_cache.similarity_search(\\n        query=prompt,\\n        k=1,\\n        distance_threshold=self.score_threshold,\\n    )\\n    if results:\\n        for document in results:\\n            generations.extend(\\n                _load_generations_from_json(document.metadata[\"return_val\"])\\n            )\\n    return generations if generations else None',\n",
       "   'd': 'Get docs.',\n",
       "   'l': False,\n",
       "   'g': ['Deserialize a ciphertext blob into a Ciphertext object.\\n    \\n    Args:\\n        ciphertext_blob (bytes): The ciphertext blob to deserialize.\\n    \\n    Returns:\\n        Ciphertext: The deserialized Ciphertext object.',\n",
       "    'Deserialize a ciphertext blob from a serialized form.\\n    \\n    Args:\\n        ciphertext_blob (bytes): The serialized ciphertext blob.\\n    \\n    Returns:\\n        Ciphertext: The deserialized ciphertext.',\n",
       "    'Deserialize a ciphertext blob into a Ciphertext object.\\n    \\n    Args:\\n        ciphertext_blob (bytes): The ciphertext blob to deserialize.\\n    \\n    Returns:\\n        Ciphertext: The deserialized ciphertext object.',\n",
       "    'Deserialize a ciphertext blob.\\n    \\n    Args:\\n        ciphertext_blob: The ciphertext blob to deserialize.\\n    \\n    Returns:\\n        A Ciphertext object.']},\n",
       "  {'c': 'def test_fireworks_streaming() -> None:\\n    llm = ChatFireworks()\\n\\n    for token in llm.stream(\"I\\'m Pickle Rick\"):\\n        assert isinstance(token.content, str)',\n",
       "   'd': 'Get the schema for a specific table.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    Args:\\n      experiment_id: An integer.\\n\\n    Returns:\\n      An Experiment.',\n",
       "    '.',\n",
       "    '.\\n    Args:\\n      experiment_id: An integer.\\n    Returns:\\n      An Experiment object.',\n",
       "    '.\\n    Args:\\n      experiment_id: int64\\n    Returns:\\n      Experiment']},\n",
       "  {'c': 'def __init__(self, **kwargs: Any) -> None:\\n    separators = self.get_separators_for_language(Language.PYTHON)\\n    super().__init__(separators=separators, **kwargs)',\n",
       "   'd': 'Initialize a PythonCodeTextSplitter.',\n",
       "   'l': True,\n",
       "   'g': ['Return a string representation of this object.',\n",
       "    '\"Return a string representation of this object.',\n",
       "    'Returns a readable representation of the object.',\n",
       "    '\"Return a string representation of this object.']},\n",
       "  {'c': '    def test_intervention_chain(self) -> None:\\n        llm = OpenAI(temperature=0, max_tokens=512)\\n        story_conditions_chain = InterventionChain.from_univariate_prompt(llm)\\n        question = \"if cindy has ten pets\"\\n        data = story_conditions_chain(question)[Constant.chain_data.value]\\n        self.assertEqual(type(data), InterventionModel)',\n",
       "   'd': 'Initialize the OpenAI object.',\n",
       "   'l': False,\n",
       "   'g': ['sections',\n",
       "    'sections',\n",
       "    'Extract all sections of markdown document.\\n\\n    :param markdown: Markdown document.\\n    :return: List of sections.',\n",
       "    '']},\n",
       "  {'c': 'def type(self) -> str:\\n    return \"human\"',\n",
       "   'd': 'Initialize with a file path.\\n\\nArgs:\\n    file_path: Either a local, S3 or web path to a PDF file.\\n    headers: Headers to use for GET request to download a file from a web path.',\n",
       "   'l': False,\n",
       "   'g': ['.set()\\n    Sets the resource for a logicalId.',\n",
       "    '.set()\\n    Sets the resource for a logical ID.',\n",
       "    '.set(self, logicalId, resource)\\n\\n    Set a resource.\\n\\n    Args:\\n        logicalId (str): Logical ID of the resource.\\n        resource (dict or SamResource): Resource data.',\n",
       "    '.set()\\n\\n    Sets the resource for the logicalId.\\n\\n    Args:\\n        logicalId (str): The logical ID of the resource.\\n        resource (SamResource): The resource to set.\\n\\n    Returns:\\n        None']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> BaseMessage:\\n',\n",
       "   'd': 'Format the prompt template.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    Formatted message.',\n",
       "   'l': True,\n",
       "   'g': ['Creates embedding matrix and initializes it with pretrained word2vec vectors.',\n",
       "    'Creates the embedding matrix for the model.',\n",
       "    'Create embedding matrix for word embeddings.',\n",
       "    'Creates the embedding matrix.']},\n",
       "  {'c': 'def _type(self) -> str:\\n    raise NotImplementedError(\\n        f\"_type property is not implemented in class {self.__class__.__name__}.\"\\n        \" This is required for serialization.\"\\n    )',\n",
       "   'd': 'Initialize the loader.\\n\\nArgs:\\n    file_path: A file, url or s3 path for input file\\n    textract_features: Features to be used for extraction, each feature\\n                       should be passed as a str that conforms to the enum\\n                       `Textract_Features`, see `amazon-textract-caller` pkg\\n    client: boto3 textract client (Optional)\\n    credentials_profile_name: AWS profile name, if not default (Optional)\\n    region_name: AWS region, eg us-east-1 (Optional)\\n    endpoint_url: endpoint url for the textract service (Optional)',\n",
       "   'l': False,\n",
       "   'g': ['List all models.',\n",
       "    'List models.\\n\\n    :param extra_headers: Extra headers to send with the request.\\n    :param extra_query: Extra query parameters to send with the request.\\n    :param extra_body: Extra body to send with the request.\\n    :param timeout: Timeout for the request.\\n    :return: A page of models.',\n",
       "    'List models.',\n",
       "    'List all models.']},\n",
       "  {'c': '    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        retriever: BaseRetriever,\\n        condense_question_prompt: BasePromptTemplate = CONDENSE_QUESTION_PROMPT,\\n        chain_type: str = \"stuff\",\\n        verbose: bool = False,\\n        condense_question_llm: Optional[BaseLanguageModel] = None,\\n        combine_docs_chain_kwargs: Optional[Dict] = None,\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> BaseConversationalRetrievalChain:\\n        combine_docs_chain_kwargs = combine_docs_chain_kwargs or {}\\n        doc_chain = load_qa_chain(\\n            llm,\\n            chain_type=chain_type,\\n            verbose=verbose,\\n            callbacks=callbacks,\\n            **combine_docs_chain_kwargs,\\n        )\\n\\n        _llm = condense_question_llm or llm\\n        condense_question_chain = LLMChain(\\n            llm=_llm,\\n            prompt=condense_question_prompt,\\n            verbose=verbose,\\n            callbacks=callbacks,\\n        )\\n        return cls(\\n            retriever=retriever,\\n            combine_docs_chain=doc_chain,\\n            question_generator=condense_question_chain,\\n            callbacks=callbacks,\\n            **kwargs,\\n        )',\n",
       "   'd': 'Validate that either folder_id or document_ids is set, but not both.',\n",
       "   'l': False,\n",
       "   'g': ['', 'Run the command \"foo\" and return the result.', '', '']},\n",
       "  {'c': '    def configure(\\n        cls,\\n        inheritable_callbacks: Callbacks = None,\\n        local_callbacks: Callbacks = None,\\n        verbose: bool = False,\\n        inheritable_tags: Optional[List[str]] = None,\\n        local_tags: Optional[List[str]] = None,\\n        inheritable_metadata: Optional[Dict[str, Any]] = None,\\n        local_metadata: Optional[Dict[str, Any]] = None,\\n    ) -> CallbackManager:\\n        return _configure(\\n            cls,\\n            inheritable_callbacks,\\n            local_callbacks,\\n            verbose,\\n            inheritable_tags,\\n            local_tags,\\n            inheritable_metadata,\\n            local_metadata,\\n        )',\n",
       "   'd': \"Call out to Aleph Alpha's asymmetric Document endpoint.\\n\\nArgs:\\n    texts: The list of texts to embed.\\n\\nReturns:\\n    List of embeddings, one for each text.\",\n",
       "   'l': False,\n",
       "   'g': ['Updates the label map path in the given configs.\\n    \\n    Args:\\n        configs (dict): The dictionary containing the configs.\\n        label_map_path (str): The path to the label map file.',\n",
       "    '',\n",
       "    'Update the label map path.',\n",
       "    'Updates the label map path in the config.\\n    \\n    Args:\\n      configs: the model config.\\n      label_map_path: the path to the label map.']},\n",
       "  {'c': 'def search_api(query: str) -> str:\\n    return \"API result\"',\n",
       "   'd': 'Return the input keys.\\n\\nReturns:\\n    List of input keys.',\n",
       "   'l': False,\n",
       "   'g': ['Save query in redis',\n",
       "    'Save query to Redis',\n",
       "    'Save a query in redis',\n",
       "    'Save a query to Redis\\n    \\n    @param client_id: ID of the client\\n    @param query: The query\\n    \\n    @return: None']},\n",
       "  {'c': '    def output_keys(self) -> List[str]:\\n        return self.output_variables',\n",
       "   'd': 'Return output key.\\n\\n:meta private:',\n",
       "   'l': True,\n",
       "   'g': ['_init_\\n\\n    Args:\\n        encoder_path (str, optional): Path to the encoder weights. Defaults to \"taesd_encoder.pth\".\\n        decoder_path (str, optional): Path to the decoder weights. Defaults to \"taesd_decoder.pth\".',\n",
       "    'Initialize the TAESD model.\\n\\n    Args:\\n        encoder_path (str, optional): Path to the encoder model weights. Defaults to \"taesd_encoder.pth\".\\n        decoder_path (str, optional): Path to the decoder model weights. Defaults to \"taesd_decoder.pth\".',\n",
       "    'Initialize the TAESD model.\\n\\n    Args:\\n        encoder_path (str, optional): Path to the encoder model weights. Defaults to \"taesd_encoder.pth\".\\n        decoder_path (str, optional): Path to the decoder model weights. Defaults to \"taesd_decoder.pth\".',\n",
       "    'Initialize the class with the encoder and decoder paths.']},\n",
       "  {'c': 'def dict(self, **kwargs: Any) -> Dict:\\n    output_parser_dict = super().dict(**kwargs)\\n    output_parser_dict[\"_type\"] = self._type\\n    return output_parser_dict',\n",
       "   'd': 'Retrieve documents relevant to a query.\\nArgs:\\n    query: string to find relevant documents for\\n    callbacks: Callback manager or list of callbacks\\n    tags: Optional list of tags associated with the retriever. Defaults to None\\n        These tags will be associated with each call to this retriever,\\n        and passed as arguments to the handlers defined in `callbacks`.\\n    metadata: Optional metadata associated with the retriever. Defaults to None\\n        This metadata will be associated with each call to this retriever,\\n        and passed as arguments to the handlers defined in `callbacks`.\\nReturns:\\n    List of relevant documents',\n",
       "   'l': False,\n",
       "   'g': ['Sends data to the control server.\\n    \\n    Args:\\n        log_data (bool, optional): Whether to log the data. Defaults to True.',\n",
       "    'Sends the data to the telemetry server. \\n    \\n    :param log_data: If True, sends the data to the server. If False, sends the data to the server as a redacted string.\\n    :type log_data: bool\\n    :return: None\\n    :rtype: NoneType',\n",
       "    'Sends telemetry data to the server.\\n    \\n    :param log_data: Whether to log the data. Defaults to True.\\n    :type log_data: bool\\n    :return: None',\n",
       "    'Sends telemetry data to the control server.']},\n",
       "  {'c': '    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        texts = [t.replace(\"\\\\n\", \" \") for t in texts]\\n        embeddings = self.client.encode(texts, **self.encode_kwargs)\\n        return embeddings.tolist()',\n",
       "   'd': 'Compute doc embeddings using a HuggingFace transformer model.\\n\\nArgs:\\n    texts: The list of texts to embed.\\n\\nReturns:\\n    List of embeddings, one for each text.',\n",
       "   'l': True,\n",
       "   'g': ['Disable the adapter layers of the base model. \\n    \\n    Args:\\n        self (PromptLearningModel): The instance of the PromptLearningModel class.\\n    \\n    Yields:\\n        None\\n    \\n    Returns:\\n        None',\n",
       "    '',\n",
       "    'Disable adapter layers for the specified adapter.\\n    \\n    Args:\\n        self (PromptTrainer): The PromptTrainer instance.\\n        \\n    Yields:\\n        None',\n",
       "    'Disable the adapter layers.']},\n",
       "  {'c': 'def __init__(\\n    self, file_path: str, password: Optional[Union[str, bytes]] = None\\n) -> None:\\n    try:\\n        import pypdf\\n    except ImportError:\\n        raise ImportError(\\n            \"pypdf package not found, please install it with \" \"`pip install pypdf`\"\\n        )\\n    self.parser = PyPDFParser(password=password)\\n    super().__init__(file_path)',\n",
       "   'd': 'Initialize with a file path.',\n",
       "   'l': True,\n",
       "   'g': ['Convert pcm to silk\\n    :param pcm_path: pcm file path\\n    :param silk_path: silk file path\\n    :return: silk file duration',\n",
       "    'Convert a .wav file to a .silk file.\\n\\n    Args:\\n        pcm_path (str): Path to the .wav file.\\n        silk_path (str): Path to the .silk file.\\n\\n    Returns:\\n        None',\n",
       "    'Convert pcm to silk\\n    \\n    Parameters\\n    ----------\\n    pcm_path : str\\n        path to pcm file\\n    silk_path : str\\n        path to silk file\\n    \\n    Returns\\n    -------\\n    None',\n",
       "    'Convert a PCM file to a SIL file using pysilk.\\n    \\n    Args:\\n        pcm_path (str): Path to the PCM file.\\n        silk_path (str): Path to the SIL file.\\n        \\n    Returns:\\n        AudioSegment: The converted SIL file.']},\n",
       "  {'c': '    def resolve_criteria(\\n        cls,\\n        criteria: Optional[Union[CRITERIA_TYPE, str]],\\n    ) -> Dict[str, str]:\\n        return resolve_criteria(criteria)',\n",
       "   'd': 'Return response when agent has been stopped due to max iterations.',\n",
       "   'l': False,\n",
       "   'g': ['Add a callback to be called when a frame is detached.\\n    \\n    Parameters\\n    ----------\\n    event: Literal[\"framedetached\"]\\n        The event to listen for.\\n    f: typing.Callable[[\"Frame\"], \"typing.Union[typing.Awaitable[None], None]\"]\\n        The callback to call when the frame is detached.\\n    \\n    Returns\\n    -------\\n    None',\n",
       "    'This function is called when the frame is detached from the parent window.\\n    \\n    Parameters\\n    ----------\\n    event: Literal[\"framedetached\"]\\n        The event type.\\n    f: typing.Callable[[\"Frame\"], \"typing.Union[typing.Awaitable[None], None]\"]\\n        The function to call when the frame is detached from the parent window.\\n        \\n    Returns\\n    -------\\n    None',\n",
       "    '',\n",
       "    '_on_event_frame_detached\\n\\n    Args:\\n        event (Literal[\"framedetached\"]): _description_\\n        f (typing.Callable[[\"Frame\"], \"typing.Union[typing.Awaitable[None], None]\"]): _description_\\n\\n    Returns:\\n        None: _description_']},\n",
       "  {'c': '    def _call(\\n        self,\\n        prompt: str,\\n        stop: Optional[List[str]] = None,\\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\\n        **kwargs: Any,\\n    ) -> str:\\n        model = self.client.get_model(self.model_uid)\\n\\n        generate_config: \"LlamaCppGenerateConfig\" = kwargs.get(\"generate_config\", {})\\n\\n        generate_config = {**self.model_kwargs, **generate_config}\\n\\n        if stop:\\n            generate_config[\"stop\"] = stop\\n\\n        if generate_config and generate_config.get(\"stream\"):\\n            combined_text_output = \"\"\\n            for token in self._stream_generate(\\n                model=model,\\n                prompt=prompt,\\n                run_manager=run_manager,\\n                generate_config=generate_config,\\n            ):\\n                combined_text_output += token\\n            return combined_text_output\\n\\n        else:\\n            completion = model.generate(prompt=prompt, generate_config=generate_config)\\n            return completion[\"choices\"][0][\"text\"]',\n",
       "   'd': 'Parse the output of an LLM call.',\n",
       "   'l': False,\n",
       "   'g': ['Returns the number of threads to use for the given parameters.',\n",
       "    '',\n",
       "    '',\n",
       "    'Get the number of threads to use\\n    \\n    :param until_found: The number of threads to use until the threads_num is found\\n    :param recent: The number of threads to use after the threads_num is found\\n    \\n    :return: The number of threads to use']},\n",
       "  {'c': '    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        all_required_field_names = get_pydantic_field_names(cls)\\n        extra = values.get(\"model_kwargs\", {})\\n        for field_name in list(values):\\n            if field_name in extra:\\n                raise ValueError(f\"Found {field_name} supplied twice.\")\\n            if field_name not in all_required_field_names:\\n                warnings.warn(\\n                    f\"\"\"WARNING! {field_name} is not default parameter.\\n                    {field_name} was transferred to model_kwargs.\\n                    Please confirm that {field_name} is what you intended.\"\"\"\\n                )\\n                extra[field_name] = values.pop(field_name)\\n\\n        invalid_model_kwargs = all_required_field_names.intersection(extra.keys())\\n        if invalid_model_kwargs:\\n            raise ValueError(\\n                f\"Parameters {invalid_model_kwargs} should be specified explicitly. \"\\n                f\"Instead they were passed in as part of `model_kwargs` parameter.\"\\n            )\\n\\n        values[\"model_kwargs\"] = extra\\n        return values',\n",
       "   'd': 'Build extra kwargs from additional params that were passed in.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Check if the request is a William request.',\n",
       "    'Check that the request is a valid SAML request.',\n",
       "    'Checks if the request is a valid SAML request.']},\n",
       "  {'c': '    def parse_result(self, result: List[Generation]) -> T:\\n        return self.parse(result[0].text)',\n",
       "   'd': 'Parse a list of candidate model Generations into a specific format.\\n\\nThe return value is parsed from only the first Generation in the result, which\\n    is assumed to be the highest-likelihood Generation.\\n\\nArgs:\\n    result: A list of Generations to be parsed. The Generations are assumed\\n        to be different candidate outputs for a single model input.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': True,\n",
       "   'g': ['Check that the target is reachable.',\n",
       "    '_summary_\\n    Check if the exploit is successful.\\n\\n    Args:\\n        target (target): Target to check.\\n\\n    Returns:\\n        bool: True if the exploit is successful, False otherwise.',\n",
       "    '_check_success_\\n\\n    This function will check if the target is vulnerable.',\n",
       "    'Test that the check function returns True when the target is vulnerable.']},\n",
       "  {'c': 'def generate_with_retry(llm: Tongyi, **kwargs: Any) -> Any:\\n    retry_decorator = _create_retry_decorator(llm)\\n\\n    @retry_decorator\\n    def _generate_with_retry(**_kwargs: Any) -> Any:\\n        resp = llm.client.call(**_kwargs)\\n        if resp.status_code == 200:\\n            return resp\\n        elif resp.status_code in [400, 401]:\\n            raise ValueError(\\n                f\"status_code: {resp.status_code} \\\\n \"\\n                f\"code: {resp.code} \\\\n message: {resp.message}\"\\n            )\\n        else:\\n            raise HTTPError(\\n                f\"HTTP error occurred: status_code: {resp.status_code} \\\\n \"\\n                f\"code: {resp.code} \\\\n message: {resp.message}\",\\n                response=resp,\\n            )\\n\\n    return _generate_with_retry(**kwargs)',\n",
       "   'd': 'Create a `LabeledCriteriaEvalChain` instance from an llm and criteria.\\n\\nParameters\\n----------\\nllm : BaseLanguageModel\\n    The language model to use for evaluation.\\ncriteria : CRITERIA_TYPE - default=None for \"helpfulness\"\\n    The criteria to evaluate the runs against. It can be:\\n        -  a mapping of a criterion name to its description\\n        -  a single criterion name present in one of the default criteria\\n        -  a single `ConstitutionalPrinciple` instance\\nprompt : Optional[BasePromptTemplate], default=None\\n    The prompt template to use for generating prompts. If not provided,\\n    a default prompt will be used.\\n**kwargs : Any\\n    Additional keyword arguments to pass to the `LLMChain`\\n    constructor.\\n\\nReturns\\n-------\\nLabeledCriteriaEvalChain\\n    An instance of the `LabeledCriteriaEvalChain` class.\\n\\nExamples\\n--------\\n>>> from langchain.llms import OpenAI\\n>>> from langchain.evaluation.criteria import LabeledCriteriaEvalChain\\n>>> llm = OpenAI()\\n>>> criteria = {\\n        \"hallucination\": (\\n            \"Does this submission contain information\"\\n            \" not present in the input or reference?\"\\n        ),\\n    }\\n>>> chain = LabeledCriteriaEvalChain.from_llm(\\n        llm=llm,\\n        criteria=criteria,\\n    )',\n",
       "   'l': False,\n",
       "   'g': ['Save model checkpoint.\\n    \\n    Args:\\n        program (fluid.core.Program): Program.\\n        ckpt_name (str): Checkpoint name.\\n    \\n    Returns:\\n        str: Checkpoint dir.',\n",
       "    'Save the model checkpoint to disk.\\n    \\n    Args:\\n        program: The program to be saved.\\n        ckpt_name: The name of the checkpoint to be saved.\\n    \\n    Returns:\\n        The directory where the checkpoint was saved.',\n",
       "    'Save model checkpoint.',\n",
       "    'Save the model checkpoint\\n    Args:\\n        ckpt_name: the name of the checkpoint']},\n",
       "  {'c': '    def input_variables(self) -> List[str]:\\n',\n",
       "   'd': 'Input variables for this prompt template.\\n\\nReturns:\\n    List of input variables.',\n",
       "   'l': True,\n",
       "   'g': ['.',\n",
       "    '.',\n",
       "    'for attribute in self.attributes',\n",
       "    '_get_attribute_description_for_attribute_name\\n    \\n    Returns the attribute description for the given attribute name.\\n    \\n    :param attribute: The name of the attribute.\\n    \\n    :return: The attribute description.']},\n",
       "  {'c': 'def run_on_dataset(\\n    client: Client,\\n    dataset_name: str,\\n    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,\\n    *,\\n    evaluation: Optional[RunEvalConfig] = None,\\n    concurrency_level: int = 5,\\n    project_name: Optional[str] = None,\\n    verbose: bool = False,\\n    tags: Optional[List[str]] = None,\\n    input_mapper: Optional[Callable[[Dict], Any]] = None,\\n    **kwargs: Any,\\n) -> Dict[str, Any]:\\n    if kwargs:\\n        warnings.warn(\\n            \"The following arguments are deprecated and \"\\n            \"will be removed in a future release: \"\\n            f\"{kwargs.keys()}.\",\\n            DeprecationWarning,\\n        )\\n    wrapped_model, project_name, dataset, examples = _prepare_eval_run(\\n        client, dataset_name, llm_or_chain_factory, project_name\\n    )\\n    if concurrency_level in (0, 1):\\n        results = _run_on_examples(\\n            client,\\n            examples,\\n            wrapped_model,\\n            project_name=project_name,\\n            verbose=verbose,\\n            tags=tags,\\n            evaluation=evaluation,\\n            input_mapper=input_mapper,\\n            data_type=dataset.data_type,\\n        )\\n    else:\\n\\n        coro = _arun_on_examples(\\n            client,\\n            examples,\\n            wrapped_model,\\n            concurrency_level=concurrency_level,\\n            project_name=project_name,\\n            verbose=verbose,\\n            tags=tags,\\n            evaluation=evaluation,\\n            input_mapper=input_mapper,\\n            data_type=dataset.data_type,\\n        )\\n        results = _handle_coroutine(coro)\\n    return TestResult(\\n        project_name=project_name,\\n        results=results,\\n    )',\n",
       "   'd': 'Run the Chain or language model on a dataset and store traces\\nto the specified project name.\\n\\nArgs:\\n    client: LangSmith client to use to access the dataset and to\\n        log feedback and run traces.\\n    dataset_name: Name of the dataset to run the chain on.\\n    llm_or_chain_factory: Language model or Chain constructor to run\\n        over the dataset. The Chain constructor is used to permit\\n        independent calls on each example without carrying over state.\\n    evaluation: Configuration for evaluators to run on the\\n        results of the chain\\n    concurrency_level: The number of async tasks to run concurrently.\\n    project_name: Name of the project to store the traces in.\\n        Defaults to {dataset_name}-{chain class name}-{datetime}.\\n    verbose: Whether to print progress.\\n    tags: Tags to add to each run in the project.\\n    input_mapper: A function to map to the inputs dictionary from an Example\\n        to the format expected by the model to be evaluated. This is useful if\\n        your model needs to deserialize more complex schema or if your dataset\\n        has inputs with keys that differ from what is expected by your chain\\n        or agent.\\n\\nReturns:\\n    A dictionary containing the run\\'s project name and the resulting model outputs.\\n\\n\\nFor the (usually faster) async version of this function, see :func:`arun_on_dataset`.\\n\\nExamples\\n--------\\n\\n.. code-block:: python\\n\\n    from langsmith import Client\\n    from langchain.chat_models import ChatOpenAI\\n    from langchain.chains import LLMChain\\n    from langchain.smith import RunEvalConfig, run_on_dataset\\n\\n    # Chains may have memory. Passing in a constructor function lets the\\n    # evaluation framework avoid cross-contamination between runs.\\n    def construct_chain():\\n        llm = ChatOpenAI(temperature=0)\\n        chain = LLMChain.from_string(\\n            llm,\\n            \"What\\'s the answer to {your_input_key}\"\\n        )\\n        return chain\\n\\n    # Load off-the-shelf evaluators via config or the EvaluatorType (string or enum)\\n    evaluation_config = RunEvalConfig(\\n        evaluators=[\\n            \"qa\",  # \"Correctness\" against a reference answer\\n            \"embedding_distance\",\\n            RunEvalConfig.Criteria(\"helpfulness\"),\\n            RunEvalConfig.Criteria({\\n                \"fifth-grader-score\": \"Do you have to be smarter than a fifth grader to answer this question?\"\\n            }),\\n        ]\\n    )\\n\\n    client = Client()\\n    run_on_dataset(\\n        client,\\n        \"<my_dataset_name>\",\\n        construct_chain,\\n        evaluation=evaluation_config,\\n    )\\n\\nYou can also create custom evaluators by subclassing the\\n:class:`StringEvaluator <langchain.evaluation.schema.StringEvaluator>`\\nor LangSmith\\'s `RunEvaluator` classes.\\n\\n.. code-block:: python\\n\\n    from typing import Optional\\n    from langchain.evaluation import StringEvaluator\\n\\n    class MyStringEvaluator(StringEvaluator):\\n\\n        @property\\n        def requires_input(self) -> bool:\\n            return False\\n\\n        @property\\n        def requires_reference(self) -> bool:\\n            return True\\n\\n        @property\\n        def evaluation_name(self) -> str:\\n            return \"exact_match\"\\n\\n        def _evaluate_strings(self, prediction, reference=None, input=None, **kwargs) -> dict:\\n            return {\"score\": prediction == reference}\\n\\n\\n    evaluation_config = RunEvalConfig(\\n        custom_evaluators = [MyStringEvaluator()],\\n    )\\n\\n    run_on_dataset(\\n        client,\\n        \"<my_dataset_name>\",\\n        construct_chain,\\n        evaluation=evaluation_config,\\n    )',\n",
       "   'l': True,\n",
       "   'g': ['Deletes a conversation from the database. \\n    \\n    Args:\\n        convo_id (str): The ID of the conversation to delete.',\n",
       "    'Deletes a conversation.\\n    \\n    Args:\\n        convo_id (str): Conversation ID.',\n",
       "    'Deletes a conversation by its id.\\n\\n    Args:\\n        convo_id (str): The id of the conversation to delete.\\n\\n    Returns:\\n        None',\n",
       "    'Delete a conversation by id.\\n\\n    Args:\\n        convo_id (str): The id of the conversation to delete.\\n\\n    Returns:\\n        None']},\n",
       "  {'c': 'def similarity_search_with_score(\\n    self, query: str, k: int = 4, filter: Optional[dict] = None, **kwargs: Any\\n) -> List[Tuple[Document, float]]:\\n    embedding = self.embedding.embed_query(query)\\n    script_query = _default_script_query(embedding, filter)\\n    response = self.client_search(\\n        self.client, self.index_name, script_query, size=k\\n    )\\n    hits = [hit for hit in response[\"hits\"][\"hits\"]]\\n    docs_and_scores = [\\n        (\\n            Document(\\n                page_content=hit[\"_source\"][\"text\"],\\n                metadata=hit[\"_source\"][\"metadata\"],\\n            ),\\n            hit[\"_score\"],\\n        )\\n        for hit in hits\\n    ]\\n    return docs_and_scores',\n",
       "   'd': 'Return docs most similar to query.\\nArgs:\\n    query: Text to look up documents similar to.\\n    k: Number of Documents to return. Defaults to 4.\\nReturns:\\n    List of Documents most similar to the query.',\n",
       "   'l': True,\n",
       "   'g': ['Return a queryset of objects that can be accessed by the current user.',\n",
       "    'Returns a queryset of objects that the user can read.',\n",
       "    'Returns a queryset of all the objects that this user can see.',\n",
       "    '_get_queryset']},\n",
       "  {'c': 'def _generate(\\n    self,\\n    prompts: List[str],\\n    stop: Optional[List[str]] = None,\\n    run_manager: Optional[CallbackManagerForLLMRun] = None,\\n    **kwargs: Any,\\n) -> LLMResult:\\n',\n",
       "   'd': 'Run the LLM on the given prompts.',\n",
       "   'l': True,\n",
       "   'g': ['',\n",
       "    'Initializes a new instance of the File class.',\n",
       "    'Constructor for the Image class.',\n",
       "    'Initializes the object with a path to the file.\\n    \\n    Args:\\n        path (str): The path to the file.']},\n",
       "  {'c': 'def _run(\\n    self,\\n    query: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> str:\\n    return str(self.api_wrapper.results(query))',\n",
       "   'd': 'Create a new TextSplitter.',\n",
       "   'l': False,\n",
       "   'g': ['def __init__(self, features):', '.', '', '.']},\n",
       "  {'c': 'def _generate(\\n    self,\\n    prompts: List[str],\\n    stop: Optional[List[str]] = None,\\n    run_manager: Optional[CallbackManagerForLLMRun] = None,\\n    **kwargs: Any,\\n) -> LLMResult:\\n',\n",
       "   'd': 'Run the LLM on the given prompts.',\n",
       "   'l': True,\n",
       "   'g': ['Returns a course staff group name.',\n",
       "    'Returns the name of the group for the staff group for the given location.',\n",
       "    'Return the name of the course staff group for the given location.',\n",
       "    '']},\n",
       "  {'c': 'def format_response_payload(self, output: bytes) -> str:\\n',\n",
       "   'd': 'Helper method to transform an Iterator of Input values into an Iterator of\\nOutput values, with callbacks.\\nUse this to implement `stream()` or `transform()` in Runnable subclasses.',\n",
       "   'l': False,\n",
       "   'g': ['.', '.', '.', '.']},\n",
       "  {'c': '    def _create_search_request(self, query: str) -> SearchRequest:\\n        from google.cloud.discoveryengine_v1beta import SearchRequest\\n\\n        query_expansion_spec = SearchRequest.QueryExpansionSpec(\\n            condition=self.query_expansion_condition,\\n        )\\n\\n        spell_correction_spec = SearchRequest.SpellCorrectionSpec(\\n            mode=self.spell_correction_mode\\n        )\\n\\n        if self.engine_data_type == 0:\\n            if self.get_extractive_answers:\\n                extractive_content_spec = (\\n                    SearchRequest.ContentSearchSpec.ExtractiveContentSpec(\\n                        max_extractive_answer_count=self.max_extractive_answer_count,\\n                    )\\n                )\\n            else:\\n                extractive_content_spec = (\\n                    SearchRequest.ContentSearchSpec.ExtractiveContentSpec(\\n                        max_extractive_segment_count=self.max_extractive_segment_count,\\n                    )\\n                )\\n            content_search_spec = SearchRequest.ContentSearchSpec(\\n                extractive_content_spec=extractive_content_spec\\n            )\\n        elif self.engine_data_type == 1:\\n            content_search_spec = None\\n        elif self.engine_data_type == 2:\\n            content_search_spec = SearchRequest.ContentSearchSpec(\\n                extractive_content_spec=SearchRequest.ContentSearchSpec.ExtractiveContentSpec(\\n                    max_extractive_answer_count=self.max_extractive_answer_count,\\n                )\\n            )\\n        else:\\n            raise NotImplementedError(\\n                \"Only data store type 0 (Unstructured), 1 (Structured),\"\\n                \"or 2 (Website with Advanced Indexing) are supported currently.\"\\n                + f\" Got {self.engine_data_type}\"\\n            )\\n\\n        return SearchRequest(\\n            query=query,\\n            filter=self.filter,\\n            serving_config=self._serving_config,\\n            page_size=self.max_documents,\\n            content_search_spec=content_search_spec,\\n            query_expansion_spec=query_expansion_spec,\\n            spell_correction_spec=spell_correction_spec,\\n        )',\n",
       "   'd': 'Add a list of texts to the Elasticsearch index.\\n\\nArgs:\\n    texts (Iterable[str]): The texts to add to the index.\\n    metadatas (List[Dict[Any, Any]], optional): A list of metadata dictionaries\\n        to associate with the texts.\\n    model_id (str, optional): The ID of the model to use for transforming the\\n        texts into vectors.\\n    refresh_indices (bool, optional): Whether to refresh the Elasticsearch\\n        indices after adding the texts.\\n    **kwargs: Arbitrary keyword arguments.\\n\\nReturns:\\n    A list of IDs for the added texts.',\n",
       "   'l': False,\n",
       "   'g': ['(float) -> float',\n",
       "    '.kde.evaluate(xs)',\n",
       "    'for each x in xs, return the density at that point.',\n",
       "    '(xs) -> density']},\n",
       "  {'c': '    def __add__(self, other: Any) -> ChatPromptTemplate:\\n        if isinstance(other, ChatPromptTemplate):\\n            return ChatPromptTemplate(messages=self.messages + other.messages)\\n        elif isinstance(\\n            other, (BaseMessagePromptTemplate, BaseMessage, BaseChatPromptTemplate)\\n        ):\\n            return ChatPromptTemplate(messages=self.messages + [other])\\n        elif isinstance(other, (list, tuple)):\\n            _other = ChatPromptTemplate.from_messages(other)\\n            return ChatPromptTemplate(messages=self.messages + _other.messages)\\n        elif isinstance(other, str):\\n            prompt = HumanMessagePromptTemplate.from_template(other)\\n            return ChatPromptTemplate(messages=self.messages + [prompt])\\n        else:\\n            raise NotImplementedError(f\"Unsupported operand type for +: {type(other)}\")',\n",
       "   'd': 'Combine two prompt templates.\\n\\nArgs:\\n    other: Another prompt template.\\n\\nReturns:\\n    Combined prompt template.',\n",
       "   'l': True,\n",
       "   'g': ['Save sample images',\n",
       "    'Generates sample images from generators and save them as images',\n",
       "    'Saves a generated sample from the validation set\\n\\n    :param batches_done: integer, number of batches done so far\\n    :return:',\n",
       "    'Saves a generated sample from the validation set\\n\\n    Parameters:\\n    batches_done (int) -- current batch number']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        path: Union[str, Path],\\n        encoding: Optional[str] = None,\\n        errors: Optional[str] = None,\\n        custom_html_tag: Optional[Tuple[str, dict]] = None,\\n        **kwargs: Optional[Any]\\n    ):\\n        try:\\n            from bs4 import BeautifulSoup\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import python packages. \"\\n                \"Please install it with `pip install beautifulsoup4`. \"\\n            )\\n\\n        try:\\n            _ = BeautifulSoup(\\n                \"<html><body>Parser builder library test.</body></html>\", **kwargs\\n            )\\n        except Exception as e:\\n            raise ValueError(\"Parsing kwargs do not appear valid\") from e\\n\\n        self.file_path = Path(path)\\n        self.encoding = encoding\\n        self.errors = errors\\n        self.custom_html_tag = custom_html_tag\\n        self.bs_kwargs = kwargs',\n",
       "   'd': 'Update cache based on prompt and llm_string.',\n",
       "   'l': False,\n",
       "   'g': ['.', '.', '.', 'Parse and unwrap a code string.']},\n",
       "  {'c': '    def run(self, query: str) -> str:\\n        res = self.lambda_client.invoke(\\n            FunctionName=self.function_name,\\n            InvocationType=\"RequestResponse\",\\n            Payload=json.dumps({\"body\": query}),\\n        )\\n\\n        try:\\n            payload_stream = res[\"Payload\"]\\n            payload_string = payload_stream.read().decode(\"utf-8\")\\n            answer = json.loads(payload_string)[\"body\"]\\n\\n        except StopIteration:\\n            return \"Failed to parse response from Lambda\"\\n\\n        if answer is None or answer == \"\":\\n\\n            return \"Request failed.\"\\n        else:\\n            return f\"Result: {answer}\"',\n",
       "   'd': 'Add documents to vectorstore.',\n",
       "   'l': False,\n",
       "   'g': [\"Returns a list of all the floating point data types. \\n    def int_dtypes(endianness='?', sizes=(16, 32, 64)):\",\n",
       "    'Return a list of floating point data types.',\n",
       "    \"Floating point types.\\n\\n    Parameters\\n    ----------\\n    endianness : {'?', 'little', 'big'}\\n        Endianness of the data.\\n    sizes : tuple\\n        Sizes of the data.\\n\\n    Returns\\n    -------\\n    dict\\n        Dictionary of floating point types.\",\n",
       "    'Return a dictionary of floating point dtypes.']},\n",
       "  {'c': '    def plan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[AgentAction, AgentFinish]:\\n        full_inputs = self.get_full_inputs(intermediate_steps, **kwargs)\\n        full_output = self.llm_chain.predict(callbacks=callbacks, **full_inputs)\\n        return self.output_parser.parse(full_output)',\n",
       "   'd': 'Type of the Message, used for serialization.',\n",
       "   'l': False,\n",
       "   'g': ['.', '.', '.', '.']},\n",
       "  {'c': '    def load(self) -> List[Document]:\\n        from pdfminer.high_level import extract_text_to_fp\\n        from pdfminer.layout import LAParams\\n        from pdfminer.utils import open_filename\\n\\n        output_string = StringIO()\\n        with open_filename(self.file_path, \"rb\") as fp:\\n            extract_text_to_fp(\\n                fp,\\n                output_string,\\n                codec=\"\",\\n                laparams=LAParams(),\\n                output_type=\"html\",\\n            )\\n        metadata = {\"source\": self.file_path}\\n        return [Document(page_content=output_string.getvalue(), metadata=metadata)]',\n",
       "   'd': 'Load file.',\n",
       "   'l': True,\n",
       "   'g': ['for the organization admin role, the user can add surveys',\n",
       "    '.\\n    Check if the user has permission to add an organization.',\n",
       "    'for the organization',\n",
       "    '.\\n    Check if user can add organization']},\n",
       "  {'c': 'def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n    out_vectors = []\\n    for text in texts:\\n        if text not in self.known_texts:\\n            self.known_texts.append(text)\\n        vector = [float(1.0)] * (self.dimensionality - 1) + [\\n            float(self.known_texts.index(text))\\n        ]\\n        out_vectors.append(vector)\\n    return out_vectors',\n",
       "   'd': 'Return consistent embeddings for each text seen so far.',\n",
       "   'l': True,\n",
       "   'g': ['Returns the maximum sequence length.',\n",
       "    'Returns the maximum sequence length',\n",
       "    'Return the maximum sequence length for the model.',\n",
       "    'Returns the maximum sequence length in the dataset.']},\n",
       "  {'c': 'def _run_on_examples(\\n    client: Client,\\n    examples: Iterator[Example],\\n    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,\\n    *,\\n    evaluation: Optional[RunEvalConfig] = None,\\n    project_name: Optional[str] = None,\\n    verbose: bool = False,\\n    tags: Optional[List[str]] = None,\\n    input_mapper: Optional[Callable[[Dict], Any]] = None,\\n    data_type: DataType = DataType.kv,\\n) -> Dict[str, Any]:\\n    results: Dict[str, Any] = {}\\n    wrapped_model = _wrap_in_chain_factory(llm_or_chain_factory)\\n    project_name = _get_project_name(project_name, wrapped_model)\\n    tracer = LangChainTracer(\\n        project_name=project_name, client=client, use_threading=False\\n    )\\n    run_evaluators, examples = _setup_evaluation(\\n        wrapped_model, examples, evaluation, data_type\\n    )\\n    examples = _validate_example_inputs(examples, wrapped_model, input_mapper)\\n    evalution_handler = EvaluatorCallbackHandler(\\n        evaluators=run_evaluators or [],\\n        client=client,\\n    )\\n    callbacks: List[BaseCallbackHandler] = [tracer, evalution_handler]\\n    for i, example in enumerate(examples):\\n        result = _run_llm_or_chain(\\n            example,\\n            wrapped_model,\\n            tags=tags,\\n            callbacks=callbacks,\\n            input_mapper=input_mapper,\\n        )\\n        if verbose:\\n            print(f\"{i+1} processed\", flush=True, end=\"\\\\r\")\\n        results[str(example.id)] = result\\n    tracer.wait_for_futures()\\n    evalution_handler.wait_for_futures()\\n    return results',\n",
       "   'd': \"Run the Chain or language model on examples and store\\ntraces to the specified project name.\\n\\nArgs:\\n    client: LangSmith client to use to log feedback and runs.\\n    examples: Examples to run the model or chain over.\\n    llm_or_chain_factory: Language model or Chain constructor to run\\n        over the dataset. The Chain constructor is used to permit\\n        independent calls on each example without carrying over state.\\n    evaluation: Optional evaluation configuration to use when evaluating\\n    project_name: Name of the project to store the traces in.\\n        Defaults to {dataset_name}-{chain class name}-{datetime}.\\n    verbose: Whether to print progress.\\n    tags: Tags to add to each run in the project.\\n    input_mapper: A function to map to the inputs dictionary from an Example\\n        to the format expected by the model to be evaluated. This is useful if\\n        your model needs to deserialize more complex schema or if your dataset\\n        has inputs with keys that differ from what is expected by your chain\\n        or agent.\\n    data_type: The dataset's data type. This is used to determine determine\\n        how to deserialize the reference data and model compatibility.\\n\\nReturns:\\n    A dictionary mapping example ids to the model outputs.\",\n",
       "   'l': True,\n",
       "   'g': ['.', '.', '.', '.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        session: Optional[CassandraSession],\\n        keyspace: Optional[str],\\n        embedding: Embeddings,\\n        table_name: str = CASSANDRA_SEMANTIC_CACHE_DEFAULT_TABLE_NAME,\\n        distance_metric: str = CASSANDRA_SEMANTIC_CACHE_DEFAULT_DISTANCE_METRIC,\\n        score_threshold: float = CASSANDRA_SEMANTIC_CACHE_DEFAULT_SCORE_THRESHOLD,\\n        ttl_seconds: Optional[int] = CASSANDRA_SEMANTIC_CACHE_DEFAULT_TTL_SECONDS,\\n        skip_provisioning: bool = False,\\n    ):\\n        try:\\n            from cassio.table import MetadataVectorCassandraTable\\n        except (ImportError, ModuleNotFoundError):\\n            raise ValueError(\\n                \"Could not import cassio python package. \"\\n                \"Please install it with `pip install cassio`.\"\\n            )\\n        self.session = session\\n        self.keyspace = keyspace\\n        self.embedding = embedding\\n        self.table_name = table_name\\n        self.distance_metric = distance_metric\\n        self.score_threshold = score_threshold\\n        self.ttl_seconds = ttl_seconds\\n\\n\\n\\n\\n\\n\\n        @lru_cache(maxsize=CASSANDRA_SEMANTIC_CACHE_EMBEDDING_CACHE_SIZE)\\n        def _cache_embedding(text: str) -> List[float]:\\n            return self.embedding.embed_query(text=text)\\n\\n        self._get_embedding = _cache_embedding\\n        self.embedding_dimension = self._get_embedding_dimension()\\n\\n        self.table = MetadataVectorCassandraTable(\\n            session=self.session,\\n            keyspace=self.keyspace,\\n            table=self.table_name,\\n            primary_key_type=[\"TEXT\"],\\n            vector_dimension=self.embedding_dimension,\\n            ttl_seconds=self.ttl_seconds,\\n            metadata_indexing=(\"allow\", {\"_llm_string_hash\"}),\\n            skip_provisioning=skip_provisioning,\\n        )',\n",
       "   'd': \"Initialize the cache with all relevant parameters.\\nArgs:\\n    session (cassandra.cluster.Session): an open Cassandra session\\n    keyspace (str): the keyspace to use for storing the cache\\n    embedding (Embedding): Embedding provider for semantic\\n        encoding and search.\\n    table_name (str): name of the Cassandra (vector) table\\n        to use as cache\\n    distance_metric (str, 'dot'): which measure to adopt for\\n        similarity searches\\n    score_threshold (optional float): numeric value to use as\\n        cutoff for the similarity searches\\n    ttl_seconds (optional int): time-to-live for cache entries\\n        (default: None, i.e. forever)\\nThe default score threshold is tuned to the default metric.\\nTune it carefully yourself if switching to another distance metric.\",\n",
       "   'l': True,\n",
       "   'g': ['.\\n\\n    Args:\\n        K (int): The number of metrics to average.\\n        device (str): The device to use for the average.',\n",
       "    '.',\n",
       "    '(optional) Set device to use for averaging.',\n",
       "    'callback for averaging metrics across multiple workers']},\n",
       "  {'c': '    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        return {\\n            \"score\": self.compute_metric(inputs[\"prediction\"], inputs[\"prediction_b\"])\\n        }',\n",
       "   'd': 'Load documents.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    'The constructor for the class.\\n    \\n    :param name: The name of the application.\\n    :param linux_command: The linux command to run.\\n    :param windows_command: The windows command to run.',\n",
       "    'Initialize the class with the following attributes:\\n    \\n    name: The name of the program.\\n    linux_command: The Linux command to run the program.\\n    windows_command: The Windows command to run the program.',\n",
       "    ':param name: The name of the command.\\n    :param linux_command: The linux command.\\n    :param windows_command: The windows command.']},\n",
       "  {'c': '    def embed_query(self, text: str) -> List[float]:\\n        instruction_pair = [self.query_instruction, text]\\n        embedding = self.client.encode([instruction_pair], **self.encode_kwargs)[0]\\n        return embedding.tolist()',\n",
       "   'd': 'Compute query embeddings using a HuggingFace instruct model.\\n\\nArgs:\\n    text: The text to embed.\\n\\nReturns:\\n    Embeddings for the text.',\n",
       "   'l': True,\n",
       "   'g': ['Get all unarchived folders',\n",
       "    'Get the unarchived folders from the snapshots.\\n    \\n    :param snapshots: The snapshots to get the unarchived folders from.\\n    :param out_dir: The output directory.\\n    :return: The unarchived folders.',\n",
       "    'Get all unarchived folders from the snapshots.',\n",
       "    'Get unarchived folders from snapshots.\\n    \\n    Args:\\n        snapshots (Snapshots): Snapshots to get unarchived folders from.\\n        out_dir (Path, optional): Output directory. Defaults to OUTPUT_DIR.\\n    \\n    Returns:\\n        Dict[str, Optional[Link]]: Unarchived folders.']},\n",
       "  {'c': 'def _run(\\n    self,\\n    query: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> str:\\n    return self.db.run_no_throw(query)',\n",
       "   'd': 'Execute the query, return the results or an error message.',\n",
       "   'l': True,\n",
       "   'g': ['KL divergence between two multivariate normal distributions. \\n    \\n    :param mean1: mean of the first distribution\\n    :param logvar1: log variance of the first distribution\\n    :param mean2: mean of the second distribution\\n    :param logvar2: log variance of the second distribution\\n    \\n    :return: KL divergence between the two distributions',\n",
       "    'KL divergence between two multivariate Gaussians',\n",
       "    'Calculate the normal KL divergence between two distributions.\\n    \\n    Args:\\n        mean1 (th.Tensor): Mean of the first distribution.\\n        logvar1 (th.Tensor): Log variance of the first distribution.\\n        mean2 (th.Tensor): Mean of the second distribution.\\n        logvar2 (th.Tensor): Log variance of the second distribution.\\n    \\n    Returns:\\n        th.Tensor: The normal KL divergence between the two distributions.',\n",
       "    'KL divergence between two normal distributions.\\n    \\n    :param mean1: mean of the first distribution.\\n    :param logvar1: log variance of the first distribution.\\n    :param mean2: mean of the second distribution.\\n    :param logvar2: log variance of the second distribution.\\n    :return: KL divergence between two normal distributions.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n    ) -> None:\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index',\n",
       "   'd': 'Parse a single string model output into some structure.\\n\\nArgs:\\n    text: String output of a language model.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': False,\n",
       "   'g': ['(Tensor) -> Tensor',\n",
       "    '(Tensor) -> Tensor\\n    Performs a forward pass through the network.\\n\\n    Args:\\n        tensor_input (Tensor): Input tensor to be passed through the network.\\n\\n    Returns:\\n        Tensor: Output tensor produced as a result of applying the network to the input.',\n",
       "    '(Tensor) -> Tensor\\n\\n    Performs a forward pass through the network.\\n\\n    Args:\\n        tensor_input (Tensor): Input tensor to the network\\n\\n    Returns:\\n        Tensor: Output of the network',\n",
       "    '.\\n\\n    Args:\\n        tensor_input (torch.Tensor): Input tensor.\\n\\n    Returns:\\n        torch.Tensor: Output tensor.']},\n",
       "  {'c': 'def refresh_schema(self) -> None:\\n    self.schema = (\\n        f\"Node properties: {self.query(node_properties_query)}\\\\n\"\\n        f\"Relationships properties: {self.query(rel_properties_query)}\\\\n\"\\n        f\"Relationships: {self.query(rel_query)}\\\\n\"\\n    )',\n",
       "   'd': 'Refreshes the schema of the FalkorDB database',\n",
       "   'l': True,\n",
       "   'g': ['Create a new instance of the class from a dictionary mapping.\\n    \\n    Parameters:\\n    - cls (class): The class to create the instance from.\\n    - mapping (dict): A dictionary mapping keys to values.\\n    \\n    Returns:\\n    - Any: A new instance of the class.',\n",
       "    '',\n",
       "    'Create a new object from a mapping.',\n",
       "    'Create a new instance of a class from a mapping of attributes.\\n    \\n    :param cls: The class to create the instance of.\\n    :param mapping: A mapping of attributes to set on the instance.\\n    :return: A new instance of the class with the specified attributes set.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        file_path: Union[str, List[str]] = \"\",\\n        mode: str = \"single\",\\n        url: str = \"https://api.unstructured.io/general/v0/general\",\\n        api_key: str = \"\",\\n        **unstructured_kwargs: Any,\\n    ):\\n        if isinstance(file_path, str):\\n            validate_unstructured_version(min_unstructured_version=\"0.6.2\")\\n        else:\\n            validate_unstructured_version(min_unstructured_version=\"0.6.3\")\\n\\n        self.url = url\\n        self.api_key = api_key\\n\\n        super().__init__(file_path=file_path, mode=mode, **unstructured_kwargs)',\n",
       "   'd': 'Initialize with file path.',\n",
       "   'l': True,\n",
       "   'g': ['.',\n",
       "    '.',\n",
       "    '.',\n",
       "    '.\\n    Returns:\\n      dm_env.TimeStep: The resulting time step.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        session: Optional[CassandraSession],\\n        keyspace: Optional[str],\\n        embedding: Embeddings,\\n        table_name: str = CASSANDRA_SEMANTIC_CACHE_DEFAULT_TABLE_NAME,\\n        distance_metric: str = CASSANDRA_SEMANTIC_CACHE_DEFAULT_DISTANCE_METRIC,\\n        score_threshold: float = CASSANDRA_SEMANTIC_CACHE_DEFAULT_SCORE_THRESHOLD,\\n        ttl_seconds: Optional[int] = CASSANDRA_SEMANTIC_CACHE_DEFAULT_TTL_SECONDS,\\n        skip_provisioning: bool = False,\\n    ):\\n        try:\\n            from cassio.table import MetadataVectorCassandraTable\\n        except (ImportError, ModuleNotFoundError):\\n            raise ValueError(\\n                \"Could not import cassio python package. \"\\n                \"Please install it with `pip install cassio`.\"\\n            )\\n        self.session = session\\n        self.keyspace = keyspace\\n        self.embedding = embedding\\n        self.table_name = table_name\\n        self.distance_metric = distance_metric\\n        self.score_threshold = score_threshold\\n        self.ttl_seconds = ttl_seconds\\n\\n\\n\\n\\n\\n\\n        @lru_cache(maxsize=CASSANDRA_SEMANTIC_CACHE_EMBEDDING_CACHE_SIZE)\\n        def _cache_embedding(text: str) -> List[float]:\\n            return self.embedding.embed_query(text=text)\\n\\n        self._get_embedding = _cache_embedding\\n        self.embedding_dimension = self._get_embedding_dimension()\\n\\n        self.table = MetadataVectorCassandraTable(\\n            session=self.session,\\n            keyspace=self.keyspace,\\n            table=self.table_name,\\n            primary_key_type=[\"TEXT\"],\\n            vector_dimension=self.embedding_dimension,\\n            ttl_seconds=self.ttl_seconds,\\n            metadata_indexing=(\"allow\", {\"_llm_string_hash\"}),\\n            skip_provisioning=skip_provisioning,\\n        )',\n",
       "   'd': \"Initialize the cache with all relevant parameters.\\nArgs:\\n    session (cassandra.cluster.Session): an open Cassandra session\\n    keyspace (str): the keyspace to use for storing the cache\\n    embedding (Embedding): Embedding provider for semantic\\n        encoding and search.\\n    table_name (str): name of the Cassandra (vector) table\\n        to use as cache\\n    distance_metric (str, 'dot'): which measure to adopt for\\n        similarity searches\\n    score_threshold (optional float): numeric value to use as\\n        cutoff for the similarity searches\\n    ttl_seconds (optional int): time-to-live for cache entries\\n        (default: None, i.e. forever)\\nThe default score threshold is tuned to the default metric.\\nTune it carefully yourself if switching to another distance metric.\",\n",
       "   'l': True,\n",
       "   'g': ['Args:\\n        weights: the weights to be regularized.\\n        name: the name of the operation.\\n    Returns:\\n        the regularization term.',\n",
       "    'Args:\\n        weights: the tensor to be regularized.\\n        name: the name of the regularization.\\n    \\n    Returns:\\n        the regularization.',\n",
       "    'Compute the maximum norm regularization term for weights.\\n    \\n    Args:\\n        weights: A `Tensor`.\\n        scale: A `Tensor`.\\n    \\n    Returns:\\n        A `Tensor`.',\n",
       "    'Args:\\n        weights: weights to be regularized\\n        name: name of the regularizer\\n    \\n    Returns:\\n        regularizer: regularization term']},\n",
       "  {'c': 'def map(self, run: Run) -> Dict[str, str]:\\n    if run.run_type != \"llm\":\\n        raise ValueError(\"LLM RunMapper only supports LLM runs.\")\\n    elif not run.outputs:\\n        if run.error:\\n            raise ValueError(\\n                f\"Cannot evaluate errored LLM run {run.id}: {run.error}\"\\n            )\\n        else:\\n            raise ValueError(\\n                f\"Run {run.id} has no outputs. Cannot evaluate this run.\"\\n            )\\n    else:\\n        try:\\n            inputs = self.serialize_inputs(run.inputs)\\n        except Exception as e:\\n            raise ValueError(\\n                f\"Could not parse LM input from run inputs {run.inputs}\"\\n            ) from e\\n        try:\\n            output_ = self.serialize_outputs(run.outputs)\\n        except Exception as e:\\n            raise ValueError(\\n                f\"Could not parse LM prediction from run outputs {run.outputs}\"\\n            ) from e\\n        return {\"input\": inputs, \"prediction\": output_}',\n",
       "   'd': 'Construct FAISS wrapper from raw documents.\\n\\nThis is a user friendly interface that:\\n    1. Embeds documents.\\n    2. Creates an in memory docstore\\n    3. Initializes the FAISS database\\n\\nThis is intended to be a quick way to get started.\\n\\nExample:\\n    .. code-block:: python\\n\\n        from langchain.vectorstores import FAISS\\n        from langchain.embeddings import OpenAIEmbeddings\\n\\n        embeddings = OpenAIEmbeddings()\\n        faiss = FAISS.from_texts(texts, embeddings)',\n",
       "   'l': False,\n",
       "   'g': ['Test for check_success',\n",
       "    '_check_success_test_case_',\n",
       "    'This function is a test function for the check function\\n    \\n    :param generic_target: GenericTarget object\\n    :type generic_target: GenericTarget\\n    :return: True if the check function is working correctly\\n    :rtype: bool',\n",
       "    'Check if target is vulnerable']},\n",
       "  {'c': '    def process_pages(\\n        self,\\n        pages: List[dict],\\n        include_restricted_content: bool,\\n        include_attachments: bool,\\n        include_comments: bool,\\n        content_format: ContentFormat,\\n        ocr_languages: Optional[str] = None,\\n        keep_markdown_format: Optional[bool] = False,\\n    ) -> List[Document]:\\n        docs = []\\n        for page in pages:\\n            if not include_restricted_content and not self.is_public_page(page):\\n                continue\\n            doc = self.process_page(\\n                page,\\n                include_attachments,\\n                include_comments,\\n                content_format,\\n                ocr_languages,\\n                keep_markdown_format,\\n            )\\n            docs.append(doc)\\n\\n        return docs',\n",
       "   'd': 'Process a list of pages into a list of documents.',\n",
       "   'l': True,\n",
       "   'g': ['_get__',\n",
       "    'Returns the value of a key in the dictionary or a default value if the key is not found.',\n",
       "    'Returns the value of the given key in the object. \\n    If the key is not found, returns the default value of the object.',\n",
       "    'get the value of k in the dict or the default value if it does not exist']},\n",
       "  {'c': 'def test_does_not_allow_extra_kwargs() -> None:\\n    template = \"This is a {foo} test.\"\\n    with pytest.raises(KeyError):\\n        formatter.format(template, foo=\"good\", bar=\"oops\")',\n",
       "   'd': 'Load documents.',\n",
       "   'l': False,\n",
       "   'g': ['(self, number) -> Window\\n\\n    Returns the window with the given number.',\n",
       "    'for windows',\n",
       "    '(self, number) -> Window\\n\\n    Returns the window with the given number.',\n",
       "    '(int) -> Window']},\n",
       "  {'c': 'def _run(\\n    self,\\n    *args: Any,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n    **kwargs: Any,\\n) -> Any:\\n    new_argument_supported = signature(self.func).parameters.get(\"callbacks\")\\n    return (\\n        self.func(\\n            *args,\\n            callbacks=run_manager.get_child() if run_manager else None,\\n            **kwargs,\\n        )\\n        if new_argument_supported\\n        else self.func(*args, **kwargs)\\n    )',\n",
       "   'd': 'Lazily parse the blob.',\n",
       "   'l': False,\n",
       "   'g': ['Returns a + b',\n",
       "    'This function does something',\n",
       "    '',\n",
       "    'This function multiplies two numbers\\n    \\n    Args:\\n        a (int): first number\\n        b (int): second number\\n        \\n    Returns:\\n        int: multiplication of a and b']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        openai_api_key = get_from_dict_or_env(\\n            values, \"openai_api_key\", \"OPENAI_API_KEY\"\\n        )\\n        openai_api_base = get_from_dict_or_env(\\n            values,\\n            \"openai_api_base\",\\n            \"OPENAI_API_BASE\",\\n            default=\"\",\\n        )\\n        openai_proxy = get_from_dict_or_env(\\n            values,\\n            \"openai_proxy\",\\n            \"OPENAI_PROXY\",\\n            default=\"\",\\n        )\\n        openai_organization = get_from_dict_or_env(\\n            values, \"openai_organization\", \"OPENAI_ORGANIZATION\", default=\"\"\\n        )\\n        try:\\n            import openai\\n\\n            openai.api_key = openai_api_key\\n            if openai_api_base:\\n                openai.api_base = openai_api_base\\n            if openai_organization:\\n                openai.organization = openai_organization\\n            if openai_proxy:\\n                openai.proxy = {\"http\": openai_proxy, \"https\": openai_proxy}\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import openai python package. \"\\n                \"Please install it with `pip install openai`.\"\\n            )\\n        try:\\n            values[\"client\"] = openai.ChatCompletion\\n        except AttributeError:\\n            raise ValueError(\\n                \"`openai` has no `ChatCompletion` attribute, this is likely \"\\n                \"due to an old version of the openai package. Try upgrading it \"\\n                \"with `pip install --upgrade openai`.\"\\n            )\\n        warnings.warn(\\n            \"You are trying to use a chat model. This way of initializing it is \"\\n            \"no longer supported. Instead, please use: \"\\n            \"`from langchain.chat_models import ChatOpenAI`\"\\n        )\\n        return values',\n",
       "   'd': 'Add a list of texts to the Elasticsearch index.\\n\\nArgs:\\n    texts (Iterable[str]): The texts to add to the index.\\n    metadatas (List[Dict[Any, Any]], optional): A list of metadata dictionaries\\n        to associate with the texts.\\n    model_id (str, optional): The ID of the model to use for transforming the\\n        texts into vectors.\\n    refresh_indices (bool, optional): Whether to refresh the Elasticsearch\\n        indices after adding the texts.\\n    **kwargs: Arbitrary keyword arguments.\\n\\nReturns:\\n    A list of IDs for the added texts.',\n",
       "   'l': False,\n",
       "   'g': [\"'Test post'\", \"'test_post'\", '', '::test:: test_post']},\n",
       "  {'c': '    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        parser: BaseOutputParser[T],\\n        prompt: BasePromptTemplate = NAIVE_FIX_PROMPT,\\n    ) -> OutputFixingParser[T]:\\n        from langchain.chains.llm import LLMChain\\n\\n        chain = LLMChain(llm=llm, prompt=prompt)\\n        return cls(parser=parser, retry_chain=chain)',\n",
       "   'd': 'PATCH the URL and return the text.',\n",
       "   'l': False,\n",
       "   'g': ['.', 'Get the header for an animated image.', '', '.']},\n",
       "  {'c': 'def __init__(\\n    self,\\n    file_path: Union[str, List[str]],\\n    mode: str = \"single\",\\n    **unstructured_kwargs: Any,\\n):\\n    self.file_path = file_path\\n    super().__init__(mode=mode, **unstructured_kwargs)',\n",
       "   'd': 'Select which examples to use based on semantic similarity.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '_test_upgrade_packages_option_no_existing_file_\\n\\n    Test that upgrade packages option works with no existing file.',\n",
       "    'Test that the upgrade packages option works with no existing file',\n",
       "    '_upgrade_packages_option_no_existing_file_']},\n",
       "  {'c': '    def get_input_schema(\\n        self, config: Optional[RunnableConfig] = None\\n    ) -> Type[BaseModel]:\\n        func = getattr(self, \"func\", None) or getattr(self, \"afunc\")\\n\\n        if isinstance(func, itemgetter):\\n\\n\\n            items = str(func).replace(\"operator.itemgetter(\", \"\")[:-1].split(\", \")\\n            if all(\\n                item[0] == \"\\'\" and item[-1] == \"\\'\" and len(item) > 2 for item in items\\n            ):\\n\\n                return create_model(\\n                    \"RunnableLambdaInput\",\\n                    **{item[1:-1]: (Any, None) for item in items},\\n                )\\n            else:\\n                return create_model(\"RunnableLambdaInput\", __root__=(List[Any], None))\\n\\n        if self.InputType != Any:\\n            return super().get_input_schema(config)\\n\\n        if dict_keys := get_function_first_arg_dict_keys(func):\\n            return create_model(\\n                \"RunnableLambdaInput\",\\n                **{key: (Any, None) for key in dict_keys},\\n            )\\n\\n        return super().get_input_schema(config)',\n",
       "   'd': 'Run the tool.',\n",
       "   'l': False,\n",
       "   'g': ['Stop all jobs in all sessions.',\n",
       "    'Stop all jobs',\n",
       "    'Stop all jobs',\n",
       "    '_stop_all_jobs\\n    Stop all jobs in all sessions.']},\n",
       "  {'c': '    def batch(\\n        self,\\n        inputs: List[Input],\\n        config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,\\n        **kwargs: Optional[Any],\\n    ) -> List[Output]:\\n        configs = self._get_config_list(config, len(inputs))\\n\\n\\n        if len(inputs) == 1:\\n            return [self.invoke(inputs[0], configs[0], **kwargs)]\\n\\n        with get_executor_for_config(configs[0]) as executor:\\n            return list(\\n                executor.map(\\n                    partial(self.invoke, **kwargs),\\n                    inputs,\\n                    (patch_config(c, executor=executor) for c in configs),\\n                )\\n            )',\n",
       "   'd': 'Default implementation of batch, which calls invoke N times.\\nSubclasses should override this method if they can batch more efficiently.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Constructor for PatchMatch object.',\n",
       "    '.Diff_Timeout = 1.0\\n\\n    self.Diff_EditCost = 4\\n\\n    self.Match_Threshold = 0.5\\n\\n    self.Match_Distance = 1000\\n\\n    self.Patch_DeleteThreshold = 0.5\\n\\n    self.Patch_Margin = 4\\n\\n    self.Match_MaxBits = 32',\n",
       "    '.',\n",
       "    '()\\n\\n    Initialize the DiffMatchPatch object.']},\n",
       "  {'c': '    def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n        from momento.responses import CacheGet\\n\\n        generations: RETURN_VAL_TYPE = []\\n\\n        get_response = self.cache_client.get(\\n            self.cache_name, self.__key(prompt, llm_string)\\n        )\\n        if isinstance(get_response, CacheGet.Hit):\\n            value = get_response.value_string\\n            generations = _load_generations_from_json(value)\\n        elif isinstance(get_response, CacheGet.Miss):\\n            pass\\n        elif isinstance(get_response, CacheGet.Error):\\n            raise get_response.inner_exception\\n        return generations if generations else None',\n",
       "   'd': 'Call out to an AzureML Managed Online endpoint.\\nArgs:\\n    prompt: The prompt to pass into the model.\\n    stop: Optional list of stop words to use when generating.\\nReturns:\\n    The string generated by the model.\\nExample:\\n    .. code-block:: python\\n        response = azureml_model(\"Tell me a joke.\")',\n",
       "   'l': False,\n",
       "   'g': ['for result',\n",
       "    '.\\n    Initialize the result object.',\n",
       "    '.\\n\\n    :param kwargs:\\n    :return:',\n",
       "    '.\\n    Initialize the result.']},\n",
       "  {'c': '    def similarity_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        pre_filter: Optional[dict] = None,\\n        post_filter_pipeline: Optional[List[Dict]] = None,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        docs_and_scores = self.similarity_search_with_score(\\n            query,\\n            k=k,\\n            pre_filter=pre_filter,\\n            post_filter_pipeline=post_filter_pipeline,\\n        )\\n        return [doc for doc, _ in docs_and_scores]',\n",
       "   'd': 'Use the tool.',\n",
       "   'l': False,\n",
       "   'g': ['Set the output keys for all datasets.\\n    \\n    Parameters\\n    ----------\\n    datasets : list of :class:`~datasets.Dataset`\\n        The datasets to set the output keys for.\\n    output_keys : list of str\\n        The output keys to set for the datasets.',\n",
       "    'Set the output keys for each dataset.\\n    \\n    Parameters\\n    ----------\\n    datasets : list of Dataset objects\\n        The datasets to set the output keys for.\\n    output_keys : list of str\\n        The output keys to set for each dataset.\\n    \\n    Returns\\n    -------\\n    None',\n",
       "    'Set the output keys for all datasets in the list.\\n    \\n    Parameters\\n    ----------\\n    datasets : list of Dataset objects\\n        The list of datasets to set the output keys for.\\n    output_keys : list of str\\n        The list of output keys to set.\\n    \\n    Returns\\n    -------\\n    None',\n",
       "    'Set output keys for all datasets. \\n    \\n    Args:\\n        datasets: list of datasets\\n        output_keys: list of output keys']},\n",
       "  {'c': 'def _call(\\n    self,\\n    inputs: Dict[str, List[Document]],\\n    run_manager: Optional[CallbackManagerForChainRun] = None,\\n) -> Dict[str, str]:\\n    _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n    docs = inputs[self.input_key]\\n\\n    other_keys = {k: v for k, v in inputs.items() if k != self.input_key}\\n    output, extra_return_dict = self.combine_docs(\\n        docs, callbacks=_run_manager.get_child(), **other_keys\\n    )\\n    extra_return_dict[self.output_key] = output\\n    return extra_return_dict',\n",
       "   'd': 'Format the prompt with the inputs.\\n\\nArgs:\\n    **kwargs: Any arguments to be passed to the prompt template.\\n\\nReturns:\\n    A formatted string.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    prompt.format(variable1=\"foo\")',\n",
       "   'l': False,\n",
       "   'g': [':param color: The color to use for the color palette.\\n    :param opacity: The opacity to use for the color palette.\\n    :param color_lookup: The color lookup to use for the color palette.',\n",
       "    '',\n",
       "    ':param color:\\n    :param opacity:\\n    :param color_lookup:',\n",
       "    ':param color:\\n    :param opacity:\\n    :param color_lookup:']},\n",
       "  {'c': '    def on_chain_error(\\n        self,\\n        error: BaseException,\\n        **kwargs: Any,\\n    ) -> None:\\n        self.ended = True\\n        return self.parent_run_manager.on_chain_error(error, **kwargs)',\n",
       "   'd': 'Run when chain errors.\\n\\nArgs:\\n    error (Exception or KeyboardInterrupt): The error.',\n",
       "   'l': True,\n",
       "   'g': ['@summary: get\\n    @param fmt: \\n    @param offset: \\n    @return:',\n",
       "    'Gets a value from the database.\\n    \\n    Args:\\n        fmt (str): The format of the value to get.\\n        offset (int): The offset to get the value from.\\n    \\n    Returns:\\n        str: The value from the database.',\n",
       "    'Get the value of a variable at a given offset.',\n",
       "    'Adds a GET operation to the operations list.']},\n",
       "  {'c': 'def input_keys(self) -> List[str]:\\n    return [\"user_input\", \"context\", \"response\"]',\n",
       "   'd': \"Instantiate a prompt cache using Momento as a backend.\\n\\nNote: to instantiate the cache client passed to MomentoCache,\\nyou must have a Momento account. See https://gomomento.com/.\\n\\nArgs:\\n    cache_client (CacheClient): The Momento cache client.\\n    cache_name (str): The name of the cache to use to store the data.\\n    ttl (Optional[timedelta], optional): The time to live for the cache items.\\n        Defaults to None, ie use the client default TTL.\\n    ensure_cache_exists (bool, optional): Create the cache if it doesn't\\n        exist. Defaults to True.\\n\\nRaises:\\n    ImportError: Momento python package is not installed.\\n    TypeError: cache_client is not of type momento.CacheClientObject\\n    ValueError: ttl is non-null and non-negative\",\n",
       "   'l': False,\n",
       "   'g': ['Removes all tasks that have been completed and are older than the time interval',\n",
       "    'Removes all tasks from the queue that have exceeded the rate limit.',\n",
       "    'Removes all tasks from the queue that are older than the limit.',\n",
       "    '']},\n",
       "  {'c': 'def test_instruct_prompt() -> None:\\n    llm = MosaicML(inject_instruction_format=True, model_kwargs={\"do_sample\": False})\\n    instruction = \"Repeat the word foo\"\\n    prompt = llm._transform_prompt(instruction)\\n    expected_prompt = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction)\\n    assert prompt == expected_prompt\\n    output = llm(prompt)\\n    assert isinstance(output, str)',\n",
       "   'd': \"Decorator to mark a function, a class, or a property as deprecated.\\n\\nWhen deprecating a classmethod, a staticmethod, or a property, the\\n``@deprecated`` decorator should go *under* ``@classmethod`` and\\n``@staticmethod`` (i.e., `deprecated` should directly decorate the\\nunderlying callable), but *over* ``@property``.\\n\\nWhen deprecating a class ``C`` intended to be used as a base class in a\\nmultiple inheritance hierarchy, ``C`` *must* define an ``__init__`` method\\n(if ``C`` instead inherited its ``__init__`` from its own base class, then\\n``@deprecated`` would mess up ``__init__`` inheritance when installing its\\nown (deprecation-emitting) ``C.__init__``).\\n\\nParameters are the same as for `warn_deprecated`, except that *obj_type*\\ndefaults to 'class' if decorating a class, 'attribute' if decorating a\\nproperty, and 'function' otherwise.\\n\\nArguments:\\n    since : str\\n        The release at which this API became deprecated.\\n    message : str, optional\\n        Override the default deprecation message. The %(since)s,\\n        %(name)s, %(alternative)s, %(obj_type)s, %(addendum)s,\\n        and %(removal)s format specifiers will be replaced by the\\n        values of the respective arguments passed to this function.\\n    name : str, optional\\n        The name of the deprecated object.\\n    alternative : str, optional\\n        An alternative API that the user may use in place of the\\n        deprecated API. The deprecation warning will tell the user\\n        about this alternative if provided.\\n    pending : bool, optional\\n        If True, uses a PendingDeprecationWarning instead of a\\n        DeprecationWarning. Cannot be used together with removal.\\n    obj_type : str, optional\\n        The object type being deprecated.\\n    addendum : str, optional\\n        Additional text appended directly to the final message.\\n    removal : str, optional\\n        The expected removal version. With the default (an empty\\n        string), a removal version is automatically computed from\\n        since. Set to other Falsy values to not schedule a removal\\n        date. Cannot be used together with pending.\\n\\nExamples\\n--------\\n\\n    .. code-block:: python\\n\\n        @deprecated('1.4.0')\\n        def the_function_to_deprecate():\\n            pass\",\n",
       "   'l': False,\n",
       "   'g': ['Enhanced connection to the database\\n    \\n    Parameters\\n    ----------\\n    session : requests.Session\\n        The requests session\\n    pool_connections : int\\n        The number of connections to the database\\n    pool_maxsize : int\\n        The maximum number of connections to the database\\n    max_retries : int\\n        The maximum number of retries to the database\\n    \\n    Returns\\n    -------\\n    requests.Session\\n        The enhanced session',\n",
       "    'This function will enhance the connection to the session object \\n    to handle the connection pooling \\n    \\n    Args:\\n        session (object): The session object \\n        pool_connections (int): The number of connections to the pool \\n        pool_maxsize (int): The maximum number of connections to the pool \\n        max_retries (int): The maximum number of retries to the pool \\n    \\n    Returns:\\n        None',\n",
       "    'This function is used to enhance the connection pool of requests. \\n    \\n    Parameters:\\n    session (requests.Session): The session object to be enhanced.\\n    pool_connections (int): The number of connections in the pool.\\n    pool_maxsize (int): The maximum number of connections in the pool.\\n    max_retries (int): The maximum number of retries for a failed request.\\n    \\n    Returns:\\n    None',\n",
       "    'Enhance the connection pool for the session. \\n    \\n    Parameters\\n    ----------\\n    session : requests.Session\\n        The session to enhance.\\n    pool_connections : int, default 20\\n        The number of connections in the pool.\\n    pool_maxsize : int, default 20\\n        The maximum number of connections in the pool.\\n    max_retries : int, default 30\\n        The maximum number of retries.']},\n",
       "  {'c': 'def test_vectara_add_documents() -> None:\\n    texts = [\"grounded generation\", \"retrieval augmented generation\", \"data privacy\"]\\n    docsearch: Vectara = Vectara.from_texts(\\n        texts,\\n        embedding=FakeEmbeddings(),\\n        metadatas=[\\n            {\"abbr\": \"gg\", \"test_num\": \"1\"},\\n            {\"abbr\": \"rag\", \"test_num\": \"1\"},\\n            {\"abbr\": \"dp\", \"test_num\": \"1\"},\\n        ],\\n        doc_metadata={\"test_num\": \"1\"},\\n    )\\n\\n\\n    new_texts = [\"large language model\", \"information retrieval\", \"question answering\"]\\n    docsearch.add_documents(\\n        [Document(page_content=t, metadata={\"abbr\": get_abbr(t)}) for t in new_texts],\\n        doc_metadata={\"test_num\": \"1\"},\\n    )\\n\\n\\n    output = docsearch.similarity_search(\\n        \"large language model\",\\n        k=2,\\n        n_sentence_context=0,\\n        filter=\"doc.test_num = 1\",\\n    )\\n    assert output[0].page_content == \"large language model\"\\n    assert output[0].metadata == {\"abbr\": \"llm\"}\\n    assert output[1].page_content == \"information retrieval\"\\n    assert output[1].metadata == {\"abbr\": \"ir\"}',\n",
       "   'd': 'Test end to end construction and search.',\n",
       "   'l': True,\n",
       "   'g': [\"'tag' is a function that takes a string and returns a list of\\n    tuples, where each tuple is a tag and its associated text.\\n\\n    'tokenize' is a boolean that determines whether to tokenize the text\\n    before tagging. If tokenize is True, the function will tokenize the text\\n    and return a list of tokens. If tokenize is False, the function will return\\n    a list of tuples where each tuple is a tag and its associated text.\\n\\n    The function returns a list of tuples where each tuple is a tag and its\\n    associated text. The tags are defined in the 'tags' variable.\\n\\n    The function returns a list of tuples where each tuple is a tag and its\\n    associated text. The tags are defined in the 'tags' variable.\\n\\n    The function returns a list of tuples where each tuple is a tag and its\\n    associated text. The tags are defined in the 'tags' variable.\\n\\n    The function returns a list of tuples where each tuple is a tag and its\\n    associated text. The tags are defined in the 'tags' variable.\\n\\n    The function returns a list of tuples where each tuple is a tag and its\\n    associated text. The tags are defined in the\",\n",
       "    '',\n",
       "    '',\n",
       "    ':param text: \\n    :param tokenize: \\n    :return:']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        client: marqo.Client,\\n        index_name: str,\\n        add_documents_settings: Optional[Dict[str, Any]] = None,\\n        searchable_attributes: Optional[List[str]] = None,\\n        page_content_builder: Optional[Callable[[Dict[str, Any]], str]] = None,\\n    ):\\n        try:\\n            import marqo\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import marqo python package. \"\\n                \"Please install it with `pip install marqo`.\"\\n            )\\n        if not isinstance(client, marqo.Client):\\n            raise ValueError(\\n                f\"client should be an instance of marqo.Client, got {type(client)}\"\\n            )\\n        self._client = client\\n        self._index_name = index_name\\n        self._add_documents_settings = (\\n            {} if add_documents_settings is None else add_documents_settings\\n        )\\n        self._searchable_attributes = searchable_attributes\\n        self.page_content_builder = page_content_builder\\n\\n        self.tensor_fields = [\"text\"]\\n\\n        self._document_batch_size = 1024',\n",
       "   'd': 'Helper method to transform an Iterator of Input values into an Iterator of\\nOutput values, with callbacks.\\nUse this to implement `stream()` or `transform()` in Runnable subclasses.',\n",
       "   'l': False,\n",
       "   'g': ['.',\n",
       "    '(self, cfg):\\n        \\n        @param cfg:\\n        @return:',\n",
       "    '(int) -> int',\n",
       "    '.']},\n",
       "  {'c': '    def run(self, command: str, fetch: str = \"all\") -> str:\\n        result = self._execute(command, fetch)\\n\\n\\n        if not result:\\n            return \"\"\\n        elif isinstance(result, list):\\n            res: Sequence = [\\n                tuple(truncate_word(c, length=self._max_string_length) for c in r)\\n                for r in result\\n            ]\\n        else:\\n            res = tuple(\\n                truncate_word(c, length=self._max_string_length) for c in result\\n            )\\n        return str(res)',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': False,\n",
       "   'g': ['_summary_\\n\\n    Returns:\\n        _type_: _description_',\n",
       "    '.\\n    Returns:\\n        Dict[str, Dict]: A dictionary of model information.',\n",
       "    '.\\n    Returns:\\n        Dict[str, Dict]:',\n",
       "    '.\\n    Returns:\\n        Dict[str, Dict]:']},\n",
       "  {'c': '    def similarity_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        embedding = self.embedding.embed_query(text=query)\\n        return self.similarity_search_by_vector(\\n            embedding=embedding,\\n            k=k,\\n        )',\n",
       "   'd': 'Run similarity search with Neo4jVector.\\n\\nArgs:\\n    query (str): Query text to search for.\\n    k (int): Number of results to return. Defaults to 4.\\n\\nReturns:\\n    List of Documents most similar to the query.',\n",
       "   'l': True,\n",
       "   'g': ['_read_tsv\\n\\n    Args:\\n        data_dir: The directory where the data files are stored.\\n\\n    Returns:\\n        list[Dict[str, str]]: List of data records.',\n",
       "    '.\\n\\n    Args:\\n      data_dir: Data directory.',\n",
       "    '_get_dev_examples.',\n",
       "    '.\\n\\n    Args:\\n      data_dir: data directory\\n\\n    Returns:\\n      examples: list of examples']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        dataset_path: str = _LANGCHAIN_DEFAULT_DEEPLAKE_PATH,\\n        token: Optional[str] = None,\\n        embedding: Optional[Embeddings] = None,\\n        embedding_function: Optional[Embeddings] = None,\\n        read_only: bool = False,\\n        ingestion_batch_size: int = 1000,\\n        num_workers: int = 0,\\n        verbose: bool = True,\\n        exec_option: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        self.ingestion_batch_size = ingestion_batch_size\\n        self.num_workers = num_workers\\n        self.verbose = verbose\\n\\n        if _DEEPLAKE_INSTALLED is False:\\n            raise ImportError(\\n                \"Could not import deeplake python package. \"\\n                \"Please install it with `pip install deeplake[enterprise]`.\"\\n            )\\n\\n        if (\\n            kwargs.get(\"runtime\") == {\"tensor_db\": True}\\n            and version_compare(deeplake.__version__, \"3.6.7\") == -1\\n        ):\\n            raise ImportError(\\n                \"To use tensor_db option you need to update deeplake to `3.6.7`. \"\\n                f\"Currently installed deeplake version is {deeplake.__version__}. \"\\n            )\\n\\n        self.dataset_path = dataset_path\\n\\n        if embedding_function:\\n            logger.warning(\\n                \"Using embedding function is deprecated and will be removed \"\\n                \"in the future. Please use embedding instead.\"\\n            )\\n\\n        self.vectorstore = DeepLakeVectorStore(\\n            path=self.dataset_path,\\n            embedding_function=embedding_function or embedding,\\n            read_only=read_only,\\n            token=token,\\n            exec_option=exec_option,\\n            verbose=verbose,\\n            **kwargs,\\n        )\\n\\n        self._embedding_function = embedding_function or embedding\\n        self._id_tensor_name = \"ids\" if \"ids\" in self.vectorstore.tensors() else \"id\"',\n",
       "   'd': 'Get documents relevant for a query.',\n",
       "   'l': False,\n",
       "   'g': ['.prepare(self, method=None, url=None, headers=None, files=None,\\n        data=None, params=None, auth=None, cookies=None, hooks=None, json=None)',\n",
       "    '.',\n",
       "    '(self, method=None, url=None, headers=None, files=None,\\n        data=None, params=None, auth=None, cookies=None, hooks=None, json=None)',\n",
       "    'Prepare the request for execution.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        init_func: Union[\\n            Callable[[Any, str], None], Callable[[Any], None], None\\n        ] = None,\\n    ):\\n        try:\\n            import gptcache\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import gptcache python package. \"\\n                \"Please install it with `pip install gptcache`.\"\\n            )\\n\\n        self.init_gptcache_func: Union[\\n            Callable[[Any, str], None], Callable[[Any], None], None\\n        ] = init_func\\n        self.gptcache_dict: Dict[str, Any] = {}',\n",
       "   'd': 'Initialize by passing in init function (default: `None`).\\n\\nArgs:\\n    init_func (Optional[Callable[[Any], None]]): init `GPTCache` function\\n    (default: `None`)\\n\\nExample:\\n.. code-block:: python\\n\\n    # Initialize GPTCache with a custom init function\\n    import gptcache\\n    from gptcache.processor.pre import get_prompt\\n    from gptcache.manager.factory import get_data_manager\\n\\n    # Avoid multiple caches using the same file,\\n    causing different llm model caches to affect each other\\n\\n    def init_gptcache(cache_obj: gptcache.Cache, llm str):\\n        cache_obj.init(\\n            pre_embedding_func=get_prompt,\\n            data_manager=manager_factory(\\n                manager=\"map\",\\n                data_dir=f\"map_cache_{llm}\"\\n            ),\\n        )\\n\\n    langchain.llm_cache = GPTCache(init_gptcache)',\n",
       "   'l': True,\n",
       "   'g': ['.',\n",
       "    '_process_all_forms(self, form):',\n",
       "    '_process_all_forms',\n",
       "    '_process_all_forms']},\n",
       "  {'c': 'def lazy_load(self) -> Iterator[Document]:\\n    if self.use_async:\\n        results = asyncio.run(self._async_get_child_links_recursive(self.url))\\n        if results is None:\\n            return iter([])\\n        else:\\n            return iter(results)\\n    else:\\n        return self._get_child_links_recursive(self.url)',\n",
       "   'd': 'Update cache.\\nFirst, retrieve the corresponding cache object using the `llm_string` parameter,\\nand then store the `prompt` and `return_val` in the cache object.',\n",
       "   'l': False,\n",
       "   'g': [\"for time_func:\\n    def fl_time(self, time_func, apply_to=['mask', 'audio'],\\n                keep_duration=False):\",\n",
       "    \".\\n    Returns a copy of the current audio with the time_func applied to the\\n    audio.\\n\\n    Parameters\\n    ----------\\n    time_func : callable\\n        The time function to apply to the audio.\\n    apply_to : list, optional\\n        The audio attributes to apply the time_func to. The default is\\n        ['mask', 'audio'].\\n    keep_duration : bool, optional\\n        If True, the duration of the audio is preserved. The default is\\n        False.\\n\\n    Returns\\n    -------\\n    audio : Audio\\n        The audio with the time_func applied to the audio.\",\n",
       "    '.\\n    \\n    Apply a function to all audio/mask channels of the audio/mask\\n    \\n    Parameters\\n    ----------\\n    time_func : callable\\n        Function to apply to all channels\\n    apply_to : list\\n        Channels to apply the function to\\n    keep_duration : bool\\n        Keep the duration of the audio\\n    \\n    Returns\\n    -------\\n    Audio\\n        Audio object with the modified channels',\n",
       "    \".fl_time(self, time_func, apply_to=['mask', 'audio'],\\n                keep_duration=False)\\n\\n    Applies time_func to all of the time-dependent attributes of self.\\n\\n    Parameters\\n    ----------\\n    time_func : function\\n        A function that takes a time argument and returns a time-dependent\\n        attribute.\\n    apply_to : list, optional\\n        A list of attributes to apply the time_func to. Defaults to\\n        ['mask', 'audio'].\\n    keep_duration : bool, optional\\n        Whether to keep the duration of the time-dependent attributes.\\n        Defaults to False.\\n\\n    Returns\\n    -------\\n    None.\"]},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        openai_api_key = get_from_dict_or_env(\\n            values, \"openai_api_key\", \"OPENAI_API_KEY\"\\n        )\\n        openai_api_base = get_from_dict_or_env(\\n            values,\\n            \"openai_api_base\",\\n            \"OPENAI_API_BASE\",\\n            default=\"\",\\n        )\\n        openai_proxy = get_from_dict_or_env(\\n            values,\\n            \"openai_proxy\",\\n            \"OPENAI_PROXY\",\\n            default=\"\",\\n        )\\n        openai_organization = get_from_dict_or_env(\\n            values, \"openai_organization\", \"OPENAI_ORGANIZATION\", default=\"\"\\n        )\\n        try:\\n            import openai\\n\\n            openai.api_key = openai_api_key\\n            if openai_api_base:\\n                openai.api_base = openai_api_base\\n            if openai_organization:\\n                openai.organization = openai_organization\\n            if openai_proxy:\\n                openai.proxy = {\"http\": openai_proxy, \"https\": openai_proxy}\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import openai python package. \"\\n                \"Please install it with `pip install openai`.\"\\n            )\\n        try:\\n            values[\"client\"] = openai.ChatCompletion\\n        except AttributeError:\\n            raise ValueError(\\n                \"`openai` has no `ChatCompletion` attribute, this is likely \"\\n                \"due to an old version of the openai package. Try upgrading it \"\\n                \"with `pip install --upgrade openai`.\"\\n            )\\n        warnings.warn(\\n            \"You are trying to use a chat model. This way of initializing it is \"\\n            \"no longer supported. Instead, please use: \"\\n            \"`from langchain.chat_models import ChatOpenAI`\"\\n        )\\n        return values',\n",
       "   'd': 'Clear semantic cache for a given llm_string.',\n",
       "   'l': False,\n",
       "   'g': ['Load the album data into the record.',\n",
       "    'Loads the current record.',\n",
       "    'Load the record from the library.',\n",
       "    'Load the current album from the library.']},\n",
       "  {'c': '    def load(self) -> List[Document]:\\n        parser = PDFPlumberParser(text_kwargs=self.text_kwargs, dedupe=self.dedupe)\\n        blob = Blob.from_path(self.file_path)\\n        return parser.parse(blob)',\n",
       "   'd': 'Generate the reference.rst file for each package.',\n",
       "   'l': False,\n",
       "   'g': ['date_string should be in the format of \"%Y-%m-%d %H:%M+%Z\"',\n",
       "    'date_string: string in format YYYY-MM-DD HH:MM+Z\\n    Returns datetime object',\n",
       "    'Parses a date string into a datetime object. \\n    \\n    Args:\\n        date_string (str): The date string to parse.\\n    \\n    Returns:\\n        datetime: The parsed datetime object.',\n",
       "    'Parses a date string in the format YYYY-MM-DD HH:MM+Z and returns a datetime object.\\n\\n    Args:\\n        date_string (str): The date string to parse.\\n\\n    Returns:\\n        datetime: The parsed datetime object.\\n\\n    Example:\\n        >>> parse_date(\"2023-05-10 14:30+00\")\\n        datetime.datetime(2023, 5, 10, 14, 30)']},\n",
       "  {'c': 'def try_load_from_hub(\\n    path: Union[str, Path],\\n    loader: Callable[[str], T],\\n    valid_prefix: str,\\n    valid_suffixes: Set[str],\\n    **kwargs: Any,\\n) -> Optional[T]:\\n    if not isinstance(path, str) or not (match := HUB_PATH_RE.match(path)):\\n        return None\\n    ref, remote_path_str = match.groups()\\n    ref = ref[1:] if ref else DEFAULT_REF\\n    remote_path = Path(remote_path_str)\\n    if remote_path.parts[0] != valid_prefix:\\n        return None\\n    if remote_path.suffix[1:] not in valid_suffixes:\\n        raise ValueError(f\"Unsupported file type, must be one of {valid_suffixes}.\")\\n\\n\\n\\n\\n\\n\\n    full_url = urljoin(URL_BASE.format(ref=ref), PurePosixPath(remote_path).__str__())\\n\\n    r = requests.get(full_url, timeout=5)\\n    if r.status_code != 200:\\n        raise ValueError(f\"Could not find file at {full_url}\")\\n    with tempfile.TemporaryDirectory() as tmpdirname:\\n        file = Path(tmpdirname) / remote_path.name\\n        with open(file, \"wb\") as f:\\n            f.write(r.content)\\n        return loader(str(file), **kwargs)',\n",
       "   'd': 'Load configuration from hub.  Returns None if path is not a hub path.',\n",
       "   'l': True,\n",
       "   'g': ['Test the default mode.',\n",
       "    'Test that the default mode is 777 - S_IWUSR - S_IWGRP - S_IWOTH',\n",
       "    'Test the default mode for files and directories.',\n",
       "    'Test that the default mode is set on the home directory.']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: List[str],\\n        metadatas: Optional[List[dict]] = None,\\n        doc_metadata: Optional[dict] = None,\\n    ) -> None:\\n        self.vectorstore.add_texts(texts, metadatas, doc_metadata or {})',\n",
       "   'd': 'Add text to the Vectara vectorstore.\\n\\nArgs:\\n    texts (List[str]): The text\\n    metadatas (List[dict]): Metadata dicts, must line up with existing store',\n",
       "   'l': True,\n",
       "   'g': ['_summary_',\n",
       "    'Returns a rendered graph for the given interface.',\n",
       "    '_graphs_view_',\n",
       "    '_summary_']},\n",
       "  {'c': 'def _create_retry_decorator(\\n    llm: ChatFireworks,\\n    run_manager: Optional[\\n        Union[AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun]\\n    ] = None,\\n) -> Callable[[Any], Any]:\\n    errors = [\\n        fireworks.client.error.RateLimitError,\\n        fireworks.client.error.ServiceUnavailableError,\\n    ]\\n    return create_base_retry_decorator(\\n        error_types=errors, max_retries=llm.max_retries, run_manager=run_manager\\n    )',\n",
       "   'd': 'Define retry mechanism.',\n",
       "   'l': True,\n",
       "   'g': ['Create a multi-layer perceptron network.\\n    \\n    Parameters\\n    ----------\\n    num_layers : int, optional\\n        Number of hidden layers.\\n    num_hidden : int, optional\\n        Number of hidden units per layer.\\n    activation : function, optional\\n        Activation function.\\n    \\n    Returns\\n    -------\\n    network_fn : function\\n        A function that takes an input tensor and returns a tuple of\\n        (output tensor, None).',\n",
       "    'A multi-layer perceptron.',\n",
       "    'A multi-layer perceptron network.\\n    \\n    Arguments:\\n    num_layers: Number of hidden layers.\\n    num_hidden: Number of hidden units.\\n    activation: Activation function.\\n    \\n    Returns:\\n    A function that takes `X` and returns `logits`.',\n",
       "    'Simple Multi-Layer Perceptron\\n    \\n    Args:\\n        num_layers: Number of layers in the network\\n        num_hidden: Number of neurons in each hidden layer\\n        activation: Activation function for hidden layers\\n    \\n    Returns:\\n        A function that takes a tensor as input and returns the output of the network.']},\n",
       "  {'c': 'def __init__(\\n    self,\\n    separators: Optional[List[str]] = None,\\n    keep_separator: bool = True,\\n    is_separator_regex: bool = False,\\n    **kwargs: Any,\\n) -> None:\\n    super().__init__(keep_separator=keep_separator, **kwargs)\\n    self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n    self._is_separator_regex = is_separator_regex',\n",
       "   'd': 'Initialize a PythonCodeTextSplitter.',\n",
       "   'l': False,\n",
       "   'g': ['Set the display name of the player.\\n    \\n    Args:\\n        display_name (str): The new display name of the player.\\n    \\n    Raises:\\n        RuntimeError: If the player has already started.',\n",
       "    'Sets the display name of the participant. \\n    \\n    Parameters\\n    ----------\\n    display_name : str\\n        The new display name of the participant.',\n",
       "    'Sets the display name of the user.\\n    \\n    Args:\\n        display_name (str): The new display name of the user.\\n    \\n    Raises:\\n        RuntimeError: If the display name can be set only before the work has started.',\n",
       "    'Set the display name for the work.\\n    \\n    Parameters\\n    ----------\\n    display_name : str\\n        The display name for the work.']},\n",
       "  {'c': '    def from_function(\\n        cls,\\n        func: Optional[Callable] = None,\\n        coroutine: Optional[Callable[..., Awaitable[Any]]] = None,\\n        name: Optional[str] = None,\\n        description: Optional[str] = None,\\n        return_direct: bool = False,\\n        args_schema: Optional[Type[BaseModel]] = None,\\n        infer_schema: bool = True,\\n        **kwargs: Any,\\n    ) -> StructuredTool:\\n        if func is not None:\\n            source_function = func\\n        elif coroutine is not None:\\n            source_function = coroutine\\n        else:\\n            raise ValueError(\"Function and/or coroutine must be provided\")\\n        name = name or source_function.__name__\\n        description = description or source_function.__doc__\\n        if description is None:\\n            raise ValueError(\\n                \"Function must have a docstring if description not provided.\"\\n            )\\n\\n\\n\\n        sig = signature(source_function)\\n        description = f\"{name}{sig} - {description.strip()}\"\\n        _args_schema = args_schema\\n        if _args_schema is None and infer_schema:\\n            _args_schema = create_schema_from_function(f\"{name}Schema\", source_function)\\n        return cls(\\n            name=name,\\n            func=func,\\n            coroutine=coroutine,\\n            args_schema=_args_schema,\\n            description=description,\\n            return_direct=return_direct,\\n            **kwargs,\\n        )',\n",
       "   'd': 'Create tool from a given function.\\n\\nA classmethod that helps to create a tool from a function.\\n\\nArgs:\\n    func: The function from which to create a tool\\n    coroutine: The async function from which to create a tool\\n    name: The name of the tool. Defaults to the function name\\n    description: The description of the tool. Defaults to the function docstring\\n    return_direct: Whether to return the result directly or as a callback\\n    args_schema: The schema of the tool\\'s input arguments\\n    infer_schema: Whether to infer the schema from the function\\'s signature\\n    **kwargs: Additional arguments to pass to the tool\\n\\nReturns:\\n    The tool\\n\\nExamples:\\n\\n    .. code-block:: python\\n\\n        def add(a: int, b: int) -> int:\\n            \"\"\"Add two numbers\"\"\"\\n            return a + b\\n        tool = StructuredTool.from_function(add)\\n        tool.run(1, 2) # 3',\n",
       "   'l': True,\n",
       "   'g': ['_test_individual_boss_on_unit_test_',\n",
       "    '_test_individual_boss_on_unit_test()',\n",
       "    '_test_individual_boss_on_unit_test_',\n",
       "    'Test IndividualBOSS on unit test data']},\n",
       "  {'c': '        def _create_doc(node: Any, text: str) -> Document:\\n            metadata = {\\n                XPATH_KEY: _xpath_for_chunk(node),\\n                DOCUMENT_ID_KEY: document[\"id\"],\\n                DOCUMENT_NAME_KEY: document[\"name\"],\\n                STRUCTURE_KEY: node.attrib.get(\"structure\", \"\"),\\n                TAG_KEY: re.sub(r\"\\\\{.*\\\\}\", \"\", node.tag),\\n            }\\n\\n            if doc_metadata:\\n                metadata.update(doc_metadata)\\n\\n            return Document(\\n                page_content=text,\\n                metadata=metadata,\\n            )',\n",
       "   'd': 'Create a Document from a node and text.',\n",
       "   'l': True,\n",
       "   'g': ['Returns a list of all the keys in the configuration file.',\n",
       "    'Returns a list of all the immutable keys',\n",
       "    'Returns a list of all the keys in the config file.',\n",
       "    'Returns a list of the immutable keys in the config file.']},\n",
       "  {'c': '    def __init__(self, redis_: Any, *, ttl: Optional[int] = None):\\n        try:\\n            from redis import Redis\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import redis python package. \"\\n                \"Please install it with `pip install redis`.\"\\n            )\\n        if not isinstance(redis_, Redis):\\n            raise ValueError(\"Please pass in Redis object.\")\\n        self.redis = redis_\\n        self.ttl = ttl',\n",
       "   'd': 'Initialize an instance of RedisCache.\\n\\nThis method initializes an object with Redis caching capabilities.\\nIt takes a `redis_` parameter, which should be an instance of a Redis\\nclient class, allowing the object to interact with a Redis\\nserver for caching purposes.\\n\\nParameters:\\n    redis_ (Any): An instance of a Redis client class\\n        (e.g., redis.Redis) used for caching.\\n        This allows the object to communicate with a\\n        Redis server for caching operations.\\n    ttl (int, optional): Time-to-live (TTL) for cached items in seconds.\\n        If provided, it sets the time duration for how long cached\\n        items will remain valid. If not provided, cached items will not\\n        have an automatic expiration.',\n",
       "   'l': True,\n",
       "   'g': [', file_path):',\n",
       "    '.\\n    Extract the tar file.\\n    :param file_path: path to tar file\\n    :return: True if successful, False otherwise',\n",
       "    'for extracting the tar file',\n",
       "    '']},\n",
       "  {'c': '    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        self.return_each_line = return_each_line\\n\\n\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )',\n",
       "   'd': 'Create a question answering chain that returns an answer with sources.\\n\\nArgs:\\n    llm: Language model to use for the chain.\\n    **kwargs: Keyword arguments to pass to `create_qa_with_structure_chain`.\\n\\nReturns:\\n    Chain (LLMChain) that can be used to answer questions with citations.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    Deletes a file.\\n\\n    :param shareName: The name of the share.\\n    :param pathName: The path of the file.\\n    :return: The result of the operation.\\n    :rtype: smb.Result\\n    :raises SessionError: If the session is not connected.',\n",
       "    '.\\n    Delete a file or folder from a share.\\n\\n    :param shareName: Name of the share to delete a file from.\\n    :param pathName: Name of the file or folder to delete.\\n    :return: True if the file or folder was deleted successfully, False otherwise.',\n",
       "    '.\\n    Deletes a file or directory on a remote share.\\n    :param shareName: The name of the share to delete the file from.\\n    :param pathName: The path to the file or directory to delete.\\n    :return: True if the file or directory was deleted successfully, False otherwise.',\n",
       "    '.\\n    Deletes a file.\\n\\n    Args:\\n        shareName (str): The share name.\\n        pathName (str): The path name.\\n\\n    Returns:\\n        bool: True if the file was deleted successfully, False otherwise.\\n\\n    Raises:\\n        SessionError: If the session is not open or the file cannot be deleted.']},\n",
       "  {'c': 'def clear(self, **kwargs: Any) -> None:\\n    with Session(self.engine) as session:\\n        session.query(self.cache_schema).delete()\\n        session.commit()',\n",
       "   'd': 'Return response when agent has been stopped due to max iterations.',\n",
       "   'l': False,\n",
       "   'g': ['Clears the queue of jobs.',\n",
       "    'Clear all jobs from the queue.',\n",
       "    'Clear all jobs.',\n",
       "    'Clear the job queue.\\n    \\n    :param tag: optional tag to remove\\n    :type tag: str']},\n",
       "  {'c': '    def validate_environment(cls, values: Dict) -> Dict:\\n        google_api_key = get_from_dict_or_env(\\n            values, \"google_api_key\", \"GOOGLE_API_KEY\"\\n        )\\n        values[\"google_api_key\"] = google_api_key\\n\\n        google_cse_id = get_from_dict_or_env(values, \"google_cse_id\", \"GOOGLE_CSE_ID\")\\n        values[\"google_cse_id\"] = google_cse_id\\n\\n        try:\\n            from googleapiclient.discovery import build\\n\\n        except ImportError:\\n            raise ImportError(\\n                \"google-api-python-client is not installed. \"\\n                \"Please install it with `pip install google-api-python-client\"\\n                \">=2.100.0`\"\\n            )\\n\\n        service = build(\"customsearch\", \"v1\", developerKey=google_api_key)\\n        values[\"search_engine\"] = service\\n\\n        return values',\n",
       "   'd': 'Validate that api key and python package exists in environment.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    Check if the interval contains the given interval.\\n\\n    :param other: Interval to check.\\n    :return: True if the interval contains the given interval.\\n    :rtype: bool',\n",
       "    'Returns True if the given value is contained in this interval.\\n\\n    :param other: The value to check.\\n    :return: True if the value is contained, False otherwise.',\n",
       "    '.contains()',\n",
       "    'for other in self.bounds']},\n",
       "  {'c': 'def args(self) -> dict:\\n    if self.args_schema is not None:\\n        return self.args_schema.schema()[\"properties\"]\\n\\n\\n    return {\"tool_input\": {\"type\": \"string\"}}',\n",
       "   'd': \"Run an Actor on the Apify platform and wait for results to be ready.\\nArgs:\\n    actor_id (str): The ID or name of the Actor on the Apify platform.\\n    run_input (Dict): The input object of the Actor that you're trying to run.\\n    dataset_mapping_function (Callable): A function that takes a single\\n        dictionary (an Apify dataset item) and converts it to an\\n        instance of the Document class.\\n    build (str, optional): Optionally specifies the actor build to run.\\n        It can be either a build tag or build number.\\n    memory_mbytes (int, optional): Optional memory limit for the run,\\n        in megabytes.\\n    timeout_secs (int, optional): Optional timeout for the run, in seconds.\\nReturns:\\n    ApifyDatasetLoader: A loader that will fetch the records from the\\n        Actor run's default dataset.\",\n",
       "   'l': False,\n",
       "   'g': ['close the tab when the middle mouse button is pressed',\n",
       "    '* @brief  重写鼠标点击事件\\n    * @param  event\\n    * @return void',\n",
       "    '@brief 鼠标点击事件\\n    @param event 事件',\n",
       "    '重写鼠标按下事件\\n    :param event:\\n    :return:']},\n",
       "  {'c': '    def marqo_bulk_similarity_search(\\n        self, queries: Iterable[Union[str, Dict[str, float]]], k: int = 4\\n    ) -> Dict[str, List[Dict[str, List[Dict[str, str]]]]]:\\n        bulk_results = self._client.bulk_search(\\n            [\\n                {\\n                    \"index\": self._index_name,\\n                    \"q\": query,\\n                    \"searchableAttributes\": self._searchable_attributes,\\n                    \"limit\": k,\\n                }\\n                for query in queries\\n            ]\\n        )\\n        return bulk_results',\n",
       "   'd': 'Construct OpenSearchVectorSearch wrapper from raw documents.\\n\\nExample:\\n    .. code-block:: python\\n\\n        from langchain import OpenSearchVectorSearch\\n        from langchain.embeddings import OpenAIEmbeddings\\n        embeddings = OpenAIEmbeddings()\\n        opensearch_vector_search = OpenSearchVectorSearch.from_texts(\\n            texts,\\n            embeddings,\\n            opensearch_url=\"http://localhost:9200\"\\n        )\\n\\nOpenSearch by default supports Approximate Search powered by nmslib, faiss\\nand lucene engines recommended for large datasets. Also supports brute force\\nsearch through Script Scoring and Painless Scripting.\\n\\nOptional Args:\\n    vector_field: Document field embeddings are stored in. Defaults to\\n    \"vector_field\".\\n\\n    text_field: Document field the text of the document is stored in. Defaults\\n    to \"text\".\\n\\nOptional Keyword Args for Approximate Search:\\n    engine: \"nmslib\", \"faiss\", \"lucene\"; default: \"nmslib\"\\n\\n    space_type: \"l2\", \"l1\", \"cosinesimil\", \"linf\", \"innerproduct\"; default: \"l2\"\\n\\n    ef_search: Size of the dynamic list used during k-NN searches. Higher values\\n    lead to more accurate but slower searches; default: 512\\n\\n    ef_construction: Size of the dynamic list used during k-NN graph creation.\\n    Higher values lead to more accurate graph but slower indexing speed;\\n    default: 512\\n\\n    m: Number of bidirectional links created for each new element. Large impact\\n    on memory consumption. Between 2 and 100; default: 16\\n\\nKeyword Args for Script Scoring or Painless Scripting:\\n    is_appx_search: False',\n",
       "   'l': False,\n",
       "   'g': ['.authenticate_user()',\n",
       "    '.authenticate_user(self, request: Request) -> AuthResponse:',\n",
       "    '.\\n    :return:\\n    :rtype:\\n    :raises:',\n",
       "    '.']},\n",
       "  {'c': '    def load(self) -> List[Document]:\\n        from playwright.sync_api import sync_playwright\\n\\n        docs: List[Document] = list()\\n\\n        with sync_playwright() as p:\\n            browser = p.chromium.launch(headless=self.headless)\\n            for url in self.urls:\\n                try:\\n                    page = browser.new_page()\\n                    response = page.goto(url)\\n                    text = self.evaluator.evaluate(page, browser, response)\\n                    metadata = {\"source\": url}\\n                    docs.append(Document(page_content=text, metadata=metadata))\\n                except Exception as e:\\n                    if self.continue_on_failure:\\n                        logger.error(\\n                            f\"Error fetching or processing {url}, exception: {e}\"\\n                        )\\n                    else:\\n                        raise e\\n            browser.close()\\n        return docs',\n",
       "   'd': 'Load the specified URLs using Playwright and create Document instances.\\n\\nReturns:\\n    List[Document]: A list of Document instances with loaded content.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        web_path: str,\\n        filter_urls: Optional[List[str]] = None,\\n        parsing_function: Optional[Callable] = None,\\n        blocksize: Optional[int] = None,\\n        blocknum: int = 0,\\n        meta_function: Optional[Callable] = None,\\n        is_local: bool = False,\\n        continue_on_failure: bool = False,\\n    ):\\n        if blocksize is not None and blocksize < 1:\\n            raise ValueError(\"Sitemap blocksize should be at least 1\")\\n\\n        if blocknum < 0:\\n            raise ValueError(\"Sitemap blocknum can not be lower then 0\")\\n\\n        try:\\n            import lxml\\n        except ImportError:\\n            raise ImportError(\\n                \"lxml package not found, please install it with \" \"`pip install lxml`\"\\n            )\\n\\n        super().__init__(web_path)\\n\\n        self.filter_urls = filter_urls\\n        self.parsing_function = parsing_function or _default_parsing_function\\n        self.meta_function = meta_function or _default_meta_function\\n        self.blocksize = blocksize\\n        self.blocknum = blocknum\\n        self.is_local = is_local\\n        self.continue_on_failure = continue_on_failure',\n",
       "   'd': 'Initialize with webpage path and optional filter URLs.\\n\\nArgs:\\n    web_path: url of the sitemap. can also be a local path\\n    filter_urls: list of strings or regexes that will be applied to filter the\\n        urls that are parsed and loaded\\n    parsing_function: Function to parse bs4.Soup output\\n    blocksize: number of sitemap locations per block\\n    blocknum: the number of the block that should be loaded - zero indexed.\\n        Default: 0\\n    meta_function: Function to parse bs4.Soup output for metadata\\n        remember when setting this method to also copy metadata[\"loc\"]\\n        to metadata[\"source\"] if you are using this field\\n    is_local: whether the sitemap is a local file. Default: False\\n    continue_on_failure: whether to continue loading the sitemap if an error\\n        occurs loading a url, emitting a warning instead of raising an\\n        exception. Setting this to True makes the loader more robust, but also\\n        may result in missing data. Default: False',\n",
       "   'l': True,\n",
       "   'g': ['Create a new game',\n",
       "    'Create a new game\\n    \\n    Args:\\n        number_to_guess (int): Number to guess\\n        \\n    Returns:\\n        None',\n",
       "    'Create a new game in memory',\n",
       "    'Create a new game']},\n",
       "  {'c': 'def _get_docs(\\n    self,\\n    question: str,\\n    *,\\n    run_manager: CallbackManagerForChainRun,\\n) -> List[Document]:\\n    return self.retriever.get_relevant_documents(\\n        question, callbacks=run_manager.get_child()\\n    )',\n",
       "   'd': 'Return values of the agent.',\n",
       "   'l': False,\n",
       "   'g': ['_pyro_param',\n",
       "    '',\n",
       "    '@param msg: \\n    @return:',\n",
       "    '_pyro_param(self, msg)\\n\\n    Return the value of a named parameter.']},\n",
       "  {'c': 'def _run(\\n    self,\\n    query: str,\\n    run_manager: Optional[CallbackManagerForToolRun] = None,\\n) -> str:\\n    return self.llm_chain.predict(query=query, dialect=self.db.dialect)',\n",
       "   'd': 'Process the media type of the request body.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    Args:\\n      blocks_args: A list of block args.\\n      global_params: A dict of global params.',\n",
       "    '.',\n",
       "    '.\\n\\n    Args:\\n      blocks_args: A list of dictionaries containing the arguments for each\\n        block.\\n      global_params: A dictionary containing the arguments for the global\\n        block.',\n",
       "    '.\\n\\n    Parameters\\n    ----------\\n    blocks_args : list\\n      A list of blocks arguments.\\n    global_params : dict\\n      Global parameters.']},\n",
       "  {'c': 'def load(self) -> List[Document]:\\n    loader = UnstructuredPDFLoader(str(self.file_path))\\n    return loader.load()',\n",
       "   'd': 'Load documents.',\n",
       "   'l': True,\n",
       "   'g': ['Find snippet files for a given file type. \\n    \\n    Arguments:\\n    ft -- file type\\n    directory -- directory to search in\\n    \\n    Returns:\\n    set of snippet file paths',\n",
       "    \"Find snippet files for a given filetype in a directory.\\n    \\n    @param ft: Filetype (e.g. 'python')\\n    @param directory: Directory to search in\\n    @return: Set of snippet files\",\n",
       "    'Find all snippet files for a given file type.',\n",
       "    'Finds all snippet files in the given directory.\\n    \\n    :param ft: the file type to search for\\n    :param directory: the directory to search\\n    :return: a set of all snippet files']},\n",
       "  {'c': 'def __new__(cls, **data: Any) -> Union[OpenAIChat, BaseOpenAI]:\\n    model_name = data.get(\"model_name\", \"\")\\n    if model_name.startswith(\"gpt-3.5-turbo\") or model_name.startswith(\"gpt-4\"):\\n        warnings.warn(\\n            \"You are trying to use a chat model. This way of initializing it is \"\\n            \"no longer supported. Instead, please use: \"\\n            \"`from langchain.chat_models import ChatOpenAI`\"\\n        )\\n        return OpenAIChat(**data)\\n    return super().__new__(cls)',\n",
       "   'd': 'PUT the URL and return the text.',\n",
       "   'l': False,\n",
       "   'g': [', X_val, Y_val, trigger, val_method=None):',\n",
       "    '.\\n    Set validation data.\\n\\n    :param batch_size:\\n    :param X_val:\\n    :param Y_val:\\n    :param trigger:\\n    :param val_method:\\n    :return:',\n",
       "    'Sets validation data for the model.',\n",
       "    'for validation']},\n",
       "  {'c': 'def clear(self, **kwargs: Any) -> None:\\n    asynchronous = kwargs.get(\"asynchronous\", False)\\n    self.redis.flushdb(asynchronous=asynchronous, **kwargs)',\n",
       "   'd': 'Input variables for this prompt template.\\n\\nReturns:\\n    List of input variables.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    \\n    Returns:\\n        str: JSON string representation of the configuration.',\n",
       "    '.\\n    Returns:\\n        str: JSON string representation of the config.',\n",
       "    '.\\n    Returns:\\n        str: A string representation of the configuration object in JSON format.',\n",
       "    '.\\n    Return a JSON string representation of the configuration.']},\n",
       "  {'c': 'def split_text(self, text: str) -> List[str]:\\n    separator = (\\n        self._separator if self._is_separator_regex else re.escape(self._separator)\\n    )\\n    splits = _split_text_with_regex(text, separator, self._keep_separator)\\n    _separator = \"\" if self._keep_separator else self._separator\\n    return self._merge_splits(splits, _separator)',\n",
       "   'd': 'Split incoming text and return chunks.',\n",
       "   'l': True,\n",
       "   'g': ['Creates a new typed tuple with the given item type and iterable.\\n    \\n    :param item_type: The type of the items in the tuple.\\n    :param iterable: An iterable of items to be added to the tuple.\\n    :return: A new typed tuple with the given item type and iterable.',\n",
       "    'Create a typed tuple from a sequence.\\n    \\n    :param item_type: Type of each item in the tuple.\\n    :param iterable: Sequence of items to be converted to a tuple.\\n    :param _tuple_cache: Tuple cache.\\n    :return: Typed tuple.',\n",
       "    'Creates a new TypedTuple object from an iterable.\\n    \\n    Args:\\n        item_type: The type of the items in the iterable.\\n        iterable: The iterable to create the TypedTuple from.\\n    \\n    Returns:\\n        A TypedTuple object.',\n",
       "    '__init__ for typedtuples\\n\\n    :param item_type: the type of items in the tuple\\n    :param iterable: optional iterable to initialize the tuple\\n    :return: a tuple of the specified type']},\n",
       "  {'c': 'def get_extraction_chain(\\n    allowed_nodes: Optional[List[str]] = None, allowed_rels: Optional[List[str]] = None\\n):\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                f\"\"\"\\n\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\n- **Nodes** represent entities and concepts. They\\'re akin to Wikipedia nodes.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\\n\\n- **Consistency**: Ensure you use basic or elementary types for node labels.\\n  - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\\n- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n{\\'- **Allowed Node Labels:**\\' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\\n{\\'- **Allowed Relationship Types**:\\' + \", \".join(allowed_rels) if allowed_rels else \"\"}\\n\\n- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\\n- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\\n- **Property Format**: Properties must be in a key-value format.\\n- **Quotation Marks**: Never use escaped single or double quotes within property values.\\n- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\\n\\n- **Maintain Entity Consistency**: When extracting entities, it\\'s vital to ensure consistency.\\nIf an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),\\nalways use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n\\nAdhere to the rules strictly. Non-compliance will result in termination.\\n          \"\"\",\\n            ),\\n            (\\n                \"human\",\\n                \"Use the given format to extract information from the following input: {input}\",\\n            ),\\n            (\"human\", \"Tip: Make sure to answer in the correct format\"),\\n        ]\\n    )\\n    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)',\n",
       "   'd': 'Get an OpenAPI spec from a text.',\n",
       "   'l': False,\n",
       "   'g': [\"'hosts' decorator overrides 'env hosts'\",\n",
       "    \"'foo' should be in the hosts of 'bar'\",\n",
       "    \"'test_hosts_decorator_overrides_env_hosts'\",\n",
       "    'test_hosts_decorator_overrides_env_hosts']},\n",
       "  {'c': 'def _run(\\n    self, text: str, run_manager: Optional[CallbackManagerForToolRun] = None\\n) -> str:\\n    try:\\n        data = _parse_input(text)\\n        return self.requests_wrapper.patch(_clean_url(data[\"url\"]), data[\"data\"])\\n    except Exception as e:\\n        return repr(e)',\n",
       "   'd': 'Run more texts through the embeddings and add to the vectorstore.\\n\\nArgs:\\n    texts: Iterable of strings to add to the vectorstore.\\n    metadatas: Optional list of metadatas associated with the texts.\\n    ids: Optional list of ids to associate with the texts.\\n    bulk_size: Bulk API request count; Default: 500\\n\\nReturns:\\n    List of ids from adding the texts into the vectorstore.\\n\\nOptional Args:\\n    vector_field: Document field embeddings are stored in. Defaults to\\n    \"vector_field\".\\n\\n    text_field: Document field the text of the document is stored in. Defaults\\n    to \"text\".',\n",
       "   'l': False,\n",
       "   'g': ['.',\n",
       "    '.\\n    This method is called when the process is started.\\n    :return:',\n",
       "    '() -> None:',\n",
       "    '.']},\n",
       "  {'c': 'def step(\\n    self, inputs: dict, callbacks: Callbacks = None, **kwargs: Any\\n) -> StepResponse:\\n',\n",
       "   'd': \"Load QA Eval Chain from LLM.\\n\\nArgs:\\n    llm (BaseLanguageModel): the base language model to use.\\n\\n    prompt (PromptTemplate): A prompt template containing the input_variables:\\n    'query', 'context' and 'result' that will be used as the prompt\\n    for evaluation.\\n    Defaults to PROMPT.\\n\\n    **kwargs: additional keyword arguments.\\n\\nReturns:\\n    ContextQAEvalChain: the loaded QA eval chain.\",\n",
       "   'l': False,\n",
       "   'g': ['for post release',\n",
       "    '.\\n    Post Release',\n",
       "    '.\\n    Post release hook.',\n",
       "    '.']},\n",
       "  {'c': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        doc_metadata: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        doc_hash = md5()\\n        for t in texts:\\n            doc_hash.update(t.encode())\\n        doc_id = doc_hash.hexdigest()\\n        if metadatas is None:\\n            metadatas = [{} for _ in texts]\\n        if doc_metadata:\\n            doc_metadata[\"source\"] = \"langchain\"\\n        else:\\n            doc_metadata = {\"source\": \"langchain\"}\\n        doc = {\\n            \"document_id\": doc_id,\\n            \"metadataJson\": json.dumps(doc_metadata),\\n            \"section\": [\\n                {\"text\": text, \"metadataJson\": json.dumps(md)}\\n                for text, md in zip(texts, metadatas)\\n            ],\\n        }\\n\\n        success_str = self._index_doc(doc)\\n        if success_str == \"E_ALREADY_EXISTS\":\\n            self._delete_doc(doc_id)\\n            self._index_doc(doc)\\n        elif success_str == \"E_NO_PERMISSIONS\":\\n            print(\\n                \"\"\"No permissions to add document to Vectara.\\n                Check your corpus ID, customer ID and API key\"\"\"\\n            )\\n        return [doc_id]',\n",
       "   'd': 'Load the specified URLs using Playwright and create Document instances.\\n\\nReturns:\\n    List[Document]: A list of Document instances with loaded content.',\n",
       "   'l': False,\n",
       "   'g': ['Returns the fields of the model.',\n",
       "    'Returns a list of the names of the fields in the model.',\n",
       "    '_fields getter',\n",
       "    'Returns the fields of the model.']},\n",
       "  {'c': '    def embed_query(self, text: str) -> List[float]:\\n        text = text.replace(\"\\\\n\", \" \")\\n        embedding = self.client.encode(\\n            self.query_instruction + text, **self.encode_kwargs\\n        )\\n        return embedding.tolist()',\n",
       "   'd': 'Compute query embeddings using a HuggingFace transformer model.\\n\\nArgs:\\n    text: The text to embed.\\n\\nReturns:\\n    Embeddings for the text.',\n",
       "   'l': True,\n",
       "   'g': ['Return the number of times that value appears in the list.',\n",
       "    'Return the number of times value occurs in the view.',\n",
       "    'Count the number of occurrences of a given value in the view.',\n",
       "    ':param value: \\n    :return:']},\n",
       "  {'c': '    def output_keys(self) -> List[str]:\\n        return [self.output_key]',\n",
       "   'd': 'Parse a single string model output into some structure.\\n\\nArgs:\\n    text: String output of a language model.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': False,\n",
       "   'g': ['_search()',\n",
       "    '',\n",
       "    '',\n",
       "    'Search using BM25.\\n    \\n    Args:\\n        term (str): Search term.\\n        weights (list): List of weights.\\n        with_score (bool): If True, return the score of the search.\\n        score_alias (str): Alias for the score.\\n        explicit_ordering (bool): If True, return the search results in explicit order.\\n    \\n    Returns:\\n        dict: Search results.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        path: Union[str, Path],\\n        *,\\n        glob: str = \"**/[!.]*\",\\n        exclude: Sequence[str] = (),\\n        suffixes: Optional[Sequence[str]] = None,\\n        show_progress: bool = False,\\n    ) -> None:\\n        if isinstance(path, Path):\\n            _path = path\\n        elif isinstance(path, str):\\n            _path = Path(path)\\n        else:\\n            raise TypeError(f\"Expected str or Path, got {type(path)}\")\\n\\n        self.path = _path.expanduser()\\n        self.glob = glob\\n        self.suffixes = set(suffixes or [])\\n        self.show_progress = show_progress\\n        self.exclude = exclude',\n",
       "   'd': 'Initialize with a path to directory and how to glob over it.\\n\\nArgs:\\n    path: Path to directory to load from\\n    glob: Glob pattern relative to the specified path\\n          by default set to pick up all non-hidden files\\n    exclude: patterns to exclude from results, use glob syntax\\n    suffixes: Provide to keep only files with these suffixes\\n              Useful when wanting to keep files with different suffixes\\n              Suffixes must include the dot, e.g. \".txt\"\\n    show_progress: If true, will show a progress bar as the files are loaded.\\n                   This forces an iteration through all matching files\\n                   to count them prior to loading them.\\n\\nExamples:\\n\\n    .. code-block:: python\\n\\n        # Recursively load all text files in a directory.\\n        loader = FileSystemBlobLoader(\"/path/to/directory\", glob=\"**/*.txt\")\\n\\n        # Recursively load all non-hidden files in a directory.\\n        loader = FileSystemBlobLoader(\"/path/to/directory\", glob=\"**/[!.]*\")\\n\\n        # Load all files in a directory without recursion.\\n        loader = FileSystemBlobLoader(\"/path/to/directory\", glob=\"*\")\\n\\n        # Recursively load all files in a directory, except for py or pyc files.\\n        loader = FileSystemBlobLoader(\\n            \"/path/to/directory\",\\n            glob=\"**/*.txt\",\\n            exclude=[\"**/*.py\", \"**/*.pyc\"]\\n        )',\n",
       "   'l': True,\n",
       "   'g': ['_to_dict__\\n\\n    Convert the Counter object to a dictionary.\\n\\n    :param get_value:\\n        A function that returns the value of the counter.\\n        Defaults to None.\\n    :return:\\n        A dictionary representation of the Counter.',\n",
       "    'Convert the counter to a dictionary.\\n    \\n    :param get_value: function to get the value from the counter\\n    :return: dictionary',\n",
       "    'Returns a dict representation of this counter.\\n    \\n    :param get_value: a callable that, given a Counter, returns the value to be returned by get()\\n    :type get_value: function\\n    :return: a dict representation of this counter\\n    :rtype: dict',\n",
       "    'Convert the counter to a dictionary.\\n    \\n    :param get_value: \\n    :return:']},\n",
       "  {'c': 'def split_text(self, text: str) -> List[str]:\\n    splits = (s.text for s in self._tokenizer(text).sents)\\n    return self._merge_splits(splits, self._separator)',\n",
       "   'd': 'Call the textgen web API and return the output.\\n\\nArgs:\\n    prompt: The prompt to use for generation.\\n    stop: A list of strings to stop generation when encountered.\\n\\nReturns:\\n    The generated text.\\n\\nExample:\\n    .. code-block:: python\\n\\n        from langchain.llms import TextGen\\n        llm = TextGen(model_url=\"http://localhost:5000\")\\n        llm(\"Write a story about llamas.\")',\n",
       "   'l': False,\n",
       "   'g': [':param cls:\\n    :param id_:\\n    :param name:\\n    :param sizes:\\n    :return:',\n",
       "    '.',\n",
       "    '.',\n",
       "    '(cls, id_, name, sizes) -> spec']},\n",
       "  {'c': '    def from_template(\\n        cls,\\n        template: str,\\n        *,\\n        template_format: str = \"f-string\",\\n        partial_variables: Optional[Dict[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> PromptTemplate:\\n        if template_format == \"jinja2\":\\n\\n            input_variables = _get_jinja2_variables_from_template(template)\\n        elif template_format == \"f-string\":\\n            input_variables = {\\n                v for _, v, _, _ in Formatter().parse(template) if v is not None\\n            }\\n        else:\\n            raise ValueError(f\"Unsupported template format: {template_format}\")\\n\\n        _partial_variables = partial_variables or {}\\n\\n        if _partial_variables:\\n            input_variables = {\\n                var for var in input_variables if var not in _partial_variables\\n            }\\n\\n        return cls(\\n            input_variables=sorted(input_variables),\\n            template=template,\\n            template_format=template_format,\\n            partial_variables=_partial_variables,\\n            **kwargs,\\n        )',\n",
       "   'd': 'Given input, decided what to do.\\n\\nArgs:\\n    intermediate_steps: Steps the LLM has taken to date,\\n        along with the observations.\\n    callbacks: Callbacks to run.\\n    **kwargs: User inputs.\\n\\nReturns:\\n    Action specifying what tool to use.',\n",
       "   'l': False,\n",
       "   'g': ['Decode a base62 string to a string.\\n    \\n    Args:\\n        ctext (T): The base62 string to decode.\\n    \\n    Returns:\\n        Optional[U]: The decoded string, or None if the decoding fails.',\n",
       "    '@param ctext:\\n    @return:',\n",
       "    'Decode a base62 encoded string into a bytes object.',\n",
       "    'Decode a base62 string to a UTF-8 string.\\n    \\n    Args:\\n        ctext (T): The base62 string to decode.\\n\\n    Returns:\\n        Optional[U]: The decoded UTF-8 string.']},\n",
       "  {'c': 'def type(self) -> str:\\n',\n",
       "   'd': 'Type of the Message, used for serialization.',\n",
       "   'l': True,\n",
       "   'g': ['(self, nodes, thunks, pre_call_clear)\\n\\n    Args:\\n        nodes (list): \\n        thunks (list): \\n        pre_call_clear (bool):',\n",
       "    '(self, nodes, thunks, pre_call_clear)',\n",
       "    '(self, nodes, thunks, pre_call_clear)',\n",
       "    '(self, nodes, thunks, pre_call_clear)']},\n",
       "  {'c': 'def clear(self, **kwargs: Any) -> None:\\n    with Session(self.engine) as session:\\n        session.query(self.cache_schema).delete()\\n        session.commit()',\n",
       "   'd': 'Add text to the Vectara vectorstore.\\n\\nArgs:\\n    texts (List[str]): The text\\n    metadatas (List[dict]): Metadata dicts, must line up with existing store',\n",
       "   'l': False,\n",
       "   'g': ['Opens a file in ASCII mode.',\n",
       "    'Opens a file in ascii mode.\\n    \\n    Parameters:\\n    filename (str): The name of the file to open.\\n    mode (str): The mode to open the file in.\\n\\n    Returns:\\n    file: A file object representing the opened file.',\n",
       "    'Opens a file in ASCII mode and returns a file object.',\n",
       "    'Opens a file in ASCII mode.']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        dataset_name: str,\\n        workspace_name: Optional[str] = None,\\n        api_url: Optional[str] = None,\\n        api_key: Optional[str] = None,\\n    ) -> None:\\n        super().__init__()\\n\\n\\n        try:\\n            import argilla as rg\\n\\n            self.ARGILLA_VERSION = rg.__version__\\n        except ImportError:\\n            raise ImportError(\\n                \"To use the Argilla callback manager you need to have the `argilla` \"\\n                \"Python package installed. Please install it with `pip install argilla`\"\\n            )\\n\\n\\n        if parse(self.ARGILLA_VERSION) < parse(\"1.8.0\"):\\n            raise ImportError(\\n                f\"The installed `argilla` version is {self.ARGILLA_VERSION} but \"\\n                \"`ArgillaCallbackHandler` requires at least version 1.8.0. Please \"\\n                \"upgrade `argilla` with `pip install --upgrade argilla`.\"\\n            )\\n\\n\\n        if api_url is None and os.getenv(\"ARGILLA_API_URL\") is None:\\n            warnings.warn(\\n                (\\n                    \"Since `api_url` is None, and the env var `ARGILLA_API_URL` is not\"\\n                    f\" set, it will default to `{self.DEFAULT_API_URL}`, which is the\"\\n                    \" default API URL in Argilla Quickstart.\"\\n                ),\\n            )\\n            api_url = self.DEFAULT_API_URL\\n\\n        if api_key is None and os.getenv(\"ARGILLA_API_KEY\") is None:\\n            self.DEFAULT_API_KEY = (\\n                \"admin.apikey\"\\n                if parse(self.ARGILLA_VERSION) < parse(\"1.11.0\")\\n                else \"owner.apikey\"\\n            )\\n\\n            warnings.warn(\\n                (\\n                    \"Since `api_key` is None, and the env var `ARGILLA_API_KEY` is not\"\\n                    f\" set, it will default to `{self.DEFAULT_API_KEY}`, which is the\"\\n                    \" default API key in Argilla Quickstart.\"\\n                ),\\n            )\\n            api_url = self.DEFAULT_API_URL\\n\\n\\n        try:\\n            rg.init(api_key=api_key, api_url=api_url)\\n        except Exception as e:\\n            raise ConnectionError(\\n                f\"Could not connect to Argilla with exception: \\'{e}\\'.\\\\n\"\\n                \"Please check your `api_key` and `api_url`, and make sure that \"\\n                \"the Argilla server is up and running. If the problem persists \"\\n                f\"please report it to {self.ISSUES_URL} as an `integration` issue.\"\\n            ) from e\\n\\n\\n        self.dataset_name = dataset_name\\n        self.workspace_name = workspace_name or rg.get_workspace()\\n\\n\\n        try:\\n            extra_args = {}\\n            if parse(self.ARGILLA_VERSION) < parse(\"1.14.0\"):\\n                warnings.warn(\\n                    f\"You have Argilla {self.ARGILLA_VERSION}, but Argilla 1.14.0 or\"\\n                    \" higher is recommended.\",\\n                    UserWarning,\\n                )\\n                extra_args = {\"with_records\": False}\\n            self.dataset = rg.FeedbackDataset.from_argilla(\\n                name=self.dataset_name,\\n                workspace=self.workspace_name,\\n                **extra_args,\\n            )\\n        except Exception as e:\\n            raise FileNotFoundError(\\n                f\"`FeedbackDataset` retrieval from Argilla failed with exception `{e}`.\"\\n                f\"\\\\nPlease check that the dataset with name={self.dataset_name} in the\"\\n                f\" workspace={self.workspace_name} exists in advance. If you need help\"\\n                \" on how to create a `langchain`-compatible `FeedbackDataset` in\"\\n                f\" Argilla, please visit {self.BLOG_URL}. If the problem persists\"\\n                f\" please report it to {self.ISSUES_URL} as an `integration` issue.\"\\n            ) from e\\n\\n        supported_fields = [\"prompt\", \"response\"]\\n        if supported_fields != [field.name for field in self.dataset.fields]:\\n            raise ValueError(\\n                f\"`FeedbackDataset` with name={self.dataset_name} in the workspace=\"\\n                f\"{self.workspace_name} had fields that are not supported yet for the\"\\n                f\"`langchain` integration. Supported fields are: {supported_fields},\"\\n                f\" and the current `FeedbackDataset` fields are {[field.name for field in self.dataset.fields]}.\"\\n                \" For more information on how to create a `langchain`-compatible\"\\n                f\" `FeedbackDataset` in Argilla, please visit {self.BLOG_URL}.\"\\n            )\\n\\n        self.prompts: Dict[str, List[str]] = {}\\n\\n        warnings.warn(\\n            (\\n                \"The `ArgillaCallbackHandler` is currently in beta and is subject to\"\\n                \" change based on updates to `langchain`. Please report any issues to\"\\n                f\" {self.ISSUES_URL} as an `integration` issue.\"\\n            ),\\n        )',\n",
       "   'd': 'Split markdown file\\nArgs:\\n    text: Markdown file',\n",
       "   'l': False,\n",
       "   'g': ['2.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.',\n",
       "    '.getAppExt()\\n\\n    Returns the application extension data.\\n\\n    :param loops: The number of loops.\\n    :type loops: int\\n    :return: The application extension data.\\n    :rtype: str',\n",
       "    '21FF0B\\\\x03\\\\x01\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x0',\n",
       "    '']},\n",
       "  {'c': '    def get_child(self, tag: Optional[str] = None) -> AsyncCallbackManager:\\n        manager = AsyncCallbackManager(handlers=[], parent_run_id=self.run_id)\\n        manager.set_handlers(self.inheritable_handlers)\\n        manager.add_tags(self.inheritable_tags)\\n        manager.add_metadata(self.inheritable_metadata)\\n        if tag is not None:\\n            manager.add_tags([tag], False)\\n        return manager',\n",
       "   'd': 'Parse a list of candidate model Generations into a specific format.\\n\\nArgs:\\n    result: A list of Generations to be parsed. The Generations are assumed\\n        to be different candidate outputs for a single model input.\\n\\nReturns:\\n    Structured output.',\n",
       "   'l': False,\n",
       "   'g': ['loglikelihood of observations',\n",
       "    '(float)\\n\\n    Compute the log likelihood of the observations given the parameters.\\n\\n    Parameters\\n    ----------\\n    params : ndarray\\n        The parameters to be used.\\n\\n    Returns\\n    -------\\n    float\\n        The log likelihood of the observations.',\n",
       "    '(float)\\n\\n    Returns the negative log-likelihood of the observations given the parameters.',\n",
       "    'Compute the log-likelihood of the observations']},\n",
       "  {'c': 'def test_faiss_add_texts_not_supported() -> None:\\n    docsearch = FAISS(FakeEmbeddings().embed_query, None, FakeDocstore(), {})\\n    with pytest.raises(ValueError):\\n        docsearch.add_texts([\"foo\"])',\n",
       "   'd': 'Load documents.',\n",
       "   'l': False,\n",
       "   'g': ['_test_count_connected_components_',\n",
       "    '.\\n    Test function for count_connected_number_of_component class.\\n    count_connected_number_of_component.py\\n<|fim_prefix|><|fim_suffix|>\\n\\n    def count_components(self,l,size):',\n",
       "    '.\\n    Test the function count_components',\n",
       "    '.\\n    Test count connected components.']},\n",
       "  {'c': '    def format_messages(self, **kwargs: Any) -> List[BaseMessage]:\\n        value = kwargs[self.variable_name]\\n        if not isinstance(value, list):\\n            raise ValueError(\\n                f\"variable {self.variable_name} should be a list of base messages, \"\\n                f\"got {value}\"\\n            )\\n        for v in value:\\n            if not isinstance(v, BaseMessage):\\n                raise ValueError(\\n                    f\"variable {self.variable_name} should be a list of base messages,\"\\n                    f\" got {value}\"\\n                )\\n        return value',\n",
       "   'd': 'Format messages from kwargs.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    List of BaseMessage.',\n",
       "   'l': True,\n",
       "   'g': ['Choose a match from the available matches.\\n    \\n    :param session: The session object.\\n    :return: The chosen match.',\n",
       "    '',\n",
       "    'This method is called by the game to choose a match.',\n",
       "    '_choose_match_impl_']},\n",
       "  {'c': '    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        import sentence_transformers\\n\\n        texts = list(map(lambda x: x.replace(\"\\\\n\", \" \"), texts))\\n        if self.multi_process:\\n            pool = self.client.start_multi_process_pool()\\n            embeddings = self.client.encode_multi_process(texts, pool)\\n            sentence_transformers.SentenceTransformer.stop_multi_process_pool(pool)\\n        else:\\n            embeddings = self.client.encode(texts, **self.encode_kwargs)\\n\\n        return embeddings.tolist()',\n",
       "   'd': 'Compute doc embeddings using a HuggingFace transformer model.\\n\\nArgs:\\n    texts: The list of texts to embed.\\n\\nReturns:\\n    List of embeddings, one for each text.',\n",
       "   'l': True,\n",
       "   'g': ['This function checks the accuracy of the model.',\n",
       "    'Function to check the accuracy of our model',\n",
       "    'This function checks the accuracy of the model on the test set.',\n",
       "    'Checks accuracy of the model\\n\\n    Args:\\n        loader (pytorch dataloader): dataloader for the dataset\\n        model (torch model): model to be evaluated\\n\\n    Returns:\\n        float: accuracy of the model']},\n",
       "  {'c': 'def args(self) -> dict:\\n    return self.args_schema.schema()[\"properties\"]',\n",
       "   'd': 'Return the output parser type for serialization.',\n",
       "   'l': False,\n",
       "   'g': ['Returns a dictionary containing the topic, the topic type, the answer, and the format of the answer.',\n",
       "    'Returns the answer to the question.',\n",
       "    'Get the page content for a given topic.\\n    \\n    Returns a dictionary with the following keys:\\n        topic: The topic to be searched.\\n        topic_type: The topic type.\\n        answer: The page content.\\n        format: The format of the answer.',\n",
       "    'Returns a dict with the answer and the format of the answer.']},\n",
       "  {'c': '    def clear(self, **kwargs: Any) -> None:\\n        from gptcache import Cache\\n\\n        for gptcache_instance in self.gptcache_dict.values():\\n            gptcache_instance = cast(Cache, gptcache_instance)\\n            gptcache_instance.flush()\\n\\n        self.gptcache_dict.clear()',\n",
       "   'd': 'Clear cache.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    Args:\\n        dataloader:\\n        current_iter:\\n        tb_logger:\\n        save_img:',\n",
       "    'validation',\n",
       "    '(self, dataloader, current_iter, tb_logger, save_img=False)\\n    Args:\\n        dataloader:\\n        current_iter:\\n        tb_logger:\\n        save_img:',\n",
       "    '']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        config: Mapping[str, Any],\\n        stream_name: str,\\n        record_handler: Optional[RecordHandler] = None,\\n        state: Optional[Any] = None,\\n    ) -> None:\\n        source_class = guard_import(\\n            \"source_gong\", pip_name=\"airbyte-source-gong\"\\n        ).SourceGong\\n        super().__init__(\\n            config=config,\\n            source_class=source_class,\\n            stream_name=stream_name,\\n            record_handler=record_handler,\\n            state=state,\\n        )',\n",
       "   'd': 'Initializes the loader.\\n\\nArgs:\\n    config: The config to pass to the source connector.\\n    stream_name: The name of the stream to load.\\n    record_handler: A function that takes in a record and an optional id and\\n        returns a Document. If None, the record will be used as the document.\\n        Defaults to None.\\n    state: The state to pass to the source connector. Defaults to None.',\n",
       "   'l': True,\n",
       "   'g': ['_Process features from tf.train.Example or features.FeatureDict.\\n\\n    Args:\\n      raw_features: Union[tf.train.Example, features.FeatureDict].\\n      random_seed: int.\\n\\n    Returns:\\n      features.FeatureDict.',\n",
       "    '_summary_\\n\\n    Args:\\n      raw_features: The raw features to process.\\n      random_seed: The random seed to use for generating random features.\\n\\n    Returns:\\n      A `features.FeatureDict` containing the processed features.',\n",
       "    '_summary_\\n\\n    Args:\\n      raw_features: A `tf.train.Example` or `features.FeatureDict`.\\n      random_seed: A random seed for shuffling.\\n\\n    Returns:\\n      A `features.FeatureDict` with processed features.',\n",
       "    '_summary_\\n\\n    Args:\\n      raw_features: Union[tf.train.Example, features.FeatureDict]\\n      random_seed: int\\n\\n    Returns:\\n      features.FeatureDict']},\n",
       "  {'c': 'def test_warn_deprecated(kwargs: Dict[str, Any], expected_message: str) -> None:\\n    with warnings.catch_warnings(record=True) as warning_list:\\n        warnings.simplefilter(\"always\")\\n\\n        warn_deprecated(**kwargs)\\n\\n        assert len(warning_list) == 1\\n        warning = warning_list[0].message\\n        assert str(warning) == expected_message',\n",
       "   'd': 'Save the agent.\\n\\nArgs:\\n    file_path: Path to file to save the agent to.\\n\\nExample:\\n.. code-block:: python\\n\\n    # If working with agent executor\\n    agent.agent.save(file_path=\"path/agent.yaml\")',\n",
       "   'l': False,\n",
       "   'g': ['Decorator for registering a model class.\\n    \\n    Args:\\n        name (str): The name of the model class.\\n    \\n    Returns:\\n        Type: The decorated model class.',\n",
       "    'Register a model class.\\n    \\n    Parameters\\n    ----------\\n    name : str, optional\\n        The name to use for the model class.\\n    \\n    Returns\\n    -------\\n    Type\\n        The model class.',\n",
       "    \"Register a model class.\\n    \\n    This function is intended to be used in a model class's __init__ method.\\n    \\n    Parameters\\n    ----------\\n    name: str\\n        The name of the model.\\n        \\n    Returns\\n    -------\\n    Type\\n        The model class.\",\n",
       "    'Register a model for name.\\n    \\n    Args:\\n        name (str, optional): Name of the model. Defaults to None.\\n    \\n    Returns:\\n        Type: Decorator for registering the model.']},\n",
       "  {'c': 'def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n',\n",
       "   'd': 'Look up based on prompt and llm_string.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '']},\n",
       "  {'c': '    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        vectors = np.array(\\n            self.embeddings.embed_documents(\\n                [inputs[\"prediction\"], inputs[\"prediction_b\"]]\\n            )\\n        )\\n        score = self._compute_score(vectors)\\n        return {\"score\": score}',\n",
       "   'd': 'The keys to extract from the run.',\n",
       "   'l': False,\n",
       "   'g': ['.encode(self, x: torch.Tensor, n_q: tp.Optional[int] = None, st: tp.Optional[int] = None) -> torch.Tensor\\n    \\n    Encodes the input tensor using the VQ-VAE.\\n    \\n    Args:\\n        x: Input tensor.\\n        n_q: Number of quantization levels.\\n        st: Start index for quantization.\\n    \\n    Returns:\\n        Encoded tensor.',\n",
       "    '::\\n\\n        Parameters:\\n            x (torch.Tensor): input tensor to be encoded\\n            n_q (int): number of codes to be encoded\\n            st (int): starting index for codes\\n        \\n        Returns:\\n            torch.Tensor: encoded tensor',\n",
       "    '.encode(self, x: torch.Tensor, n_q: tp.Optional[int] = None, st: tp.Optional[int] = None) -> torch.Tensor\\n    \\n    Encode x into codes.\\n    \\n    Args:\\n        x: input tensor of shape (B, T, C)\\n        n_q: number of codes to encode.\\n        st: starting index.\\n    \\n    Returns:\\n        codes: encoded tensor of shape (B, T, n_q, C)',\n",
       "    'Encodes a tensor of floats to a tensor of integers.\\n    Args:\\n        x: tensor of floats\\n        n_q: number of bits to encode to\\n        st: start index\\n    Returns:\\n        tensor of integers']},\n",
       "  {'c': '    def load(self, query: str) -> List[Document]:\\n        try:\\n            import fitz\\n        except ImportError:\\n            raise ImportError(\\n                \"PyMuPDF package not found, please install it with \"\\n                \"`pip install pymupdf`\"\\n            )\\n\\n        try:\\n\\n            query = query.replace(\":\", \"\").replace(\"-\", \"\")\\n            if self.is_arxiv_identifier(query):\\n                results = self.arxiv_search(\\n                    id_list=query[: self.ARXIV_MAX_QUERY_LENGTH].split(),\\n                    max_results=self.load_max_docs,\\n                ).results()\\n            else:\\n                results = self.arxiv_search(\\n                    query[: self.ARXIV_MAX_QUERY_LENGTH], max_results=self.load_max_docs\\n                ).results()\\n        except self.arxiv_exceptions as ex:\\n            logger.debug(\"Error on arxiv: %s\", ex)\\n            return []\\n\\n        docs: List[Document] = []\\n        for result in results:\\n            try:\\n                doc_file_name: str = result.download_pdf()\\n                with fitz.open(doc_file_name) as doc_file:\\n                    text: str = \"\".join(page.get_text() for page in doc_file)\\n            except (FileNotFoundError, fitz.fitz.FileDataError) as f_ex:\\n                logger.debug(f_ex)\\n                continue\\n            if self.load_all_available_meta:\\n                extra_metadata = {\\n                    \"entry_id\": result.entry_id,\\n                    \"published_first_time\": str(result.published.date()),\\n                    \"comment\": result.comment,\\n                    \"journal_ref\": result.journal_ref,\\n                    \"doi\": result.doi,\\n                    \"primary_category\": result.primary_category,\\n                    \"categories\": result.categories,\\n                    \"links\": [link.href for link in result.links],\\n                }\\n            else:\\n                extra_metadata = {}\\n            metadata = {\\n                \"Published\": str(result.updated.date()),\\n                \"Title\": result.title,\\n                \"Authors\": \", \".join(a.name for a in result.authors),\\n                \"Summary\": result.summary,\\n                **extra_metadata,\\n            }\\n            doc = Document(\\n                page_content=text[: self.doc_content_chars_max], metadata=metadata\\n            )\\n            docs.append(doc)\\n            os.remove(doc_file_name)\\n        return docs',\n",
       "   'd': 'Run Arxiv search and get the article texts plus the article meta information.\\nSee https://lukasschwab.me/arxiv.py/index.html#Search\\n\\nReturns: a list of documents with the document.page_content in text format\\n\\nPerforms an arxiv search, downloads the top k results as PDFs, loads\\nthem as Documents, and returns them in a List.\\n\\nArgs:\\n    query: a plaintext search query',\n",
       "   'l': True,\n",
       "   'g': ['_children(self, tag=None, recursive=False)\\n\\n    Returns a list of child elements of the current element.\\n\\n    :param tag: The tag name of the child elements.\\n    :param recursive: Whether to recursively search for child elements.\\n    :return: A list of child elements.',\n",
       "    '_recursive_children',\n",
       "    '_Get all children of this element.',\n",
       "    '_children\\n\\n    Returns a list of all direct children of this element.\\n\\n    :param tag: the tag name to filter by\\n    :param recursive: whether to recurse into children\\n    :return: a list of all direct children of this element']},\n",
       "  {'c': '    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[PromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> QAEvalChain:\\n        prompt = prompt or PROMPT\\n        expected_input_vars = {\"query\", \"answer\", \"result\"}\\n        if expected_input_vars != set(prompt.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt.input_variables}\"\\n            )\\n        return cls(llm=llm, prompt=prompt, **kwargs)',\n",
       "   'd': 'Constructs a new RocksetChatMessageHistory.\\n\\nArgs:\\n    - session_id: The ID of the chat session\\n    - client: The RocksetClient object to use to query\\n    - collection: The name of the collection to use to store chat\\n                  messages. If a collection with the given name\\n                  does not exist in the workspace, it is created.\\n    - workspace: The workspace containing `collection`. Defaults\\n                 to `\"commons\"`\\n    - messages_key: The DB column containing message history.\\n                    Defaults to `\"messages\"`\\n    - sync: Whether to wait for messages to be added. Defaults\\n            to `False`. NOTE: setting this to `True` will slow\\n            down performance.\\n    - message_uuid_method: The method that generates message IDs.\\n            If set, all messages will have an `id` field within the\\n            `additional_kwargs` property. If this param is not set\\n            and `sync` is `False`, message IDs will not be created.\\n            If this param is not set and `sync` is `True`, the\\n            `uuid.uuid4` method will be used to create message IDs.',\n",
       "   'l': False,\n",
       "   'g': ['.',\n",
       "    '(self, params, **cfg)',\n",
       "    '(generator)',\n",
       "    '(self, params, **cfg)\\n\\n    def __iter__(self):']},\n",
       "  {'c': '    def split_text(self, text: str) -> List[Document]:\\n        lines = text.split(\"\\\\n\")\\n\\n        lines_with_metadata: List[LineType] = []\\n\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n\\n\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            for sep, name in self.headers_to_split_on:\\n\\n                if stripped_line.startswith(sep) and (\\n\\n\\n                    len(stripped_line) == len(sep)\\n                    or stripped_line[len(sep)] == \" \"\\n                ):\\n\\n                    if name is not None:\\n\\n                        current_header_level = sep.count(\"\\n\\n\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n\\n\\n                            popped_header = header_stack.pop()\\n\\n\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])\\n\\n\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n\\n                        initial_metadata[name] = header[\"data\"]\\n\\n\\n\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n\\n\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
       "   'd': 'Return whether the chain requires a reference.\\n\\nReturns:\\n    bool: True if the chain requires a reference, False otherwise.',\n",
       "   'l': False,\n",
       "   'g': ['(object)', '(MemoryStorage)', '.', 'for the memory storage']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n    ) -> None:\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index',\n",
       "   'd': \"Create a new TextSplitter.\\n\\nArgs:\\n    chunk_size: Maximum size of chunks to return\\n    chunk_overlap: Overlap in characters between chunks\\n    length_function: Function that measures the length of given chunks\\n    keep_separator: Whether to keep the separator in the chunks\\n    add_start_index: If `True`, includes chunk's start index in metadata\",\n",
       "   'l': True,\n",
       "   'g': ['::summary::\\n    Stops TensorBoard writer.',\n",
       "    '_summary_',\n",
       "    'Close the TensorBoard writer.',\n",
       "    '_summary_\\n    Close TensorBoard writer.']},\n",
       "  {'c': '    def similarity_search_with_score(\\n        self,\\n        query: str,\\n        k: int = DEFAULT_TOPN,\\n        text_in_page_content: Optional[str] = None,\\n        meta_filter: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> List[Tuple[Document, float]]:\\n        if self.awadb_client is None:\\n            raise ValueError(\"AwaDB client is None!!!\")\\n\\n        embedding = None\\n        if self.using_table_name in self.table2embeddings:\\n            embedding = self.table2embeddings[self.using_table_name].embed_query(query)\\n        else:\\n            from awadb import AwaEmbedding\\n\\n            embedding = AwaEmbedding().Embedding(query)\\n\\n        results: List[Tuple[Document, float]] = []\\n\\n        not_include_fields: Set[str] = {\"text_embedding\", \"_id\"}\\n        retrieval_docs = self.similarity_search_by_vector(\\n            embedding,\\n            k,\\n            text_in_page_content=text_in_page_content,\\n            meta_filter=meta_filter,\\n            not_include_fields_in_metadata=not_include_fields,\\n        )\\n\\n        for doc in retrieval_docs:\\n            score = doc.metadata[\"score\"]\\n            del doc.metadata[\"score\"]\\n            doc_tuple = (doc, score)\\n            results.append(doc_tuple)\\n\\n        return results',\n",
       "   'd': 'Extract images from page and get the text with RapidOCR.',\n",
       "   'l': False,\n",
       "   'g': ['This function calculates the token limit for a given model.\\n    \\n    Parameters:\\n    model (str): The name of the model.\\n    \\n    Returns:\\n    int: The token limit for the given model.',\n",
       "    'Returns the token limit for the given model.\\n\\n    Args:\\n        model (str, optional): The name of the model. Defaults to \"gpt-3.5-turbo-0301\".\\n\\n    Returns:\\n        int: The token limit for the given model.',\n",
       "    'Returns the token limit for the given model.',\n",
       "    'Returns the maximum number of tokens that can be used by the model.']},\n",
       "  {'c': 'def _llm_type(self) -> str:\\n    return \"azure\"',\n",
       "   'd': 'Update based on prompt and llm_string.',\n",
       "   'l': False,\n",
       "   'g': ['_get_param_groups__\\n    \\n    :return:',\n",
       "    '.\\n    Returns:\\n        Dict[str, List[Parameter]]:',\n",
       "    '_get_param_groups_helper_method_',\n",
       "    '.']},\n",
       "  {'c': 'def template_is_valid(cls, values: Dict) -> Dict:\\n    if values[\"validate_template\"]:\\n        check_valid_template(\\n            values[\"prefix\"] + values[\"suffix\"],\\n            values[\"template_format\"],\\n            values[\"input_variables\"] + list(values[\"partial_variables\"]),\\n        )\\n    elif values.get(\"template_format\"):\\n        values[\"input_variables\"] = [\\n            var\\n            for var in get_template_variables(\\n                values[\"prefix\"] + values[\"suffix\"], values[\"template_format\"]\\n            )\\n            if var not in values[\"partial_variables\"]\\n        ]\\n    return values',\n",
       "   'd': 'Run on new LLM token. Only available when streaming is enabled.',\n",
       "   'l': False,\n",
       "   'g': ['()\\n\\n    def __init__(self, leaf):\\n        self[\"leaf\"] = leaf',\n",
       "    '() -> int:',\n",
       "    '.\\n    Returns:\\n        int: the number of leaf nodes in the tree.',\n",
       "    '() -> bool:']},\n",
       "  {'c': 'def on_tool_error(\\n    self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\\n) -> None:\\n    self.step += 1\\n    self.errors += 1',\n",
       "   'd': 'Clear session memory from DynamoDB',\n",
       "   'l': False,\n",
       "   'g': [\"The function returns the number of possible combinations of a sylvester's triangle.\\n    \\n    Parameters:\\n    - number: The number of rows in the sylvester's triangle.\\n    \\n    Returns:\\n    - The number of possible combinations of a sylvester's triangle.\\n    \\n    Raises:\\n    - ValueError: If the input value of [n] is not an integer or less than 1.\\n    \\n    Examples:\\n    >>> sylvester(1)\\n    2\\n    >>> sylvester(2)\\n    3\\n    >>> sylvester(3)\\n    6\\n    >>> sylvester(4)\\n    10\\n    >>> sylvester(5)\\n    15\\n    >>> sylvester(6)\\n    21\\n    >>> sylvester(7)\\n    28\\n    >>> sylvester(8)\\n    36\\n    >>> sylvester(9)\\n    45\\n    >>> sylvester(10)\\n    55\\n    >>> sylvester(11)\\n    66\\n    >>> sylvester(12)\\n    78\\n    >>> sylvester(13)\\n    91\",\n",
       "    'The function returns the number of possible combinations of \\n    letters in the name of the person with the given number of letters.',\n",
       "    'This function calculates the sum of the squares of the first n natural numbers.\\n    \\n    Parameters:\\n    - number (int): The input number.\\n    \\n    Returns:\\n    - int: The sum of the squares of the first n natural numbers.',\n",
       "    'This function calculates the sum of the numbers from 1 to n.\\n    \\n    Parameters\\n    ----------\\n    number : int\\n        The number of the sum.\\n\\n    Returns\\n    -------\\n    int\\n        The sum of the numbers from 1 to n.']},\n",
       "  {'c': '    def input_keys(self) -> List[str]:\\n        return [self.input_docs_key, self.question_key]',\n",
       "   'd': 'Create a new TextSplitter.',\n",
       "   'l': False,\n",
       "   'g': ['.\\n    Generates the context for the LLM.\\n\\n    Args:\\n        message (str): The message to generate the context for.\\n\\n    Returns:\\n        str: The generated context.',\n",
       "    '.\\n    Generates a context string for the given message.',\n",
       "    '.\\n\\n    Args:\\n        message (str): The message to generate the context for.\\n\\n    Returns:\\n        str: The context for the message.',\n",
       "    '.\\n    Generates the context for the LLM.\\n\\n    Args:\\n        message (str): The message to generate the context for.\\n\\n    Returns:\\n        str: The generated context.']},\n",
       "  {'c': 'def requires_input(self) -> bool:\\n    return False',\n",
       "   'd': 'Whether this evaluator requires an input string.',\n",
       "   'l': True,\n",
       "   'g': ['to be called by the expert devices',\n",
       "    'to compute the expert to gates transformation.',\n",
       "    'to the gates for the expert devices.',\n",
       "    'to the gates, which are the inputs to the expert.']},\n",
       "  {'c': 'def __init__(\\n    self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n) -> None:\\n    super().__init__(**kwargs)\\n    self._separator = separator\\n    self._is_separator_regex = is_separator_regex',\n",
       "   'd': 'Create a new TextSplitter.',\n",
       "   'l': True,\n",
       "   'g': ['', '', '', '_run_']},\n",
       "  {'c': '    def _run(\\n        self,\\n        *args: Any,\\n        **kwargs: Any,\\n    ) -> Any:\\n',\n",
       "   'd': 'Validate environment.\\nValidate that an Apify API token is set and the apify-client\\nPython package exists in the current environment.',\n",
       "   'l': False,\n",
       "   'g': ['', '', 'Parse the arguments of the command.', '']},\n",
       "  {'c': '    def messages(self) -> List[BaseMessage]:\\n        from botocore.exceptions import ClientError\\n\\n        response = None\\n        try:\\n            response = self.table.get_item(Key={\"SessionId\": self.session_id})\\n        except ClientError as error:\\n            if error.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":\\n                logger.warning(\"No record found with session id: %s\", self.session_id)\\n            else:\\n                logger.error(error)\\n\\n        if response and \"Item\" in response:\\n            items = response[\"Item\"][\"History\"]\\n        else:\\n            items = []\\n\\n        messages = messages_from_dict(items)\\n        return messages',\n",
       "   'd': 'Retrieve the messages from DynamoDB',\n",
       "   'l': True,\n",
       "   'g': ['(x)\\n    \\n    Args:\\n        sample (torch.FloatTensor): [description]\\n        return_dict (bool, optional): [description]. Defaults to True.\\n\\n    Returns:\\n        Union[DecoderOutput, torch.FloatTensor]: [description]',\n",
       "    '(x: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:',\n",
       "    '(x: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:',\n",
       "    '(x: torch.FloatTensor) -> Union[DecoderOutput, torch.FloatTensor]:']},\n",
       "  {'c': 'def validate_channel_or_videoIds_is_set(\\n    cls, values: Dict[str, Any]\\n) -> Dict[str, Any]:\\n    if not values.get(\"channel_name\") and not values.get(\"video_ids\"):\\n        raise ValueError(\"Must specify either channel_name or video_ids\")\\n    return values',\n",
       "   'd': 'Validate that either folder_id or document_ids is set, but not both.',\n",
       "   'l': True,\n",
       "   'g': ['Set cached dataframe for pair and timeframe\\n    \\n    Args:\\n        pair (str): pair\\n        timeframe (str): timeframe\\n        dataframe (DataFrame): dataframe\\n    \\n    Returns:\\n        None',\n",
       "    'Sets the cached dataframe for a given pair and timeframe. \\n    \\n    Args:\\n        pair (str): The pair to cache the dataframe for.\\n        timeframe (str): The timeframe to cache the dataframe for.\\n        dataframe (DataFrame): The dataframe to cache.\\n    \\n    Returns:\\n        None',\n",
       "    'Sets the cached dataframe for a given pair and timeframe.\\n    \\n    Parameters:\\n        pair (str): The pair for which the cached dataframe is being set.\\n        timeframe (str): The timeframe for which the cached dataframe is being set.\\n        dataframe (DataFrame): The DataFrame to be cached.',\n",
       "    'Set the cached dataframe for a given pair and timeframe.\\n    \\n    Args:\\n        pair (str): The pair to cache the dataframe for.\\n        timeframe (str): The timeframe to cache the dataframe for.\\n        dataframe (DataFrame): The dataframe to cache.']},\n",
       "  {'c': 'def dict(self, **kwargs: Any) -> Dict:\\n    _dict = super().dict()\\n    del _dict[\"output_parser\"]\\n    return _dict',\n",
       "   'd': 'Checks if a link is working.\\n\\nIf an error is identified when the request for the link occurs,\\nthe return will be a tuple with the first value True and the second\\nvalue a string containing the error message.\\n\\nIf no errors are identified, the return will be a tuple with the\\nfirst value False and the second an empty string.',\n",
       "   'l': False,\n",
       "   'g': ['', 'Shell +', 'Runs the django shell_plus command', '_summary_']},\n",
       "  {'c': '    def from_template(cls, template: str, **kwargs: Any) -> ChatPromptTemplate:\\n        prompt_template = PromptTemplate.from_template(template, **kwargs)\\n        message = HumanMessagePromptTemplate(prompt=prompt_template)\\n        return cls.from_messages([message])',\n",
       "   'd': 'Create a class from a template.\\n\\nArgs:\\n    template: template string.\\n    **kwargs: keyword arguments to pass to the constructor.\\n\\nReturns:\\n    A new instance of this class.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    :param service:\\n    :param session:\\n    :param region:\\n    :return:',\n",
       "    '.\\n    :param service:\\n    :param session:\\n    :param region:\\n    :return:',\n",
       "    'for the given service, create a client and store it in the cache.',\n",
       "    '.\\n\\n    :param service: The service name.\\n    :param session: The boto3 session.\\n    :param region: The region.\\n    :return: The client.']},\n",
       "  {'c': 'def load(self) -> List[Document]:\\n    return list(self.lazy_load())',\n",
       "   'd': 'Load given path as pages.',\n",
       "   'l': True,\n",
       "   'g': ['(Override) Run all flows in the trial',\n",
       "    \"(self, trial_parameters: dict, workspace: str = 'workspace', callback=None, **kwargs):\",\n",
       "    \"(\\n        self,\\n        trial_parameters: dict,\\n        workspace: str = 'workspace',\\n        callback=None,\\n        **kwargs,\\n    )\",\n",
       "    \".\\n    Run the experiment.\\n\\n    Args:\\n        trial_parameters (dict): Trial parameters.\\n        workspace (str, optional): Workspace to save the results. Defaults to 'workspace'.\\n        callback (function, optional): Callback function. Defaults to None.\\n\\n    Returns:\\n        None\"]},\n",
       "  {'c': 'def _get_combined_score(\\n    self,\\n    document: Document,\\n    vector_relevance: Optional[float],\\n    current_time: datetime.datetime,\\n) -> float:\\n    hours_passed = _get_hours_passed(\\n        current_time,\\n        document.metadata[\"last_accessed_at\"],\\n    )\\n    score = (1.0 - self.decay_rate) ** hours_passed\\n    for key in self.other_score_keys:\\n        if key in document.metadata:\\n            score += document.metadata[key]\\n    if vector_relevance is not None:\\n        score += vector_relevance\\n    return score',\n",
       "   'd': 'Return the combined score for a document.',\n",
       "   'l': True,\n",
       "   'g': ['.\\n    The first process in the process pool.',\n",
       "    '.\\n    * Returns:\\n        Yields:',\n",
       "    'for the first time, we are the main process.',\n",
       "    'Main process.']},\n",
       "  {'c': 'def __init__(\\n    self,\\n    separators: Optional[List[str]] = None,\\n    keep_separator: bool = True,\\n    is_separator_regex: bool = False,\\n    **kwargs: Any,\\n) -> None:\\n    super().__init__(keep_separator=keep_separator, **kwargs)\\n    self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n    self._is_separator_regex = is_separator_regex',\n",
       "   'd': 'Create a new TextSplitter.',\n",
       "   'l': True,\n",
       "   'g': ['Returns the sort key for the given key.', '', '', '']},\n",
       "  {'c': '    def format(self, **kwargs: Any) -> BaseMessage:\\n        text = self.prompt.format(**kwargs)\\n        return AIMessage(content=text, additional_kwargs=self.additional_kwargs)',\n",
       "   'd': 'Format the prompt template.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    Formatted message.',\n",
       "   'l': True,\n",
       "   'g': ['Get environment variables for the application.',\n",
       "    'get environment variables from user',\n",
       "    'get_env()',\n",
       "    'Override the get_env method to add the user env to the environment.']},\n",
       "  {'c': 'def return_stopped_response(\\n    self,\\n    early_stopping_method: str,\\n    intermediate_steps: List[Tuple[AgentAction, str]],\\n    **kwargs: Any,\\n) -> AgentFinish:\\n    if early_stopping_method == \"force\":\\n\\n        return AgentFinish(\\n            {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\\n        )\\n    elif early_stopping_method == \"generate\":\\n\\n        thoughts = \"\"\\n        for action, observation in intermediate_steps:\\n            thoughts += action.log\\n            thoughts += (\\n                f\"\\\\n{self.observation_prefix}{observation}\\\\n{self.llm_prefix}\"\\n            )\\n\\n        thoughts += (\\n            \"\\\\n\\\\nI now need to return a final answer based on the previous steps:\"\\n        )\\n        new_inputs = {\"agent_scratchpad\": thoughts, \"stop\": self._stop}\\n        full_inputs = {**kwargs, **new_inputs}\\n        full_output = self.llm_chain.predict(**full_inputs)\\n\\n        parsed_output = self.output_parser.parse(full_output)\\n        if isinstance(parsed_output, AgentFinish):\\n\\n            return parsed_output\\n        else:\\n\\n\\n            return AgentFinish({\"output\": full_output}, full_output)\\n    else:\\n        raise ValueError(\\n            \"early_stopping_method should be one of `force` or `generate`, \"\\n            f\"got {early_stopping_method}\"\\n        )',\n",
       "   'd': 'Return response when agent has been stopped due to max iterations.',\n",
       "   'l': True,\n",
       "   'g': ['\"\\n    Prints a message to the console.\\n    \\n    :param message: The message to print.\\n    :param exit_code: The exit code to return.\\n    :return: None',\n",
       "    'Print a message to the console in red and bold.\\n    \\n    :param message: The message to print.\\n    :param exit_code: The exit code to use.',\n",
       "    '\"\\n    Print a message to the user and exit.',\n",
       "    '::message:: str\\n    ::exit_code:: int']},\n",
       "  {'c': '    def output_keys(self) -> List[str]:\\n        return self.combine_docs_chain.output_keys',\n",
       "   'd': 'Return output key.\\n\\n:meta private:',\n",
       "   'l': True,\n",
       "   'g': ['Prints a deployment.\\n    \\n    Args:\\n        deployment (dict): The deployment to print.',\n",
       "    'Prints the deployment information.\\n    \\n    Args:\\n        deployment (dict): The deployment information.\\n    \\n    Returns:\\n        dict: The deployment information with the API key obfuscated.',\n",
       "    'Print the deployment.\\n\\n    Args:\\n        deployment (dict): The deployment to print.\\n\\n    Returns:\\n        dict: The deployment.',\n",
       "    'Print deployment dict']},\n",
       "  {'c': '    def format_messages(self, **kwargs: Any) -> List[BaseMessage]:\\n        return [self.format(**kwargs)]',\n",
       "   'd': 'Format messages from kwargs.\\n\\nArgs:\\n    **kwargs: Keyword arguments to use for formatting.\\n\\nReturns:\\n    List of BaseMessages.',\n",
       "   'l': True,\n",
       "   'g': ['Converts a string to UTF-8.',\n",
       "    'Converts a string to UTF-8.',\n",
       "    'Convert to utf-8',\n",
       "    'Converts a string to utf8.']},\n",
       "  {'c': '    def requires_reference(self) -> bool:\\n        return False',\n",
       "   'd': 'Return whether the chain requires a reference.\\n\\nReturns:\\n    bool: True if the chain requires a reference, False otherwise.',\n",
       "   'l': True,\n",
       "   'g': ['.close()',\n",
       "    '.close()\\n    \\n    def clear(self):',\n",
       "    '.close()',\n",
       "    '.close()']},\n",
       "  {'c': 'def parse(self, text: str) -> List[str]:\\n    return text.strip().split(\", \")',\n",
       "   'd': 'Pushes an object to the hub and returns the URL.',\n",
       "   'l': False,\n",
       "   'g': ['',\n",
       "    '.\\n\\n    Parameters\\n    ----------\\n    scores : array-like, shape (n_samples,)\\n        Scores of the detectors.\\n\\n    Returns\\n    -------\\n    candidates : array-like, shape (n_selected,)\\n        Indices of the selected detectors.',\n",
       "    '.',\n",
       "    '.\\n\\n    Args:\\n        scores (array_like): array of scores\\n    Returns:\\n        list: list of indices of selected detectors']},\n",
       "  {'c': 'def test_invalid_suffix() -> None:\\n    path = \"lc://chains/path.invalid\"\\n    loader = Mock()\\n    valid_suffixes = {\"json\"}\\n\\n    with pytest.raises(ValueError, match=\"Unsupported file type.\"):\\n        try_load_from_hub(path, loader, \"chains\", valid_suffixes)\\n\\n    loader.assert_not_called()',\n",
       "   'd': 'Test that a hub path with an invalid suffix raises an error.',\n",
       "   'l': True,\n",
       "   'g': ['.', '.', '.', '.']},\n",
       "  {'c': 'def requires_input(self) -> bool:\\n    return False',\n",
       "   'd': 'Whether this evaluator requires an input string.',\n",
       "   'l': True,\n",
       "   'g': ['Returns a dict containing the results of the identification.\\n    \\n    :param text: The text to identify.\\n    :param min_rarity: The minimum rarity of the result.\\n    :param max_rarity: The maximum rarity of the result.\\n    :param included_tags: The tags to include in the result.\\n    :param excluded_tags: The tags to exclude from the result.\\n    :return: A dict containing the results of the identification.',\n",
       "    'Returns a dict with the results of the identification process.',\n",
       "    '',\n",
       "    ':param text: Text to be searched for\\n    :param min_rarity: Minimum rarity of the result\\n    :param max_rarity: Maximum rarity of the result\\n    :param included_tags: List of tags to include in the result\\n    :param excluded_tags: List of tags to exclude from the result\\n    :return: Dictionary with the following keys:\\n        - id: The ID of the result\\n        - text: The text of the result\\n        - rarity: The rarity of the result\\n        - tags: The tags of the result']},\n",
       "  {'c': '    def __init__(\\n        self,\\n        url: str,\\n        max_depth: Optional[int] = 2,\\n        use_async: Optional[bool] = None,\\n        extractor: Optional[Callable[[str], str]] = None,\\n        metadata_extractor: Optional[Callable[[str, str], str]] = None,\\n        exclude_dirs: Optional[Sequence[str]] = (),\\n        timeout: Optional[int] = 10,\\n        prevent_outside: Optional[bool] = True,\\n        link_regex: Union[str, re.Pattern, None] = None,\\n        headers: Optional[dict] = None,\\n        check_response_status: bool = False,\\n    ) -> None:\\n        self.url = url\\n        self.max_depth = max_depth if max_depth is not None else 2\\n        self.use_async = use_async if use_async is not None else False\\n        self.extractor = extractor if extractor is not None else lambda x: x\\n        self.metadata_extractor = (\\n            metadata_extractor\\n            if metadata_extractor is not None\\n            else _metadata_extractor\\n        )\\n        self.exclude_dirs = exclude_dirs if exclude_dirs is not None else ()\\n\\n        if any(url.startswith(exclude_dir) for exclude_dir in self.exclude_dirs):\\n            raise ValueError(\\n                f\"Base url is included in exclude_dirs. Received base_url: {url} and \"\\n                f\"exclude_dirs: {self.exclude_dirs}\"\\n            )\\n\\n        self.timeout = timeout\\n        self.prevent_outside = prevent_outside if prevent_outside is not None else True\\n        self.link_regex = link_regex\\n        self._lock = asyncio.Lock() if self.use_async else None\\n        self.headers = headers\\n        self.check_response_status = check_response_status',\n",
       "   'd': 'Compute query embeddings using a HuggingFace instruct model.\\n\\nArgs:\\n    text: The text to embed.\\n\\nReturns:\\n    Embeddings for the text.',\n",
       "   'l': False,\n",
       "   'g': ['(str) -> NoneType',\n",
       "    '.',\n",
       "    '(str) -> str',\n",
       "    'for each row in result, add a new row to self.rows']},\n",
       "  {'c': '    def _call(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        chain_input = {**inputs}\\n        if self.agent_tools:\\n            chain_input[\"tool_descriptions\"] = self._tools_description\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        raw_output = self.eval_chain.run(\\n            chain_input, callbacks=_run_manager.get_child()\\n        )\\n        parsed_output = self.output_parser.parse(raw_output)\\n\\n        if self.return_reasoning:\\n            return {\"score\": parsed_output.score, \"reasoning\": parsed_output.reasoning}\\n\\n        return {\"score\": parsed_output.score}',\n",
       "   'd': 'Wait for the given futures to complete.',\n",
       "   'l': False,\n",
       "   'g': [\"'\\n    Adds summaries for losses\\n    Args:\\n        total_loss: Total loss from loss op\\n    Returns:\\n        loss_averages_op: op for updating moving averages of losses\",\n",
       "    'Adds summaries for losses and the total loss to tensorboard.',\n",
       "    \"'Add summaries for losses and gradients.\",\n",
       "    'Add summaries for losses and accuracies.']}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data = {\n",
    "    mid: [td | {'g': gd} for td, gd in zip(test_data, updated_docstrings_0[mid])]\n",
    "    for mid in updated_docstrings_0.keys()\n",
    "}\n",
    "eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name microsoft/codebert-base. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('microsoft/codebert-base', device='cuda:0')\n",
    "\n",
    "def perform_test(\n",
    "    codes: List[str], docstrings: List[str], generated_docstrings: List[List[str]],\n",
    "    distance_function: str, normalize: bool, sample_many: bool\n",
    ") -> List[float]:\n",
    "    assert len(codes) == len(docstrings) == len(generated_docstrings)\n",
    "\n",
    "    n_codes = len(codes)\n",
    "    n_samples = 4 if sample_many else 1\n",
    "\n",
    "    codes = [code for code in codes for _ in range(n_samples)]\n",
    "    docstrings = [docstring for docstring in docstrings for _ in range(n_samples)]\n",
    "    if sample_many:\n",
    "        generated_docstrings = [d for ds in generated_docstrings for d in ds]\n",
    "    else:\n",
    "        generated_docstrings = [d[0] for d in generated_docstrings]\n",
    "\n",
    "    assert len(codes) == len(docstrings) == len(generated_docstrings)\n",
    "\n",
    "    code_embeddings = model.encode(codes, normalize_embeddings=normalize)\n",
    "    docstring_embeddings = model.encode(docstrings, normalize_embeddings=normalize)\n",
    "    generated_docstring_embeddings = model.encode(generated_docstrings, normalize_embeddings=normalize)\n",
    "\n",
    "    model.similarity_fn_name = distance_function\n",
    "\n",
    "    docstring_similarities = model.similarity_pairwise(code_embeddings, docstring_embeddings)\n",
    "    generated_docstring_similarities = model.similarity_pairwise(code_embeddings, generated_docstring_embeddings)\n",
    "\n",
    "    if distance_function == 'euclidean':\n",
    "        docstring_similarities = -docstring_similarities\n",
    "        generated_docstring_similarities = -generated_docstring_similarities\n",
    "\n",
    "    if sample_many:\n",
    "        docstring_similarities = docstring_similarities.reshape(n_codes, n_samples)\n",
    "        docstring_similarities = torch.median(docstring_similarities, dim=1).values\n",
    "\n",
    "        generated_docstring_similarities = generated_docstring_similarities.reshape(n_codes, n_samples)\n",
    "        generated_docstring_similarities = torch.median(generated_docstring_similarities, dim=1).values\n",
    "\n",
    "    ratios = torch.div(docstring_similarities, generated_docstring_similarities)\n",
    "    return ratios.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [1:01:10<00:00, 458.76s/it]\n",
      "100%|██████████| 8/8 [34:32<00:00, 259.02s/it]  \n"
     ]
    }
   ],
   "source": [
    "ratio_results = {}\n",
    "\n",
    "for mid, data in eval_data.items():\n",
    "    codes = [d['c'] for d in data]\n",
    "    docstrings = [d['d'] for d in data]\n",
    "    generated_docstrings = [d['g'] for d in data]\n",
    "    labels = [d['l'] for d in data]\n",
    "\n",
    "    ratio_results[mid] = {}\n",
    "\n",
    "    for distance_function, normalize, sample_many in tqdm(itertools.product(\n",
    "        ['cosine', 'euclidean'],\n",
    "        [True, False],\n",
    "        [True, False]\n",
    "    ), total=2*2*2):\n",
    "        ratios = perform_test(codes, docstrings, generated_docstrings, distance_function, normalize, sample_many)\n",
    "        ratio_results[mid][(distance_function, normalize, sample_many)] = ratios\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checkpoints/finetuned_0': {('cosine', True, True): [1.215024709701538,\n",
       "   1.1801878213882446,\n",
       "   1.5343092679977417,\n",
       "   0.9335689544677734,\n",
       "   1.3937071561813354,\n",
       "   0.9440323710441589,\n",
       "   1.2586134672164917,\n",
       "   1.456436038017273,\n",
       "   0.9222041964530945,\n",
       "   1.7389726638793945,\n",
       "   1.6034759283065796,\n",
       "   1.3851253986358643,\n",
       "   1.064948558807373,\n",
       "   0.9437777400016785,\n",
       "   1.535832405090332,\n",
       "   0.9963194727897644,\n",
       "   1.5824706554412842,\n",
       "   1.024376392364502,\n",
       "   0.9975019693374634,\n",
       "   1.5580897331237793,\n",
       "   0.9825649261474609,\n",
       "   1.0361788272857666,\n",
       "   0.975216805934906,\n",
       "   1.615065097808838,\n",
       "   1.5632058382034302,\n",
       "   1.5235546827316284,\n",
       "   1.2201893329620361,\n",
       "   1.0456717014312744,\n",
       "   0.9650365710258484,\n",
       "   0.9124303460121155,\n",
       "   1.270965814590454,\n",
       "   1.0830395221710205,\n",
       "   1.3693957328796387,\n",
       "   1.6603580713272095,\n",
       "   1.536556601524353,\n",
       "   1.25315523147583,\n",
       "   1.6403770446777344,\n",
       "   1.399306297302246,\n",
       "   0.9636114835739136,\n",
       "   0.9664019346237183,\n",
       "   1.7461392879486084,\n",
       "   1.0109235048294067,\n",
       "   1.0111559629440308,\n",
       "   1.0289230346679688,\n",
       "   1.033232569694519,\n",
       "   1.5856809616088867,\n",
       "   1.5556163787841797,\n",
       "   1.0840706825256348,\n",
       "   0.9174164533615112,\n",
       "   0.9765291810035706,\n",
       "   1.5553375482559204,\n",
       "   1.6512117385864258,\n",
       "   0.9172935485839844,\n",
       "   1.0672276020050049,\n",
       "   1.4702306985855103,\n",
       "   1.0043152570724487,\n",
       "   0.9427614808082581,\n",
       "   1.444201111793518,\n",
       "   1.2373621463775635,\n",
       "   1.0260041952133179,\n",
       "   1.634823203086853,\n",
       "   1.0046159029006958,\n",
       "   0.9910733699798584,\n",
       "   1.0599061250686646,\n",
       "   1.4243083000183105,\n",
       "   1.0294798612594604,\n",
       "   1.4967061281204224,\n",
       "   0.9821077585220337,\n",
       "   0.954333484172821,\n",
       "   1.0298042297363281,\n",
       "   1.0658373832702637,\n",
       "   1.5040290355682373,\n",
       "   0.9799648523330688,\n",
       "   1.5588816404342651,\n",
       "   1.4514578580856323,\n",
       "   1.0052011013031006,\n",
       "   1.6740214824676514,\n",
       "   1.6875942945480347,\n",
       "   1.3646769523620605,\n",
       "   1.4602471590042114,\n",
       "   0.9992771148681641,\n",
       "   0.9961969256401062,\n",
       "   0.8880087733268738,\n",
       "   1.1544687747955322,\n",
       "   0.9357317686080933,\n",
       "   1.027686357498169,\n",
       "   1.5949811935424805,\n",
       "   1.0754565000534058,\n",
       "   1.6031770706176758,\n",
       "   1.2161437273025513,\n",
       "   1.7884362936019897,\n",
       "   1.0193766355514526,\n",
       "   0.9611062407493591,\n",
       "   1.6424155235290527,\n",
       "   1.0105451345443726,\n",
       "   0.9897129535675049,\n",
       "   1.5712571144104004,\n",
       "   0.8639070987701416,\n",
       "   1.2198729515075684,\n",
       "   1.4108673334121704,\n",
       "   1.0240814685821533,\n",
       "   1.8195825815200806,\n",
       "   1.5826646089553833,\n",
       "   0.9912734031677246,\n",
       "   0.9414337873458862,\n",
       "   0.9266393184661865,\n",
       "   0.9672094583511353,\n",
       "   1.7844548225402832,\n",
       "   1.502553105354309,\n",
       "   1.6601004600524902,\n",
       "   1.275862693786621,\n",
       "   1.5921235084533691,\n",
       "   0.959517776966095,\n",
       "   0.9721444249153137,\n",
       "   1.5796929597854614,\n",
       "   1.2984871864318848,\n",
       "   1.4517372846603394,\n",
       "   1.145400047302246,\n",
       "   1.0954457521438599,\n",
       "   1.6103730201721191,\n",
       "   1.6546175479888916,\n",
       "   1.834252119064331,\n",
       "   1.2227760553359985,\n",
       "   0.9893301129341125,\n",
       "   0.8850424289703369,\n",
       "   1.0636379718780518,\n",
       "   1.4014708995819092,\n",
       "   1.4967138767242432,\n",
       "   1.5230703353881836,\n",
       "   1.622737169265747,\n",
       "   0.9952871203422546,\n",
       "   0.9956913590431213,\n",
       "   0.9300427436828613,\n",
       "   1.3171435594558716,\n",
       "   1.0244718790054321,\n",
       "   1.0183824300765991,\n",
       "   1.5561782121658325,\n",
       "   0.9401514530181885,\n",
       "   0.9055017828941345,\n",
       "   1.5095000267028809,\n",
       "   0.9390719532966614,\n",
       "   1.4726989269256592,\n",
       "   1.6805918216705322,\n",
       "   1.147100806236267,\n",
       "   0.9146212935447693,\n",
       "   0.9835090637207031,\n",
       "   1.5451968908309937,\n",
       "   1.5528463125228882,\n",
       "   1.5728394985198975,\n",
       "   1.0471082925796509,\n",
       "   1.0008535385131836,\n",
       "   1.5761851072311401,\n",
       "   0.9772958159446716,\n",
       "   1.5577458143234253,\n",
       "   1.5593878030776978,\n",
       "   0.9700985550880432,\n",
       "   0.9927766919136047,\n",
       "   0.9736664891242981,\n",
       "   0.9520086646080017,\n",
       "   0.9779797792434692,\n",
       "   1.0688261985778809,\n",
       "   1.5707679986953735,\n",
       "   1.728203535079956,\n",
       "   1.0365440845489502,\n",
       "   0.9236395359039307,\n",
       "   1.0314486026763916,\n",
       "   0.9858591556549072,\n",
       "   1.0523009300231934,\n",
       "   0.9844624400138855,\n",
       "   0.8979872465133667,\n",
       "   1.520505666732788,\n",
       "   0.9428144693374634,\n",
       "   1.0827891826629639,\n",
       "   0.9706099033355713,\n",
       "   0.9633612036705017,\n",
       "   0.9184370040893555,\n",
       "   1.0117616653442383,\n",
       "   1.5569227933883667,\n",
       "   1.4556940793991089,\n",
       "   1.0058842897415161,\n",
       "   1.5265393257141113,\n",
       "   1.4279769659042358,\n",
       "   1.02170729637146,\n",
       "   1.3188592195510864,\n",
       "   0.9193068146705627,\n",
       "   1.1883771419525146,\n",
       "   0.9719161987304688,\n",
       "   1.162802815437317,\n",
       "   1.1085634231567383,\n",
       "   1.0742318630218506,\n",
       "   1.7783232927322388,\n",
       "   0.9617831110954285,\n",
       "   1.913895845413208,\n",
       "   0.8835693001747131,\n",
       "   0.9726011157035828,\n",
       "   1.5268502235412598,\n",
       "   1.8085238933563232,\n",
       "   1.0696223974227905,\n",
       "   1.5994945764541626,\n",
       "   1.6315592527389526,\n",
       "   0.8313369154930115,\n",
       "   1.1707359552383423,\n",
       "   1.5051119327545166,\n",
       "   1.3198518753051758,\n",
       "   1.0116735696792603,\n",
       "   0.9268602728843689,\n",
       "   1.5767357349395752,\n",
       "   1.0071314573287964,\n",
       "   0.9958072900772095,\n",
       "   1.5773133039474487,\n",
       "   1.034435510635376,\n",
       "   1.0158355236053467,\n",
       "   0.9252803325653076,\n",
       "   1.0765998363494873,\n",
       "   1.4428085088729858,\n",
       "   1.6080708503723145,\n",
       "   0.9536820650100708,\n",
       "   1.5956039428710938,\n",
       "   1.0839077234268188,\n",
       "   1.046086311340332,\n",
       "   1.0567079782485962,\n",
       "   0.9725559949874878,\n",
       "   0.9709985852241516,\n",
       "   0.9425356388092041,\n",
       "   1.1355173587799072,\n",
       "   0.9970365166664124,\n",
       "   1.6123262643814087,\n",
       "   0.9491892457008362,\n",
       "   1.7078107595443726,\n",
       "   1.0994008779525757,\n",
       "   1.7232671976089478,\n",
       "   1.01065993309021,\n",
       "   1.0889872312545776,\n",
       "   1.5573203563690186,\n",
       "   0.9874385595321655,\n",
       "   1.5860685110092163,\n",
       "   0.9238314032554626,\n",
       "   1.7573201656341553,\n",
       "   1.3027074337005615,\n",
       "   1.7461392879486084,\n",
       "   1.0556707382202148,\n",
       "   1.2072051763534546,\n",
       "   1.495665431022644,\n",
       "   1.5661208629608154,\n",
       "   1.5345962047576904,\n",
       "   0.9723284840583801,\n",
       "   0.9970983862876892,\n",
       "   0.9586696028709412,\n",
       "   1.1391940116882324,\n",
       "   0.8847947716712952,\n",
       "   0.9623484015464783,\n",
       "   1.0930156707763672,\n",
       "   1.343626856803894,\n",
       "   1.5206916332244873,\n",
       "   1.5623196363449097,\n",
       "   0.96750408411026,\n",
       "   0.9987722635269165,\n",
       "   0.9911842942237854,\n",
       "   1.551658272743225,\n",
       "   0.9923023581504822,\n",
       "   1.3646769523620605,\n",
       "   2.046804189682007,\n",
       "   1.0536959171295166,\n",
       "   1.0517990589141846,\n",
       "   1.529292345046997,\n",
       "   1.5909336805343628,\n",
       "   0.9439467191696167,\n",
       "   1.5084030628204346,\n",
       "   1.4978573322296143,\n",
       "   1.0477920770645142,\n",
       "   0.9573593735694885,\n",
       "   1.7214757204055786,\n",
       "   1.8181055784225464,\n",
       "   0.8700271248817444,\n",
       "   1.054687738418579,\n",
       "   0.9194582104682922,\n",
       "   1.0092291831970215,\n",
       "   0.9402467608451843,\n",
       "   1.722203016281128,\n",
       "   1.0563527345657349,\n",
       "   1.3851253986358643,\n",
       "   1.0251036882400513,\n",
       "   1.7086403369903564,\n",
       "   0.9155793786048889,\n",
       "   1.0698461532592773,\n",
       "   0.9923543334007263,\n",
       "   0.9741048812866211,\n",
       "   1.107139229774475,\n",
       "   0.9337311387062073,\n",
       "   1.566564917564392,\n",
       "   1.6928960084915161,\n",
       "   1.5394642353057861,\n",
       "   1.6078734397888184,\n",
       "   0.9348987936973572,\n",
       "   1.5359539985656738,\n",
       "   0.9559752345085144,\n",
       "   1.5860786437988281,\n",
       "   1.4731545448303223,\n",
       "   1.5939860343933105,\n",
       "   1.0404390096664429,\n",
       "   1.1439114809036255,\n",
       "   1.7927926778793335,\n",
       "   1.0630412101745605,\n",
       "   0.818041980266571,\n",
       "   0.95777827501297,\n",
       "   1.3362047672271729,\n",
       "   1.0464574098587036,\n",
       "   1.5848993062973022,\n",
       "   0.9579952359199524,\n",
       "   1.0669771432876587,\n",
       "   0.932231068611145,\n",
       "   0.9786744713783264,\n",
       "   0.9723433256149292,\n",
       "   0.9551302790641785,\n",
       "   1.3312560319900513,\n",
       "   1.06786048412323,\n",
       "   0.9573044180870056,\n",
       "   0.9588267207145691,\n",
       "   1.0071139335632324,\n",
       "   0.9554629325866699,\n",
       "   1.434101939201355,\n",
       "   1.6437431573867798,\n",
       "   0.9985105991363525,\n",
       "   1.8275765180587769,\n",
       "   0.9389759302139282,\n",
       "   1.5810036659240723,\n",
       "   0.9966833591461182,\n",
       "   1.5112062692642212,\n",
       "   1.5534536838531494,\n",
       "   0.947834312915802,\n",
       "   0.9296550154685974,\n",
       "   1.0699639320373535,\n",
       "   1.4191713333129883,\n",
       "   0.9664839506149292,\n",
       "   0.9668486714363098,\n",
       "   0.9346640706062317,\n",
       "   0.8814444541931152,\n",
       "   1.3437786102294922,\n",
       "   1.621201753616333,\n",
       "   1.3522356748580933,\n",
       "   1.5598000288009644,\n",
       "   0.938273549079895,\n",
       "   1.5652663707733154,\n",
       "   1.0212743282318115,\n",
       "   1.6118615865707397,\n",
       "   1.2283681631088257,\n",
       "   1.1825312376022339,\n",
       "   1.0126757621765137,\n",
       "   1.5747089385986328,\n",
       "   1.7858110666275024,\n",
       "   0.9422730803489685,\n",
       "   0.9793405532836914,\n",
       "   1.5795763731002808,\n",
       "   0.9836630821228027,\n",
       "   0.9943992495536804,\n",
       "   1.55598783493042,\n",
       "   1.5899325609207153,\n",
       "   1.7912931442260742,\n",
       "   0.9637202024459839,\n",
       "   1.5527198314666748,\n",
       "   1.567886471748352,\n",
       "   1.5376360416412354,\n",
       "   1.0355671644210815,\n",
       "   1.7077492475509644,\n",
       "   1.4685697555541992,\n",
       "   1.370864987373352,\n",
       "   1.0707136392593384,\n",
       "   1.3300079107284546,\n",
       "   1.070365071296692,\n",
       "   0.9838764667510986,\n",
       "   1.1278506517410278,\n",
       "   1.0773167610168457,\n",
       "   1.479321002960205,\n",
       "   1.0770305395126343,\n",
       "   1.8026010990142822,\n",
       "   0.8667320013046265,\n",
       "   0.9677326679229736,\n",
       "   1.0179232358932495,\n",
       "   1.6143200397491455,\n",
       "   0.9577766060829163,\n",
       "   1.25900137424469,\n",
       "   1.005363941192627,\n",
       "   1.6055766344070435,\n",
       "   1.0282753705978394,\n",
       "   1.118257999420166,\n",
       "   0.9603552222251892,\n",
       "   1.663164734840393,\n",
       "   1.1118768453598022,\n",
       "   0.9213732481002808,\n",
       "   1.5259181261062622,\n",
       "   1.6061012744903564,\n",
       "   1.5567725896835327,\n",
       "   1.343626856803894,\n",
       "   1.0595083236694336,\n",
       "   0.9147692918777466,\n",
       "   1.0890177488327026,\n",
       "   1.628177523612976,\n",
       "   1.0149226188659668,\n",
       "   1.0777027606964111,\n",
       "   0.969658374786377,\n",
       "   1.296104907989502,\n",
       "   1.2048462629318237,\n",
       "   1.1022452116012573,\n",
       "   1.0171712636947632,\n",
       "   1.6015387773513794,\n",
       "   0.9917083978652954,\n",
       "   0.8999290466308594,\n",
       "   1.5886014699935913,\n",
       "   1.0556042194366455,\n",
       "   1.2461496591567993,\n",
       "   1.454301118850708,\n",
       "   1.6267292499542236,\n",
       "   0.9998563528060913,\n",
       "   1.0013877153396606,\n",
       "   1.6748417615890503,\n",
       "   0.8771500587463379,\n",
       "   1.1076829433441162,\n",
       "   1.8113682270050049,\n",
       "   1.6093310117721558,\n",
       "   1.4436798095703125,\n",
       "   1.5361634492874146,\n",
       "   1.895896315574646,\n",
       "   1.030733585357666,\n",
       "   1.4753323793411255,\n",
       "   1.4714807271957397,\n",
       "   1.095598816871643,\n",
       "   1.125461459159851,\n",
       "   1.4688677787780762,\n",
       "   1.7121362686157227,\n",
       "   1.5615627765655518,\n",
       "   1.4228187799453735,\n",
       "   1.0334559679031372,\n",
       "   1.5902096033096313,\n",
       "   0.9615867137908936,\n",
       "   0.9446974396705627,\n",
       "   1.3138728141784668,\n",
       "   1.0229949951171875,\n",
       "   1.13666570186615,\n",
       "   0.9698854684829712,\n",
       "   1.6411455869674683,\n",
       "   1.5409624576568604,\n",
       "   1.014225721359253,\n",
       "   1.6950355768203735,\n",
       "   0.9868803024291992,\n",
       "   1.5933128595352173,\n",
       "   1.7421373128890991,\n",
       "   1.2764681577682495,\n",
       "   1.030734896659851,\n",
       "   1.0470004081726074,\n",
       "   1.4727492332458496,\n",
       "   1.0330044031143188,\n",
       "   1.6037094593048096,\n",
       "   1.8829319477081299,\n",
       "   1.7029685974121094,\n",
       "   1.811219573020935,\n",
       "   1.4442813396453857,\n",
       "   0.9678835272789001,\n",
       "   0.9365245699882507,\n",
       "   1.0038974285125732,\n",
       "   0.9812369346618652,\n",
       "   1.6316829919815063,\n",
       "   1.051087498664856,\n",
       "   1.5681016445159912,\n",
       "   1.1105877161026,\n",
       "   1.0384751558303833,\n",
       "   1.5994837284088135,\n",
       "   1.4240925312042236,\n",
       "   1.097865343093872,\n",
       "   0.9362594485282898,\n",
       "   1.6854625940322876,\n",
       "   0.9158271551132202,\n",
       "   1.572202205657959,\n",
       "   1.034712314605713,\n",
       "   1.012986421585083,\n",
       "   0.9989782571792603,\n",
       "   0.9942393898963928,\n",
       "   1.6875288486480713,\n",
       "   0.950086772441864,\n",
       "   0.8928606510162354,\n",
       "   1.208999752998352,\n",
       "   1.1477075815200806,\n",
       "   1.3787678480148315,\n",
       "   0.9811475276947021,\n",
       "   1.006426215171814,\n",
       "   1.0883256196975708,\n",
       "   1.126257300376892,\n",
       "   1.1691001653671265,\n",
       "   0.9554484486579895,\n",
       "   1.005495548248291,\n",
       "   1.0383546352386475,\n",
       "   0.9683875441551208,\n",
       "   0.9501832723617554,\n",
       "   1.0139095783233643,\n",
       "   1.1113260984420776,\n",
       "   0.9765726923942566,\n",
       "   0.9393401145935059,\n",
       "   1.6764520406723022,\n",
       "   1.0189732313156128,\n",
       "   1.0004942417144775,\n",
       "   1.0823004245758057,\n",
       "   0.970511794090271,\n",
       "   1.6043062210083008,\n",
       "   1.0459802150726318,\n",
       "   1.062394142150879,\n",
       "   1.4220041036605835,\n",
       "   1.4742629528045654,\n",
       "   1.0105730295181274,\n",
       "   0.9447066187858582,\n",
       "   1.0037775039672852,\n",
       "   1.0824410915374756,\n",
       "   1.6311078071594238,\n",
       "   0.9544586539268494],\n",
       "  ('cosine', True, False): [1.215024709701538,\n",
       "   1.1801878213882446,\n",
       "   1.5343092679977417,\n",
       "   0.903221845626831,\n",
       "   1.0117932558059692,\n",
       "   0.9683955907821655,\n",
       "   1.0416202545166016,\n",
       "   0.8878309726715088,\n",
       "   0.9223654270172119,\n",
       "   1.7389726638793945,\n",
       "   1.6034759283065796,\n",
       "   1.3851255178451538,\n",
       "   1.064948558807373,\n",
       "   0.943777859210968,\n",
       "   1.5358322858810425,\n",
       "   0.9963194727897644,\n",
       "   0.9975734949111938,\n",
       "   1.024376392364502,\n",
       "   0.9975019097328186,\n",
       "   1.5580897331237793,\n",
       "   0.9825649857521057,\n",
       "   1.0361788272857666,\n",
       "   0.9752167463302612,\n",
       "   1.615065097808838,\n",
       "   1.5632058382034302,\n",
       "   1.5235546827316284,\n",
       "   1.0856753587722778,\n",
       "   1.0456717014312744,\n",
       "   0.9641268849372864,\n",
       "   0.8678069710731506,\n",
       "   1.2709660530090332,\n",
       "   1.0830395221710205,\n",
       "   1.3693957328796387,\n",
       "   1.6603580713272095,\n",
       "   1.536556601524353,\n",
       "   1.2531551122665405,\n",
       "   1.640377163887024,\n",
       "   1.399306297302246,\n",
       "   0.9636114835739136,\n",
       "   0.9851092100143433,\n",
       "   1.7461392879486084,\n",
       "   1.0109235048294067,\n",
       "   1.0295262336730957,\n",
       "   0.9273030161857605,\n",
       "   1.0332326889038086,\n",
       "   1.5856810808181763,\n",
       "   1.5556163787841797,\n",
       "   1.0840706825256348,\n",
       "   0.8737190961837769,\n",
       "   1.564797043800354,\n",
       "   1.5553375482559204,\n",
       "   1.6512117385864258,\n",
       "   0.8998662233352661,\n",
       "   1.0672276020050049,\n",
       "   1.4702306985855103,\n",
       "   0.9774971008300781,\n",
       "   0.9427614808082581,\n",
       "   1.4442009925842285,\n",
       "   0.9271070957183838,\n",
       "   1.0192872285842896,\n",
       "   1.634823203086853,\n",
       "   0.9801622033119202,\n",
       "   0.991073489189148,\n",
       "   1.015936017036438,\n",
       "   1.4243083000183105,\n",
       "   1.0275403261184692,\n",
       "   1.4967058897018433,\n",
       "   0.9915468096733093,\n",
       "   0.9211553335189819,\n",
       "   1.0974080562591553,\n",
       "   1.0018409490585327,\n",
       "   0.9221382141113281,\n",
       "   1.0466272830963135,\n",
       "   1.5588816404342651,\n",
       "   0.9600731730461121,\n",
       "   0.9318429827690125,\n",
       "   1.674021601676941,\n",
       "   1.0257855653762817,\n",
       "   0.9658502340316772,\n",
       "   1.4602470397949219,\n",
       "   0.9992771148681641,\n",
       "   1.5539908409118652,\n",
       "   0.8440041542053223,\n",
       "   1.0507971048355103,\n",
       "   0.9149311780929565,\n",
       "   1.0276864767074585,\n",
       "   1.5949811935424805,\n",
       "   1.0754565000534058,\n",
       "   1.6031770706176758,\n",
       "   1.2234885692596436,\n",
       "   1.7884362936019897,\n",
       "   1.0089640617370605,\n",
       "   0.9611062407493591,\n",
       "   1.6424155235290527,\n",
       "   1.010545015335083,\n",
       "   0.9897129535675049,\n",
       "   1.57125723361969,\n",
       "   0.8639071583747864,\n",
       "   1.0277882814407349,\n",
       "   1.1036406755447388,\n",
       "   1.0240815877914429,\n",
       "   1.8195827007293701,\n",
       "   1.5826643705368042,\n",
       "   0.9822062253952026,\n",
       "   0.9414340257644653,\n",
       "   0.9230279326438904,\n",
       "   0.9228522181510925,\n",
       "   1.7844550609588623,\n",
       "   1.502553105354309,\n",
       "   1.6601004600524902,\n",
       "   1.275862693786621,\n",
       "   1.5921236276626587,\n",
       "   0.9559922814369202,\n",
       "   0.9684054851531982,\n",
       "   1.5796929597854614,\n",
       "   1.2984871864318848,\n",
       "   1.4517372846603394,\n",
       "   1.145400047302246,\n",
       "   1.0954457521438599,\n",
       "   0.9910772442817688,\n",
       "   1.6546175479888916,\n",
       "   1.834252119064331,\n",
       "   1.0420156717300415,\n",
       "   0.986642599105835,\n",
       "   0.8833627104759216,\n",
       "   1.0636379718780518,\n",
       "   1.4014707803726196,\n",
       "   1.4967141151428223,\n",
       "   1.5230703353881836,\n",
       "   0.9376459717750549,\n",
       "   0.9952870011329651,\n",
       "   0.9803332686424255,\n",
       "   0.9300427436828613,\n",
       "   1.3171435594558716,\n",
       "   0.9876964688301086,\n",
       "   0.945036768913269,\n",
       "   1.00873863697052,\n",
       "   0.937954306602478,\n",
       "   0.9055016040802002,\n",
       "   1.5095000267028809,\n",
       "   0.9390718340873718,\n",
       "   0.8953657746315002,\n",
       "   1.6805918216705322,\n",
       "   1.1683192253112793,\n",
       "   0.9146212935447693,\n",
       "   0.9299865365028381,\n",
       "   1.5451968908309937,\n",
       "   0.9355311393737793,\n",
       "   1.5728394985198975,\n",
       "   1.0471082925796509,\n",
       "   0.9455510973930359,\n",
       "   0.9812451004981995,\n",
       "   0.9717985987663269,\n",
       "   1.5577458143234253,\n",
       "   1.559388279914856,\n",
       "   0.9583392143249512,\n",
       "   0.9927766919136047,\n",
       "   0.9810827374458313,\n",
       "   0.8767619132995605,\n",
       "   0.9344195127487183,\n",
       "   0.9833101630210876,\n",
       "   1.570767879486084,\n",
       "   1.728203535079956,\n",
       "   0.948112964630127,\n",
       "   0.8862253427505493,\n",
       "   1.1095507144927979,\n",
       "   0.9858590960502625,\n",
       "   1.0523009300231934,\n",
       "   1.5605930089950562,\n",
       "   0.8979872465133667,\n",
       "   1.520505666732788,\n",
       "   0.9562863707542419,\n",
       "   1.0827891826629639,\n",
       "   0.9687350988388062,\n",
       "   0.9415143132209778,\n",
       "   0.9279553294181824,\n",
       "   1.0117616653442383,\n",
       "   1.5569227933883667,\n",
       "   1.4556939601898193,\n",
       "   0.9617518782615662,\n",
       "   1.5265393257141113,\n",
       "   1.4279768466949463,\n",
       "   1.1874712705612183,\n",
       "   1.3188592195510864,\n",
       "   0.9193068146705627,\n",
       "   1.1883772611618042,\n",
       "   0.8710188269615173,\n",
       "   1.109413504600525,\n",
       "   1.1085633039474487,\n",
       "   1.001477599143982,\n",
       "   1.7783232927322388,\n",
       "   0.9617830514907837,\n",
       "   1.9138963222503662,\n",
       "   0.876066267490387,\n",
       "   0.9726012349128723,\n",
       "   1.5268502235412598,\n",
       "   1.8085238933563232,\n",
       "   1.0599608421325684,\n",
       "   1.5994945764541626,\n",
       "   1.6315592527389526,\n",
       "   0.8313368558883667,\n",
       "   1.1707360744476318,\n",
       "   1.5051119327545166,\n",
       "   1.013127088546753,\n",
       "   1.0139859914779663,\n",
       "   1.1392993927001953,\n",
       "   0.9980624914169312,\n",
       "   1.007131576538086,\n",
       "   0.9958072900772095,\n",
       "   1.5773133039474487,\n",
       "   0.9000405073165894,\n",
       "   0.8891431093215942,\n",
       "   0.8769994378089905,\n",
       "   1.0765999555587769,\n",
       "   1.4428085088729858,\n",
       "   1.03731369972229,\n",
       "   1.2244033813476562,\n",
       "   1.5956040620803833,\n",
       "   1.0839076042175293,\n",
       "   1.007020354270935,\n",
       "   1.0567080974578857,\n",
       "   0.9785428047180176,\n",
       "   1.5945661067962646,\n",
       "   0.9425356984138489,\n",
       "   1.1296045780181885,\n",
       "   0.9970365166664124,\n",
       "   1.6123260259628296,\n",
       "   0.9491892457008362,\n",
       "   1.7078102827072144,\n",
       "   1.1213321685791016,\n",
       "   1.7232673168182373,\n",
       "   1.01065993309021,\n",
       "   1.0665156841278076,\n",
       "   0.9774854183197021,\n",
       "   0.9874385595321655,\n",
       "   1.5860682725906372,\n",
       "   0.9238315224647522,\n",
       "   1.7573201656341553,\n",
       "   1.3027074337005615,\n",
       "   1.7461392879486084,\n",
       "   0.9406553506851196,\n",
       "   0.9926390647888184,\n",
       "   1.495665431022644,\n",
       "   1.566120982170105,\n",
       "   1.5345964431762695,\n",
       "   0.9288236498832703,\n",
       "   1.49824059009552,\n",
       "   0.957863450050354,\n",
       "   1.0233501195907593,\n",
       "   0.8231244087219238,\n",
       "   0.9698854684829712,\n",
       "   1.0930157899856567,\n",
       "   0.954444169998169,\n",
       "   1.0219647884368896,\n",
       "   1.5623195171356201,\n",
       "   0.9946677684783936,\n",
       "   0.9987724423408508,\n",
       "   0.9607810378074646,\n",
       "   0.9726779460906982,\n",
       "   0.9829646944999695,\n",
       "   1.3646767139434814,\n",
       "   2.046804189682007,\n",
       "   1.0069044828414917,\n",
       "   1.0517990589141846,\n",
       "   1.5292925834655762,\n",
       "   1.5909335613250732,\n",
       "   0.9439467787742615,\n",
       "   1.508402943611145,\n",
       "   1.4978573322296143,\n",
       "   1.6500874757766724,\n",
       "   0.9492559432983398,\n",
       "   1.156669020652771,\n",
       "   1.8181053400039673,\n",
       "   0.8700270652770996,\n",
       "   1.6671040058135986,\n",
       "   0.9194583296775818,\n",
       "   1.009229302406311,\n",
       "   0.9402467012405396,\n",
       "   1.7222028970718384,\n",
       "   0.9973982572555542,\n",
       "   1.3851255178451538,\n",
       "   0.9959258437156677,\n",
       "   1.7086403369903564,\n",
       "   0.9258524179458618,\n",
       "   1.0002377033233643,\n",
       "   0.9923543334007263,\n",
       "   0.9741050004959106,\n",
       "   1.107139229774475,\n",
       "   1.1302552223205566,\n",
       "   1.566564917564392,\n",
       "   1.6928960084915161,\n",
       "   1.5394642353057861,\n",
       "   1.6078734397888184,\n",
       "   0.934898853302002,\n",
       "   1.5359537601470947,\n",
       "   0.9559752345085144,\n",
       "   1.5860788822174072,\n",
       "   1.4731546640396118,\n",
       "   1.5939862728118896,\n",
       "   1.0389487743377686,\n",
       "   1.143911600112915,\n",
       "   1.7927926778793335,\n",
       "   1.088530421257019,\n",
       "   0.808058500289917,\n",
       "   0.8766415119171143,\n",
       "   1.6860111951828003,\n",
       "   1.046457290649414,\n",
       "   0.8729693293571472,\n",
       "   1.504265308380127,\n",
       "   1.0732855796813965,\n",
       "   0.9318742752075195,\n",
       "   0.9763776659965515,\n",
       "   0.9723431468009949,\n",
       "   0.9551303386688232,\n",
       "   1.3312559127807617,\n",
       "   1.06786048412323,\n",
       "   0.917057991027832,\n",
       "   0.9700889587402344,\n",
       "   0.9563620686531067,\n",
       "   0.9541972875595093,\n",
       "   0.7626109719276428,\n",
       "   1.6437431573867798,\n",
       "   0.9985106587409973,\n",
       "   1.8275765180587769,\n",
       "   0.9389759302139282,\n",
       "   1.002087950706482,\n",
       "   1.166717529296875,\n",
       "   1.5112063884735107,\n",
       "   1.5534536838531494,\n",
       "   0.9204469919204712,\n",
       "   0.9246217012405396,\n",
       "   1.0158635377883911,\n",
       "   0.9228422045707703,\n",
       "   0.9486513137817383,\n",
       "   0.9750100374221802,\n",
       "   0.9094845652580261,\n",
       "   0.8811466097831726,\n",
       "   1.3437786102294922,\n",
       "   1.621201753616333,\n",
       "   1.3522355556488037,\n",
       "   1.5598000288009644,\n",
       "   0.9467933177947998,\n",
       "   1.5652663707733154,\n",
       "   1.044372797012329,\n",
       "   0.9582842588424683,\n",
       "   1.2214635610580444,\n",
       "   0.9796605110168457,\n",
       "   0.9209861755371094,\n",
       "   1.5747089385986328,\n",
       "   1.7858108282089233,\n",
       "   0.9528385996818542,\n",
       "   0.9793403148651123,\n",
       "   1.046554446220398,\n",
       "   1.4880001544952393,\n",
       "   0.9943994283676147,\n",
       "   1.55598783493042,\n",
       "   1.5899324417114258,\n",
       "   1.3717080354690552,\n",
       "   0.9487929344177246,\n",
       "   1.5527198314666748,\n",
       "   1.5678865909576416,\n",
       "   1.537636160850525,\n",
       "   1.0355671644210815,\n",
       "   1.7077492475509644,\n",
       "   1.4685698747634888,\n",
       "   1.370864987373352,\n",
       "   1.0707135200500488,\n",
       "   0.9912613034248352,\n",
       "   1.083155870437622,\n",
       "   0.9775459170341492,\n",
       "   1.0602880716323853,\n",
       "   1.0385209321975708,\n",
       "   1.4793211221694946,\n",
       "   1.236742377281189,\n",
       "   1.8026008605957031,\n",
       "   0.8462927937507629,\n",
       "   0.9599657654762268,\n",
       "   1.0179232358932495,\n",
       "   1.6143200397491455,\n",
       "   0.9577765464782715,\n",
       "   1.25900137424469,\n",
       "   1.0000630617141724,\n",
       "   1.6055766344070435,\n",
       "   1.0712398290634155,\n",
       "   0.9864272475242615,\n",
       "   1.404087781906128,\n",
       "   1.663164734840393,\n",
       "   1.1118768453598022,\n",
       "   0.9122037291526794,\n",
       "   1.5259182453155518,\n",
       "   1.6061010360717773,\n",
       "   0.9305139183998108,\n",
       "   1.3436269760131836,\n",
       "   1.0574430227279663,\n",
       "   0.9364653825759888,\n",
       "   1.0890177488327026,\n",
       "   1.0179444551467896,\n",
       "   0.9872037768363953,\n",
       "   1.072979211807251,\n",
       "   0.9696584939956665,\n",
       "   0.9827507138252258,\n",
       "   1.8485292196273804,\n",
       "   1.113207221031189,\n",
       "   0.9229304194450378,\n",
       "   1.601538896560669,\n",
       "   0.9214668273925781,\n",
       "   0.8999289274215698,\n",
       "   1.5886014699935913,\n",
       "   1.0556042194366455,\n",
       "   1.2461496591567993,\n",
       "   0.8811407685279846,\n",
       "   1.6267292499542236,\n",
       "   0.9602566361427307,\n",
       "   1.0013877153396606,\n",
       "   1.6748418807983398,\n",
       "   1.563759207725525,\n",
       "   1.8823217153549194,\n",
       "   1.8113682270050049,\n",
       "   1.6093310117721558,\n",
       "   1.4436798095703125,\n",
       "   1.5361634492874146,\n",
       "   1.895896315574646,\n",
       "   1.0307334661483765,\n",
       "   0.9819294214248657,\n",
       "   0.9694508910179138,\n",
       "   1.095598816871643,\n",
       "   1.1254615783691406,\n",
       "   1.4688677787780762,\n",
       "   1.7121362686157227,\n",
       "   1.5615627765655518,\n",
       "   1.4228187799453735,\n",
       "   0.9919072389602661,\n",
       "   1.590209722518921,\n",
       "   1.6299389600753784,\n",
       "   0.944697380065918,\n",
       "   1.0356639623641968,\n",
       "   1.0229949951171875,\n",
       "   1.1538995504379272,\n",
       "   0.969885528087616,\n",
       "   1.6411455869674683,\n",
       "   1.54096257686615,\n",
       "   1.014225721359253,\n",
       "   1.6950352191925049,\n",
       "   0.979583203792572,\n",
       "   0.9841192960739136,\n",
       "   1.7421373128890991,\n",
       "   1.27646803855896,\n",
       "   1.012453317642212,\n",
       "   1.047000527381897,\n",
       "   1.0161620378494263,\n",
       "   1.0330045223236084,\n",
       "   1.6037095785140991,\n",
       "   1.1538418531417847,\n",
       "   1.7029685974121094,\n",
       "   1.0661742687225342,\n",
       "   1.4442812204360962,\n",
       "   1.0207349061965942,\n",
       "   0.9248790144920349,\n",
       "   0.9314736723899841,\n",
       "   0.9398676753044128,\n",
       "   1.6316829919815063,\n",
       "   0.9749537110328674,\n",
       "   1.5681016445159912,\n",
       "   1.1573212146759033,\n",
       "   1.013862133026123,\n",
       "   1.5994839668273926,\n",
       "   0.9581359624862671,\n",
       "   0.991782546043396,\n",
       "   0.9316478967666626,\n",
       "   1.685462474822998,\n",
       "   0.9158271551132202,\n",
       "   1.0277035236358643,\n",
       "   0.9465233087539673,\n",
       "   0.9887605905532837,\n",
       "   0.9989780187606812,\n",
       "   0.9765586256980896,\n",
       "   1.6875289678573608,\n",
       "   1.3867110013961792,\n",
       "   0.8928606510162354,\n",
       "   1.208999752998352,\n",
       "   0.9806333780288696,\n",
       "   1.378767728805542,\n",
       "   0.9811476469039917,\n",
       "   1.006426215171814,\n",
       "   1.0591065883636475,\n",
       "   0.9582107663154602,\n",
       "   1.1691001653671265,\n",
       "   1.5178183317184448,\n",
       "   1.0406969785690308,\n",
       "   1.0631442070007324,\n",
       "   1.0635546445846558,\n",
       "   0.9359728097915649,\n",
       "   1.0139096975326538,\n",
       "   1.1113260984420776,\n",
       "   0.9751112461090088,\n",
       "   0.9393400549888611,\n",
       "   1.6764520406723022,\n",
       "   0.9682612419128418,\n",
       "   0.942036509513855,\n",
       "   1.0833687782287598,\n",
       "   0.9705116152763367,\n",
       "   1.6043063402175903,\n",
       "   1.0459802150726318,\n",
       "   0.934005856513977,\n",
       "   1.422003984451294,\n",
       "   1.4742629528045654,\n",
       "   1.0105730295181274,\n",
       "   0.9191710948944092,\n",
       "   0.9826745986938477,\n",
       "   1.0824410915374756,\n",
       "   1.6311078071594238,\n",
       "   0.9467483758926392],\n",
       "  ('cosine', False, True): [1.2150251865386963,\n",
       "   1.1801875829696655,\n",
       "   1.5343093872070312,\n",
       "   0.9335688352584839,\n",
       "   1.393707036972046,\n",
       "   0.9440323114395142,\n",
       "   1.258613109588623,\n",
       "   1.456436276435852,\n",
       "   0.9222040772438049,\n",
       "   1.7389732599258423,\n",
       "   1.6034762859344482,\n",
       "   1.3851255178451538,\n",
       "   1.0649484395980835,\n",
       "   0.9437777996063232,\n",
       "   1.535832405090332,\n",
       "   0.9963192343711853,\n",
       "   1.5824706554412842,\n",
       "   1.024376392364502,\n",
       "   0.9975020885467529,\n",
       "   1.5580899715423584,\n",
       "   0.9825646877288818,\n",
       "   1.0361788272857666,\n",
       "   0.975216805934906,\n",
       "   1.6150652170181274,\n",
       "   1.5632058382034302,\n",
       "   1.523554801940918,\n",
       "   1.2201893329620361,\n",
       "   1.0456717014312744,\n",
       "   0.9650366306304932,\n",
       "   0.9124304056167603,\n",
       "   1.2709659337997437,\n",
       "   1.083039402961731,\n",
       "   1.3693958520889282,\n",
       "   1.660358190536499,\n",
       "   1.5365567207336426,\n",
       "   1.25315523147583,\n",
       "   1.6403770446777344,\n",
       "   1.3993065357208252,\n",
       "   0.9636117815971375,\n",
       "   0.9664022922515869,\n",
       "   1.7461395263671875,\n",
       "   1.0109233856201172,\n",
       "   1.0111562013626099,\n",
       "   1.0289229154586792,\n",
       "   1.0332324504852295,\n",
       "   1.5856810808181763,\n",
       "   1.5556163787841797,\n",
       "   1.0840704441070557,\n",
       "   0.9174163937568665,\n",
       "   0.9765293598175049,\n",
       "   1.55533766746521,\n",
       "   1.6512117385864258,\n",
       "   0.9172934889793396,\n",
       "   1.0672273635864258,\n",
       "   1.4702305793762207,\n",
       "   1.0043152570724487,\n",
       "   0.9427615404129028,\n",
       "   1.4442012310028076,\n",
       "   1.2373621463775635,\n",
       "   1.0260045528411865,\n",
       "   1.6348235607147217,\n",
       "   1.0046159029006958,\n",
       "   0.9910733699798584,\n",
       "   1.0599061250686646,\n",
       "   1.4243083000183105,\n",
       "   1.0294800996780396,\n",
       "   1.4967066049575806,\n",
       "   0.9821076393127441,\n",
       "   0.9543336033821106,\n",
       "   1.0298041105270386,\n",
       "   1.0658375024795532,\n",
       "   1.5040289163589478,\n",
       "   0.9799647331237793,\n",
       "   1.5588818788528442,\n",
       "   1.451458215713501,\n",
       "   1.0052011013031006,\n",
       "   1.6740217208862305,\n",
       "   1.6875942945480347,\n",
       "   1.3646769523620605,\n",
       "   1.460247278213501,\n",
       "   0.9992772340774536,\n",
       "   0.9961968064308167,\n",
       "   0.8880088329315186,\n",
       "   1.1544685363769531,\n",
       "   0.9357317686080933,\n",
       "   1.027686595916748,\n",
       "   1.5949811935424805,\n",
       "   1.0754563808441162,\n",
       "   1.6031768321990967,\n",
       "   1.2161437273025513,\n",
       "   1.7884361743927002,\n",
       "   1.0193763971328735,\n",
       "   0.9611065983772278,\n",
       "   1.6424158811569214,\n",
       "   1.010545253753662,\n",
       "   0.9897129535675049,\n",
       "   1.5712568759918213,\n",
       "   0.8639072775840759,\n",
       "   1.2198729515075684,\n",
       "   1.410867691040039,\n",
       "   1.0240815877914429,\n",
       "   1.8195827007293701,\n",
       "   1.5826646089553833,\n",
       "   0.9912733435630798,\n",
       "   0.9414337873458862,\n",
       "   0.9266391396522522,\n",
       "   0.9672093987464905,\n",
       "   1.7844548225402832,\n",
       "   1.50255286693573,\n",
       "   1.6601002216339111,\n",
       "   1.275862693786621,\n",
       "   1.5921239852905273,\n",
       "   0.959517776966095,\n",
       "   0.9721441864967346,\n",
       "   1.579693078994751,\n",
       "   1.2984870672225952,\n",
       "   1.451737403869629,\n",
       "   1.1454001665115356,\n",
       "   1.095445990562439,\n",
       "   1.6103731393814087,\n",
       "   1.6546176671981812,\n",
       "   1.834252119064331,\n",
       "   1.2227760553359985,\n",
       "   0.9893299341201782,\n",
       "   0.8850424885749817,\n",
       "   1.0636379718780518,\n",
       "   1.4014710187911987,\n",
       "   1.4967142343521118,\n",
       "   1.5230704545974731,\n",
       "   1.622737169265747,\n",
       "   0.9952871203422546,\n",
       "   0.9956912398338318,\n",
       "   0.9300425052642822,\n",
       "   1.3171439170837402,\n",
       "   1.0244718790054321,\n",
       "   1.01838219165802,\n",
       "   1.5561779737472534,\n",
       "   0.9401514530181885,\n",
       "   0.905501663684845,\n",
       "   1.5094999074935913,\n",
       "   0.9390717148780823,\n",
       "   1.4726992845535278,\n",
       "   1.6805917024612427,\n",
       "   1.1471006870269775,\n",
       "   0.9146213531494141,\n",
       "   0.983508825302124,\n",
       "   1.545196771621704,\n",
       "   1.5528464317321777,\n",
       "   1.5728397369384766,\n",
       "   1.0471080541610718,\n",
       "   1.0008533000946045,\n",
       "   1.5761852264404297,\n",
       "   0.9772956967353821,\n",
       "   1.5577455759048462,\n",
       "   1.5593878030776978,\n",
       "   0.9700986742973328,\n",
       "   0.9927765727043152,\n",
       "   0.9736666679382324,\n",
       "   0.9520083069801331,\n",
       "   0.9779796600341797,\n",
       "   1.0688261985778809,\n",
       "   1.5707679986953735,\n",
       "   1.728203535079956,\n",
       "   1.0365442037582397,\n",
       "   0.9236394762992859,\n",
       "   1.0314486026763916,\n",
       "   0.9858593940734863,\n",
       "   1.0523011684417725,\n",
       "   0.9844624400138855,\n",
       "   0.8979872465133667,\n",
       "   1.5205057859420776,\n",
       "   0.9428145289421082,\n",
       "   1.0827891826629639,\n",
       "   0.9706099629402161,\n",
       "   0.9633612036705017,\n",
       "   0.9184370040893555,\n",
       "   1.0117616653442383,\n",
       "   1.5569225549697876,\n",
       "   1.4556941986083984,\n",
       "   1.0058842897415161,\n",
       "   1.5265392065048218,\n",
       "   1.4279770851135254,\n",
       "   1.02170729637146,\n",
       "   1.3188594579696655,\n",
       "   0.9193069934844971,\n",
       "   1.1883769035339355,\n",
       "   0.9719163179397583,\n",
       "   1.162803053855896,\n",
       "   1.1085636615753174,\n",
       "   1.0742316246032715,\n",
       "   1.7783235311508179,\n",
       "   0.9617830514907837,\n",
       "   1.9138964414596558,\n",
       "   0.8835692405700684,\n",
       "   0.9726009368896484,\n",
       "   1.5268505811691284,\n",
       "   1.8085240125656128,\n",
       "   1.0696223974227905,\n",
       "   1.5994945764541626,\n",
       "   1.6315592527389526,\n",
       "   0.8313369154930115,\n",
       "   1.1707361936569214,\n",
       "   1.5051120519638062,\n",
       "   1.3198517560958862,\n",
       "   1.0116733312606812,\n",
       "   0.9268600344657898,\n",
       "   1.5767358541488647,\n",
       "   1.0071316957473755,\n",
       "   0.9958072304725647,\n",
       "   1.5773131847381592,\n",
       "   1.034435510635376,\n",
       "   1.0158357620239258,\n",
       "   0.9252803921699524,\n",
       "   1.0765997171401978,\n",
       "   1.442808747291565,\n",
       "   1.6080708503723145,\n",
       "   0.9536820650100708,\n",
       "   1.5956041812896729,\n",
       "   1.0839077234268188,\n",
       "   1.046086072921753,\n",
       "   1.0567079782485962,\n",
       "   0.9725558757781982,\n",
       "   0.9709985256195068,\n",
       "   0.9425356984138489,\n",
       "   1.1355171203613281,\n",
       "   0.9970364570617676,\n",
       "   1.61232590675354,\n",
       "   0.9491894245147705,\n",
       "   1.707810878753662,\n",
       "   1.0994006395339966,\n",
       "   1.7232671976089478,\n",
       "   1.0106598138809204,\n",
       "   1.0889872312545776,\n",
       "   1.5573203563690186,\n",
       "   0.987438440322876,\n",
       "   1.5860685110092163,\n",
       "   0.9238315224647522,\n",
       "   1.7573201656341553,\n",
       "   1.3027076721191406,\n",
       "   1.7461395263671875,\n",
       "   1.0556706190109253,\n",
       "   1.2072051763534546,\n",
       "   1.495665192604065,\n",
       "   1.5661206245422363,\n",
       "   1.5345964431762695,\n",
       "   0.9723287224769592,\n",
       "   0.9970985651016235,\n",
       "   0.9586697816848755,\n",
       "   1.1391940116882324,\n",
       "   0.8847947716712952,\n",
       "   0.962348222732544,\n",
       "   1.0930155515670776,\n",
       "   1.3436270952224731,\n",
       "   1.5206921100616455,\n",
       "   1.5623198747634888,\n",
       "   0.9675042033195496,\n",
       "   0.9987723231315613,\n",
       "   0.9911841750144958,\n",
       "   1.5516587495803833,\n",
       "   0.9923022389411926,\n",
       "   1.3646769523620605,\n",
       "   2.046804189682007,\n",
       "   1.053695797920227,\n",
       "   1.051798939704895,\n",
       "   1.5292929410934448,\n",
       "   1.5909340381622314,\n",
       "   0.9439467191696167,\n",
       "   1.5084030628204346,\n",
       "   1.4978575706481934,\n",
       "   1.047791838645935,\n",
       "   0.9573593735694885,\n",
       "   1.7214760780334473,\n",
       "   1.8181052207946777,\n",
       "   0.8700273036956787,\n",
       "   1.0546878576278687,\n",
       "   0.9194584488868713,\n",
       "   1.0092294216156006,\n",
       "   0.9402467608451843,\n",
       "   1.7222031354904175,\n",
       "   1.056352972984314,\n",
       "   1.3851255178451538,\n",
       "   1.0251035690307617,\n",
       "   1.708640456199646,\n",
       "   0.9155793786048889,\n",
       "   1.0698463916778564,\n",
       "   0.9923545122146606,\n",
       "   0.9741049408912659,\n",
       "   1.107139229774475,\n",
       "   0.9337311387062073,\n",
       "   1.5665652751922607,\n",
       "   1.6928958892822266,\n",
       "   1.5394642353057861,\n",
       "   1.607873558998108,\n",
       "   0.9348984956741333,\n",
       "   1.5359538793563843,\n",
       "   0.9559754133224487,\n",
       "   1.5860788822174072,\n",
       "   1.4731546640396118,\n",
       "   1.5939861536026,\n",
       "   1.0404390096664429,\n",
       "   1.143911600112915,\n",
       "   1.7927929162979126,\n",
       "   1.063041090965271,\n",
       "   0.818041980266571,\n",
       "   0.9577780961990356,\n",
       "   1.3362047672271729,\n",
       "   1.0464575290679932,\n",
       "   1.5848994255065918,\n",
       "   0.9579951763153076,\n",
       "   1.0669770240783691,\n",
       "   0.9322310090065002,\n",
       "   0.9786745309829712,\n",
       "   0.972343385219574,\n",
       "   0.9551301002502441,\n",
       "   1.33125638961792,\n",
       "   1.067860722541809,\n",
       "   0.9573043584823608,\n",
       "   0.9588267207145691,\n",
       "   1.0071139335632324,\n",
       "   0.9554629921913147,\n",
       "   1.4341020584106445,\n",
       "   1.6437429189682007,\n",
       "   0.9985106587409973,\n",
       "   1.8275763988494873,\n",
       "   0.938975989818573,\n",
       "   1.5810035467147827,\n",
       "   0.9966831803321838,\n",
       "   1.5112063884735107,\n",
       "   1.553453803062439,\n",
       "   0.9478344321250916,\n",
       "   0.9296549558639526,\n",
       "   1.069964051246643,\n",
       "   1.4191714525222778,\n",
       "   0.9664841294288635,\n",
       "   0.9668486714363098,\n",
       "   0.9346640706062317,\n",
       "   0.8814443945884705,\n",
       "   1.3437786102294922,\n",
       "   1.6212018728256226,\n",
       "   1.3522356748580933,\n",
       "   1.5598000288009644,\n",
       "   0.9382736086845398,\n",
       "   1.565266489982605,\n",
       "   1.0212743282318115,\n",
       "   1.6118619441986084,\n",
       "   1.2283681631088257,\n",
       "   1.182531476020813,\n",
       "   1.0126756429672241,\n",
       "   1.5747092962265015,\n",
       "   1.785811424255371,\n",
       "   0.9422730803489685,\n",
       "   0.9793404340744019,\n",
       "   1.5795762538909912,\n",
       "   0.9836631417274475,\n",
       "   0.9943992495536804,\n",
       "   1.5559879541397095,\n",
       "   1.589932918548584,\n",
       "   1.7912931442260742,\n",
       "   0.9637203216552734,\n",
       "   1.5527198314666748,\n",
       "   1.5678863525390625,\n",
       "   1.537636160850525,\n",
       "   1.0355671644210815,\n",
       "   1.707749605178833,\n",
       "   1.4685697555541992,\n",
       "   1.3708651065826416,\n",
       "   1.0707135200500488,\n",
       "   1.3300079107284546,\n",
       "   1.0703649520874023,\n",
       "   0.9838762879371643,\n",
       "   1.127850890159607,\n",
       "   1.0773165225982666,\n",
       "   1.479320764541626,\n",
       "   1.0770305395126343,\n",
       "   1.8026009798049927,\n",
       "   0.8667320609092712,\n",
       "   0.9677327275276184,\n",
       "   1.0179232358932495,\n",
       "   1.614319920539856,\n",
       "   0.9577766060829163,\n",
       "   1.25900137424469,\n",
       "   1.005364179611206,\n",
       "   1.6055763959884644,\n",
       "   1.0282752513885498,\n",
       "   1.118257761001587,\n",
       "   0.9603552222251892,\n",
       "   1.663164734840393,\n",
       "   1.1118768453598022,\n",
       "   0.9213731288909912,\n",
       "   1.5259182453155518,\n",
       "   1.606101155281067,\n",
       "   1.5567724704742432,\n",
       "   1.3436270952224731,\n",
       "   1.0595083236694336,\n",
       "   0.9147691130638123,\n",
       "   1.0890178680419922,\n",
       "   1.628177523612976,\n",
       "   1.0149226188659668,\n",
       "   1.0777028799057007,\n",
       "   0.9696581959724426,\n",
       "   1.2961050271987915,\n",
       "   1.2048463821411133,\n",
       "   1.102245569229126,\n",
       "   1.0171712636947632,\n",
       "   1.6015387773513794,\n",
       "   0.9917082190513611,\n",
       "   0.8999292254447937,\n",
       "   1.5886013507843018,\n",
       "   1.0556044578552246,\n",
       "   1.2461495399475098,\n",
       "   1.454301118850708,\n",
       "   1.6267292499542236,\n",
       "   0.9998564720153809,\n",
       "   1.0013874769210815,\n",
       "   1.6748418807983398,\n",
       "   0.8771501779556274,\n",
       "   1.107682704925537,\n",
       "   1.8113682270050049,\n",
       "   1.6093312501907349,\n",
       "   1.443679690361023,\n",
       "   1.5361634492874146,\n",
       "   1.8958964347839355,\n",
       "   1.0307334661483765,\n",
       "   1.475332498550415,\n",
       "   1.4714809656143188,\n",
       "   1.095598816871643,\n",
       "   1.125461459159851,\n",
       "   1.4688678979873657,\n",
       "   1.712136149406433,\n",
       "   1.5615628957748413,\n",
       "   1.4228190183639526,\n",
       "   1.0334559679031372,\n",
       "   1.5902096033096313,\n",
       "   0.9615867137908936,\n",
       "   0.9446973204612732,\n",
       "   1.313873052597046,\n",
       "   1.022994875907898,\n",
       "   1.1366658210754395,\n",
       "   0.9698853492736816,\n",
       "   1.6411457061767578,\n",
       "   1.5409629344940186,\n",
       "   1.014225721359253,\n",
       "   1.6950355768203735,\n",
       "   0.9868800640106201,\n",
       "   1.593313217163086,\n",
       "   1.7421373128890991,\n",
       "   1.2764681577682495,\n",
       "   1.030734896659851,\n",
       "   1.0470006465911865,\n",
       "   1.4727492332458496,\n",
       "   1.0330040454864502,\n",
       "   1.6037094593048096,\n",
       "   1.882932186126709,\n",
       "   1.7029683589935303,\n",
       "   1.8112194538116455,\n",
       "   1.4442813396453857,\n",
       "   0.967883825302124,\n",
       "   0.9365243911743164,\n",
       "   1.0038974285125732,\n",
       "   0.9812367558479309,\n",
       "   1.6316828727722168,\n",
       "   1.051087498664856,\n",
       "   1.5681016445159912,\n",
       "   1.1105878353118896,\n",
       "   1.0384747982025146,\n",
       "   1.5994839668273926,\n",
       "   1.4240925312042236,\n",
       "   1.097865343093872,\n",
       "   0.9362596273422241,\n",
       "   1.6854627132415771,\n",
       "   0.915827214717865,\n",
       "   1.5722023248672485,\n",
       "   1.0347124338150024,\n",
       "   1.0129863023757935,\n",
       "   0.9989782571792603,\n",
       "   0.9942393898963928,\n",
       "   1.6875290870666504,\n",
       "   0.9500868320465088,\n",
       "   0.8928607106208801,\n",
       "   1.2089998722076416,\n",
       "   1.1477078199386597,\n",
       "   1.378767728805542,\n",
       "   0.981147825717926,\n",
       "   1.006426215171814,\n",
       "   1.0883257389068604,\n",
       "   1.126257061958313,\n",
       "   1.1690999269485474,\n",
       "   0.9554484486579895,\n",
       "   1.0054954290390015,\n",
       "   1.0383548736572266,\n",
       "   0.9683876633644104,\n",
       "   0.9501833319664001,\n",
       "   1.0139096975326538,\n",
       "   1.1113263368606567,\n",
       "   0.9765726327896118,\n",
       "   0.9393400549888611,\n",
       "   1.6764522790908813,\n",
       "   1.0189731121063232,\n",
       "   1.0004942417144775,\n",
       "   1.0823004245758057,\n",
       "   0.970511794090271,\n",
       "   1.6043063402175903,\n",
       "   1.0459802150726318,\n",
       "   1.062394142150879,\n",
       "   1.422004222869873,\n",
       "   1.4742631912231445,\n",
       "   1.0105730295181274,\n",
       "   0.9447066187858582,\n",
       "   1.0037776231765747,\n",
       "   1.082440972328186,\n",
       "   1.6311075687408447,\n",
       "   0.9544587135314941],\n",
       "  ('cosine', False, False): [1.2150249481201172,\n",
       "   1.180187702178955,\n",
       "   1.5343091487884521,\n",
       "   0.903221845626831,\n",
       "   1.0117932558059692,\n",
       "   0.9683955907821655,\n",
       "   1.041619896888733,\n",
       "   0.8878310918807983,\n",
       "   0.9223654866218567,\n",
       "   1.7389732599258423,\n",
       "   1.6034762859344482,\n",
       "   1.3851256370544434,\n",
       "   1.064948558807373,\n",
       "   0.9437779188156128,\n",
       "   1.535832405090332,\n",
       "   0.9963192939758301,\n",
       "   0.9975733160972595,\n",
       "   1.024376392364502,\n",
       "   0.9975019097328186,\n",
       "   1.5580897331237793,\n",
       "   0.9825649261474609,\n",
       "   1.0361789464950562,\n",
       "   0.9752166867256165,\n",
       "   1.6150652170181274,\n",
       "   1.5632058382034302,\n",
       "   1.523554801940918,\n",
       "   1.0856754779815674,\n",
       "   1.0456717014312744,\n",
       "   0.9641269445419312,\n",
       "   0.8678067922592163,\n",
       "   1.2709660530090332,\n",
       "   1.0830392837524414,\n",
       "   1.3693958520889282,\n",
       "   1.660358190536499,\n",
       "   1.5365567207336426,\n",
       "   1.25315523147583,\n",
       "   1.640377163887024,\n",
       "   1.3993065357208252,\n",
       "   0.9636118412017822,\n",
       "   0.9851093292236328,\n",
       "   1.7461395263671875,\n",
       "   1.0109233856201172,\n",
       "   1.0295265913009644,\n",
       "   0.927302896976471,\n",
       "   1.033232569694519,\n",
       "   1.5856808423995972,\n",
       "   1.5556163787841797,\n",
       "   1.0840704441070557,\n",
       "   0.8737190365791321,\n",
       "   1.564797282218933,\n",
       "   1.55533766746521,\n",
       "   1.6512117385864258,\n",
       "   0.8998661637306213,\n",
       "   1.0672273635864258,\n",
       "   1.4702305793762207,\n",
       "   0.9774969816207886,\n",
       "   0.9427614808082581,\n",
       "   1.444201111793518,\n",
       "   0.9271070957183838,\n",
       "   1.0192874670028687,\n",
       "   1.6348235607147217,\n",
       "   0.9801623821258545,\n",
       "   0.991073489189148,\n",
       "   1.0159358978271484,\n",
       "   1.4243084192276,\n",
       "   1.0275403261184692,\n",
       "   1.496706247329712,\n",
       "   0.9915467500686646,\n",
       "   0.9211555123329163,\n",
       "   1.0974081754684448,\n",
       "   1.0018409490585327,\n",
       "   0.922137975692749,\n",
       "   1.0466272830963135,\n",
       "   1.5588818788528442,\n",
       "   0.9600731730461121,\n",
       "   0.9318428635597229,\n",
       "   1.67402184009552,\n",
       "   1.0257855653762817,\n",
       "   0.9658500552177429,\n",
       "   1.460247278213501,\n",
       "   0.9992771148681641,\n",
       "   1.5539908409118652,\n",
       "   0.8440042734146118,\n",
       "   1.0507968664169312,\n",
       "   0.9149312973022461,\n",
       "   1.0276864767074585,\n",
       "   1.5949811935424805,\n",
       "   1.0754563808441162,\n",
       "   1.6031768321990967,\n",
       "   1.2234883308410645,\n",
       "   1.7884361743927002,\n",
       "   1.0089640617370605,\n",
       "   0.9611064791679382,\n",
       "   1.6424158811569214,\n",
       "   1.010545253753662,\n",
       "   0.9897129535675049,\n",
       "   1.5712568759918213,\n",
       "   0.8639072775840759,\n",
       "   1.027788519859314,\n",
       "   1.1036409139633179,\n",
       "   1.0240815877914429,\n",
       "   1.8195827007293701,\n",
       "   1.5826647281646729,\n",
       "   0.9822060465812683,\n",
       "   0.9414337873458862,\n",
       "   0.9230279922485352,\n",
       "   0.9228523373603821,\n",
       "   1.7844550609588623,\n",
       "   1.5025527477264404,\n",
       "   1.6601002216339111,\n",
       "   1.275862693786621,\n",
       "   1.5921236276626587,\n",
       "   0.9559923410415649,\n",
       "   0.968405544757843,\n",
       "   1.5796929597854614,\n",
       "   1.2984868288040161,\n",
       "   1.451737403869629,\n",
       "   1.145399808883667,\n",
       "   1.0954457521438599,\n",
       "   0.9910771250724792,\n",
       "   1.654617428779602,\n",
       "   1.834251880645752,\n",
       "   1.0420156717300415,\n",
       "   0.9866428375244141,\n",
       "   0.8833624720573425,\n",
       "   1.0636379718780518,\n",
       "   1.4014711380004883,\n",
       "   1.4967139959335327,\n",
       "   1.5230704545974731,\n",
       "   0.9376458525657654,\n",
       "   0.9952871203422546,\n",
       "   0.9803333282470703,\n",
       "   0.9300424456596375,\n",
       "   1.3171439170837402,\n",
       "   0.9876963496208191,\n",
       "   0.9450368285179138,\n",
       "   1.0087388753890991,\n",
       "   0.9379541873931885,\n",
       "   0.9055016040802002,\n",
       "   1.5094999074935913,\n",
       "   0.9390717148780823,\n",
       "   0.8953660726547241,\n",
       "   1.6805917024612427,\n",
       "   1.1683193445205688,\n",
       "   0.9146210551261902,\n",
       "   0.9299867153167725,\n",
       "   1.545196771621704,\n",
       "   0.9355310797691345,\n",
       "   1.5728397369384766,\n",
       "   1.0471080541610718,\n",
       "   0.9455506205558777,\n",
       "   0.9812452793121338,\n",
       "   0.9717985987663269,\n",
       "   1.5577458143234253,\n",
       "   1.5593880414962769,\n",
       "   0.9583394527435303,\n",
       "   0.9927763938903809,\n",
       "   0.9810827374458313,\n",
       "   0.8767619132995605,\n",
       "   0.9344195127487183,\n",
       "   0.983310341835022,\n",
       "   1.570767879486084,\n",
       "   1.728203535079956,\n",
       "   0.9481130242347717,\n",
       "   0.8862253427505493,\n",
       "   1.1095505952835083,\n",
       "   0.9858593344688416,\n",
       "   1.052301049232483,\n",
       "   1.5605930089950562,\n",
       "   0.8979873657226562,\n",
       "   1.5205057859420776,\n",
       "   0.9562864303588867,\n",
       "   1.0827891826629639,\n",
       "   0.9687349796295166,\n",
       "   0.9415143728256226,\n",
       "   0.9279553294181824,\n",
       "   1.0117616653442383,\n",
       "   1.5569225549697876,\n",
       "   1.4556941986083984,\n",
       "   0.9617518186569214,\n",
       "   1.5265394449234009,\n",
       "   1.4279769659042358,\n",
       "   1.1874713897705078,\n",
       "   1.3188594579696655,\n",
       "   0.919306755065918,\n",
       "   1.1883772611618042,\n",
       "   0.8710188269615173,\n",
       "   1.109413504600525,\n",
       "   1.1085635423660278,\n",
       "   1.001477599143982,\n",
       "   1.7783235311508179,\n",
       "   0.9617829918861389,\n",
       "   1.9138962030410767,\n",
       "   0.8760660290718079,\n",
       "   0.9726011753082275,\n",
       "   1.5268503427505493,\n",
       "   1.8085240125656128,\n",
       "   1.0599606037139893,\n",
       "   1.5994945764541626,\n",
       "   1.6315592527389526,\n",
       "   0.8313367962837219,\n",
       "   1.170736312866211,\n",
       "   1.5051120519638062,\n",
       "   1.0131269693374634,\n",
       "   1.0139861106872559,\n",
       "   1.1392992734909058,\n",
       "   0.9980625510215759,\n",
       "   1.0071313381195068,\n",
       "   0.9958072304725647,\n",
       "   1.5773130655288696,\n",
       "   0.9000406861305237,\n",
       "   0.8891432285308838,\n",
       "   0.8769994974136353,\n",
       "   1.0765997171401978,\n",
       "   1.442808747291565,\n",
       "   1.0373135805130005,\n",
       "   1.2244032621383667,\n",
       "   1.5956041812896729,\n",
       "   1.0839077234268188,\n",
       "   1.007020354270935,\n",
       "   1.0567079782485962,\n",
       "   0.9785430431365967,\n",
       "   1.5945662260055542,\n",
       "   0.9425356388092041,\n",
       "   1.1296045780181885,\n",
       "   0.9970365166664124,\n",
       "   1.61232590675354,\n",
       "   0.949189305305481,\n",
       "   1.7078102827072144,\n",
       "   1.1213324069976807,\n",
       "   1.7232674360275269,\n",
       "   1.0106598138809204,\n",
       "   1.0665156841278076,\n",
       "   0.9774854779243469,\n",
       "   0.9874384999275208,\n",
       "   1.5860685110092163,\n",
       "   0.9238314032554626,\n",
       "   1.7573201656341553,\n",
       "   1.3027076721191406,\n",
       "   1.7461395263671875,\n",
       "   0.9406553506851196,\n",
       "   0.9926391243934631,\n",
       "   1.495665192604065,\n",
       "   1.5661206245422363,\n",
       "   1.5345964431762695,\n",
       "   0.928823709487915,\n",
       "   1.4982408285140991,\n",
       "   0.957863450050354,\n",
       "   1.0233501195907593,\n",
       "   0.8231245279312134,\n",
       "   0.969885528087616,\n",
       "   1.0930160284042358,\n",
       "   0.954444169998169,\n",
       "   1.0219647884368896,\n",
       "   1.5623196363449097,\n",
       "   0.9946678280830383,\n",
       "   0.9987723231315613,\n",
       "   0.960780918598175,\n",
       "   0.9726778268814087,\n",
       "   0.9829646944999695,\n",
       "   1.3646767139434814,\n",
       "   2.046804428100586,\n",
       "   1.0069042444229126,\n",
       "   1.051798939704895,\n",
       "   1.5292928218841553,\n",
       "   1.5909337997436523,\n",
       "   0.9439467787742615,\n",
       "   1.508402943611145,\n",
       "   1.4978575706481934,\n",
       "   1.6500874757766724,\n",
       "   0.9492558240890503,\n",
       "   1.1566691398620605,\n",
       "   1.8181055784225464,\n",
       "   0.8700272440910339,\n",
       "   1.6671043634414673,\n",
       "   0.9194583892822266,\n",
       "   1.0092294216156006,\n",
       "   0.9402467012405396,\n",
       "   1.722203016281128,\n",
       "   0.997398316860199,\n",
       "   1.3851256370544434,\n",
       "   0.9959259629249573,\n",
       "   1.708640456199646,\n",
       "   0.9258524775505066,\n",
       "   1.0002374649047852,\n",
       "   0.9923545122146606,\n",
       "   0.9741050004959106,\n",
       "   1.1071391105651855,\n",
       "   1.1302549839019775,\n",
       "   1.5665655136108398,\n",
       "   1.6928958892822266,\n",
       "   1.5394642353057861,\n",
       "   1.6078733205795288,\n",
       "   0.9348987340927124,\n",
       "   1.5359541177749634,\n",
       "   0.9559755325317383,\n",
       "   1.5860788822174072,\n",
       "   1.4731547832489014,\n",
       "   1.5939863920211792,\n",
       "   1.0389485359191895,\n",
       "   1.1439114809036255,\n",
       "   1.7927929162979126,\n",
       "   1.0885303020477295,\n",
       "   0.808058500289917,\n",
       "   0.8766416311264038,\n",
       "   1.6860111951828003,\n",
       "   1.046457290649414,\n",
       "   0.872969388961792,\n",
       "   1.5042651891708374,\n",
       "   1.0732852220535278,\n",
       "   0.93187415599823,\n",
       "   0.9763776659965515,\n",
       "   0.9723432660102844,\n",
       "   0.9551301598548889,\n",
       "   1.3312562704086304,\n",
       "   1.06786048412323,\n",
       "   0.9170579314231873,\n",
       "   0.9700890183448792,\n",
       "   0.9563620090484619,\n",
       "   0.9541974663734436,\n",
       "   0.7626109719276428,\n",
       "   1.6437429189682007,\n",
       "   0.9985107779502869,\n",
       "   1.8275763988494873,\n",
       "   0.938975989818573,\n",
       "   1.002087950706482,\n",
       "   1.166717290878296,\n",
       "   1.5112063884735107,\n",
       "   1.553453803062439,\n",
       "   0.9204469919204712,\n",
       "   0.9246218204498291,\n",
       "   1.015863299369812,\n",
       "   0.922842264175415,\n",
       "   0.9486514329910278,\n",
       "   0.9750099778175354,\n",
       "   0.9094846248626709,\n",
       "   0.8811466097831726,\n",
       "   1.3437786102294922,\n",
       "   1.6212018728256226,\n",
       "   1.3522354364395142,\n",
       "   1.5598000288009644,\n",
       "   0.9467935562133789,\n",
       "   1.565266489982605,\n",
       "   1.044372797012329,\n",
       "   0.9582843780517578,\n",
       "   1.2214637994766235,\n",
       "   0.9796607494354248,\n",
       "   0.9209862351417542,\n",
       "   1.5747090578079224,\n",
       "   1.7858113050460815,\n",
       "   0.9528385996818542,\n",
       "   0.9793404340744019,\n",
       "   1.046554446220398,\n",
       "   1.4880002737045288,\n",
       "   0.9943994283676147,\n",
       "   1.555988073348999,\n",
       "   1.589932918548584,\n",
       "   1.3717079162597656,\n",
       "   0.9487929344177246,\n",
       "   1.5527198314666748,\n",
       "   1.567886471748352,\n",
       "   1.537636160850525,\n",
       "   1.0355671644210815,\n",
       "   1.707749605178833,\n",
       "   1.4685697555541992,\n",
       "   1.370864987373352,\n",
       "   1.0707135200500488,\n",
       "   0.9912614822387695,\n",
       "   1.083155870437622,\n",
       "   0.9775458574295044,\n",
       "   1.0602880716323853,\n",
       "   1.0385208129882812,\n",
       "   1.4793212413787842,\n",
       "   1.2367420196533203,\n",
       "   1.8026010990142822,\n",
       "   0.8462929129600525,\n",
       "   0.9599657654762268,\n",
       "   1.0179232358932495,\n",
       "   1.6143200397491455,\n",
       "   0.9577764868736267,\n",
       "   1.25900137424469,\n",
       "   1.000063180923462,\n",
       "   1.6055763959884644,\n",
       "   1.0712398290634155,\n",
       "   0.9864272475242615,\n",
       "   1.404087781906128,\n",
       "   1.663164734840393,\n",
       "   1.1118768453598022,\n",
       "   0.9122034907341003,\n",
       "   1.5259180068969727,\n",
       "   1.606101155281067,\n",
       "   0.9305139780044556,\n",
       "   1.3436272144317627,\n",
       "   1.0574430227279663,\n",
       "   0.9364652037620544,\n",
       "   1.0890178680419922,\n",
       "   1.0179444551467896,\n",
       "   0.9872036576271057,\n",
       "   1.072979211807251,\n",
       "   0.9696582555770874,\n",
       "   0.9827504754066467,\n",
       "   1.8485294580459595,\n",
       "   1.1132069826126099,\n",
       "   0.9229304790496826,\n",
       "   1.6015386581420898,\n",
       "   0.9214667677879333,\n",
       "   0.8999292254447937,\n",
       "   1.5886013507843018,\n",
       "   1.0556042194366455,\n",
       "   1.2461495399475098,\n",
       "   0.8811407089233398,\n",
       "   1.6267292499542236,\n",
       "   0.9602566957473755,\n",
       "   1.001387596130371,\n",
       "   1.6748418807983398,\n",
       "   1.5637593269348145,\n",
       "   1.882321834564209,\n",
       "   1.8113682270050049,\n",
       "   1.6093312501907349,\n",
       "   1.4436798095703125,\n",
       "   1.536163330078125,\n",
       "   1.895896315574646,\n",
       "   1.030733585357666,\n",
       "   0.9819294214248657,\n",
       "   0.9694508910179138,\n",
       "   1.0955989360809326,\n",
       "   1.1254616975784302,\n",
       "   1.4688677787780762,\n",
       "   1.712136149406433,\n",
       "   1.5615628957748413,\n",
       "   1.4228190183639526,\n",
       "   0.9919072985649109,\n",
       "   1.5902098417282104,\n",
       "   1.6299387216567993,\n",
       "   0.9446973204612732,\n",
       "   1.0356639623641968,\n",
       "   1.022994875907898,\n",
       "   1.1538997888565063,\n",
       "   0.9698852300643921,\n",
       "   1.6411457061767578,\n",
       "   1.540962815284729,\n",
       "   1.014225721359253,\n",
       "   1.6950350999832153,\n",
       "   0.9795830249786377,\n",
       "   0.984119176864624,\n",
       "   1.7421374320983887,\n",
       "   1.27646803855896,\n",
       "   1.0124534368515015,\n",
       "   1.047000527381897,\n",
       "   1.0161617994308472,\n",
       "   1.0330042839050293,\n",
       "   1.6037095785140991,\n",
       "   1.1538419723510742,\n",
       "   1.7029683589935303,\n",
       "   1.0661739110946655,\n",
       "   1.4442811012268066,\n",
       "   1.0207349061965942,\n",
       "   0.9248790144920349,\n",
       "   0.9314736723899841,\n",
       "   0.9398674964904785,\n",
       "   1.6316828727722168,\n",
       "   0.9749534726142883,\n",
       "   1.5681016445159912,\n",
       "   1.1573212146759033,\n",
       "   1.013862133026123,\n",
       "   1.5994842052459717,\n",
       "   0.9581360220909119,\n",
       "   0.991782546043396,\n",
       "   0.9316480755805969,\n",
       "   1.6854627132415771,\n",
       "   0.9158270955085754,\n",
       "   1.0277035236358643,\n",
       "   0.9465231895446777,\n",
       "   0.9887609481811523,\n",
       "   0.9989780783653259,\n",
       "   0.9765588045120239,\n",
       "   1.6875290870666504,\n",
       "   1.3867110013961792,\n",
       "   0.8928606510162354,\n",
       "   1.2089998722076416,\n",
       "   0.9806333780288696,\n",
       "   1.378767728805542,\n",
       "   0.9811475872993469,\n",
       "   1.006426215171814,\n",
       "   1.0591063499450684,\n",
       "   0.9582105278968811,\n",
       "   1.1690999269485474,\n",
       "   1.5178186893463135,\n",
       "   1.0406967401504517,\n",
       "   1.0631442070007324,\n",
       "   1.0635546445846558,\n",
       "   0.9359728097915649,\n",
       "   1.0139096975326538,\n",
       "   1.1113263368606567,\n",
       "   0.975111186504364,\n",
       "   0.9393400549888611,\n",
       "   1.6764522790908813,\n",
       "   0.9682613015174866,\n",
       "   0.942036509513855,\n",
       "   1.0833690166473389,\n",
       "   0.970511794090271,\n",
       "   1.6043065786361694,\n",
       "   1.045980453491211,\n",
       "   0.9340055584907532,\n",
       "   1.422004222869873,\n",
       "   1.4742631912231445,\n",
       "   1.0105730295181274,\n",
       "   0.9191712737083435,\n",
       "   0.9826745390892029,\n",
       "   1.082440972328186,\n",
       "   1.6311075687408447,\n",
       "   0.9467484354972839],\n",
       "  ('euclidean', True, True): [1.2973521947860718,\n",
       "   0.500749945640564,\n",
       "   0.6114131212234497,\n",
       "   1.313923716545105,\n",
       "   0.7191166877746582,\n",
       "   1.2992444038391113,\n",
       "   0.6731559634208679,\n",
       "   0.6788283586502075,\n",
       "   1.6265437602996826,\n",
       "   0.31603172421455383,\n",
       "   0.4199957847595215,\n",
       "   0.6013990640640259,\n",
       "   0.7129268646240234,\n",
       "   1.450329065322876,\n",
       "   0.6281730532646179,\n",
       "   1.0166139602661133,\n",
       "   0.632472813129425,\n",
       "   1.058840274810791,\n",
       "   1.134447693824768,\n",
       "   0.590972363948822,\n",
       "   1.1120413541793823,\n",
       "   0.909715473651886,\n",
       "   1.3366711139678955,\n",
       "   0.8768725991249084,\n",
       "   1.4756816625595093,\n",
       "   0.6450247764587402,\n",
       "   0.5374057292938232,\n",
       "   0.8892667889595032,\n",
       "   1.3250672817230225,\n",
       "   2.0218098163604736,\n",
       "   0.9475119709968567,\n",
       "   0.8614165186882019,\n",
       "   0.6026570200920105,\n",
       "   0.5260066390037537,\n",
       "   0.5818139910697937,\n",
       "   0.7566788792610168,\n",
       "   1.3230226039886475,\n",
       "   0.6064263582229614,\n",
       "   1.217475414276123,\n",
       "   1.1582800149917603,\n",
       "   0.7753462791442871,\n",
       "   1.1316953897476196,\n",
       "   0.9957082867622375,\n",
       "   1.0481246709823608,\n",
       "   0.9036412835121155,\n",
       "   0.4431091248989105,\n",
       "   0.6062033176422119,\n",
       "   0.5643165707588196,\n",
       "   1.5229827165603638,\n",
       "   1.171074628829956,\n",
       "   0.5654734969139099,\n",
       "   0.5134574174880981,\n",
       "   1.6986991167068481,\n",
       "   0.8301032185554504,\n",
       "   0.6532213687896729,\n",
       "   0.9815480709075928,\n",
       "   1.3158892393112183,\n",
       "   0.4214310944080353,\n",
       "   1.2059788703918457,\n",
       "   0.9286588430404663,\n",
       "   0.4986882507801056,\n",
       "   1.0659040212631226,\n",
       "   1.3201937675476074,\n",
       "   0.9387556910514832,\n",
       "   0.9122641682624817,\n",
       "   0.9381088018417358,\n",
       "   0.2925907075405121,\n",
       "   1.213332176208496,\n",
       "   1.315551519393921,\n",
       "   1.2708818912506104,\n",
       "   0.9869577884674072,\n",
       "   1.2797991037368774,\n",
       "   1.121809720993042,\n",
       "   0.9062955379486084,\n",
       "   0.9610432982444763,\n",
       "   1.220751404762268,\n",
       "   0.5684919953346252,\n",
       "   0.4243881404399872,\n",
       "   0.7141544818878174,\n",
       "   0.6533411145210266,\n",
       "   1.0023123025894165,\n",
       "   1.136392593383789,\n",
       "   1.8256921768188477,\n",
       "   0.5406605005264282,\n",
       "   1.3882955312728882,\n",
       "   0.8495745062828064,\n",
       "   0.4798940122127533,\n",
       "   0.8399457335472107,\n",
       "   0.5948549509048462,\n",
       "   0.6126391887664795,\n",
       "   0.45046690106391907,\n",
       "   0.9314275979995728,\n",
       "   1.1636643409729004,\n",
       "   0.6629031300544739,\n",
       "   0.9679962992668152,\n",
       "   1.0292117595672607,\n",
       "   0.5088862180709839,\n",
       "   1.4717475175857544,\n",
       "   0.9220908880233765,\n",
       "   0.8979020118713379,\n",
       "   1.2114872932434082,\n",
       "   0.8240376114845276,\n",
       "   0.5487624406814575,\n",
       "   1.039416790008545,\n",
       "   1.2989389896392822,\n",
       "   1.6545820236206055,\n",
       "   1.3156489133834839,\n",
       "   0.4576045274734497,\n",
       "   0.5045343041419983,\n",
       "   0.8951026201248169,\n",
       "   0.25747379660606384,\n",
       "   0.44737112522125244,\n",
       "   1.4566630125045776,\n",
       "   1.1457160711288452,\n",
       "   0.6511096358299255,\n",
       "   0.5789405107498169,\n",
       "   0.5767989754676819,\n",
       "   0.8024064302444458,\n",
       "   0.5763835310935974,\n",
       "   0.8771660327911377,\n",
       "   0.5977327823638916,\n",
       "   0.25493690371513367,\n",
       "   0.5464672446250916,\n",
       "   1.0826616287231445,\n",
       "   1.3311301469802856,\n",
       "   0.977048397064209,\n",
       "   0.5268230438232422,\n",
       "   0.5020357370376587,\n",
       "   0.5199544429779053,\n",
       "   1.4203088283538818,\n",
       "   1.0284209251403809,\n",
       "   1.0609363317489624,\n",
       "   1.1863703727722168,\n",
       "   0.5440505743026733,\n",
       "   1.0449659824371338,\n",
       "   1.1875876188278198,\n",
       "   0.956189751625061,\n",
       "   1.4949018955230713,\n",
       "   1.4372918605804443,\n",
       "   1.2993489503860474,\n",
       "   1.1675962209701538,\n",
       "   0.6341074109077454,\n",
       "   0.5926850438117981,\n",
       "   0.6021186709403992,\n",
       "   1.7354484796524048,\n",
       "   1.0943981409072876,\n",
       "   0.6274691224098206,\n",
       "   0.8788830637931824,\n",
       "   0.5192031264305115,\n",
       "   1.4290745258331299,\n",
       "   1.3523764610290527,\n",
       "   1.046266794204712,\n",
       "   1.1863471269607544,\n",
       "   0.5479485392570496,\n",
       "   0.5693778395652771,\n",
       "   1.3863983154296875,\n",
       "   1.023733139038086,\n",
       "   1.100366234779358,\n",
       "   1.205214500427246,\n",
       "   1.1495118141174316,\n",
       "   1.046718716621399,\n",
       "   0.5000373125076294,\n",
       "   0.2667524814605713,\n",
       "   1.1888586282730103,\n",
       "   1.4236341714859009,\n",
       "   0.8515644073486328,\n",
       "   1.0877115726470947,\n",
       "   0.8681358098983765,\n",
       "   1.1890085935592651,\n",
       "   1.4176995754241943,\n",
       "   0.6563737392425537,\n",
       "   1.3115214109420776,\n",
       "   1.0474952459335327,\n",
       "   1.1539994478225708,\n",
       "   1.5362690687179565,\n",
       "   1.8252216577529907,\n",
       "   0.9834091067314148,\n",
       "   0.4335990846157074,\n",
       "   0.6774958968162537,\n",
       "   1.0060539245605469,\n",
       "   0.6007581353187561,\n",
       "   0.5863369703292847,\n",
       "   1.0905067920684814,\n",
       "   0.4863303601741791,\n",
       "   1.6364637613296509,\n",
       "   0.6578720211982727,\n",
       "   1.87824285030365,\n",
       "   0.6662425994873047,\n",
       "   0.3515886068344116,\n",
       "   0.6983234882354736,\n",
       "   0.30918267369270325,\n",
       "   1.1921637058258057,\n",
       "   0.2814442217350006,\n",
       "   1.3939827680587769,\n",
       "   1.2354062795639038,\n",
       "   0.6196567416191101,\n",
       "   0.6884530186653137,\n",
       "   0.8343212604522705,\n",
       "   1.1379387378692627,\n",
       "   0.5861575603485107,\n",
       "   1.5770777463912964,\n",
       "   0.8026384711265564,\n",
       "   0.49716421961784363,\n",
       "   0.5362706780433655,\n",
       "   0.9139044284820557,\n",
       "   1.2568320035934448,\n",
       "   1.0069137811660767,\n",
       "   0.9719423055648804,\n",
       "   1.0218924283981323,\n",
       "   0.5994313359260559,\n",
       "   1.421533226966858,\n",
       "   1.524773359298706,\n",
       "   1.6489336490631104,\n",
       "   0.8777902722358704,\n",
       "   0.6595761179924011,\n",
       "   0.9010079503059387,\n",
       "   1.1362472772598267,\n",
       "   0.5927116274833679,\n",
       "   0.7797667384147644,\n",
       "   0.937569797039032,\n",
       "   0.8257730007171631,\n",
       "   1.257988691329956,\n",
       "   1.3468266725540161,\n",
       "   1.4117685556411743,\n",
       "   0.4685826301574707,\n",
       "   1.0932575464248657,\n",
       "   0.7876633405685425,\n",
       "   1.1763659715652466,\n",
       "   0.47521889209747314,\n",
       "   0.5959550738334656,\n",
       "   1.0437424182891846,\n",
       "   0.9443128108978271,\n",
       "   0.7791494727134705,\n",
       "   1.0850574970245361,\n",
       "   1.2269275188446045,\n",
       "   0.511427104473114,\n",
       "   1.4316799640655518,\n",
       "   0.41187819838523865,\n",
       "   0.38095784187316895,\n",
       "   0.3493649661540985,\n",
       "   1.1377536058425903,\n",
       "   0.47462373971939087,\n",
       "   0.5785130262374878,\n",
       "   0.5333126783370972,\n",
       "   1.4486000537872314,\n",
       "   1.2136321067810059,\n",
       "   1.018164038658142,\n",
       "   1.1336209774017334,\n",
       "   0.9199398756027222,\n",
       "   1.5344772338867188,\n",
       "   1.1404240131378174,\n",
       "   0.8951122760772705,\n",
       "   0.5779944062232971,\n",
       "   0.6399624347686768,\n",
       "   0.6079592704772949,\n",
       "   1.2371630668640137,\n",
       "   1.0058300495147705,\n",
       "   1.4045443534851074,\n",
       "   0.5366741418838501,\n",
       "   1.1458425521850586,\n",
       "   0.5122089982032776,\n",
       "   0.5076634287834167,\n",
       "   0.8129353523254395,\n",
       "   0.903827965259552,\n",
       "   0.6451364159584045,\n",
       "   0.5954299569129944,\n",
       "   1.2379024028778076,\n",
       "   1.6049299240112305,\n",
       "   0.9193138480186462,\n",
       "   0.7436355352401733,\n",
       "   1.7088488340377808,\n",
       "   0.617749810218811,\n",
       "   0.2749914228916168,\n",
       "   1.374599575996399,\n",
       "   0.8619998097419739,\n",
       "   1.364905595779419,\n",
       "   0.9163434505462646,\n",
       "   1.2447402477264404,\n",
       "   0.5339562892913818,\n",
       "   1.0057624578475952,\n",
       "   0.6013990640640259,\n",
       "   1.016886591911316,\n",
       "   0.5194184184074402,\n",
       "   1.397834300994873,\n",
       "   0.999192476272583,\n",
       "   1.0744366645812988,\n",
       "   1.087193250656128,\n",
       "   0.8383756875991821,\n",
       "   1.516154170036316,\n",
       "   0.5439769625663757,\n",
       "   0.5551496744155884,\n",
       "   0.6220262050628662,\n",
       "   0.3687913119792938,\n",
       "   1.3207125663757324,\n",
       "   0.6163486242294312,\n",
       "   1.1741634607315063,\n",
       "   0.3999767601490021,\n",
       "   1.064760684967041,\n",
       "   0.49060118198394775,\n",
       "   0.8676745891571045,\n",
       "   0.700320839881897,\n",
       "   0.28159379959106445,\n",
       "   0.6479864120483398,\n",
       "   1.5640548467636108,\n",
       "   1.266335129737854,\n",
       "   0.8617305755615234,\n",
       "   0.8928510546684265,\n",
       "   0.5786069631576538,\n",
       "   1.2586448192596436,\n",
       "   0.7489709258079529,\n",
       "   1.509485125541687,\n",
       "   1.2712070941925049,\n",
       "   1.166442632675171,\n",
       "   1.3119560480117798,\n",
       "   0.6379470825195312,\n",
       "   0.8013652563095093,\n",
       "   1.410161018371582,\n",
       "   1.2597862482070923,\n",
       "   1.1841069459915161,\n",
       "   1.348751187324524,\n",
       "   0.725148618221283,\n",
       "   0.4279569089412689,\n",
       "   1.0373526811599731,\n",
       "   0.2716687023639679,\n",
       "   1.6979864835739136,\n",
       "   0.9858086109161377,\n",
       "   1.426497459411621,\n",
       "   0.5528342723846436,\n",
       "   0.571631133556366,\n",
       "   1.3849891424179077,\n",
       "   1.570154070854187,\n",
       "   0.7451101541519165,\n",
       "   1.247788667678833,\n",
       "   1.2009638547897339,\n",
       "   1.2784162759780884,\n",
       "   1.2416280508041382,\n",
       "   1.3593645095825195,\n",
       "   0.9964612722396851,\n",
       "   0.43272310495376587,\n",
       "   0.5796219706535339,\n",
       "   0.6157219409942627,\n",
       "   1.3223938941955566,\n",
       "   0.5417237281799316,\n",
       "   0.9503518342971802,\n",
       "   1.0985716581344604,\n",
       "   0.3998855650424957,\n",
       "   1.011560320854187,\n",
       "   1.1308903694152832,\n",
       "   1.3189349174499512,\n",
       "   0.5160050988197327,\n",
       "   1.4445933103561401,\n",
       "   1.0692952871322632,\n",
       "   0.8781406283378601,\n",
       "   1.4498090744018555,\n",
       "   1.0117900371551514,\n",
       "   0.46479207277297974,\n",
       "   0.3115498125553131,\n",
       "   0.4805417060852051,\n",
       "   1.1528892517089844,\n",
       "   0.8847672939300537,\n",
       "   0.5330682992935181,\n",
       "   0.4933285415172577,\n",
       "   1.0155307054519653,\n",
       "   0.3298211991786957,\n",
       "   0.6233955025672913,\n",
       "   1.1627312898635864,\n",
       "   0.6525892019271851,\n",
       "   1.0590041875839233,\n",
       "   0.8319193124771118,\n",
       "   1.1773499250411987,\n",
       "   0.7083535194396973,\n",
       "   0.9076920747756958,\n",
       "   0.4975515902042389,\n",
       "   1.0481866598129272,\n",
       "   0.5123801231384277,\n",
       "   1.7418885231018066,\n",
       "   1.3280856609344482,\n",
       "   0.9878702163696289,\n",
       "   1.264738917350769,\n",
       "   1.3543540239334106,\n",
       "   0.6666419506072998,\n",
       "   0.9993591904640198,\n",
       "   0.5287944078445435,\n",
       "   0.9730116724967957,\n",
       "   1.075568437576294,\n",
       "   1.2756226062774658,\n",
       "   0.44588810205459595,\n",
       "   0.7332125902175903,\n",
       "   2.03547739982605,\n",
       "   0.5122362375259399,\n",
       "   0.9459331035614014,\n",
       "   1.0135717391967773,\n",
       "   0.5779944062232971,\n",
       "   0.7136263251304626,\n",
       "   1.4853469133377075,\n",
       "   0.594596803188324,\n",
       "   0.9307412505149841,\n",
       "   0.9630773067474365,\n",
       "   0.6636391878128052,\n",
       "   1.3357661962509155,\n",
       "   1.076360821723938,\n",
       "   0.6177409887313843,\n",
       "   0.8355518579483032,\n",
       "   1.0056536197662354,\n",
       "   1.238423466682434,\n",
       "   1.210911750793457,\n",
       "   1.2008123397827148,\n",
       "   0.5240064263343811,\n",
       "   0.698732852935791,\n",
       "   0.5739012360572815,\n",
       "   1.8403478860855103,\n",
       "   0.413173645734787,\n",
       "   1.0513572692871094,\n",
       "   1.0697050094604492,\n",
       "   0.9636999368667603,\n",
       "   1.6646811962127686,\n",
       "   0.6498799920082092,\n",
       "   0.6725305914878845,\n",
       "   0.6314675211906433,\n",
       "   0.5607325434684753,\n",
       "   0.5334206223487854,\n",
       "   0.392753928899765,\n",
       "   0.9503070116043091,\n",
       "   1.1289175748825073,\n",
       "   1.0648225545883179,\n",
       "   0.8379549384117126,\n",
       "   0.730649471282959,\n",
       "   1.7091625928878784,\n",
       "   0.40502139925956726,\n",
       "   0.4367012083530426,\n",
       "   0.5046741962432861,\n",
       "   1.0592252016067505,\n",
       "   0.5643791556358337,\n",
       "   1.383909821510315,\n",
       "   1.2020736932754517,\n",
       "   0.7287406921386719,\n",
       "   1.0205802917480469,\n",
       "   0.32101091742515564,\n",
       "   1.1839889287948608,\n",
       "   0.9307115077972412,\n",
       "   0.5853955149650574,\n",
       "   1.1678646802902222,\n",
       "   0.35250529646873474,\n",
       "   1.1217690706253052,\n",
       "   0.4396202266216278,\n",
       "   0.4554162919521332,\n",
       "   0.7316470742225647,\n",
       "   0.8476479053497314,\n",
       "   1.026926040649414,\n",
       "   0.8614345788955688,\n",
       "   1.0420385599136353,\n",
       "   0.36810368299484253,\n",
       "   0.5483102202415466,\n",
       "   0.5552979111671448,\n",
       "   0.32456448674201965,\n",
       "   0.4980268180370331,\n",
       "   1.55780029296875,\n",
       "   1.455452561378479,\n",
       "   1.3938817977905273,\n",
       "   1.1820714473724365,\n",
       "   1.3304883241653442,\n",
       "   1.1259047985076904,\n",
       "   0.3515801429748535,\n",
       "   0.791807234287262,\n",
       "   0.8940550088882446,\n",
       "   0.39461272954940796,\n",
       "   1.2170593738555908,\n",
       "   0.5510610938072205,\n",
       "   1.6645222902297974,\n",
       "   0.5272719264030457,\n",
       "   1.2543103694915771,\n",
       "   0.8156132102012634,\n",
       "   1.0353469848632812,\n",
       "   1.0330390930175781,\n",
       "   1.003594160079956,\n",
       "   1.1008970737457275,\n",
       "   0.4882814884185791,\n",
       "   1.325806975364685,\n",
       "   2.2107174396514893,\n",
       "   0.9057076573371887,\n",
       "   1.1576215028762817,\n",
       "   0.9901672601699829,\n",
       "   1.07698655128479,\n",
       "   1.0170841217041016,\n",
       "   0.5256263017654419,\n",
       "   0.7752575874328613,\n",
       "   0.6422412991523743,\n",
       "   1.3945072889328003,\n",
       "   1.0391004085540771,\n",
       "   1.2225134372711182,\n",
       "   1.2944616079330444,\n",
       "   1.4269050359725952,\n",
       "   0.9912737607955933,\n",
       "   0.6657747626304626,\n",
       "   1.2463618516921997,\n",
       "   1.1797420978546143,\n",
       "   0.3535586893558502,\n",
       "   1.0602222681045532,\n",
       "   1.4759654998779297,\n",
       "   0.7644215226173401,\n",
       "   1.1263725757598877,\n",
       "   0.4222773611545563,\n",
       "   0.8934417963027954,\n",
       "   1.3423629999160767,\n",
       "   0.9686961770057678,\n",
       "   0.3501431941986084,\n",
       "   0.9335525631904602,\n",
       "   1.2981956005096436,\n",
       "   1.1800518035888672,\n",
       "   0.7404255270957947,\n",
       "   0.7957745790481567,\n",
       "   1.1902899742126465],\n",
       "  ('euclidean', True, False): [0.714379608631134,\n",
       "   0.4545353949069977,\n",
       "   0.6114131808280945,\n",
       "   1.6519627571105957,\n",
       "   0.9354960918426514,\n",
       "   1.123713493347168,\n",
       "   0.699371874332428,\n",
       "   1.3758301734924316,\n",
       "   1.52510404586792,\n",
       "   0.3160317838191986,\n",
       "   0.4199957847595215,\n",
       "   0.6013990044593811,\n",
       "   0.7129269242286682,\n",
       "   1.27279531955719,\n",
       "   0.6281731128692627,\n",
       "   1.0166141986846924,\n",
       "   1.004993200302124,\n",
       "   0.9452072978019714,\n",
       "   1.0290749073028564,\n",
       "   0.5909723043441772,\n",
       "   1.0933550596237183,\n",
       "   0.909715473651886,\n",
       "   1.2228187322616577,\n",
       "   0.4675908088684082,\n",
       "   0.5377312302589417,\n",
       "   0.6450247764587402,\n",
       "   0.5374057292938232,\n",
       "   0.8892670273780823,\n",
       "   1.3387900590896606,\n",
       "   2.0218098163604736,\n",
       "   0.6663951277732849,\n",
       "   0.8614165186882019,\n",
       "   0.6026571393013,\n",
       "   0.5260066390037537,\n",
       "   0.5818139910697937,\n",
       "   0.6264737844467163,\n",
       "   0.6079186201095581,\n",
       "   0.6064263582229614,\n",
       "   1.217475414276123,\n",
       "   1.0578879117965698,\n",
       "   0.3493649661540985,\n",
       "   0.951553225517273,\n",
       "   0.9185819625854492,\n",
       "   1.2596423625946045,\n",
       "   0.9036412835121155,\n",
       "   0.4431091547012329,\n",
       "   0.6062032580375671,\n",
       "   0.5643165707588196,\n",
       "   1.9226295948028564,\n",
       "   0.6308254599571228,\n",
       "   0.5654734969139099,\n",
       "   0.5134573578834534,\n",
       "   1.7539970874786377,\n",
       "   0.7728427052497864,\n",
       "   0.6532213687896729,\n",
       "   1.1203687191009521,\n",
       "   1.3158894777297974,\n",
       "   0.4214309751987457,\n",
       "   1.372389793395996,\n",
       "   0.9286590218544006,\n",
       "   0.4986882507801056,\n",
       "   1.078951120376587,\n",
       "   1.0218135118484497,\n",
       "   0.9567151665687561,\n",
       "   0.6200234889984131,\n",
       "   0.9381088018417358,\n",
       "   0.2925906777381897,\n",
       "   1.0521271228790283,\n",
       "   1.431847095489502,\n",
       "   0.6933616399765015,\n",
       "   0.9869577884674072,\n",
       "   1.2797991037368774,\n",
       "   0.8350124359130859,\n",
       "   0.3865146338939667,\n",
       "   1.2987041473388672,\n",
       "   1.220751166343689,\n",
       "   0.5684919953346252,\n",
       "   0.8835874795913696,\n",
       "   1.2618931531906128,\n",
       "   0.6533411145210266,\n",
       "   1.0023123025894165,\n",
       "   0.5495225191116333,\n",
       "   1.831047773361206,\n",
       "   0.5406605005264282,\n",
       "   1.4034755229949951,\n",
       "   0.7863985300064087,\n",
       "   0.4798940122127533,\n",
       "   0.733292281627655,\n",
       "   0.5948549509048462,\n",
       "   0.4336276948451996,\n",
       "   0.45046690106391907,\n",
       "   0.9314275979995728,\n",
       "   1.1628292798995972,\n",
       "   0.6629031300544739,\n",
       "   0.9679965972900391,\n",
       "   1.0292117595672607,\n",
       "   0.5088862776756287,\n",
       "   1.2931307554244995,\n",
       "   0.9220905900001526,\n",
       "   0.8979020118713379,\n",
       "   0.8870658278465271,\n",
       "   0.5123007893562317,\n",
       "   0.5487624406814575,\n",
       "   1.039416790008545,\n",
       "   1.121720314025879,\n",
       "   1.6545816659927368,\n",
       "   1.348574161529541,\n",
       "   0.4576045274734497,\n",
       "   0.3610616624355316,\n",
       "   0.5292666554450989,\n",
       "   0.25747376680374146,\n",
       "   0.44737106561660767,\n",
       "   1.4566624164581299,\n",
       "   1.1457163095474243,\n",
       "   0.6511095762252808,\n",
       "   0.5789404511451721,\n",
       "   0.5767989754676819,\n",
       "   0.8024064898490906,\n",
       "   0.5388897657394409,\n",
       "   1.0578879117965698,\n",
       "   0.5977327823638916,\n",
       "   0.2549368739128113,\n",
       "   0.8066396713256836,\n",
       "   1.082661509513855,\n",
       "   1.3311302661895752,\n",
       "   0.7865053415298462,\n",
       "   0.5268230438232422,\n",
       "   0.5020357370376587,\n",
       "   0.5199544429779053,\n",
       "   1.5196475982666016,\n",
       "   1.02842116355896,\n",
       "   1.0908573865890503,\n",
       "   1.1863704919815063,\n",
       "   0.5440505743026733,\n",
       "   1.048758625984192,\n",
       "   1.1875877380371094,\n",
       "   0.9561896920204163,\n",
       "   1.4949018955230713,\n",
       "   1.3690437078475952,\n",
       "   0.6218453645706177,\n",
       "   1.1675963401794434,\n",
       "   1.4766517877578735,\n",
       "   0.40498417615890503,\n",
       "   0.5797373652458191,\n",
       "   1.6195096969604492,\n",
       "   1.4160916805267334,\n",
       "   0.6274691224098206,\n",
       "   1.494632363319397,\n",
       "   0.5192031860351562,\n",
       "   0.8616518974304199,\n",
       "   1.3523764610290527,\n",
       "   1.046266794204712,\n",
       "   1.1863471269607544,\n",
       "   0.5479485392570496,\n",
       "   0.569377601146698,\n",
       "   1.4171719551086426,\n",
       "   1.023733139038086,\n",
       "   1.0664488077163696,\n",
       "   1.5438168048858643,\n",
       "   1.1767292022705078,\n",
       "   1.076615333557129,\n",
       "   0.5000373721122742,\n",
       "   0.2667524814605713,\n",
       "   1.1888587474822998,\n",
       "   1.4507876634597778,\n",
       "   0.6664867997169495,\n",
       "   1.0877113342285156,\n",
       "   0.833763062953949,\n",
       "   0.6142350435256958,\n",
       "   1.402494192123413,\n",
       "   0.6563737392425537,\n",
       "   1.204745888710022,\n",
       "   0.8610116839408875,\n",
       "   1.1539989709854126,\n",
       "   1.7205034494400024,\n",
       "   1.5707316398620605,\n",
       "   0.970184326171875,\n",
       "   0.4335990846157074,\n",
       "   0.6774959564208984,\n",
       "   1.3110311031341553,\n",
       "   0.6007581353187561,\n",
       "   0.5863370895385742,\n",
       "   0.6524088978767395,\n",
       "   0.4863303601741791,\n",
       "   1.6364644765853882,\n",
       "   0.6578719615936279,\n",
       "   1.878243088722229,\n",
       "   0.7188016176223755,\n",
       "   0.351588636636734,\n",
       "   0.98687344789505,\n",
       "   0.30918267369270325,\n",
       "   1.1921637058258057,\n",
       "   0.28144416213035583,\n",
       "   1.3939825296401978,\n",
       "   1.2354055643081665,\n",
       "   0.6196567416191101,\n",
       "   0.45542922616004944,\n",
       "   0.8343209028244019,\n",
       "   0.6101762652397156,\n",
       "   0.5861575603485107,\n",
       "   1.5005834102630615,\n",
       "   0.8026383519172668,\n",
       "   0.4971643090248108,\n",
       "   0.9397037625312805,\n",
       "   0.8814800977706909,\n",
       "   0.8159072399139404,\n",
       "   1.012898325920105,\n",
       "   0.9719423651695251,\n",
       "   1.0208181142807007,\n",
       "   0.5994313955307007,\n",
       "   1.5938400030136108,\n",
       "   2.016460657119751,\n",
       "   1.703571081161499,\n",
       "   0.8777902126312256,\n",
       "   0.6595761179924011,\n",
       "   0.9010081887245178,\n",
       "   0.7436603903770447,\n",
       "   0.5927115678787231,\n",
       "   0.7797666788101196,\n",
       "   0.9375696182250977,\n",
       "   0.7854539155960083,\n",
       "   1.1423876285552979,\n",
       "   0.5865368843078613,\n",
       "   1.2529189586639404,\n",
       "   0.4685825705528259,\n",
       "   1.0164518356323242,\n",
       "   0.5359658598899841,\n",
       "   1.1666871309280396,\n",
       "   0.47521886229515076,\n",
       "   0.5038289427757263,\n",
       "   0.5318754315376282,\n",
       "   0.9443128108978271,\n",
       "   0.7791493535041809,\n",
       "   1.085057258605957,\n",
       "   1.054722785949707,\n",
       "   0.5114270448684692,\n",
       "   1.3982861042022705,\n",
       "   0.41187819838523865,\n",
       "   0.38095784187316895,\n",
       "   0.3493649661540985,\n",
       "   1.1377536058425903,\n",
       "   1.0837509632110596,\n",
       "   0.578512966632843,\n",
       "   0.5333126783370972,\n",
       "   0.548233151435852,\n",
       "   1.2136316299438477,\n",
       "   0.6090651154518127,\n",
       "   1.133621096611023,\n",
       "   0.9199398756027222,\n",
       "   1.7008366584777832,\n",
       "   1.1066149473190308,\n",
       "   0.8951120376586914,\n",
       "   1.2619019746780396,\n",
       "   0.9576089978218079,\n",
       "   0.6079592704772949,\n",
       "   1.0288671255111694,\n",
       "   1.0058298110961914,\n",
       "   1.4918227195739746,\n",
       "   1.1147689819335938,\n",
       "   1.1458425521850586,\n",
       "   0.5122090578079224,\n",
       "   0.25142526626586914,\n",
       "   0.967165470123291,\n",
       "   0.903827965259552,\n",
       "   0.6451364755630493,\n",
       "   0.5954299569129944,\n",
       "   1.1648328304290771,\n",
       "   0.5635419487953186,\n",
       "   0.6272189617156982,\n",
       "   0.35075974464416504,\n",
       "   1.720840573310852,\n",
       "   0.617749810218811,\n",
       "   0.27499139308929443,\n",
       "   1.374599575996399,\n",
       "   0.5058689713478088,\n",
       "   1.3296390771865845,\n",
       "   0.9163433909416199,\n",
       "   1.189754605293274,\n",
       "   0.5339562892913818,\n",
       "   1.0057623386383057,\n",
       "   0.6013990044593811,\n",
       "   1.016886591911316,\n",
       "   0.5194184184074402,\n",
       "   1.2038164138793945,\n",
       "   0.9991925954818726,\n",
       "   1.0744366645812988,\n",
       "   1.0871928930282593,\n",
       "   0.8383756875991821,\n",
       "   0.7320988774299622,\n",
       "   0.5439770817756653,\n",
       "   0.5551496744155884,\n",
       "   0.6220262050628662,\n",
       "   0.3687913417816162,\n",
       "   1.2106300592422485,\n",
       "   0.6163486242294312,\n",
       "   1.1373586654663086,\n",
       "   0.39997678995132446,\n",
       "   0.5196924209594727,\n",
       "   0.49060115218162537,\n",
       "   0.867674708366394,\n",
       "   0.700320839881897,\n",
       "   0.28159379959106445,\n",
       "   0.5715048909187317,\n",
       "   1.5640548467636108,\n",
       "   1.646742820739746,\n",
       "   0.5401174426078796,\n",
       "   0.8928513526916504,\n",
       "   2.1490421295166016,\n",
       "   0.666918933391571,\n",
       "   0.7335543036460876,\n",
       "   1.5094859600067139,\n",
       "   1.2712070941925049,\n",
       "   1.1003119945526123,\n",
       "   1.3119558095932007,\n",
       "   0.6379470825195312,\n",
       "   0.801365315914154,\n",
       "   1.4967541694641113,\n",
       "   1.1450252532958984,\n",
       "   1.5167691707611084,\n",
       "   1.3487510681152344,\n",
       "   3.708575487136841,\n",
       "   0.42795687913894653,\n",
       "   1.0202425718307495,\n",
       "   0.2716687023639679,\n",
       "   1.2555644512176514,\n",
       "   0.9861928224563599,\n",
       "   0.6417422890663147,\n",
       "   0.5528342127799988,\n",
       "   0.571631133556366,\n",
       "   1.3849891424179077,\n",
       "   1.570154070854187,\n",
       "   0.8983557224273682,\n",
       "   1.247788667678833,\n",
       "   1.330431342124939,\n",
       "   1.085133671760559,\n",
       "   1.4145188331604004,\n",
       "   1.3593645095825195,\n",
       "   0.607358992099762,\n",
       "   0.43272313475608826,\n",
       "   0.5796219706535339,\n",
       "   0.6157219409942627,\n",
       "   1.2469314336776733,\n",
       "   0.5417237281799316,\n",
       "   0.8438480496406555,\n",
       "   1.098571538925171,\n",
       "   0.3998856246471405,\n",
       "   1.0983467102050781,\n",
       "   1.4268689155578613,\n",
       "   0.5973930358886719,\n",
       "   0.5160052180290222,\n",
       "   1.1537548303604126,\n",
       "   1.0692954063415527,\n",
       "   0.8781406283378601,\n",
       "   0.6398770809173584,\n",
       "   1.0117902755737305,\n",
       "   0.4647921323776245,\n",
       "   0.3115498423576355,\n",
       "   0.48054176568984985,\n",
       "   1.1528894901275635,\n",
       "   0.513697624206543,\n",
       "   0.5330682992935181,\n",
       "   0.49332860112190247,\n",
       "   0.8877553343772888,\n",
       "   0.3298211991786957,\n",
       "   0.6233955025672913,\n",
       "   0.708656907081604,\n",
       "   0.652588963508606,\n",
       "   1.059004306793213,\n",
       "   0.7590620517730713,\n",
       "   1.1773498058319092,\n",
       "   0.7083535194396973,\n",
       "   0.9076920747756958,\n",
       "   0.4975515604019165,\n",
       "   0.6537977457046509,\n",
       "   0.2581122815608978,\n",
       "   1.8266618251800537,\n",
       "   1.3280855417251587,\n",
       "   0.9437224864959717,\n",
       "   0.4619102478027344,\n",
       "   1.2786396741867065,\n",
       "   0.6666419506072998,\n",
       "   0.9993586540222168,\n",
       "   0.5287944078445435,\n",
       "   0.8691349029541016,\n",
       "   1.0755685567855835,\n",
       "   0.5480851531028748,\n",
       "   0.44588810205459595,\n",
       "   0.6946094632148743,\n",
       "   2.2926673889160156,\n",
       "   0.5122362375259399,\n",
       "   0.60255366563797,\n",
       "   1.2468758821487427,\n",
       "   0.5779945850372314,\n",
       "   0.7136263251304626,\n",
       "   1.2126749753952026,\n",
       "   0.566439688205719,\n",
       "   0.9307413697242737,\n",
       "   1.091477870941162,\n",
       "   0.6744166016578674,\n",
       "   1.2779468297958374,\n",
       "   1.0763611793518066,\n",
       "   0.40853193402290344,\n",
       "   0.7905926704406738,\n",
       "   1.3404159545898438,\n",
       "   0.6066832542419434,\n",
       "   1.3863575458526611,\n",
       "   1.2008124589920044,\n",
       "   0.5240064263343811,\n",
       "   0.6661778688430786,\n",
       "   0.48055124282836914,\n",
       "   2.0736265182495117,\n",
       "   0.413173645734787,\n",
       "   1.376552939414978,\n",
       "   0.9964095950126648,\n",
       "   0.48439672589302063,\n",
       "   0.6348481178283691,\n",
       "   0.3351445198059082,\n",
       "   0.3475206196308136,\n",
       "   0.6314674615859985,\n",
       "   0.5607326030731201,\n",
       "   0.5334206223487854,\n",
       "   0.392753928899765,\n",
       "   0.9026569128036499,\n",
       "   1.1289175748825073,\n",
       "   1.0648226737976074,\n",
       "   0.8379548788070679,\n",
       "   0.7306494116783142,\n",
       "   0.7068439722061157,\n",
       "   0.40502139925956726,\n",
       "   0.4367012083530426,\n",
       "   0.5046741962432861,\n",
       "   1.062818169593811,\n",
       "   0.5643791556358337,\n",
       "   0.5011104941368103,\n",
       "   1.170231819152832,\n",
       "   0.8037750720977783,\n",
       "   0.9532610177993774,\n",
       "   0.30634745955467224,\n",
       "   1.1839890480041504,\n",
       "   0.628287672996521,\n",
       "   0.5853955149650574,\n",
       "   0.9528712630271912,\n",
       "   0.35250526666641235,\n",
       "   1.1217693090438843,\n",
       "   1.1049154996871948,\n",
       "   0.24734386801719666,\n",
       "   0.7316470742225647,\n",
       "   0.8476479649543762,\n",
       "   0.7427042126655579,\n",
       "   0.9417489767074585,\n",
       "   0.9322466254234314,\n",
       "   0.3681037127971649,\n",
       "   0.5483102798461914,\n",
       "   0.5552979111671448,\n",
       "   0.6777642369270325,\n",
       "   0.49802690744400024,\n",
       "   0.9083859920501709,\n",
       "   1.4554526805877686,\n",
       "   1.3938816785812378,\n",
       "   1.194098711013794,\n",
       "   0.5427887439727783,\n",
       "   1.1259047985076904,\n",
       "   0.3515801429748535,\n",
       "   0.5416588187217712,\n",
       "   0.9553187489509583,\n",
       "   0.3946128189563751,\n",
       "   1.2170593738555908,\n",
       "   1.2642337083816528,\n",
       "   1.7133243083953857,\n",
       "   0.5272719264030457,\n",
       "   1.254310965538025,\n",
       "   0.8156130313873291,\n",
       "   1.2012611627578735,\n",
       "   1.0428880453109741,\n",
       "   1.0035945177078247,\n",
       "   1.100897192955017,\n",
       "   0.4882814884185791,\n",
       "   0.5514556169509888,\n",
       "   1.5713993310928345,\n",
       "   0.5226085782051086,\n",
       "   1.157621145248413,\n",
       "   0.5827203392982483,\n",
       "   1.0757251977920532,\n",
       "   0.9760836362838745,\n",
       "   0.5256262421607971,\n",
       "   1.1611047983169556,\n",
       "   0.6058744788169861,\n",
       "   0.6402468681335449,\n",
       "   0.8088845610618591,\n",
       "   0.8667681813240051,\n",
       "   0.773481011390686,\n",
       "   1.5743439197540283,\n",
       "   0.9331403374671936,\n",
       "   0.6657748818397522,\n",
       "   1.2472237348556519,\n",
       "   1.1797419786453247,\n",
       "   0.3535586893558502,\n",
       "   1.2278977632522583,\n",
       "   1.5199633836746216,\n",
       "   0.7587765455245972,\n",
       "   1.1263726949691772,\n",
       "   0.4222773611545563,\n",
       "   0.8644235730171204,\n",
       "   1.3423627614974976,\n",
       "   0.5993313789367676,\n",
       "   0.35014328360557556,\n",
       "   0.9335524439811707,\n",
       "   1.6032238006591797,\n",
       "   1.180051565170288,\n",
       "   0.7404255867004395,\n",
       "   0.5483139753341675,\n",
       "   1.1902899742126465],\n",
       "  ('euclidean', False, True): [1.3687469959259033,\n",
       "   0.4866729974746704,\n",
       "   0.6453565359115601,\n",
       "   1.3607302904129028,\n",
       "   0.7498835325241089,\n",
       "   1.3560644388198853,\n",
       "   0.6736453175544739,\n",
       "   0.6996161341667175,\n",
       "   1.719299554824829,\n",
       "   0.34027087688446045,\n",
       "   0.43995678424835205,\n",
       "   0.6422197818756104,\n",
       "   0.7244364619255066,\n",
       "   1.5265666246414185,\n",
       "   0.64933180809021,\n",
       "   1.0922436714172363,\n",
       "   0.6579777002334595,\n",
       "   1.0869077444076538,\n",
       "   1.1734274625778198,\n",
       "   0.6156514286994934,\n",
       "   1.093596339225769,\n",
       "   0.9282931685447693,\n",
       "   1.378570318222046,\n",
       "   0.8783392310142517,\n",
       "   1.5247509479522705,\n",
       "   0.6690059304237366,\n",
       "   0.5310549139976501,\n",
       "   0.8979835510253906,\n",
       "   1.308821201324463,\n",
       "   2.018754005432129,\n",
       "   0.9863123297691345,\n",
       "   0.895882248878479,\n",
       "   0.6164361834526062,\n",
       "   0.5490787625312805,\n",
       "   0.5996875166893005,\n",
       "   0.7578603029251099,\n",
       "   1.3847805261611938,\n",
       "   0.6365237832069397,\n",
       "   1.236127495765686,\n",
       "   1.2059754133224487,\n",
       "   0.7591766715049744,\n",
       "   1.1652859449386597,\n",
       "   1.0311896800994873,\n",
       "   1.0615990161895752,\n",
       "   0.9226699471473694,\n",
       "   0.46766436100006104,\n",
       "   0.6247072815895081,\n",
       "   0.5779039859771729,\n",
       "   1.5460124015808105,\n",
       "   1.1905261278152466,\n",
       "   0.6067209839820862,\n",
       "   0.5185645818710327,\n",
       "   1.7478852272033691,\n",
       "   0.9097851514816284,\n",
       "   0.6938118934631348,\n",
       "   0.9848330020904541,\n",
       "   1.3912086486816406,\n",
       "   0.45251819491386414,\n",
       "   1.2600284814834595,\n",
       "   0.9152933955192566,\n",
       "   0.5250971913337708,\n",
       "   1.1297776699066162,\n",
       "   1.449847936630249,\n",
       "   0.9696104526519775,\n",
       "   0.9521344900131226,\n",
       "   0.9959749579429626,\n",
       "   0.29863157868385315,\n",
       "   1.2376024723052979,\n",
       "   1.3284155130386353,\n",
       "   1.319785237312317,\n",
       "   1.0065926313400269,\n",
       "   1.3507146835327148,\n",
       "   1.2073020935058594,\n",
       "   0.9687860012054443,\n",
       "   0.939206600189209,\n",
       "   1.2816131114959717,\n",
       "   0.5954527258872986,\n",
       "   0.44106006622314453,\n",
       "   0.7420947551727295,\n",
       "   0.6942553520202637,\n",
       "   1.0715974569320679,\n",
       "   1.1690236330032349,\n",
       "   1.8976600170135498,\n",
       "   0.5385289788246155,\n",
       "   1.478903889656067,\n",
       "   0.8530473113059998,\n",
       "   0.4897020757198334,\n",
       "   0.8428699970245361,\n",
       "   0.6214337944984436,\n",
       "   0.6106318235397339,\n",
       "   0.46444037556648254,\n",
       "   1.013098120689392,\n",
       "   1.27866792678833,\n",
       "   0.6941745281219482,\n",
       "   1.0192053318023682,\n",
       "   1.0506937503814697,\n",
       "   0.5382604002952576,\n",
       "   1.5202209949493408,\n",
       "   0.9807860255241394,\n",
       "   0.8957507610321045,\n",
       "   1.2850004434585571,\n",
       "   0.8114187121391296,\n",
       "   0.5611280202865601,\n",
       "   1.0549272298812866,\n",
       "   1.3462642431259155,\n",
       "   1.6475147008895874,\n",
       "   1.3697772026062012,\n",
       "   0.46787410974502563,\n",
       "   0.5373542904853821,\n",
       "   0.8717549443244934,\n",
       "   0.26111799478530884,\n",
       "   0.4414054751396179,\n",
       "   1.4283612966537476,\n",
       "   1.2139921188354492,\n",
       "   0.6675499677658081,\n",
       "   0.6078843474388123,\n",
       "   0.6022205948829651,\n",
       "   0.8238072395324707,\n",
       "   0.5656227469444275,\n",
       "   0.9079517126083374,\n",
       "   0.6119208931922913,\n",
       "   0.2525971233844757,\n",
       "   0.5286299586296082,\n",
       "   1.0927748680114746,\n",
       "   1.4059202671051025,\n",
       "   1.016430377960205,\n",
       "   0.5438300967216492,\n",
       "   0.5132135152816772,\n",
       "   0.5329561233520508,\n",
       "   1.4492361545562744,\n",
       "   1.0110305547714233,\n",
       "   1.0709964036941528,\n",
       "   1.172343373298645,\n",
       "   0.5652436017990112,\n",
       "   1.0166949033737183,\n",
       "   1.2187057733535767,\n",
       "   0.9604068994522095,\n",
       "   1.5190349817276,\n",
       "   1.4512698650360107,\n",
       "   1.3456969261169434,\n",
       "   1.2231372594833374,\n",
       "   0.663276195526123,\n",
       "   0.5907922983169556,\n",
       "   0.5797970294952393,\n",
       "   1.823311448097229,\n",
       "   1.1453121900558472,\n",
       "   0.6382508873939514,\n",
       "   0.8897542357444763,\n",
       "   0.5391364097595215,\n",
       "   1.5130335092544556,\n",
       "   1.4383939504623413,\n",
       "   1.0351158380508423,\n",
       "   1.2477550506591797,\n",
       "   0.5745441317558289,\n",
       "   0.5789439678192139,\n",
       "   1.4552886486053467,\n",
       "   1.0407891273498535,\n",
       "   1.1750894784927368,\n",
       "   1.2741564512252808,\n",
       "   1.1482445001602173,\n",
       "   1.0530345439910889,\n",
       "   0.5252816677093506,\n",
       "   0.2629193961620331,\n",
       "   1.2394953966140747,\n",
       "   1.4994624853134155,\n",
       "   0.88392573595047,\n",
       "   1.1219069957733154,\n",
       "   0.8478237986564636,\n",
       "   1.2362987995147705,\n",
       "   1.4675939083099365,\n",
       "   0.6755331754684448,\n",
       "   1.3336752653121948,\n",
       "   1.0945860147476196,\n",
       "   1.1913923025131226,\n",
       "   1.5557163953781128,\n",
       "   1.9199789762496948,\n",
       "   1.0139352083206177,\n",
       "   0.4481181204319,\n",
       "   0.6961821913719177,\n",
       "   1.008503794670105,\n",
       "   0.6106361150741577,\n",
       "   0.6048102974891663,\n",
       "   1.0944050550460815,\n",
       "   0.5218933820724487,\n",
       "   1.6739389896392822,\n",
       "   0.6831259727478027,\n",
       "   1.9606437683105469,\n",
       "   0.6486172080039978,\n",
       "   0.3413001298904419,\n",
       "   0.7235002517700195,\n",
       "   0.306024432182312,\n",
       "   1.1873679161071777,\n",
       "   0.2730386257171631,\n",
       "   1.4809643030166626,\n",
       "   1.2391911745071411,\n",
       "   0.643815815448761,\n",
       "   0.7149121165275574,\n",
       "   0.8986683487892151,\n",
       "   1.1817548274993896,\n",
       "   0.6204314827919006,\n",
       "   1.5506625175476074,\n",
       "   0.838530957698822,\n",
       "   0.5215109586715698,\n",
       "   0.5460297465324402,\n",
       "   0.9201298952102661,\n",
       "   1.3330973386764526,\n",
       "   0.9815062284469604,\n",
       "   1.0136419534683228,\n",
       "   1.0602450370788574,\n",
       "   0.6272202134132385,\n",
       "   1.4042465686798096,\n",
       "   1.5636380910873413,\n",
       "   1.845255732536316,\n",
       "   0.9186741709709167,\n",
       "   0.6863878965377808,\n",
       "   0.9139853715896606,\n",
       "   1.1821364164352417,\n",
       "   0.603299617767334,\n",
       "   0.7705227136611938,\n",
       "   0.9770325422286987,\n",
       "   0.8363766670227051,\n",
       "   1.2812178134918213,\n",
       "   1.3705954551696777,\n",
       "   1.4366215467453003,\n",
       "   0.4531106948852539,\n",
       "   1.159193992614746,\n",
       "   0.8321234583854675,\n",
       "   1.15962815284729,\n",
       "   0.48720869421958923,\n",
       "   0.602769672870636,\n",
       "   1.0822702646255493,\n",
       "   0.9668511748313904,\n",
       "   0.786427915096283,\n",
       "   1.120030164718628,\n",
       "   1.2498329877853394,\n",
       "   0.52206951379776,\n",
       "   1.5090688467025757,\n",
       "   0.4126182496547699,\n",
       "   0.3773142099380493,\n",
       "   0.3500087261199951,\n",
       "   1.140869140625,\n",
       "   0.46633389592170715,\n",
       "   0.596400797367096,\n",
       "   0.564470648765564,\n",
       "   1.4919390678405762,\n",
       "   1.2670645713806152,\n",
       "   1.0163670778274536,\n",
       "   1.2259941101074219,\n",
       "   0.910998523235321,\n",
       "   1.612134337425232,\n",
       "   1.1647971868515015,\n",
       "   0.9347596168518066,\n",
       "   0.6043503284454346,\n",
       "   0.6789218783378601,\n",
       "   0.6465762257575989,\n",
       "   1.2703638076782227,\n",
       "   1.0057212114334106,\n",
       "   1.466318964958191,\n",
       "   0.5493578910827637,\n",
       "   1.162831425666809,\n",
       "   0.528076171875,\n",
       "   0.494754821062088,\n",
       "   0.8399355411529541,\n",
       "   0.9142575860023499,\n",
       "   0.6787635684013367,\n",
       "   0.6173227429389954,\n",
       "   1.2972499132156372,\n",
       "   1.670602798461914,\n",
       "   1.0065988302230835,\n",
       "   0.7327443361282349,\n",
       "   1.7814861536026,\n",
       "   0.615924596786499,\n",
       "   0.27496209740638733,\n",
       "   1.3828141689300537,\n",
       "   0.8221216201782227,\n",
       "   1.3855640888214111,\n",
       "   0.919066309928894,\n",
       "   1.242087960243225,\n",
       "   0.5552262663841248,\n",
       "   0.9997557997703552,\n",
       "   0.6422197818756104,\n",
       "   1.0587773323059082,\n",
       "   0.5431973934173584,\n",
       "   1.4936842918395996,\n",
       "   1.027328610420227,\n",
       "   1.1269339323043823,\n",
       "   1.1313704252243042,\n",
       "   0.8590257167816162,\n",
       "   1.5351063013076782,\n",
       "   0.5741688013076782,\n",
       "   0.5880258083343506,\n",
       "   0.6605789661407471,\n",
       "   0.38201114535331726,\n",
       "   1.3925310373306274,\n",
       "   0.6465386748313904,\n",
       "   1.1965978145599365,\n",
       "   0.42216214537620544,\n",
       "   1.0822062492370605,\n",
       "   0.498617947101593,\n",
       "   0.8652104139328003,\n",
       "   0.6945610642433167,\n",
       "   0.28376802802085876,\n",
       "   0.6550620198249817,\n",
       "   1.568176507949829,\n",
       "   1.3118867874145508,\n",
       "   0.8758221864700317,\n",
       "   0.8968632817268372,\n",
       "   0.5917835831642151,\n",
       "   1.290953516960144,\n",
       "   0.7838027477264404,\n",
       "   1.5627013444900513,\n",
       "   1.358622670173645,\n",
       "   1.1988465785980225,\n",
       "   1.3657690286636353,\n",
       "   0.6459164619445801,\n",
       "   0.7976279258728027,\n",
       "   1.5042757987976074,\n",
       "   1.2826200723648071,\n",
       "   1.2033486366271973,\n",
       "   1.4015004634857178,\n",
       "   0.7512814402580261,\n",
       "   0.45043668150901794,\n",
       "   1.0734624862670898,\n",
       "   0.274008572101593,\n",
       "   1.760499358177185,\n",
       "   1.0209999084472656,\n",
       "   1.4413890838623047,\n",
       "   0.5690121650695801,\n",
       "   0.6133047342300415,\n",
       "   1.4236845970153809,\n",
       "   1.5739322900772095,\n",
       "   0.7500831484794617,\n",
       "   1.220069408416748,\n",
       "   1.2621355056762695,\n",
       "   1.3151713609695435,\n",
       "   1.2587077617645264,\n",
       "   1.3278595209121704,\n",
       "   1.0399137735366821,\n",
       "   0.4279339015483856,\n",
       "   0.6017356514930725,\n",
       "   0.6513342261314392,\n",
       "   1.3541477918624878,\n",
       "   0.5600175261497498,\n",
       "   0.9453433156013489,\n",
       "   1.135129451751709,\n",
       "   0.37289783358573914,\n",
       "   1.0023045539855957,\n",
       "   1.1862438917160034,\n",
       "   1.4104114770889282,\n",
       "   0.5499657988548279,\n",
       "   1.4634044170379639,\n",
       "   1.094742774963379,\n",
       "   0.8780617713928223,\n",
       "   1.5441867113113403,\n",
       "   1.0417180061340332,\n",
       "   0.4848816692829132,\n",
       "   0.32354307174682617,\n",
       "   0.49324730038642883,\n",
       "   1.1838494539260864,\n",
       "   0.8788110613822937,\n",
       "   0.5641840696334839,\n",
       "   0.5016558766365051,\n",
       "   1.0568333864212036,\n",
       "   0.3346823751926422,\n",
       "   0.6458671689033508,\n",
       "   1.1924521923065186,\n",
       "   0.6681726574897766,\n",
       "   1.1220344305038452,\n",
       "   0.8621901273727417,\n",
       "   1.2544621229171753,\n",
       "   0.695724606513977,\n",
       "   0.9341983795166016,\n",
       "   0.5251232981681824,\n",
       "   1.066947340965271,\n",
       "   0.5136777758598328,\n",
       "   1.759800672531128,\n",
       "   1.341850757598877,\n",
       "   1.0407001972198486,\n",
       "   1.249726414680481,\n",
       "   1.397818684577942,\n",
       "   0.6844727993011475,\n",
       "   1.0193167924880981,\n",
       "   0.5533517599105835,\n",
       "   0.9360746145248413,\n",
       "   1.1055333614349365,\n",
       "   1.3381409645080566,\n",
       "   0.44512978196144104,\n",
       "   0.7391529083251953,\n",
       "   2.0778422355651855,\n",
       "   0.5364860892295837,\n",
       "   0.955412745475769,\n",
       "   1.0201029777526855,\n",
       "   0.6043503284454346,\n",
       "   0.7161015272140503,\n",
       "   1.569469928741455,\n",
       "   0.5833660364151001,\n",
       "   0.9816334247589111,\n",
       "   1.0124015808105469,\n",
       "   0.6865732669830322,\n",
       "   1.3861949443817139,\n",
       "   1.0997810363769531,\n",
       "   0.6134110689163208,\n",
       "   0.836477518081665,\n",
       "   1.0520942211151123,\n",
       "   1.2505563497543335,\n",
       "   1.2586404085159302,\n",
       "   1.1958763599395752,\n",
       "   0.546877920627594,\n",
       "   0.6998218297958374,\n",
       "   0.5655149221420288,\n",
       "   1.876149296760559,\n",
       "   0.4306924641132355,\n",
       "   1.1063851118087769,\n",
       "   1.11307692527771,\n",
       "   1.020797848701477,\n",
       "   1.7372382879257202,\n",
       "   0.6553639769554138,\n",
       "   0.6819436550140381,\n",
       "   0.6515609622001648,\n",
       "   0.5720989108085632,\n",
       "   0.5505211353302002,\n",
       "   0.3921316862106323,\n",
       "   1.008102297782898,\n",
       "   1.1383954286575317,\n",
       "   1.0464316606521606,\n",
       "   0.8769935369491577,\n",
       "   0.7458680868148804,\n",
       "   1.7697011232376099,\n",
       "   0.423494428396225,\n",
       "   0.4577988386154175,\n",
       "   0.5066663026809692,\n",
       "   1.0587027072906494,\n",
       "   0.5913937091827393,\n",
       "   1.4448089599609375,\n",
       "   1.2388280630111694,\n",
       "   0.7231143116950989,\n",
       "   1.0568536520004272,\n",
       "   0.3140801191329956,\n",
       "   1.2076642513275146,\n",
       "   0.9606320858001709,\n",
       "   0.6075804233551025,\n",
       "   1.25347101688385,\n",
       "   0.3622612953186035,\n",
       "   1.0997506380081177,\n",
       "   0.47112396359443665,\n",
       "   0.4223342835903168,\n",
       "   0.7655949592590332,\n",
       "   0.8429760932922363,\n",
       "   1.1046298742294312,\n",
       "   0.8856599926948547,\n",
       "   1.0910767316818237,\n",
       "   0.3634016513824463,\n",
       "   0.5282787680625916,\n",
       "   0.5709663033485413,\n",
       "   0.3296886682510376,\n",
       "   0.5043754577636719,\n",
       "   1.6248470544815063,\n",
       "   1.4902372360229492,\n",
       "   1.4166507720947266,\n",
       "   1.2365012168884277,\n",
       "   1.3900772333145142,\n",
       "   1.089422583580017,\n",
       "   0.35140591859817505,\n",
       "   0.7973366379737854,\n",
       "   0.9172472953796387,\n",
       "   0.4114294648170471,\n",
       "   1.3112828731536865,\n",
       "   0.5657700896263123,\n",
       "   1.6388652324676514,\n",
       "   0.5464658737182617,\n",
       "   1.2480332851409912,\n",
       "   0.8441849946975708,\n",
       "   1.049652338027954,\n",
       "   1.0512257814407349,\n",
       "   1.0007339715957642,\n",
       "   1.1323603391647339,\n",
       "   0.5119727253913879,\n",
       "   1.3736834526062012,\n",
       "   2.2595345973968506,\n",
       "   0.9031075835227966,\n",
       "   1.192334771156311,\n",
       "   1.000667691230774,\n",
       "   1.0892667770385742,\n",
       "   1.0604451894760132,\n",
       "   0.5234282612800598,\n",
       "   0.7730786800384521,\n",
       "   0.6328429579734802,\n",
       "   1.4631510972976685,\n",
       "   1.1343423128128052,\n",
       "   1.2526222467422485,\n",
       "   1.3093534708023071,\n",
       "   1.431511402130127,\n",
       "   1.0006266832351685,\n",
       "   0.6599087715148926,\n",
       "   1.32050621509552,\n",
       "   1.2086375951766968,\n",
       "   0.37555474042892456,\n",
       "   1.039521336555481,\n",
       "   1.4852224588394165,\n",
       "   0.805689811706543,\n",
       "   1.10277259349823,\n",
       "   0.4382367730140686,\n",
       "   0.9037628173828125,\n",
       "   1.4402027130126953,\n",
       "   1.0254684686660767,\n",
       "   0.36822715401649475,\n",
       "   0.9418348670005798,\n",
       "   1.2862790822982788,\n",
       "   1.1780918836593628,\n",
       "   0.7630501389503479,\n",
       "   0.8278738856315613,\n",
       "   1.2070749998092651],\n",
       "  ('euclidean', False, False): [0.7257487177848816,\n",
       "   0.44079044461250305,\n",
       "   0.6453565955162048,\n",
       "   1.737955927848816,\n",
       "   0.9368528127670288,\n",
       "   1.1823526620864868,\n",
       "   0.6736451983451843,\n",
       "   1.4422634840011597,\n",
       "   1.6149108409881592,\n",
       "   0.3402709364891052,\n",
       "   0.43995678424835205,\n",
       "   0.6422197818756104,\n",
       "   0.724436342716217,\n",
       "   1.3212701082229614,\n",
       "   0.6493316888809204,\n",
       "   1.092244029045105,\n",
       "   0.9969993829727173,\n",
       "   0.9590622782707214,\n",
       "   1.0773221254348755,\n",
       "   0.6156513690948486,\n",
       "   1.0849958658218384,\n",
       "   0.9282934069633484,\n",
       "   1.2618006467819214,\n",
       "   0.48216190934181213,\n",
       "   0.5486834645271301,\n",
       "   0.6690059304237366,\n",
       "   0.5310548543930054,\n",
       "   0.8979836702346802,\n",
       "   1.3165591955184937,\n",
       "   2.018754005432129,\n",
       "   0.6833351254463196,\n",
       "   0.895882248878479,\n",
       "   0.6164363026618958,\n",
       "   0.5490787625312805,\n",
       "   0.5996875166893005,\n",
       "   0.6394430994987488,\n",
       "   0.6380056738853455,\n",
       "   0.6365237832069397,\n",
       "   1.2361271381378174,\n",
       "   1.0866247415542603,\n",
       "   0.3500087261199951,\n",
       "   0.9476178288459778,\n",
       "   0.9414106607437134,\n",
       "   1.3105663061141968,\n",
       "   0.9226700067520142,\n",
       "   0.46766436100006104,\n",
       "   0.6247072219848633,\n",
       "   0.5779039859771729,\n",
       "   1.9991027116775513,\n",
       "   0.65824294090271,\n",
       "   0.6067209839820862,\n",
       "   0.5185645222663879,\n",
       "   1.8220971822738647,\n",
       "   0.8507455587387085,\n",
       "   0.6938118934631348,\n",
       "   1.1416360139846802,\n",
       "   1.3912087678909302,\n",
       "   0.45251813530921936,\n",
       "   1.4443155527114868,\n",
       "   0.9152935743331909,\n",
       "   0.5250971913337708,\n",
       "   1.1850038766860962,\n",
       "   1.046406865119934,\n",
       "   0.9732990264892578,\n",
       "   0.662108838558197,\n",
       "   0.9853987693786621,\n",
       "   0.29863154888153076,\n",
       "   1.006240725517273,\n",
       "   1.486116647720337,\n",
       "   0.7413421273231506,\n",
       "   1.0065926313400269,\n",
       "   1.3507146835327148,\n",
       "   0.9110603332519531,\n",
       "   0.40597596764564514,\n",
       "   1.3307653665542603,\n",
       "   1.2816129922866821,\n",
       "   0.5954527258872986,\n",
       "   0.9004970788955688,\n",
       "   1.2953940629959106,\n",
       "   0.6942553520202637,\n",
       "   1.071597695350647,\n",
       "   0.5693202018737793,\n",
       "   1.8976600170135498,\n",
       "   0.5385289788246155,\n",
       "   1.4789036512374878,\n",
       "   0.7972621321678162,\n",
       "   0.4897020757198334,\n",
       "   0.7520859241485596,\n",
       "   0.6214337944984436,\n",
       "   0.4002279043197632,\n",
       "   0.46444037556648254,\n",
       "   1.013098120689392,\n",
       "   1.27866792678833,\n",
       "   0.6941745281219482,\n",
       "   1.0192055702209473,\n",
       "   1.0506936311721802,\n",
       "   0.5382604002952576,\n",
       "   1.3129496574401855,\n",
       "   0.9807860255241394,\n",
       "   0.8957507610321045,\n",
       "   0.9196656346321106,\n",
       "   0.5235675573348999,\n",
       "   0.5611280798912048,\n",
       "   1.054927110671997,\n",
       "   1.159218430519104,\n",
       "   1.6475143432617188,\n",
       "   1.3697772026062012,\n",
       "   0.46787410974502563,\n",
       "   0.3831605613231659,\n",
       "   0.5358534455299377,\n",
       "   0.26111793518066406,\n",
       "   0.44140544533729553,\n",
       "   1.428360939025879,\n",
       "   1.2139924764633179,\n",
       "   0.6675499677658081,\n",
       "   0.607884407043457,\n",
       "   0.6022205948829651,\n",
       "   0.823807418346405,\n",
       "   0.5174877643585205,\n",
       "   1.1263928413391113,\n",
       "   0.6119208931922913,\n",
       "   0.2525971233844757,\n",
       "   0.8339874148368835,\n",
       "   1.0927748680114746,\n",
       "   1.3842384815216064,\n",
       "   0.7972207069396973,\n",
       "   0.5438300967216492,\n",
       "   0.5132135152816772,\n",
       "   0.5329561233520508,\n",
       "   1.521005630493164,\n",
       "   1.011030673980713,\n",
       "   1.0981853008270264,\n",
       "   1.1723434925079346,\n",
       "   0.5652436017990112,\n",
       "   1.0166949033737183,\n",
       "   1.2187057733535767,\n",
       "   0.9604068994522095,\n",
       "   1.5190349817276,\n",
       "   1.3956047296524048,\n",
       "   0.6481265425682068,\n",
       "   1.223137378692627,\n",
       "   1.5710997581481934,\n",
       "   0.4142124354839325,\n",
       "   0.5630947351455688,\n",
       "   1.6820188760757446,\n",
       "   1.4598417282104492,\n",
       "   0.6382508873939514,\n",
       "   1.563968300819397,\n",
       "   0.5391364693641663,\n",
       "   0.8739398717880249,\n",
       "   1.4383939504623413,\n",
       "   1.0351157188415527,\n",
       "   1.2477548122406006,\n",
       "   0.5745441317558289,\n",
       "   0.5789437294006348,\n",
       "   1.4929440021514893,\n",
       "   1.0407891273498535,\n",
       "   1.1334072351455688,\n",
       "   1.6428871154785156,\n",
       "   1.1807348728179932,\n",
       "   1.1242400407791138,\n",
       "   0.5252817869186401,\n",
       "   0.2629193961620331,\n",
       "   1.2394953966140747,\n",
       "   1.4994628429412842,\n",
       "   0.6544301509857178,\n",
       "   1.1219068765640259,\n",
       "   0.8080441355705261,\n",
       "   0.641869306564331,\n",
       "   1.4615188837051392,\n",
       "   0.6755331754684448,\n",
       "   1.2123916149139404,\n",
       "   0.8855928778648376,\n",
       "   1.1860815286636353,\n",
       "   1.7406855821609497,\n",
       "   1.6539734601974487,\n",
       "   1.0139352083206177,\n",
       "   0.4481181204319,\n",
       "   0.6961823105812073,\n",
       "   1.3109827041625977,\n",
       "   0.6106360554695129,\n",
       "   0.604810357093811,\n",
       "   0.6566122174263,\n",
       "   0.5218933820724487,\n",
       "   1.6739393472671509,\n",
       "   0.6831259727478027,\n",
       "   1.9606437683105469,\n",
       "   0.7015284299850464,\n",
       "   0.34130018949508667,\n",
       "   1.0430984497070312,\n",
       "   0.306024432182312,\n",
       "   1.1873679161071777,\n",
       "   0.2730385959148407,\n",
       "   1.480963945388794,\n",
       "   1.2391902208328247,\n",
       "   0.643815815448761,\n",
       "   0.4611203372478485,\n",
       "   0.8986679911613464,\n",
       "   0.6381512880325317,\n",
       "   0.6204314827919006,\n",
       "   1.4755663871765137,\n",
       "   0.8385308980941772,\n",
       "   0.5215109586715698,\n",
       "   0.935821533203125,\n",
       "   0.8822345733642578,\n",
       "   0.8511415719985962,\n",
       "   0.9861621260643005,\n",
       "   1.0136421918869019,\n",
       "   1.043243646621704,\n",
       "   0.6272203326225281,\n",
       "   1.5574320554733276,\n",
       "   2.0972678661346436,\n",
       "   1.89162278175354,\n",
       "   0.9186741709709167,\n",
       "   0.6863878965377808,\n",
       "   0.9139856696128845,\n",
       "   0.7714884877204895,\n",
       "   0.603299617767334,\n",
       "   0.7705226540565491,\n",
       "   0.977032482624054,\n",
       "   0.7830610871315002,\n",
       "   1.1533393859863281,\n",
       "   0.6110135912895203,\n",
       "   1.2522941827774048,\n",
       "   0.45311060547828674,\n",
       "   1.045925498008728,\n",
       "   0.5647526383399963,\n",
       "   1.204813838005066,\n",
       "   0.48720869421958923,\n",
       "   0.4863552451133728,\n",
       "   0.5604802966117859,\n",
       "   0.9668511748313904,\n",
       "   0.7864277958869934,\n",
       "   1.1200299263000488,\n",
       "   1.063832402229309,\n",
       "   0.5220695734024048,\n",
       "   1.4489445686340332,\n",
       "   0.4126182496547699,\n",
       "   0.3773142099380493,\n",
       "   0.3500087261199951,\n",
       "   1.140869379043579,\n",
       "   1.086063027381897,\n",
       "   0.596400797367096,\n",
       "   0.564470648765564,\n",
       "   0.5727230906486511,\n",
       "   1.2670644521713257,\n",
       "   0.633052408695221,\n",
       "   1.2259942293167114,\n",
       "   0.9109984636306763,\n",
       "   1.767066478729248,\n",
       "   1.1140283346176147,\n",
       "   0.9347593784332275,\n",
       "   1.3275020122528076,\n",
       "   0.9998618364334106,\n",
       "   0.6465762257575989,\n",
       "   1.066281795501709,\n",
       "   1.005721092224121,\n",
       "   1.548637866973877,\n",
       "   1.136651635169983,\n",
       "   1.162831425666809,\n",
       "   0.5280762314796448,\n",
       "   0.24335096776485443,\n",
       "   0.9707783460617065,\n",
       "   0.9142575860023499,\n",
       "   0.6787635684013367,\n",
       "   0.6173226833343506,\n",
       "   1.204202651977539,\n",
       "   0.5853298306465149,\n",
       "   0.6797995567321777,\n",
       "   0.35597798228263855,\n",
       "   1.7906724214553833,\n",
       "   0.6159246563911438,\n",
       "   0.27496203780174255,\n",
       "   1.3828141689300537,\n",
       "   0.5070896148681641,\n",
       "   1.3283179998397827,\n",
       "   0.919066309928894,\n",
       "   1.197161078453064,\n",
       "   0.5552262663841248,\n",
       "   0.9997556209564209,\n",
       "   0.6422197818756104,\n",
       "   1.0587773323059082,\n",
       "   0.5431973934173584,\n",
       "   1.2749134302139282,\n",
       "   1.027328610420227,\n",
       "   1.1269339323043823,\n",
       "   1.1313703060150146,\n",
       "   0.8590256571769714,\n",
       "   0.7381551265716553,\n",
       "   0.5741688013076782,\n",
       "   0.5880258083343506,\n",
       "   0.6605789661407471,\n",
       "   0.38201114535331726,\n",
       "   1.2598192691802979,\n",
       "   0.6465386152267456,\n",
       "   1.1153732538223267,\n",
       "   0.42216211557388306,\n",
       "   0.5429599285125732,\n",
       "   0.49861788749694824,\n",
       "   0.8652104139328003,\n",
       "   0.6945611238479614,\n",
       "   0.28376802802085876,\n",
       "   0.5973643064498901,\n",
       "   1.5681766271591187,\n",
       "   1.716565728187561,\n",
       "   0.5592262744903564,\n",
       "   0.8968635201454163,\n",
       "   2.258617877960205,\n",
       "   0.6866031289100647,\n",
       "   0.7668276429176331,\n",
       "   1.5519992113113403,\n",
       "   1.358622670173645,\n",
       "   1.1256020069122314,\n",
       "   1.3657687902450562,\n",
       "   0.6459165215492249,\n",
       "   0.7976279854774475,\n",
       "   1.594462275505066,\n",
       "   1.1533297300338745,\n",
       "   1.5806043148040771,\n",
       "   1.4015003442764282,\n",
       "   3.883368730545044,\n",
       "   0.45043665170669556,\n",
       "   1.0734620094299316,\n",
       "   0.274008572101593,\n",
       "   1.2503248453140259,\n",
       "   1.0553709268569946,\n",
       "   0.6435633301734924,\n",
       "   0.5690121650695801,\n",
       "   0.6133047342300415,\n",
       "   1.42368483543396,\n",
       "   1.5739322900772095,\n",
       "   0.9290655255317688,\n",
       "   1.220069408416748,\n",
       "   1.3991334438323975,\n",
       "   1.1061110496520996,\n",
       "   1.447482943534851,\n",
       "   1.3278595209121704,\n",
       "   0.6324245929718018,\n",
       "   0.427933931350708,\n",
       "   0.6017356514930725,\n",
       "   0.6513341665267944,\n",
       "   1.2442529201507568,\n",
       "   0.5600175261497498,\n",
       "   0.8432410359382629,\n",
       "   1.1351290941238403,\n",
       "   0.3710305988788605,\n",
       "   1.1098499298095703,\n",
       "   1.4785983562469482,\n",
       "   0.6271103024482727,\n",
       "   0.5499659180641174,\n",
       "   1.1637212038040161,\n",
       "   1.0947428941726685,\n",
       "   0.8780617713928223,\n",
       "   0.6651003956794739,\n",
       "   1.0417181253433228,\n",
       "   0.484881728887558,\n",
       "   0.32354313135147095,\n",
       "   0.4932474195957184,\n",
       "   1.1838496923446655,\n",
       "   0.5372762084007263,\n",
       "   0.5641840696334839,\n",
       "   0.5016559362411499,\n",
       "   0.8807199001312256,\n",
       "   0.3346823751926422,\n",
       "   0.645867109298706,\n",
       "   0.7357816696166992,\n",
       "   0.6681725978851318,\n",
       "   1.1220345497131348,\n",
       "   0.783340573310852,\n",
       "   1.2544620037078857,\n",
       "   0.695724606513977,\n",
       "   0.9341985583305359,\n",
       "   0.5251232385635376,\n",
       "   0.6883984804153442,\n",
       "   0.2551255226135254,\n",
       "   1.8787119388580322,\n",
       "   1.341850757598877,\n",
       "   0.9833834171295166,\n",
       "   0.4680776000022888,\n",
       "   1.293298363685608,\n",
       "   0.6844727993011475,\n",
       "   1.01931631565094,\n",
       "   0.5533517599105835,\n",
       "   0.8539395332336426,\n",
       "   1.105533480644226,\n",
       "   0.5791457891464233,\n",
       "   0.44512978196144104,\n",
       "   0.7135886549949646,\n",
       "   2.3681893348693848,\n",
       "   0.5364860892295837,\n",
       "   0.6157494187355042,\n",
       "   1.3007359504699707,\n",
       "   0.6043505668640137,\n",
       "   0.7161015272140503,\n",
       "   1.2365655899047852,\n",
       "   0.5623366236686707,\n",
       "   0.9816334247589111,\n",
       "   1.1215792894363403,\n",
       "   0.6749280095100403,\n",
       "   1.3339096307754517,\n",
       "   1.0997812747955322,\n",
       "   0.40783584117889404,\n",
       "   0.8190032839775085,\n",
       "   1.4242279529571533,\n",
       "   0.6220806241035461,\n",
       "   1.39885675907135,\n",
       "   1.1958763599395752,\n",
       "   0.546877920627594,\n",
       "   0.6683913469314575,\n",
       "   0.4733003079891205,\n",
       "   2.1253325939178467,\n",
       "   0.4306924641132355,\n",
       "   1.4435077905654907,\n",
       "   1.037014365196228,\n",
       "   0.5018161535263062,\n",
       "   0.6625329256057739,\n",
       "   0.33888718485832214,\n",
       "   0.35625115036964417,\n",
       "   0.6515610218048096,\n",
       "   0.5720990300178528,\n",
       "   0.5505211353302002,\n",
       "   0.3921316862106323,\n",
       "   0.9669743180274963,\n",
       "   1.1383954286575317,\n",
       "   1.0464317798614502,\n",
       "   0.8769934773445129,\n",
       "   0.745867908000946,\n",
       "   0.7313107848167419,\n",
       "   0.423494428396225,\n",
       "   0.4577988386154175,\n",
       "   0.5066663026809692,\n",
       "   1.0621432065963745,\n",
       "   0.5913936495780945,\n",
       "   0.5253543853759766,\n",
       "   1.1792700290679932,\n",
       "   0.797559380531311,\n",
       "   0.973879337310791,\n",
       "   0.2862391173839569,\n",
       "   1.2076646089553833,\n",
       "   0.6632099151611328,\n",
       "   0.6075804829597473,\n",
       "   0.9645853042602539,\n",
       "   0.36226126551628113,\n",
       "   1.0997509956359863,\n",
       "   1.1516443490982056,\n",
       "   0.24555645883083344,\n",
       "   0.765595018863678,\n",
       "   0.8429761528968811,\n",
       "   0.7777427434921265,\n",
       "   0.9653602242469788,\n",
       "   0.9700911641120911,\n",
       "   0.3634016513824463,\n",
       "   0.5282787680625916,\n",
       "   0.5709663033485413,\n",
       "   0.6711762547492981,\n",
       "   0.5043755173683167,\n",
       "   0.9251598119735718,\n",
       "   1.4902373552322388,\n",
       "   1.4166507720947266,\n",
       "   1.2543138265609741,\n",
       "   0.5664067268371582,\n",
       "   1.0894227027893066,\n",
       "   0.35140591859817505,\n",
       "   0.562394917011261,\n",
       "   0.9881948232650757,\n",
       "   0.41142961382865906,\n",
       "   1.311282753944397,\n",
       "   1.2795064449310303,\n",
       "   1.7117441892623901,\n",
       "   0.5464659929275513,\n",
       "   1.248034119606018,\n",
       "   0.8441846966743469,\n",
       "   1.2422335147857666,\n",
       "   1.0630840063095093,\n",
       "   1.0007342100143433,\n",
       "   1.1323604583740234,\n",
       "   0.5119727849960327,\n",
       "   0.5736777186393738,\n",
       "   1.5309107303619385,\n",
       "   0.5151401162147522,\n",
       "   1.1923344135284424,\n",
       "   0.6164764761924744,\n",
       "   1.0892670154571533,\n",
       "   1.0166716575622559,\n",
       "   0.523428201675415,\n",
       "   1.2109835147857666,\n",
       "   0.6125954985618591,\n",
       "   0.6936299800872803,\n",
       "   0.8364459872245789,\n",
       "   0.9097495079040527,\n",
       "   0.8409786820411682,\n",
       "   1.5700651407241821,\n",
       "   0.9282135367393494,\n",
       "   0.6599088907241821,\n",
       "   1.32050621509552,\n",
       "   1.2086373567581177,\n",
       "   0.37555474042892456,\n",
       "   1.2811018228530884,\n",
       "   1.5266985893249512,\n",
       "   0.8056895732879639,\n",
       "   1.102772831916809,\n",
       "   0.438236802816391,\n",
       "   0.8701990842819214,\n",
       "   1.4402025938034058,\n",
       "   0.627324104309082,\n",
       "   0.36822718381881714,\n",
       "   0.9418346285820007,\n",
       "   1.5703338384628296,\n",
       "   1.1780916452407837,\n",
       "   0.7630502581596375,\n",
       "   0.5806347727775574,\n",
       "   1.2070748805999756]},\n",
       " 'google/codegemma-2b': {('cosine', True, True): [0.9272906184196472,\n",
       "   1.1336967945098877,\n",
       "   1.5343092679977417,\n",
       "   0.9825559854507446,\n",
       "   1.0144176483154297,\n",
       "   0.9899806976318359,\n",
       "   1.1500698328018188,\n",
       "   0.8440626859664917,\n",
       "   1.4476103782653809,\n",
       "   1.1489343643188477,\n",
       "   1.6034759283065796,\n",
       "   0.9654756784439087,\n",
       "   1.1208692789077759,\n",
       "   1.0214300155639648,\n",
       "   0.9275938868522644,\n",
       "   0.9923262596130371,\n",
       "   0.9753646850585938,\n",
       "   0.9104536771774292,\n",
       "   1.0486122369766235,\n",
       "   1.5580897331237793,\n",
       "   1.0104689598083496,\n",
       "   1.0805598497390747,\n",
       "   1.5150456428527832,\n",
       "   1.0347771644592285,\n",
       "   0.9966756701469421,\n",
       "   1.039595365524292,\n",
       "   0.9969750046730042,\n",
       "   1.0277613401412964,\n",
       "   0.9769922494888306,\n",
       "   0.9579587578773499,\n",
       "   1.1628981828689575,\n",
       "   0.9822947382926941,\n",
       "   1.3693957328796387,\n",
       "   1.0466350317001343,\n",
       "   0.9880717396736145,\n",
       "   1.072219967842102,\n",
       "   1.0207698345184326,\n",
       "   1.0245108604431152,\n",
       "   1.0504306554794312,\n",
       "   0.9885725975036621,\n",
       "   1.0330877304077148,\n",
       "   0.9781445264816284,\n",
       "   1.6598228216171265,\n",
       "   0.9273030161857605,\n",
       "   0.9501910209655762,\n",
       "   0.9750527739524841,\n",
       "   0.9605211615562439,\n",
       "   1.0682626962661743,\n",
       "   0.9411173462867737,\n",
       "   0.9637314081192017,\n",
       "   1.5553375482559204,\n",
       "   1.0125384330749512,\n",
       "   0.9912687540054321,\n",
       "   1.1329883337020874,\n",
       "   1.4702306985855103,\n",
       "   0.9706808924674988,\n",
       "   0.9412256479263306,\n",
       "   1.0112494230270386,\n",
       "   0.9792020320892334,\n",
       "   1.3024249076843262,\n",
       "   1.0399391651153564,\n",
       "   1.0715522766113281,\n",
       "   0.9517104029655457,\n",
       "   1.677180528640747,\n",
       "   1.1598126888275146,\n",
       "   0.9623908996582031,\n",
       "   1.0146716833114624,\n",
       "   0.9959635138511658,\n",
       "   1.0144315958023071,\n",
       "   0.966513454914093,\n",
       "   1.0096272230148315,\n",
       "   0.9850105047225952,\n",
       "   1.0215734243392944,\n",
       "   0.970822811126709,\n",
       "   0.9947397112846375,\n",
       "   1.1143049001693726,\n",
       "   1.0250115394592285,\n",
       "   0.9823386073112488,\n",
       "   0.9991793632507324,\n",
       "   1.4602471590042114,\n",
       "   0.9428137540817261,\n",
       "   0.9986558556556702,\n",
       "   0.8791605830192566,\n",
       "   1.066432237625122,\n",
       "   0.9639496803283691,\n",
       "   1.0070959329605103,\n",
       "   0.9506239891052246,\n",
       "   1.0648696422576904,\n",
       "   1.6031770706176758,\n",
       "   1.2287770509719849,\n",
       "   0.958223819732666,\n",
       "   1.2921998500823975,\n",
       "   1.2701647281646729,\n",
       "   0.9820688962936401,\n",
       "   1.2764159440994263,\n",
       "   1.2453067302703857,\n",
       "   0.9554093480110168,\n",
       "   0.8828827738761902,\n",
       "   1.0958831310272217,\n",
       "   1.1036407947540283,\n",
       "   1.5124425888061523,\n",
       "   1.1054432392120361,\n",
       "   0.9318966865539551,\n",
       "   0.9827917218208313,\n",
       "   1.2142438888549805,\n",
       "   1.1009284257888794,\n",
       "   0.921123206615448,\n",
       "   0.9846242070198059,\n",
       "   1.0387240648269653,\n",
       "   0.9871371984481812,\n",
       "   1.149261713027954,\n",
       "   1.057513952255249,\n",
       "   1.2308590412139893,\n",
       "   1.0631906986236572,\n",
       "   1.2373908758163452,\n",
       "   1.0406397581100464,\n",
       "   0.9805343151092529,\n",
       "   1.236027717590332,\n",
       "   1.040908694267273,\n",
       "   1.6103730201721191,\n",
       "   1.2999238967895508,\n",
       "   1.109576940536499,\n",
       "   1.138546109199524,\n",
       "   1.0214488506317139,\n",
       "   0.9205691814422607,\n",
       "   1.0599628686904907,\n",
       "   1.0167977809906006,\n",
       "   1.4967138767242432,\n",
       "   0.9438368678092957,\n",
       "   1.0083376169204712,\n",
       "   1.198404312133789,\n",
       "   1.2518233060836792,\n",
       "   1.2273900508880615,\n",
       "   1.049788475036621,\n",
       "   1.0566720962524414,\n",
       "   1.2454341650009155,\n",
       "   0.969444751739502,\n",
       "   0.9228507876396179,\n",
       "   0.920360267162323,\n",
       "   0.895349383354187,\n",
       "   0.9614202976226807,\n",
       "   0.9931104183197021,\n",
       "   1.0582212209701538,\n",
       "   1.472969889640808,\n",
       "   0.9854463934898376,\n",
       "   0.9109699726104736,\n",
       "   0.9184667468070984,\n",
       "   0.9640173316001892,\n",
       "   1.0165112018585205,\n",
       "   1.239812970161438,\n",
       "   0.922115683555603,\n",
       "   0.9787710905075073,\n",
       "   0.9886686205863953,\n",
       "   1.0399540662765503,\n",
       "   1.5593878030776978,\n",
       "   1.0074553489685059,\n",
       "   1.1201878786087036,\n",
       "   1.0388520956039429,\n",
       "   1.0747286081314087,\n",
       "   1.0227482318878174,\n",
       "   0.9749135375022888,\n",
       "   0.9923564195632935,\n",
       "   1.728203535079956,\n",
       "   0.9685898423194885,\n",
       "   1.1848928928375244,\n",
       "   1.111284852027893,\n",
       "   0.9862316250801086,\n",
       "   1.447262167930603,\n",
       "   0.9124954342842102,\n",
       "   1.157699465751648,\n",
       "   0.8605778813362122,\n",
       "   1.5957471132278442,\n",
       "   1.2388203144073486,\n",
       "   0.9995375275611877,\n",
       "   1.020312786102295,\n",
       "   1.5921870470046997,\n",
       "   0.9357045292854309,\n",
       "   0.9811723828315735,\n",
       "   0.9573860168457031,\n",
       "   1.0058842897415161,\n",
       "   0.9861718416213989,\n",
       "   1.4279769659042358,\n",
       "   1.1728451251983643,\n",
       "   1.0308960676193237,\n",
       "   0.9361748099327087,\n",
       "   1.1996089220046997,\n",
       "   0.9551278948783875,\n",
       "   1.0392427444458008,\n",
       "   1.2346192598342896,\n",
       "   1.167410969734192,\n",
       "   1.17740797996521,\n",
       "   1.073903203010559,\n",
       "   1.2115113735198975,\n",
       "   0.9727990627288818,\n",
       "   1.024294137954712,\n",
       "   0.8695574998855591,\n",
       "   1.1140456199645996,\n",
       "   1.0019234418869019,\n",
       "   1.5994945764541626,\n",
       "   1.0091365575790405,\n",
       "   0.8911682963371277,\n",
       "   0.9380251169204712,\n",
       "   1.038521409034729,\n",
       "   1.0028448104858398,\n",
       "   1.050410270690918,\n",
       "   0.9336427450180054,\n",
       "   1.035736083984375,\n",
       "   1.0051411390304565,\n",
       "   0.9838946461677551,\n",
       "   1.5773133039474487,\n",
       "   1.2207493782043457,\n",
       "   1.0511152744293213,\n",
       "   0.8865908980369568,\n",
       "   1.0449265241622925,\n",
       "   1.4428085088729858,\n",
       "   0.9818993210792542,\n",
       "   0.9753418564796448,\n",
       "   0.934237539768219,\n",
       "   1.330804705619812,\n",
       "   1.0321602821350098,\n",
       "   1.1864724159240723,\n",
       "   0.9892580509185791,\n",
       "   1.594565987586975,\n",
       "   0.9548974633216858,\n",
       "   1.1798654794692993,\n",
       "   1.6697791814804077,\n",
       "   1.6123262643814087,\n",
       "   0.994255006313324,\n",
       "   1.102869987487793,\n",
       "   1.0619233846664429,\n",
       "   0.9339162707328796,\n",
       "   0.9755712151527405,\n",
       "   1.152829885482788,\n",
       "   1.000130534172058,\n",
       "   0.980655312538147,\n",
       "   1.069137692451477,\n",
       "   0.8871686458587646,\n",
       "   1.7573201656341553,\n",
       "   0.9941307306289673,\n",
       "   1.163871169090271,\n",
       "   0.9518194794654846,\n",
       "   1.0378553867340088,\n",
       "   0.9643920660018921,\n",
       "   0.9643034934997559,\n",
       "   1.0586364269256592,\n",
       "   0.9003197550773621,\n",
       "   1.0143120288848877,\n",
       "   0.9775475859642029,\n",
       "   1.3686870336532593,\n",
       "   0.8303422927856445,\n",
       "   0.9578446745872498,\n",
       "   0.926936686038971,\n",
       "   1.343626856803894,\n",
       "   1.065536379814148,\n",
       "   1.0260063409805298,\n",
       "   1.1011017560958862,\n",
       "   0.9901984333992004,\n",
       "   1.0367704629898071,\n",
       "   0.9806725382804871,\n",
       "   1.3497213125228882,\n",
       "   0.998813807964325,\n",
       "   1.2634857892990112,\n",
       "   1.0291469097137451,\n",
       "   0.9826818704605103,\n",
       "   0.9962612986564636,\n",
       "   1.0180567502975464,\n",
       "   0.9605679512023926,\n",
       "   1.2150956392288208,\n",
       "   0.9791962504386902,\n",
       "   1.082526683807373,\n",
       "   0.9561400413513184,\n",
       "   1.1062291860580444,\n",
       "   1.1578943729400635,\n",
       "   0.8724992871284485,\n",
       "   1.0229439735412598,\n",
       "   1.1828703880310059,\n",
       "   1.005593180656433,\n",
       "   1.2474955320358276,\n",
       "   1.0412452220916748,\n",
       "   1.2350594997406006,\n",
       "   0.96907639503479,\n",
       "   1.1450096368789673,\n",
       "   1.0294091701507568,\n",
       "   1.507418155670166,\n",
       "   1.0892142057418823,\n",
       "   1.0238887071609497,\n",
       "   0.9921561479568481,\n",
       "   1.058595895767212,\n",
       "   1.3011555671691895,\n",
       "   1.566564917564392,\n",
       "   1.1428141593933105,\n",
       "   1.0099642276763916,\n",
       "   1.052531123161316,\n",
       "   0.9103674292564392,\n",
       "   0.880340576171875,\n",
       "   0.9767807722091675,\n",
       "   0.974358081817627,\n",
       "   0.9829844236373901,\n",
       "   0.9910615086555481,\n",
       "   1.3025506734848022,\n",
       "   0.9269017577171326,\n",
       "   1.7927926778793335,\n",
       "   1.3206658363342285,\n",
       "   0.8245614171028137,\n",
       "   0.8860971927642822,\n",
       "   1.3358038663864136,\n",
       "   0.9110040664672852,\n",
       "   1.0346976518630981,\n",
       "   0.8766473531723022,\n",
       "   0.9871677756309509,\n",
       "   1.0197958946228027,\n",
       "   1.0322420597076416,\n",
       "   1.0325491428375244,\n",
       "   0.9848859906196594,\n",
       "   1.062584638595581,\n",
       "   1.0031018257141113,\n",
       "   1.1027247905731201,\n",
       "   0.9555186033248901,\n",
       "   0.974376916885376,\n",
       "   1.0005450248718262,\n",
       "   0.8198236227035522,\n",
       "   1.0755712985992432,\n",
       "   1.0487468242645264,\n",
       "   1.2272573709487915,\n",
       "   0.8859935402870178,\n",
       "   0.9797823429107666,\n",
       "   1.027185320854187,\n",
       "   0.9545608758926392,\n",
       "   0.9269816279411316,\n",
       "   0.9373742341995239,\n",
       "   0.9904908537864685,\n",
       "   1.0580554008483887,\n",
       "   0.9673399925231934,\n",
       "   0.9613816142082214,\n",
       "   0.9639678597450256,\n",
       "   1.0092182159423828,\n",
       "   1.1555176973342896,\n",
       "   0.9672703742980957,\n",
       "   1.0931529998779297,\n",
       "   1.0126473903656006,\n",
       "   0.967171311378479,\n",
       "   0.9755302667617798,\n",
       "   0.9618228077888489,\n",
       "   1.016692876815796,\n",
       "   0.9505001306533813,\n",
       "   1.290687918663025,\n",
       "   0.9966591596603394,\n",
       "   0.9793994426727295,\n",
       "   0.8929052352905273,\n",
       "   1.0362313985824585,\n",
       "   0.9097246527671814,\n",
       "   1.046629786491394,\n",
       "   0.9635319113731384,\n",
       "   0.8855942487716675,\n",
       "   1.24394690990448,\n",
       "   0.9827135801315308,\n",
       "   1.047784686088562,\n",
       "   1.1229346990585327,\n",
       "   1.5484576225280762,\n",
       "   1.0342247486114502,\n",
       "   0.9857659935951233,\n",
       "   1.0168876647949219,\n",
       "   0.9959184527397156,\n",
       "   0.9976409077644348,\n",
       "   1.0001487731933594,\n",
       "   0.9319332242012024,\n",
       "   1.3402718305587769,\n",
       "   1.0598301887512207,\n",
       "   1.0432873964309692,\n",
       "   0.9885876178741455,\n",
       "   1.0983250141143799,\n",
       "   1.0427099466323853,\n",
       "   0.9993520379066467,\n",
       "   0.9722311496734619,\n",
       "   1.055289387702942,\n",
       "   0.8388887047767639,\n",
       "   0.9668740630149841,\n",
       "   1.0113027095794678,\n",
       "   1.2019497156143188,\n",
       "   1.0094388723373413,\n",
       "   1.0286730527877808,\n",
       "   1.0854347944259644,\n",
       "   1.6055766344070435,\n",
       "   1.1115258932113647,\n",
       "   1.0168269872665405,\n",
       "   0.9295861721038818,\n",
       "   0.999162495136261,\n",
       "   1.0660070180892944,\n",
       "   0.9225770235061646,\n",
       "   0.9556340575218201,\n",
       "   1.6061012744903564,\n",
       "   1.0061538219451904,\n",
       "   1.0010203123092651,\n",
       "   1.0959967374801636,\n",
       "   1.0607792139053345,\n",
       "   1.0584149360656738,\n",
       "   1.0551443099975586,\n",
       "   0.9929119348526001,\n",
       "   1.2040362358093262,\n",
       "   1.0157699584960938,\n",
       "   1.1149743795394897,\n",
       "   1.1661759614944458,\n",
       "   1.0321100950241089,\n",
       "   1.2560657262802124,\n",
       "   0.9695538878440857,\n",
       "   1.2614198923110962,\n",
       "   0.9087784290313721,\n",
       "   1.027451992034912,\n",
       "   1.3942201137542725,\n",
       "   1.1417381763458252,\n",
       "   0.8852874040603638,\n",
       "   1.0377141237258911,\n",
       "   0.9786816239356995,\n",
       "   0.931294322013855,\n",
       "   1.063823938369751,\n",
       "   1.2183269262313843,\n",
       "   1.0462886095046997,\n",
       "   1.1543571949005127,\n",
       "   0.9865873456001282,\n",
       "   1.0012868642807007,\n",
       "   1.0395578145980835,\n",
       "   1.2366925477981567,\n",
       "   1.2646229267120361,\n",
       "   0.9998030066490173,\n",
       "   1.0158408880233765,\n",
       "   0.96242755651474,\n",
       "   1.0635759830474854,\n",
       "   0.8157411813735962,\n",
       "   1.0109795331954956,\n",
       "   1.0096763372421265,\n",
       "   1.015821933746338,\n",
       "   0.9917389154434204,\n",
       "   1.5902096033096313,\n",
       "   1.2965673208236694,\n",
       "   0.9259105920791626,\n",
       "   1.0036982297897339,\n",
       "   0.9996500015258789,\n",
       "   1.1441527605056763,\n",
       "   0.9660672545433044,\n",
       "   1.0409328937530518,\n",
       "   0.9870318174362183,\n",
       "   1.0464880466461182,\n",
       "   1.1485766172409058,\n",
       "   0.94205641746521,\n",
       "   0.9860734343528748,\n",
       "   1.1082744598388672,\n",
       "   1.038869857788086,\n",
       "   1.1710351705551147,\n",
       "   0.9818572402000427,\n",
       "   0.9680832624435425,\n",
       "   1.0143988132476807,\n",
       "   1.0131241083145142,\n",
       "   1.19364333152771,\n",
       "   1.7029685974121094,\n",
       "   1.207898736000061,\n",
       "   0.992737352848053,\n",
       "   0.968363881111145,\n",
       "   0.9447084665298462,\n",
       "   1.0004146099090576,\n",
       "   1.0200494527816772,\n",
       "   0.9273263812065125,\n",
       "   0.9773097634315491,\n",
       "   1.2663952112197876,\n",
       "   1.0740247964859009,\n",
       "   1.1123369932174683,\n",
       "   1.0342357158660889,\n",
       "   1.4240925312042236,\n",
       "   1.0843908786773682,\n",
       "   1.2377057075500488,\n",
       "   1.0501000881195068,\n",
       "   0.8970341086387634,\n",
       "   1.0516247749328613,\n",
       "   1.0180120468139648,\n",
       "   1.050520896911621,\n",
       "   0.9214790463447571,\n",
       "   1.0398920774459839,\n",
       "   1.0582245588302612,\n",
       "   0.9753074049949646,\n",
       "   1.0627555847167969,\n",
       "   1.0025700330734253,\n",
       "   1.0198439359664917,\n",
       "   1.3787678480148315,\n",
       "   0.8943222165107727,\n",
       "   1.0815725326538086,\n",
       "   1.1977038383483887,\n",
       "   1.1404790878295898,\n",
       "   1.1192539930343628,\n",
       "   0.9849802851676941,\n",
       "   1.0547770261764526,\n",
       "   1.0457662343978882,\n",
       "   0.9782618880271912,\n",
       "   1.0143696069717407,\n",
       "   1.0360122919082642,\n",
       "   1.611741065979004,\n",
       "   1.4534326791763306,\n",
       "   0.9420437216758728,\n",
       "   0.9846511483192444,\n",
       "   1.1131137609481812,\n",
       "   0.9855779409408569,\n",
       "   1.0070304870605469,\n",
       "   1.024842619895935,\n",
       "   1.6043062210083008,\n",
       "   1.0766440629959106,\n",
       "   0.9661604166030884,\n",
       "   0.9393745064735413,\n",
       "   1.0313526391983032,\n",
       "   1.0599350929260254,\n",
       "   1.1799921989440918,\n",
       "   1.2637357711791992,\n",
       "   1.016697883605957,\n",
       "   1.0734764337539673,\n",
       "   0.9755783081054688],\n",
       "  ('cosine', True, False): [1.2150245904922485,\n",
       "   1.1336969137191772,\n",
       "   1.5343095064163208,\n",
       "   1.0583245754241943,\n",
       "   0.9936563968658447,\n",
       "   0.9414050579071045,\n",
       "   1.0451236963272095,\n",
       "   0.8578886389732361,\n",
       "   1.4476102590560913,\n",
       "   1.1149449348449707,\n",
       "   1.6034759283065796,\n",
       "   0.9358716607093811,\n",
       "   1.139454960823059,\n",
       "   1.0214300155639648,\n",
       "   0.885944128036499,\n",
       "   0.9923261404037476,\n",
       "   0.9753645658493042,\n",
       "   0.9090650677680969,\n",
       "   1.0563664436340332,\n",
       "   1.5580897331237793,\n",
       "   0.9757463932037354,\n",
       "   0.9741328358650208,\n",
       "   1.5150457620620728,\n",
       "   1.0318928956985474,\n",
       "   1.0126820802688599,\n",
       "   1.0674771070480347,\n",
       "   1.102059006690979,\n",
       "   1.0529311895370483,\n",
       "   0.9727941751480103,\n",
       "   0.8546579480171204,\n",
       "   1.162898302078247,\n",
       "   0.9822947382926941,\n",
       "   1.3693958520889282,\n",
       "   0.9452997446060181,\n",
       "   0.9408446550369263,\n",
       "   1.072219967842102,\n",
       "   1.0207699537277222,\n",
       "   1.0003767013549805,\n",
       "   1.0504305362701416,\n",
       "   0.9885724186897278,\n",
       "   1.0330877304077148,\n",
       "   0.9781445264816284,\n",
       "   1.6598228216171265,\n",
       "   0.9273029565811157,\n",
       "   0.9355007410049438,\n",
       "   0.9750528335571289,\n",
       "   0.9458585977554321,\n",
       "   1.0682626962661743,\n",
       "   0.8954203128814697,\n",
       "   0.9637314081192017,\n",
       "   1.5553375482559204,\n",
       "   0.9762982726097107,\n",
       "   0.9912686944007874,\n",
       "   1.1329883337020874,\n",
       "   1.4702306985855103,\n",
       "   0.9706807732582092,\n",
       "   0.9412257671356201,\n",
       "   0.9685907959938049,\n",
       "   1.2373621463775635,\n",
       "   1.0205317735671997,\n",
       "   1.039939045906067,\n",
       "   1.583490014076233,\n",
       "   1.0466958284378052,\n",
       "   0.9491657614707947,\n",
       "   1.1598124504089355,\n",
       "   0.9653775095939636,\n",
       "   1.013397216796875,\n",
       "   1.0001740455627441,\n",
       "   1.0144315958023071,\n",
       "   0.9614269733428955,\n",
       "   1.0096272230148315,\n",
       "   1.5040289163589478,\n",
       "   0.9794304370880127,\n",
       "   0.9683710932731628,\n",
       "   0.9751569628715515,\n",
       "   1.1143046617507935,\n",
       "   1.0980228185653687,\n",
       "   0.9748780131340027,\n",
       "   0.9991793632507324,\n",
       "   1.4602470397949219,\n",
       "   0.9428137540817261,\n",
       "   1.0003890991210938,\n",
       "   0.8791607022285461,\n",
       "   1.055397868156433,\n",
       "   0.8799489736557007,\n",
       "   1.0070959329605103,\n",
       "   1.0116945505142212,\n",
       "   1.0648696422576904,\n",
       "   1.6031770706176758,\n",
       "   1.2458158731460571,\n",
       "   1.2074240446090698,\n",
       "   1.292199730873108,\n",
       "   0.9188202619552612,\n",
       "   0.9611597061157227,\n",
       "   1.0165464878082275,\n",
       "   1.2453067302703857,\n",
       "   1.57125723361969,\n",
       "   0.882882833480835,\n",
       "   0.9692849516868591,\n",
       "   0.8257103562355042,\n",
       "   1.0450997352600098,\n",
       "   0.9405036568641663,\n",
       "   0.9318966865539551,\n",
       "   0.9622092843055725,\n",
       "   1.2142441272735596,\n",
       "   1.1009281873703003,\n",
       "   0.912277102470398,\n",
       "   0.9398545622825623,\n",
       "   1.0387240648269653,\n",
       "   1.0392801761627197,\n",
       "   1.0250089168548584,\n",
       "   1.057513952255249,\n",
       "   1.2308588027954102,\n",
       "   1.0631905794143677,\n",
       "   0.867476761341095,\n",
       "   1.040639877319336,\n",
       "   0.9805343151092529,\n",
       "   1.236027479171753,\n",
       "   1.018398642539978,\n",
       "   1.6103730201721191,\n",
       "   1.2999238967895508,\n",
       "   1.1808089017868042,\n",
       "   1.1963704824447632,\n",
       "   1.0594425201416016,\n",
       "   0.9858234524726868,\n",
       "   1.0599628686904907,\n",
       "   0.9855924844741821,\n",
       "   1.4967141151428223,\n",
       "   0.9438368678092957,\n",
       "   0.9512228965759277,\n",
       "   0.9969630241394043,\n",
       "   1.2518230676651,\n",
       "   1.2273900508880615,\n",
       "   1.0497885942459106,\n",
       "   1.056672215461731,\n",
       "   1.2454341650009155,\n",
       "   0.9750089645385742,\n",
       "   0.9137797951698303,\n",
       "   0.9469821453094482,\n",
       "   0.8840749859809875,\n",
       "   0.9405922889709473,\n",
       "   0.9709494113922119,\n",
       "   1.1414353847503662,\n",
       "   1.472969889640808,\n",
       "   0.9942684769630432,\n",
       "   0.9109699726104736,\n",
       "   1.0105303525924683,\n",
       "   0.9603007435798645,\n",
       "   1.01651132106781,\n",
       "   1.239812970161438,\n",
       "   0.927319347858429,\n",
       "   0.8868100643157959,\n",
       "   0.9886685013771057,\n",
       "   1.009616494178772,\n",
       "   1.559388279914856,\n",
       "   1.0074552297592163,\n",
       "   0.9992036819458008,\n",
       "   1.0259746313095093,\n",
       "   1.0747287273406982,\n",
       "   1.0227481126785278,\n",
       "   0.9935339689254761,\n",
       "   0.9923563599586487,\n",
       "   1.132615327835083,\n",
       "   0.9249799251556396,\n",
       "   0.9954441785812378,\n",
       "   1.1163312196731567,\n",
       "   0.9773638248443604,\n",
       "   1.1286001205444336,\n",
       "   0.8881195187568665,\n",
       "   1.1576993465423584,\n",
       "   0.863594651222229,\n",
       "   1.5957471132278442,\n",
       "   1.133286476135254,\n",
       "   1.0235244035720825,\n",
       "   0.9990200996398926,\n",
       "   1.5921870470046997,\n",
       "   0.9357045292854309,\n",
       "   0.9728037118911743,\n",
       "   0.9261583685874939,\n",
       "   0.9560503363609314,\n",
       "   1.0541328191757202,\n",
       "   1.4279768466949463,\n",
       "   1.1728451251983643,\n",
       "   1.036295771598816,\n",
       "   0.9446380734443665,\n",
       "   1.0097334384918213,\n",
       "   0.9435684084892273,\n",
       "   1.1561670303344727,\n",
       "   1.234619379043579,\n",
       "   1.743850827217102,\n",
       "   1.17740797996521,\n",
       "   0.9417786598205566,\n",
       "   1.9138963222503662,\n",
       "   0.9310701489448547,\n",
       "   0.9796786308288574,\n",
       "   0.8803285956382751,\n",
       "   1.1244474649429321,\n",
       "   1.010167121887207,\n",
       "   0.9596574306488037,\n",
       "   0.9380812048912048,\n",
       "   0.7864644527435303,\n",
       "   0.9345992207527161,\n",
       "   0.9327945709228516,\n",
       "   1.0072869062423706,\n",
       "   1.050410270690918,\n",
       "   0.9202278256416321,\n",
       "   0.9584290385246277,\n",
       "   1.005141258239746,\n",
       "   0.9404318332672119,\n",
       "   1.5773133039474487,\n",
       "   1.2207492589950562,\n",
       "   1.0511151552200317,\n",
       "   0.8790715336799622,\n",
       "   1.0449265241622925,\n",
       "   0.9054344892501831,\n",
       "   0.9818993210792542,\n",
       "   0.9859001040458679,\n",
       "   0.9250399470329285,\n",
       "   1.330804705619812,\n",
       "   1.0305241346359253,\n",
       "   1.1864726543426514,\n",
       "   1.052384376525879,\n",
       "   1.5945661067962646,\n",
       "   0.9548974633216858,\n",
       "   1.136231541633606,\n",
       "   1.6697793006896973,\n",
       "   1.6123260259628296,\n",
       "   1.2171180248260498,\n",
       "   1.1028698682785034,\n",
       "   1.0528124570846558,\n",
       "   0.9321726560592651,\n",
       "   0.9755712747573853,\n",
       "   1.7892082929611206,\n",
       "   1.2250038385391235,\n",
       "   1.3228821754455566,\n",
       "   1.0832040309906006,\n",
       "   0.8824364542961121,\n",
       "   1.7573201656341553,\n",
       "   0.9941307306289673,\n",
       "   1.0883840322494507,\n",
       "   0.9388975501060486,\n",
       "   1.0291547775268555,\n",
       "   0.9546414017677307,\n",
       "   0.9869635701179504,\n",
       "   1.5345964431762695,\n",
       "   0.8755900859832764,\n",
       "   1.0480669736862183,\n",
       "   1.0556132793426514,\n",
       "   0.9628750681877136,\n",
       "   0.9304927587509155,\n",
       "   0.9578447341918945,\n",
       "   0.9153656959533691,\n",
       "   1.3436269760131836,\n",
       "   1.065536379814148,\n",
       "   1.5623195171356201,\n",
       "   0.9879124164581299,\n",
       "   0.9901984333992004,\n",
       "   1.0367703437805176,\n",
       "   0.9806727170944214,\n",
       "   1.3497211933135986,\n",
       "   0.998813807964325,\n",
       "   1.2634860277175903,\n",
       "   1.1228687763214111,\n",
       "   0.995901882648468,\n",
       "   0.9962614178657532,\n",
       "   1.0180566310882568,\n",
       "   1.5429127216339111,\n",
       "   1.2150954008102417,\n",
       "   0.9023897051811218,\n",
       "   1.0374699831008911,\n",
       "   0.9571825265884399,\n",
       "   1.1381232738494873,\n",
       "   1.150274634361267,\n",
       "   0.8514634370803833,\n",
       "   1.0229438543319702,\n",
       "   0.91840660572052,\n",
       "   1.0045971870422363,\n",
       "   0.9303116798400879,\n",
       "   1.0412453413009644,\n",
       "   1.2350594997406006,\n",
       "   0.9311808943748474,\n",
       "   1.1450096368789673,\n",
       "   0.9183052778244019,\n",
       "   1.5074180364608765,\n",
       "   1.0892143249511719,\n",
       "   0.9810537695884705,\n",
       "   0.935600221157074,\n",
       "   1.058595895767212,\n",
       "   1.3011555671691895,\n",
       "   1.566564917564392,\n",
       "   1.1490939855575562,\n",
       "   1.5394643545150757,\n",
       "   1.052531123161316,\n",
       "   0.9103675484657288,\n",
       "   0.8741306662559509,\n",
       "   0.976780891418457,\n",
       "   1.0772464275360107,\n",
       "   0.972142219543457,\n",
       "   0.9910616874694824,\n",
       "   1.3025506734848022,\n",
       "   0.9225377440452576,\n",
       "   1.7927926778793335,\n",
       "   1.3206658363342285,\n",
       "   0.8161088824272156,\n",
       "   0.8860970735549927,\n",
       "   1.0319420099258423,\n",
       "   0.9110040068626404,\n",
       "   1.5848994255065918,\n",
       "   0.8783963918685913,\n",
       "   0.9871677756309509,\n",
       "   1.019795536994934,\n",
       "   0.9638464450836182,\n",
       "   0.9335503578186035,\n",
       "   0.9803004264831543,\n",
       "   1.1067336797714233,\n",
       "   1.025712251663208,\n",
       "   1.3200312852859497,\n",
       "   0.9592037796974182,\n",
       "   0.9743770360946655,\n",
       "   0.9621870517730713,\n",
       "   0.812055766582489,\n",
       "   0.9841029047966003,\n",
       "   1.048746943473816,\n",
       "   1.0915502309799194,\n",
       "   0.8845862746238708,\n",
       "   0.9653915166854858,\n",
       "   1.4363633394241333,\n",
       "   0.9547156095504761,\n",
       "   0.926796019077301,\n",
       "   0.9681702256202698,\n",
       "   1.0213507413864136,\n",
       "   1.0580554008483887,\n",
       "   0.9624446034431458,\n",
       "   0.9567349553108215,\n",
       "   1.0330476760864258,\n",
       "   1.0092182159423828,\n",
       "   1.1555176973342896,\n",
       "   0.9486506581306458,\n",
       "   1.0642584562301636,\n",
       "   0.9965342879295349,\n",
       "   0.9339828491210938,\n",
       "   0.9755302667617798,\n",
       "   0.9056553840637207,\n",
       "   0.990668773651123,\n",
       "   1.0443661212921143,\n",
       "   1.2527408599853516,\n",
       "   1.0108944177627563,\n",
       "   0.9673596024513245,\n",
       "   0.8705576062202454,\n",
       "   1.0108740329742432,\n",
       "   0.9097246527671814,\n",
       "   1.046629548072815,\n",
       "   0.9478378891944885,\n",
       "   0.8759847283363342,\n",
       "   1.2439470291137695,\n",
       "   0.9464340806007385,\n",
       "   0.9914770126342773,\n",
       "   1.0982236862182617,\n",
       "   1.5484577417373657,\n",
       "   0.9456055164337158,\n",
       "   0.9857661128044128,\n",
       "   1.0095216035842896,\n",
       "   1.0895791053771973,\n",
       "   1.071781873703003,\n",
       "   0.9947216510772705,\n",
       "   0.846188485622406,\n",
       "   1.0301378965377808,\n",
       "   1.0598301887512207,\n",
       "   1.0063754320144653,\n",
       "   0.9970608949661255,\n",
       "   1.0983250141143799,\n",
       "   1.0333443880081177,\n",
       "   0.9893378615379333,\n",
       "   0.9722311496734619,\n",
       "   1.0542247295379639,\n",
       "   0.8335110545158386,\n",
       "   0.9656711220741272,\n",
       "   0.9990636110305786,\n",
       "   1.2019495964050293,\n",
       "   1.0094388723373413,\n",
       "   1.0286730527877808,\n",
       "   1.085434913635254,\n",
       "   1.6055766344070435,\n",
       "   0.932246208190918,\n",
       "   1.016826868057251,\n",
       "   0.9286520481109619,\n",
       "   0.999162495136261,\n",
       "   0.9976546168327332,\n",
       "   0.9225770235061646,\n",
       "   0.9452182054519653,\n",
       "   1.0236274003982544,\n",
       "   1.556772232055664,\n",
       "   0.9764604568481445,\n",
       "   1.0400190353393555,\n",
       "   1.060779333114624,\n",
       "   1.0584146976470947,\n",
       "   1.0519664287567139,\n",
       "   1.4503858089447021,\n",
       "   1.0171247720718384,\n",
       "   1.0065613985061646,\n",
       "   1.1149742603302002,\n",
       "   1.1531317234039307,\n",
       "   1.1182302236557007,\n",
       "   1.2560657262802124,\n",
       "   0.9068588018417358,\n",
       "   1.2614198923110962,\n",
       "   0.9087784290313721,\n",
       "   1.0163742303848267,\n",
       "   1.3942201137542725,\n",
       "   1.0418281555175781,\n",
       "   1.4543009996414185,\n",
       "   1.0377140045166016,\n",
       "   0.9786818623542786,\n",
       "   1.0600831508636475,\n",
       "   0.9424213171005249,\n",
       "   1.2183268070220947,\n",
       "   1.0490936040878296,\n",
       "   1.1554986238479614,\n",
       "   1.1443431377410889,\n",
       "   0.9203817248344421,\n",
       "   1.0069916248321533,\n",
       "   1.1958414316177368,\n",
       "   1.2646229267120361,\n",
       "   1.475332260131836,\n",
       "   0.9587578773498535,\n",
       "   0.96242755651474,\n",
       "   1.250985026359558,\n",
       "   0.8115347623825073,\n",
       "   0.985878586769104,\n",
       "   0.9987738728523254,\n",
       "   1.015821933746338,\n",
       "   1.0089693069458008,\n",
       "   0.8866391777992249,\n",
       "   1.2965673208236694,\n",
       "   0.9517568945884705,\n",
       "   0.9756173491477966,\n",
       "   0.9348729252815247,\n",
       "   1.4478788375854492,\n",
       "   0.9660671949386597,\n",
       "   1.0409328937530518,\n",
       "   0.991023600101471,\n",
       "   1.0807843208312988,\n",
       "   1.1485763788223267,\n",
       "   0.9342408180236816,\n",
       "   0.9614375829696655,\n",
       "   1.1082743406295776,\n",
       "   1.0388697385787964,\n",
       "   1.164562702178955,\n",
       "   0.9833579659461975,\n",
       "   0.9534536600112915,\n",
       "   0.9228572249412537,\n",
       "   1.0449516773223877,\n",
       "   1.19364333152771,\n",
       "   1.7029685974121094,\n",
       "   1.207898736000061,\n",
       "   0.992737352848053,\n",
       "   0.9530617594718933,\n",
       "   0.9447084665298462,\n",
       "   0.9415937662124634,\n",
       "   1.0200495719909668,\n",
       "   0.9190655946731567,\n",
       "   0.9767082929611206,\n",
       "   1.2663949728012085,\n",
       "   1.0740246772766113,\n",
       "   1.1123369932174683,\n",
       "   1.0273290872573853,\n",
       "   1.043105125427246,\n",
       "   1.0533891916275024,\n",
       "   0.987925112247467,\n",
       "   0.9635908603668213,\n",
       "   0.9325057864189148,\n",
       "   1.0516248941421509,\n",
       "   1.0180120468139648,\n",
       "   1.0972946882247925,\n",
       "   0.9214789271354675,\n",
       "   0.9631465077400208,\n",
       "   1.0425937175750732,\n",
       "   0.9671041965484619,\n",
       "   0.8754407167434692,\n",
       "   0.997285783290863,\n",
       "   0.9973737001419067,\n",
       "   1.378767728805542,\n",
       "   0.889884352684021,\n",
       "   0.9855936765670776,\n",
       "   1.4226601123809814,\n",
       "   1.1078542470932007,\n",
       "   1.090381145477295,\n",
       "   0.8983678221702576,\n",
       "   1.0349665880203247,\n",
       "   0.9258896708488464,\n",
       "   1.0268547534942627,\n",
       "   1.011172890663147,\n",
       "   1.0456733703613281,\n",
       "   1.611741304397583,\n",
       "   1.4534326791763306,\n",
       "   0.8808153867721558,\n",
       "   0.9911544322967529,\n",
       "   1.4783480167388916,\n",
       "   0.9855779409408569,\n",
       "   1.0070303678512573,\n",
       "   0.9678162932395935,\n",
       "   1.0528427362442017,\n",
       "   1.0796384811401367,\n",
       "   0.9332358837127686,\n",
       "   0.9314178824424744,\n",
       "   1.0276981592178345,\n",
       "   1.059935212135315,\n",
       "   1.4190382957458496,\n",
       "   1.2637360095977783,\n",
       "   0.9957271814346313,\n",
       "   1.0534987449645996,\n",
       "   0.9128823280334473],\n",
       "  ('cosine', False, True): [0.9272906184196472,\n",
       "   1.1336966753005981,\n",
       "   1.5343093872070312,\n",
       "   0.9825558066368103,\n",
       "   1.0144175291061401,\n",
       "   0.9899805188179016,\n",
       "   1.1500693559646606,\n",
       "   0.8440627455711365,\n",
       "   1.4476102590560913,\n",
       "   1.1489343643188477,\n",
       "   1.6034762859344482,\n",
       "   0.9654757380485535,\n",
       "   1.1208692789077759,\n",
       "   1.0214300155639648,\n",
       "   0.9275937676429749,\n",
       "   0.9923260807991028,\n",
       "   0.9753648042678833,\n",
       "   0.9104534983634949,\n",
       "   1.0486122369766235,\n",
       "   1.5580899715423584,\n",
       "   1.01046884059906,\n",
       "   1.0805598497390747,\n",
       "   1.5150456428527832,\n",
       "   1.034777283668518,\n",
       "   0.9966756701469421,\n",
       "   1.0395954847335815,\n",
       "   0.9969751238822937,\n",
       "   1.0277613401412964,\n",
       "   0.9769925475120544,\n",
       "   0.9579588770866394,\n",
       "   1.1628984212875366,\n",
       "   0.9822947382926941,\n",
       "   1.3693958520889282,\n",
       "   1.0466350317001343,\n",
       "   0.988071858882904,\n",
       "   1.072219967842102,\n",
       "   1.0207699537277222,\n",
       "   1.0245109796524048,\n",
       "   1.0504311323165894,\n",
       "   0.9885728359222412,\n",
       "   1.033087968826294,\n",
       "   0.9781443476676941,\n",
       "   1.6598230600357056,\n",
       "   0.9273030161857605,\n",
       "   0.9501909017562866,\n",
       "   0.9750528931617737,\n",
       "   0.9605211019515991,\n",
       "   1.0682625770568848,\n",
       "   0.9411172270774841,\n",
       "   0.9637314677238464,\n",
       "   1.55533766746521,\n",
       "   1.0125383138656616,\n",
       "   0.9912686944007874,\n",
       "   1.1329882144927979,\n",
       "   1.4702305793762207,\n",
       "   0.9706808924674988,\n",
       "   0.9412258863449097,\n",
       "   1.011249303817749,\n",
       "   0.979202151298523,\n",
       "   1.3024252653121948,\n",
       "   1.0399391651153564,\n",
       "   1.0715522766113281,\n",
       "   0.9517103433609009,\n",
       "   1.6771807670593262,\n",
       "   1.1598128080368042,\n",
       "   0.9623908996582031,\n",
       "   1.0146719217300415,\n",
       "   0.995963454246521,\n",
       "   1.0144317150115967,\n",
       "   0.9665133953094482,\n",
       "   1.0096272230148315,\n",
       "   0.9850105047225952,\n",
       "   1.0215731859207153,\n",
       "   0.9708229303359985,\n",
       "   0.9947398900985718,\n",
       "   1.1143049001693726,\n",
       "   1.0250115394592285,\n",
       "   0.9823383688926697,\n",
       "   0.9991792440414429,\n",
       "   1.460247278213501,\n",
       "   0.9428138136863708,\n",
       "   0.9986558556556702,\n",
       "   0.8791607618331909,\n",
       "   1.0664318799972534,\n",
       "   0.9639495015144348,\n",
       "   1.0070960521697998,\n",
       "   0.9506239295005798,\n",
       "   1.0648696422576904,\n",
       "   1.6031768321990967,\n",
       "   1.2287769317626953,\n",
       "   0.9582237601280212,\n",
       "   1.2922000885009766,\n",
       "   1.2701654434204102,\n",
       "   0.9820690155029297,\n",
       "   1.276416301727295,\n",
       "   1.2453068494796753,\n",
       "   0.9554092288017273,\n",
       "   0.8828830122947693,\n",
       "   1.0958831310272217,\n",
       "   1.1036409139633179,\n",
       "   1.512442708015442,\n",
       "   1.1054432392120361,\n",
       "   0.9318968057632446,\n",
       "   0.9827916026115417,\n",
       "   1.2142438888549805,\n",
       "   1.100928544998169,\n",
       "   0.9211232662200928,\n",
       "   0.9846242070198059,\n",
       "   1.0387238264083862,\n",
       "   0.9871371984481812,\n",
       "   1.1492615938186646,\n",
       "   1.0575140714645386,\n",
       "   1.2308590412139893,\n",
       "   1.0631906986236572,\n",
       "   1.2373911142349243,\n",
       "   1.040639877319336,\n",
       "   0.9805344939231873,\n",
       "   1.236027717590332,\n",
       "   1.0409088134765625,\n",
       "   1.6103731393814087,\n",
       "   1.2999238967895508,\n",
       "   1.109576940536499,\n",
       "   1.138546109199524,\n",
       "   1.0214488506317139,\n",
       "   0.9205691814422607,\n",
       "   1.0599628686904907,\n",
       "   1.0167979001998901,\n",
       "   1.4967142343521118,\n",
       "   0.9438368678092957,\n",
       "   1.0083376169204712,\n",
       "   1.1984044313430786,\n",
       "   1.2518233060836792,\n",
       "   1.2273900508880615,\n",
       "   1.0497887134552002,\n",
       "   1.0566720962524414,\n",
       "   1.245434284210205,\n",
       "   0.969444751739502,\n",
       "   0.9228505492210388,\n",
       "   0.9203601479530334,\n",
       "   0.8953494429588318,\n",
       "   0.9614201188087463,\n",
       "   0.9931105375289917,\n",
       "   1.0582212209701538,\n",
       "   1.4729700088500977,\n",
       "   0.9854463934898376,\n",
       "   0.9109698534011841,\n",
       "   0.9184667468070984,\n",
       "   0.9640173316001892,\n",
       "   1.016511082649231,\n",
       "   1.2398128509521484,\n",
       "   0.9221155643463135,\n",
       "   0.9787712693214417,\n",
       "   0.9886685609817505,\n",
       "   1.0399539470672607,\n",
       "   1.5593878030776978,\n",
       "   1.0074554681777954,\n",
       "   1.1201878786087036,\n",
       "   1.0388520956039429,\n",
       "   1.0747284889221191,\n",
       "   1.0227481126785278,\n",
       "   0.9749135375022888,\n",
       "   0.9923563599586487,\n",
       "   1.728203535079956,\n",
       "   0.9685898423194885,\n",
       "   1.184893012046814,\n",
       "   1.1112849712371826,\n",
       "   0.9862316250801086,\n",
       "   1.4472624063491821,\n",
       "   0.9124953746795654,\n",
       "   1.1576993465423584,\n",
       "   0.8605778813362122,\n",
       "   1.5957473516464233,\n",
       "   1.2388204336166382,\n",
       "   0.9995375275611877,\n",
       "   1.0203125476837158,\n",
       "   1.5921874046325684,\n",
       "   0.9357045888900757,\n",
       "   0.9811723232269287,\n",
       "   0.9573860168457031,\n",
       "   1.0058842897415161,\n",
       "   0.9861718416213989,\n",
       "   1.4279770851135254,\n",
       "   1.1728451251983643,\n",
       "   1.0308961868286133,\n",
       "   0.936174750328064,\n",
       "   1.1996090412139893,\n",
       "   0.9551280736923218,\n",
       "   1.0392428636550903,\n",
       "   1.2346196174621582,\n",
       "   1.167410969734192,\n",
       "   1.1774077415466309,\n",
       "   1.0739030838012695,\n",
       "   1.2115116119384766,\n",
       "   0.9727989435195923,\n",
       "   1.0242940187454224,\n",
       "   0.8695574998855591,\n",
       "   1.1140457391738892,\n",
       "   1.0019234418869019,\n",
       "   1.5994945764541626,\n",
       "   1.009136438369751,\n",
       "   0.8911683559417725,\n",
       "   0.938025176525116,\n",
       "   1.0385215282440186,\n",
       "   1.0028446912765503,\n",
       "   1.050410270690918,\n",
       "   0.9336424469947815,\n",
       "   1.0357362031936646,\n",
       "   1.005141258239746,\n",
       "   0.9838945865631104,\n",
       "   1.5773131847381592,\n",
       "   1.2207494974136353,\n",
       "   1.0511155128479004,\n",
       "   0.8865910172462463,\n",
       "   1.044926404953003,\n",
       "   1.442808747291565,\n",
       "   0.9818993210792542,\n",
       "   0.9753417372703552,\n",
       "   0.934237539768219,\n",
       "   1.3308049440383911,\n",
       "   1.0321601629257202,\n",
       "   1.1864725351333618,\n",
       "   0.9892579913139343,\n",
       "   1.5945662260055542,\n",
       "   0.9548973441123962,\n",
       "   1.1798654794692993,\n",
       "   1.6697791814804077,\n",
       "   1.61232590675354,\n",
       "   0.994255006313324,\n",
       "   1.1028701066970825,\n",
       "   1.0619232654571533,\n",
       "   0.9339163303375244,\n",
       "   0.9755711555480957,\n",
       "   1.1528297662734985,\n",
       "   1.000130534172058,\n",
       "   0.9806551933288574,\n",
       "   1.069137692451477,\n",
       "   0.8871688842773438,\n",
       "   1.7573201656341553,\n",
       "   0.9941308498382568,\n",
       "   1.1638712882995605,\n",
       "   0.9518193006515503,\n",
       "   1.037855625152588,\n",
       "   0.9643918871879578,\n",
       "   0.9643033146858215,\n",
       "   1.0586364269256592,\n",
       "   0.9003198742866516,\n",
       "   1.0143122673034668,\n",
       "   0.9775477647781372,\n",
       "   1.3686872720718384,\n",
       "   0.8303424715995789,\n",
       "   0.9578446745872498,\n",
       "   0.9269364476203918,\n",
       "   1.3436270952224731,\n",
       "   1.065536618232727,\n",
       "   1.0260063409805298,\n",
       "   1.1011019945144653,\n",
       "   0.9901983141899109,\n",
       "   1.0367701053619385,\n",
       "   0.9806727766990662,\n",
       "   1.3497214317321777,\n",
       "   0.9988135695457458,\n",
       "   1.2634859085083008,\n",
       "   1.0291470289230347,\n",
       "   0.9826815724372864,\n",
       "   0.9962615370750427,\n",
       "   1.018056869506836,\n",
       "   0.9605679512023926,\n",
       "   1.2150957584381104,\n",
       "   0.9791964888572693,\n",
       "   1.082526445388794,\n",
       "   0.9561400413513184,\n",
       "   1.1062294244766235,\n",
       "   1.157894253730774,\n",
       "   0.8724993467330933,\n",
       "   1.0229437351226807,\n",
       "   1.1828707456588745,\n",
       "   1.0055932998657227,\n",
       "   1.2474957704544067,\n",
       "   1.0412452220916748,\n",
       "   1.2350598573684692,\n",
       "   0.96907639503479,\n",
       "   1.1450093984603882,\n",
       "   1.0294091701507568,\n",
       "   1.507418155670166,\n",
       "   1.0892143249511719,\n",
       "   1.0238887071609497,\n",
       "   0.9921563863754272,\n",
       "   1.0585957765579224,\n",
       "   1.3011558055877686,\n",
       "   1.5665652751922607,\n",
       "   1.142814040184021,\n",
       "   1.0099642276763916,\n",
       "   1.0525312423706055,\n",
       "   0.9103673100471497,\n",
       "   0.8803403973579407,\n",
       "   0.9767811298370361,\n",
       "   0.9743582010269165,\n",
       "   0.9829844236373901,\n",
       "   0.9910614490509033,\n",
       "   1.3025507926940918,\n",
       "   0.9269018173217773,\n",
       "   1.7927929162979126,\n",
       "   1.3206660747528076,\n",
       "   0.8245614171028137,\n",
       "   0.8860970139503479,\n",
       "   1.3358041048049927,\n",
       "   0.9110041260719299,\n",
       "   1.0346976518630981,\n",
       "   0.8766472935676575,\n",
       "   0.9871674180030823,\n",
       "   1.0197960138320923,\n",
       "   1.0322420597076416,\n",
       "   1.0325490236282349,\n",
       "   0.9848859310150146,\n",
       "   1.0625847578048706,\n",
       "   1.0031019449234009,\n",
       "   1.1027246713638306,\n",
       "   0.9555186033248901,\n",
       "   0.9743767380714417,\n",
       "   1.0005450248718262,\n",
       "   0.8198235034942627,\n",
       "   1.075571060180664,\n",
       "   1.0487467050552368,\n",
       "   1.2272570133209229,\n",
       "   0.885993480682373,\n",
       "   0.9797822833061218,\n",
       "   1.0271849632263184,\n",
       "   0.9545608162879944,\n",
       "   0.9269817471504211,\n",
       "   0.9373741745948792,\n",
       "   0.9904908537864685,\n",
       "   1.0580555200576782,\n",
       "   0.9673399329185486,\n",
       "   0.9613816738128662,\n",
       "   0.9639677405357361,\n",
       "   1.0092183351516724,\n",
       "   1.155517816543579,\n",
       "   0.9672701954841614,\n",
       "   1.0931528806686401,\n",
       "   1.0126471519470215,\n",
       "   0.967171311378479,\n",
       "   0.9755302667617798,\n",
       "   0.9618225693702698,\n",
       "   1.016692876815796,\n",
       "   0.9505002498626709,\n",
       "   1.2906876802444458,\n",
       "   0.9966592192649841,\n",
       "   0.979399561882019,\n",
       "   0.8929053544998169,\n",
       "   1.036231517791748,\n",
       "   0.9097246527671814,\n",
       "   1.046629548072815,\n",
       "   0.9635317325592041,\n",
       "   0.885594367980957,\n",
       "   1.243947148323059,\n",
       "   0.9827136397361755,\n",
       "   1.047784686088562,\n",
       "   1.1229345798492432,\n",
       "   1.5484575033187866,\n",
       "   1.0342246294021606,\n",
       "   0.985765814781189,\n",
       "   1.0168874263763428,\n",
       "   0.9959185719490051,\n",
       "   0.9976409077644348,\n",
       "   1.0001487731933594,\n",
       "   0.9319332242012024,\n",
       "   1.3402719497680664,\n",
       "   1.0598301887512207,\n",
       "   1.0432873964309692,\n",
       "   0.9885876178741455,\n",
       "   1.0983251333236694,\n",
       "   1.0427098274230957,\n",
       "   0.9993518590927124,\n",
       "   0.9722310900688171,\n",
       "   1.055289387702942,\n",
       "   0.8388887643814087,\n",
       "   0.9668741226196289,\n",
       "   1.0113027095794678,\n",
       "   1.2019497156143188,\n",
       "   1.0094388723373413,\n",
       "   1.0286731719970703,\n",
       "   1.0854347944259644,\n",
       "   1.6055763959884644,\n",
       "   1.1115256547927856,\n",
       "   1.016826868057251,\n",
       "   0.9295861721038818,\n",
       "   0.9991625547409058,\n",
       "   1.066007137298584,\n",
       "   0.922576904296875,\n",
       "   0.9556339979171753,\n",
       "   1.606101155281067,\n",
       "   1.0061537027359009,\n",
       "   1.0010204315185547,\n",
       "   1.0959964990615845,\n",
       "   1.060779333114624,\n",
       "   1.0584149360656738,\n",
       "   1.0551443099975586,\n",
       "   0.9929118752479553,\n",
       "   1.2040363550186157,\n",
       "   1.0157699584960938,\n",
       "   1.1149742603302002,\n",
       "   1.1661759614944458,\n",
       "   1.0321100950241089,\n",
       "   1.2560657262802124,\n",
       "   0.9695537090301514,\n",
       "   1.2614197731018066,\n",
       "   0.9087784886360168,\n",
       "   1.0274516344070435,\n",
       "   1.3942203521728516,\n",
       "   1.141737937927246,\n",
       "   0.8852874636650085,\n",
       "   1.0377140045166016,\n",
       "   0.9786818623542786,\n",
       "   0.9312942028045654,\n",
       "   1.0638238191604614,\n",
       "   1.2183274030685425,\n",
       "   1.0462886095046997,\n",
       "   1.1543571949005127,\n",
       "   0.9865873456001282,\n",
       "   1.0012867450714111,\n",
       "   1.0395578145980835,\n",
       "   1.2366927862167358,\n",
       "   1.2646232843399048,\n",
       "   0.9998033046722412,\n",
       "   1.0158408880233765,\n",
       "   0.96242755651474,\n",
       "   1.063576102256775,\n",
       "   0.815741240978241,\n",
       "   1.0109796524047852,\n",
       "   1.009676456451416,\n",
       "   1.015822172164917,\n",
       "   0.9917389750480652,\n",
       "   1.5902096033096313,\n",
       "   1.2965673208236694,\n",
       "   0.9259105920791626,\n",
       "   1.0036982297897339,\n",
       "   0.999650239944458,\n",
       "   1.144153118133545,\n",
       "   0.9660672545433044,\n",
       "   1.0409328937530518,\n",
       "   0.9870319366455078,\n",
       "   1.0464880466461182,\n",
       "   1.1485764980316162,\n",
       "   0.9420562982559204,\n",
       "   0.9860736727714539,\n",
       "   1.1082743406295776,\n",
       "   1.0388699769973755,\n",
       "   1.1710351705551147,\n",
       "   0.9818575978279114,\n",
       "   0.9680832624435425,\n",
       "   1.014398455619812,\n",
       "   1.0131241083145142,\n",
       "   1.19364333152771,\n",
       "   1.7029683589935303,\n",
       "   1.207898497581482,\n",
       "   0.992737352848053,\n",
       "   0.968363881111145,\n",
       "   0.9447084665298462,\n",
       "   1.0004147291183472,\n",
       "   1.0200492143630981,\n",
       "   0.9273264408111572,\n",
       "   0.9773097038269043,\n",
       "   1.2663953304290771,\n",
       "   1.0740247964859009,\n",
       "   1.1123368740081787,\n",
       "   1.0342357158660889,\n",
       "   1.4240925312042236,\n",
       "   1.0843908786773682,\n",
       "   1.2377058267593384,\n",
       "   1.0500998497009277,\n",
       "   0.8970340490341187,\n",
       "   1.0516247749328613,\n",
       "   1.0180120468139648,\n",
       "   1.050520896911621,\n",
       "   0.9214792251586914,\n",
       "   1.039892315864563,\n",
       "   1.0582245588302612,\n",
       "   0.975307285785675,\n",
       "   1.0627557039260864,\n",
       "   1.0025701522827148,\n",
       "   1.0198439359664917,\n",
       "   1.378767728805542,\n",
       "   0.8943224549293518,\n",
       "   1.0815726518630981,\n",
       "   1.1977038383483887,\n",
       "   1.1404789686203003,\n",
       "   1.1192538738250732,\n",
       "   0.9849802851676941,\n",
       "   1.054776906967163,\n",
       "   1.0457662343978882,\n",
       "   0.9782620072364807,\n",
       "   1.0143697261810303,\n",
       "   1.0360122919082642,\n",
       "   1.6117417812347412,\n",
       "   1.4534326791763306,\n",
       "   0.9420438408851624,\n",
       "   0.9846512079238892,\n",
       "   1.1131136417388916,\n",
       "   0.9855779409408569,\n",
       "   1.0070304870605469,\n",
       "   1.024842619895935,\n",
       "   1.6043063402175903,\n",
       "   1.0766443014144897,\n",
       "   0.9661602973937988,\n",
       "   0.939374566078186,\n",
       "   1.0313526391983032,\n",
       "   1.0599350929260254,\n",
       "   1.1799921989440918,\n",
       "   1.2637358903884888,\n",
       "   1.0166980028152466,\n",
       "   1.0734764337539673,\n",
       "   0.9755785465240479],\n",
       "  ('cosine', False, False): [1.2150248289108276,\n",
       "   1.1336966753005981,\n",
       "   1.5343092679977417,\n",
       "   1.0583245754241943,\n",
       "   0.9936563372612,\n",
       "   0.941405177116394,\n",
       "   1.0451236963272095,\n",
       "   0.8578886389732361,\n",
       "   1.4476103782653809,\n",
       "   1.1149450540542603,\n",
       "   1.6034762859344482,\n",
       "   0.9358716607093811,\n",
       "   1.1394548416137695,\n",
       "   1.0214301347732544,\n",
       "   0.8859440684318542,\n",
       "   0.9923259615898132,\n",
       "   0.9753643870353699,\n",
       "   0.9090650677680969,\n",
       "   1.0563664436340332,\n",
       "   1.5580897331237793,\n",
       "   0.9757463932037354,\n",
       "   0.9741329550743103,\n",
       "   1.5150457620620728,\n",
       "   1.0318928956985474,\n",
       "   1.0126819610595703,\n",
       "   1.0674771070480347,\n",
       "   1.102059006690979,\n",
       "   1.0529311895370483,\n",
       "   0.9727943539619446,\n",
       "   0.8546581268310547,\n",
       "   1.1628984212875366,\n",
       "   0.9822946786880493,\n",
       "   1.3693959712982178,\n",
       "   0.9452998042106628,\n",
       "   0.940844714641571,\n",
       "   1.0722198486328125,\n",
       "   1.0207699537277222,\n",
       "   1.0003769397735596,\n",
       "   1.0504311323165894,\n",
       "   0.9885726571083069,\n",
       "   1.033087968826294,\n",
       "   0.9781443476676941,\n",
       "   1.6598230600357056,\n",
       "   0.927302896976471,\n",
       "   0.9355006814002991,\n",
       "   0.9750526547431946,\n",
       "   0.9458584189414978,\n",
       "   1.0682625770568848,\n",
       "   0.8954203128814697,\n",
       "   0.9637314081192017,\n",
       "   1.55533766746521,\n",
       "   0.9762983322143555,\n",
       "   0.9912686944007874,\n",
       "   1.1329882144927979,\n",
       "   1.4702306985855103,\n",
       "   0.9706807732582092,\n",
       "   0.9412258267402649,\n",
       "   0.9685909152030945,\n",
       "   1.237362265586853,\n",
       "   1.0205323696136475,\n",
       "   1.039939284324646,\n",
       "   1.5834906101226807,\n",
       "   1.0466958284378052,\n",
       "   0.9491657018661499,\n",
       "   1.1598128080368042,\n",
       "   0.9653775691986084,\n",
       "   1.0133973360061646,\n",
       "   1.0001740455627441,\n",
       "   1.0144317150115967,\n",
       "   0.9614269733428955,\n",
       "   1.0096272230148315,\n",
       "   1.5040289163589478,\n",
       "   0.9794304966926575,\n",
       "   0.9683712124824524,\n",
       "   0.9751570224761963,\n",
       "   1.1143049001693726,\n",
       "   1.0980228185653687,\n",
       "   0.9748780727386475,\n",
       "   0.9991792440414429,\n",
       "   1.460247278213501,\n",
       "   0.9428138136863708,\n",
       "   1.0003890991210938,\n",
       "   0.8791605830192566,\n",
       "   1.055397868156433,\n",
       "   0.879949152469635,\n",
       "   1.0070960521697998,\n",
       "   1.0116945505142212,\n",
       "   1.0648696422576904,\n",
       "   1.6031768321990967,\n",
       "   1.245815634727478,\n",
       "   1.2074239253997803,\n",
       "   1.2921996116638184,\n",
       "   0.9188203811645508,\n",
       "   0.9611597657203674,\n",
       "   1.0165467262268066,\n",
       "   1.2453067302703857,\n",
       "   1.5712568759918213,\n",
       "   0.8828830122947693,\n",
       "   0.9692849516868591,\n",
       "   0.8257105350494385,\n",
       "   1.0450999736785889,\n",
       "   0.9405034184455872,\n",
       "   0.9318968653678894,\n",
       "   0.962209165096283,\n",
       "   1.2142438888549805,\n",
       "   1.1009279489517212,\n",
       "   0.9122771620750427,\n",
       "   0.9398545622825623,\n",
       "   1.0387238264083862,\n",
       "   1.0392799377441406,\n",
       "   1.0250091552734375,\n",
       "   1.0575140714645386,\n",
       "   1.2308590412139893,\n",
       "   1.0631906986236572,\n",
       "   0.8674768209457397,\n",
       "   1.0406396389007568,\n",
       "   0.9805344343185425,\n",
       "   1.2360273599624634,\n",
       "   1.018398642539978,\n",
       "   1.6103732585906982,\n",
       "   1.2999237775802612,\n",
       "   1.1808091402053833,\n",
       "   1.1963704824447632,\n",
       "   1.0594425201416016,\n",
       "   0.9858231544494629,\n",
       "   1.0599628686904907,\n",
       "   0.9855924844741821,\n",
       "   1.4967141151428223,\n",
       "   0.9438368678092957,\n",
       "   0.9512228965759277,\n",
       "   0.9969632029533386,\n",
       "   1.2518231868743896,\n",
       "   1.227389931678772,\n",
       "   1.0497888326644897,\n",
       "   1.056672215461731,\n",
       "   1.245434284210205,\n",
       "   0.9750090837478638,\n",
       "   0.9137798547744751,\n",
       "   0.9469823837280273,\n",
       "   0.8840751051902771,\n",
       "   0.9405921697616577,\n",
       "   0.9709495306015015,\n",
       "   1.1414355039596558,\n",
       "   1.4729701280593872,\n",
       "   0.9942682385444641,\n",
       "   0.9109699130058289,\n",
       "   1.0105303525924683,\n",
       "   0.9603008031845093,\n",
       "   1.01651132106781,\n",
       "   1.2398128509521484,\n",
       "   0.9273191690444946,\n",
       "   0.8868101239204407,\n",
       "   0.9886684417724609,\n",
       "   1.009616494178772,\n",
       "   1.5593880414962769,\n",
       "   1.0074553489685059,\n",
       "   0.9992033839225769,\n",
       "   1.0259746313095093,\n",
       "   1.0747287273406982,\n",
       "   1.0227481126785278,\n",
       "   0.9935340285301208,\n",
       "   0.9923563003540039,\n",
       "   1.132615327835083,\n",
       "   0.9249797463417053,\n",
       "   0.9954441785812378,\n",
       "   1.1163312196731567,\n",
       "   0.9773637652397156,\n",
       "   1.1286002397537231,\n",
       "   0.8881194591522217,\n",
       "   1.157699465751648,\n",
       "   0.8635947108268738,\n",
       "   1.5957473516464233,\n",
       "   1.1332865953445435,\n",
       "   1.0235241651535034,\n",
       "   0.999019980430603,\n",
       "   1.5921874046325684,\n",
       "   0.9357045888900757,\n",
       "   0.9728035926818848,\n",
       "   0.9261583089828491,\n",
       "   0.9560502767562866,\n",
       "   1.0541330575942993,\n",
       "   1.4279769659042358,\n",
       "   1.1728451251983643,\n",
       "   1.036296010017395,\n",
       "   0.9446381330490112,\n",
       "   1.0097334384918213,\n",
       "   0.9435684084892273,\n",
       "   1.1561670303344727,\n",
       "   1.2346196174621582,\n",
       "   1.7438509464263916,\n",
       "   1.17740797996521,\n",
       "   0.9417786002159119,\n",
       "   1.9138962030410767,\n",
       "   0.9310700297355652,\n",
       "   0.9796786904335022,\n",
       "   0.8803286552429199,\n",
       "   1.1244474649429321,\n",
       "   1.010166883468628,\n",
       "   0.9596575498580933,\n",
       "   0.9380810260772705,\n",
       "   0.7864643931388855,\n",
       "   0.9345993995666504,\n",
       "   0.9327947497367859,\n",
       "   1.007286787033081,\n",
       "   1.050410270690918,\n",
       "   0.9202277064323425,\n",
       "   0.9584290385246277,\n",
       "   1.0051411390304565,\n",
       "   0.9404318928718567,\n",
       "   1.5773130655288696,\n",
       "   1.2207497358322144,\n",
       "   1.0511155128479004,\n",
       "   0.8790715932846069,\n",
       "   1.044926404953003,\n",
       "   0.9054345488548279,\n",
       "   0.9818993806838989,\n",
       "   0.9859001636505127,\n",
       "   0.925040066242218,\n",
       "   1.3308049440383911,\n",
       "   1.0305241346359253,\n",
       "   1.1864725351333618,\n",
       "   1.052384614944458,\n",
       "   1.5945662260055542,\n",
       "   0.9548972845077515,\n",
       "   1.136231780052185,\n",
       "   1.6697793006896973,\n",
       "   1.61232590675354,\n",
       "   1.2171181440353394,\n",
       "   1.1028698682785034,\n",
       "   1.0528125762939453,\n",
       "   0.9321727752685547,\n",
       "   0.9755711555480957,\n",
       "   1.7892082929611206,\n",
       "   1.225003957748413,\n",
       "   1.3228821754455566,\n",
       "   1.0832041501998901,\n",
       "   0.8824363350868225,\n",
       "   1.7573201656341553,\n",
       "   0.9941308498382568,\n",
       "   1.0883840322494507,\n",
       "   0.9388974905014038,\n",
       "   1.0291547775268555,\n",
       "   0.9546414017677307,\n",
       "   0.9869632720947266,\n",
       "   1.5345964431762695,\n",
       "   0.8755900859832764,\n",
       "   1.0480670928955078,\n",
       "   1.0556131601333618,\n",
       "   0.9628751277923584,\n",
       "   0.9304928779602051,\n",
       "   0.9578446745872498,\n",
       "   0.9153656959533691,\n",
       "   1.3436272144317627,\n",
       "   1.065536618232727,\n",
       "   1.5623196363449097,\n",
       "   0.9879124760627747,\n",
       "   0.9901983141899109,\n",
       "   1.036769986152649,\n",
       "   0.9806725978851318,\n",
       "   1.3497213125228882,\n",
       "   0.9988135099411011,\n",
       "   1.2634861469268799,\n",
       "   1.1228684186935425,\n",
       "   0.9959017038345337,\n",
       "   0.9962613582611084,\n",
       "   1.0180567502975464,\n",
       "   1.5429127216339111,\n",
       "   1.2150956392288208,\n",
       "   0.9023897647857666,\n",
       "   1.037469744682312,\n",
       "   0.9571825265884399,\n",
       "   1.1381235122680664,\n",
       "   1.1502748727798462,\n",
       "   0.8514635562896729,\n",
       "   1.0229438543319702,\n",
       "   0.9184065461158752,\n",
       "   1.0045973062515259,\n",
       "   0.9303117394447327,\n",
       "   1.0412453413009644,\n",
       "   1.2350598573684692,\n",
       "   0.9311808347702026,\n",
       "   1.1450093984603882,\n",
       "   0.9183052182197571,\n",
       "   1.507418155670166,\n",
       "   1.0892143249511719,\n",
       "   0.9810537695884705,\n",
       "   0.935600221157074,\n",
       "   1.058595895767212,\n",
       "   1.3011555671691895,\n",
       "   1.5665655136108398,\n",
       "   1.149093747138977,\n",
       "   1.5394642353057861,\n",
       "   1.0525310039520264,\n",
       "   0.9103675484657288,\n",
       "   0.8741307854652405,\n",
       "   0.9767811298370361,\n",
       "   1.0772463083267212,\n",
       "   0.9721422791481018,\n",
       "   0.9910615682601929,\n",
       "   1.3025507926940918,\n",
       "   0.9225376844406128,\n",
       "   1.7927929162979126,\n",
       "   1.3206658363342285,\n",
       "   0.8161088824272156,\n",
       "   0.8860970735549927,\n",
       "   1.0319420099258423,\n",
       "   0.9110041260719299,\n",
       "   1.5848994255065918,\n",
       "   0.8783964514732361,\n",
       "   0.9871675372123718,\n",
       "   1.019795536994934,\n",
       "   0.9638465046882629,\n",
       "   0.9335503578186035,\n",
       "   0.9803003668785095,\n",
       "   1.1067336797714233,\n",
       "   1.025712251663208,\n",
       "   1.3200312852859497,\n",
       "   0.9592039585113525,\n",
       "   0.9743769764900208,\n",
       "   0.9621871113777161,\n",
       "   0.8120558261871338,\n",
       "   0.984102725982666,\n",
       "   1.0487468242645264,\n",
       "   1.0915499925613403,\n",
       "   0.8845862746238708,\n",
       "   0.9653914570808411,\n",
       "   1.4363631010055542,\n",
       "   0.9547154307365417,\n",
       "   0.9267961382865906,\n",
       "   0.9681702256202698,\n",
       "   1.0213508605957031,\n",
       "   1.0580555200576782,\n",
       "   0.9624446034431458,\n",
       "   0.9567350745201111,\n",
       "   1.0330479145050049,\n",
       "   1.0092183351516724,\n",
       "   1.155517816543579,\n",
       "   0.9486506581306458,\n",
       "   1.0642584562301636,\n",
       "   0.9965340495109558,\n",
       "   0.9339828491210938,\n",
       "   0.975530207157135,\n",
       "   0.9056552648544312,\n",
       "   0.990668773651123,\n",
       "   1.0443661212921143,\n",
       "   1.2527408599853516,\n",
       "   1.010894775390625,\n",
       "   0.9673596620559692,\n",
       "   0.8705576062202454,\n",
       "   1.0108741521835327,\n",
       "   0.9097244739532471,\n",
       "   1.046629548072815,\n",
       "   0.9478378891944885,\n",
       "   0.8759846091270447,\n",
       "   1.2439472675323486,\n",
       "   0.9464341402053833,\n",
       "   0.9914771318435669,\n",
       "   1.0982236862182617,\n",
       "   1.5484576225280762,\n",
       "   0.945605456829071,\n",
       "   0.985765814781189,\n",
       "   1.009521484375,\n",
       "   1.0895789861679077,\n",
       "   1.071781873703003,\n",
       "   0.994721531867981,\n",
       "   0.8461886048316956,\n",
       "   1.0301380157470703,\n",
       "   1.0598304271697998,\n",
       "   1.0063753128051758,\n",
       "   0.9970608353614807,\n",
       "   1.0983250141143799,\n",
       "   1.0333442687988281,\n",
       "   0.9893378615379333,\n",
       "   0.9722309708595276,\n",
       "   1.054224967956543,\n",
       "   0.8335111737251282,\n",
       "   0.9656712412834167,\n",
       "   0.9990636110305786,\n",
       "   1.2019495964050293,\n",
       "   1.0094388723373413,\n",
       "   1.0286731719970703,\n",
       "   1.0854346752166748,\n",
       "   1.6055763959884644,\n",
       "   0.932246208190918,\n",
       "   1.0168267488479614,\n",
       "   0.9286520481109619,\n",
       "   0.9991623759269714,\n",
       "   0.9976547360420227,\n",
       "   0.9225767850875854,\n",
       "   0.945218026638031,\n",
       "   1.0236274003982544,\n",
       "   1.556772232055664,\n",
       "   0.9764605164527893,\n",
       "   1.0400192737579346,\n",
       "   1.060779094696045,\n",
       "   1.0584146976470947,\n",
       "   1.0519664287567139,\n",
       "   1.4503856897354126,\n",
       "   1.0171246528625488,\n",
       "   1.0065613985061646,\n",
       "   1.114974021911621,\n",
       "   1.1531317234039307,\n",
       "   1.1182301044464111,\n",
       "   1.2560657262802124,\n",
       "   0.9068585634231567,\n",
       "   1.2614197731018066,\n",
       "   0.9087784886360168,\n",
       "   1.0163739919662476,\n",
       "   1.3942203521728516,\n",
       "   1.0418280363082886,\n",
       "   1.4543009996414185,\n",
       "   1.0377140045166016,\n",
       "   0.9786819219589233,\n",
       "   1.060083031654358,\n",
       "   0.9424213171005249,\n",
       "   1.2183269262313843,\n",
       "   1.0490937232971191,\n",
       "   1.1554983854293823,\n",
       "   1.1443432569503784,\n",
       "   0.9203817844390869,\n",
       "   1.0069915056228638,\n",
       "   1.1958414316177368,\n",
       "   1.2646232843399048,\n",
       "   1.4753323793411255,\n",
       "   0.9587578773498535,\n",
       "   0.9624276161193848,\n",
       "   1.2509853839874268,\n",
       "   0.8115347623825073,\n",
       "   0.9858784675598145,\n",
       "   0.9987739324569702,\n",
       "   1.015822172164917,\n",
       "   1.0089694261550903,\n",
       "   0.8866391777992249,\n",
       "   1.2965673208236694,\n",
       "   0.9517568945884705,\n",
       "   0.9756172299385071,\n",
       "   0.9348730444908142,\n",
       "   1.4478791952133179,\n",
       "   0.9660671949386597,\n",
       "   1.0409328937530518,\n",
       "   0.9910237193107605,\n",
       "   1.0807843208312988,\n",
       "   1.148576259613037,\n",
       "   0.9342406392097473,\n",
       "   0.9614375829696655,\n",
       "   1.1082743406295776,\n",
       "   1.038869857788086,\n",
       "   1.164562702178955,\n",
       "   0.9833581447601318,\n",
       "   0.9534534811973572,\n",
       "   0.9228573441505432,\n",
       "   1.0449517965316772,\n",
       "   1.19364333152771,\n",
       "   1.7029683589935303,\n",
       "   1.207898497581482,\n",
       "   0.9927374124526978,\n",
       "   0.9530618190765381,\n",
       "   0.9447084665298462,\n",
       "   0.9415938854217529,\n",
       "   1.0200494527816772,\n",
       "   0.919065535068512,\n",
       "   0.9767081141471863,\n",
       "   1.266395092010498,\n",
       "   1.0740246772766113,\n",
       "   1.1123369932174683,\n",
       "   1.0273292064666748,\n",
       "   1.0431050062179565,\n",
       "   1.0533891916275024,\n",
       "   0.9879254102706909,\n",
       "   0.9635909795761108,\n",
       "   0.93250572681427,\n",
       "   1.0516247749328613,\n",
       "   1.0180120468139648,\n",
       "   1.097294807434082,\n",
       "   0.9214789271354675,\n",
       "   0.9631466865539551,\n",
       "   1.0425939559936523,\n",
       "   0.9671041965484619,\n",
       "   0.8754407167434692,\n",
       "   0.9972859621047974,\n",
       "   0.9973737001419067,\n",
       "   1.378767728805542,\n",
       "   0.8898845314979553,\n",
       "   0.985593855381012,\n",
       "   1.422659993171692,\n",
       "   1.1078540086746216,\n",
       "   1.0903810262680054,\n",
       "   0.8983678221702576,\n",
       "   1.0349664688110352,\n",
       "   0.925889790058136,\n",
       "   1.0268547534942627,\n",
       "   1.011172890663147,\n",
       "   1.0456732511520386,\n",
       "   1.6117417812347412,\n",
       "   1.4534326791763306,\n",
       "   0.8808153867721558,\n",
       "   0.9911543726921082,\n",
       "   1.4783486127853394,\n",
       "   0.9855779409408569,\n",
       "   1.0070304870605469,\n",
       "   0.9678163528442383,\n",
       "   1.0528428554534912,\n",
       "   1.0796386003494263,\n",
       "   0.9332355856895447,\n",
       "   0.9314180612564087,\n",
       "   1.0276981592178345,\n",
       "   1.0599350929260254,\n",
       "   1.4190382957458496,\n",
       "   1.2637358903884888,\n",
       "   0.995727002620697,\n",
       "   1.05349862575531,\n",
       "   0.912882387638092],\n",
       "  ('euclidean', True, True): [1.3304568529129028,\n",
       "   0.513915479183197,\n",
       "   0.6114131212234497,\n",
       "   1.0929230451583862,\n",
       "   1.0415186882019043,\n",
       "   1.263265609741211,\n",
       "   0.6854116916656494,\n",
       "   1.9651528596878052,\n",
       "   0.5684921145439148,\n",
       "   0.5602031946182251,\n",
       "   0.4199957847595215,\n",
       "   1.1908034086227417,\n",
       "   0.6073839068412781,\n",
       "   0.9592156410217285,\n",
       "   1.3143436908721924,\n",
       "   1.0357600450515747,\n",
       "   1.089496374130249,\n",
       "   1.4163217544555664,\n",
       "   0.7262527346611023,\n",
       "   0.9346826076507568,\n",
       "   0.9775612354278564,\n",
       "   1.0107613801956177,\n",
       "   0.4291050434112549,\n",
       "   0.8804395198822021,\n",
       "   1.130224585533142,\n",
       "   0.938300371170044,\n",
       "   1.0706619024276733,\n",
       "   0.9334242939949036,\n",
       "   1.2237770557403564,\n",
       "   2.079089403152466,\n",
       "   0.9537210464477539,\n",
       "   1.1581954956054688,\n",
       "   0.6026570200920105,\n",
       "   1.0295084714889526,\n",
       "   1.202958345413208,\n",
       "   0.8218576312065125,\n",
       "   1.3798006772994995,\n",
       "   0.9778512120246887,\n",
       "   0.8524706363677979,\n",
       "   1.0433679819107056,\n",
       "   0.8124393224716187,\n",
       "   1.1590287685394287,\n",
       "   0.5295519828796387,\n",
       "   1.259642243385315,\n",
       "   1.3888019323349,\n",
       "   1.4023280143737793,\n",
       "   1.175126314163208,\n",
       "   0.6160815954208374,\n",
       "   1.5607343912124634,\n",
       "   1.0896294116973877,\n",
       "   0.5654734969139099,\n",
       "   1.0989230871200562,\n",
       "   1.0278335809707642,\n",
       "   0.6657267212867737,\n",
       "   1.0442126989364624,\n",
       "   1.1678394079208374,\n",
       "   1.3700629472732544,\n",
       "   1.3496383428573608,\n",
       "   1.0778539180755615,\n",
       "   0.7690892815589905,\n",
       "   0.9087964296340942,\n",
       "   0.8869764804840088,\n",
       "   1.1911700963974,\n",
       "   0.8898837566375732,\n",
       "   0.7579584717750549,\n",
       "   1.1361596584320068,\n",
       "   0.8375662565231323,\n",
       "   1.0602912902832031,\n",
       "   1.4018521308898926,\n",
       "   1.3978153467178345,\n",
       "   1.0852574110031128,\n",
       "   1.055016279220581,\n",
       "   1.049516201019287,\n",
       "   1.4105429649353027,\n",
       "   1.0307937860488892,\n",
       "   0.900878369808197,\n",
       "   0.9442565441131592,\n",
       "   1.185045838356018,\n",
       "   1.0139161348342896,\n",
       "   1.5076946020126343,\n",
       "   1.5655275583267212,\n",
       "   1.1533840894699097,\n",
       "   1.5136194229125977,\n",
       "   0.5249539017677307,\n",
       "   1.1166566610336304,\n",
       "   1.0204484462738037,\n",
       "   1.5701302289962769,\n",
       "   0.7933517098426819,\n",
       "   0.9890672564506531,\n",
       "   0.5928906798362732,\n",
       "   1.3331068754196167,\n",
       "   0.6579855680465698,\n",
       "   1.1352875232696533,\n",
       "   1.0460107326507568,\n",
       "   0.8832849264144897,\n",
       "   0.6966419219970703,\n",
       "   1.3793514966964722,\n",
       "   1.5303024053573608,\n",
       "   1.122549295425415,\n",
       "   0.8979019522666931,\n",
       "   0.45165055990219116,\n",
       "   0.9832845330238342,\n",
       "   1.3623055219650269,\n",
       "   1.0919393301010132,\n",
       "   0.7950189709663391,\n",
       "   0.7674655318260193,\n",
       "   1.4033617973327637,\n",
       "   1.081304669380188,\n",
       "   0.7972986698150635,\n",
       "   1.1601709127426147,\n",
       "   0.5192610025405884,\n",
       "   0.8509972095489502,\n",
       "   0.563060462474823,\n",
       "   0.9695950746536255,\n",
       "   0.7646349668502808,\n",
       "   0.9042347073554993,\n",
       "   1.2869040966033936,\n",
       "   1.114108681678772,\n",
       "   0.7780324220657349,\n",
       "   0.8228181600570679,\n",
       "   0.8606634140014648,\n",
       "   0.5070599317550659,\n",
       "   0.7371933460235596,\n",
       "   0.9024754166603088,\n",
       "   1.2432847023010254,\n",
       "   0.8396567702293396,\n",
       "   0.9910509586334229,\n",
       "   0.5020357370376587,\n",
       "   1.4166628122329712,\n",
       "   1.11216139793396,\n",
       "   1.0180071592330933,\n",
       "   0.6200759410858154,\n",
       "   0.7640388011932373,\n",
       "   0.8655062913894653,\n",
       "   0.9134858846664429,\n",
       "   0.7379645109176636,\n",
       "   1.275303602218628,\n",
       "   1.8713265657424927,\n",
       "   1.4060851335525513,\n",
       "   1.5450260639190674,\n",
       "   1.1520839929580688,\n",
       "   1.0385223627090454,\n",
       "   0.8714773058891296,\n",
       "   0.571079432964325,\n",
       "   1.052857756614685,\n",
       "   1.791332721710205,\n",
       "   1.3083326816558838,\n",
       "   1.2229382991790771,\n",
       "   0.9659170508384705,\n",
       "   0.6335355043411255,\n",
       "   2.080432176589966,\n",
       "   1.125978708267212,\n",
       "   1.067486047744751,\n",
       "   0.9379414319992065,\n",
       "   0.8234905004501343,\n",
       "   1.3110504150390625,\n",
       "   0.9599155783653259,\n",
       "   0.9290689826011658,\n",
       "   1.1779605150222778,\n",
       "   1.254441738128662,\n",
       "   1.1267814636230469,\n",
       "   1.4968867301940918,\n",
       "   0.4648769199848175,\n",
       "   1.2581570148468018,\n",
       "   0.7813493013381958,\n",
       "   0.6721017956733704,\n",
       "   1.0877113342285156,\n",
       "   0.7061634063720703,\n",
       "   1.53889000415802,\n",
       "   0.7931811809539795,\n",
       "   1.88105046749115,\n",
       "   1.2050710916519165,\n",
       "   0.8066897988319397,\n",
       "   1.1218394041061401,\n",
       "   1.0052695274353027,\n",
       "   1.1239936351776123,\n",
       "   1.3002723455429077,\n",
       "   1.1408555507659912,\n",
       "   1.1359072923660278,\n",
       "   1.0415035486221313,\n",
       "   1.0863152742385864,\n",
       "   0.5863369703292847,\n",
       "   0.9478058218955994,\n",
       "   0.9904048442840576,\n",
       "   1.4112814664840698,\n",
       "   0.6487902402877808,\n",
       "   1.1862750053405762,\n",
       "   0.9017327427864075,\n",
       "   0.364715039730072,\n",
       "   0.6995590925216675,\n",
       "   0.5496937036514282,\n",
       "   0.8128942251205444,\n",
       "   0.4916694164276123,\n",
       "   1.1578019857406616,\n",
       "   0.9595097303390503,\n",
       "   1.8193446397781372,\n",
       "   0.7302956581115723,\n",
       "   1.105305552482605,\n",
       "   0.9627875685691833,\n",
       "   1.2178641557693481,\n",
       "   1.9341500997543335,\n",
       "   1.1664128303527832,\n",
       "   1.347295880317688,\n",
       "   0.9858096837997437,\n",
       "   0.9177994728088379,\n",
       "   1.1895556449890137,\n",
       "   1.5168681144714355,\n",
       "   1.021284580230713,\n",
       "   1.4718170166015625,\n",
       "   0.5994313359260559,\n",
       "   0.7093684077262878,\n",
       "   0.9482055306434631,\n",
       "   1.5790077447891235,\n",
       "   1.140547752380371,\n",
       "   1.2757943868637085,\n",
       "   1.0652745962142944,\n",
       "   1.1024576425552368,\n",
       "   1.2919169664382935,\n",
       "   0.5707327723503113,\n",
       "   0.7943845987319946,\n",
       "   0.9751248955726624,\n",
       "   1.3057998418807983,\n",
       "   1.1460075378417969,\n",
       "   1.2148269414901733,\n",
       "   0.44860556721687317,\n",
       "   0.4330156743526459,\n",
       "   0.5359659194946289,\n",
       "   1.3192732334136963,\n",
       "   0.7670870423316956,\n",
       "   0.6505690813064575,\n",
       "   1.3356724977493286,\n",
       "   1.5658003091812134,\n",
       "   0.7071078419685364,\n",
       "   1.0004925727844238,\n",
       "   1.1508018970489502,\n",
       "   0.8181107044219971,\n",
       "   2.0715479850769043,\n",
       "   0.41187819838523865,\n",
       "   1.0952329635620117,\n",
       "   0.5657723546028137,\n",
       "   1.1069464683532715,\n",
       "   0.7986906170845032,\n",
       "   1.1763900518417358,\n",
       "   1.1958651542663574,\n",
       "   1.475873351097107,\n",
       "   1.5719678401947021,\n",
       "   1.0757319927215576,\n",
       "   1.2331445217132568,\n",
       "   0.5639526844024658,\n",
       "   1.6678440570831299,\n",
       "   2.2694787979125977,\n",
       "   1.1394480466842651,\n",
       "   1.0441009998321533,\n",
       "   1.1039175987243652,\n",
       "   0.9791459441184998,\n",
       "   0.8702880144119263,\n",
       "   1.09196937084198,\n",
       "   0.8224267959594727,\n",
       "   1.192100167274475,\n",
       "   1.4444844722747803,\n",
       "   1.118696928024292,\n",
       "   0.47054967284202576,\n",
       "   0.8880389332771301,\n",
       "   1.1500072479248047,\n",
       "   1.0076922178268433,\n",
       "   0.9933441281318665,\n",
       "   1.2022463083267212,\n",
       "   0.6854182481765747,\n",
       "   1.199891448020935,\n",
       "   0.7705168128013611,\n",
       "   1.5743482112884521,\n",
       "   0.7037729024887085,\n",
       "   0.46885865926742554,\n",
       "   1.4369604587554932,\n",
       "   1.0116496086120605,\n",
       "   0.7523657083511353,\n",
       "   0.9554023742675781,\n",
       "   1.0376497507095337,\n",
       "   0.9224635362625122,\n",
       "   0.7384783625602722,\n",
       "   1.3724541664123535,\n",
       "   0.8456700444221497,\n",
       "   1.5251375436782837,\n",
       "   1.0107834339141846,\n",
       "   0.8547378778457642,\n",
       "   1.013679027557373,\n",
       "   1.2603189945220947,\n",
       "   0.9700120687484741,\n",
       "   0.6042535901069641,\n",
       "   0.5439769625663757,\n",
       "   0.9160436987876892,\n",
       "   1.142762303352356,\n",
       "   0.7740769386291504,\n",
       "   1.3501708507537842,\n",
       "   1.6750438213348389,\n",
       "   1.0642807483673096,\n",
       "   1.6902623176574707,\n",
       "   1.1479295492172241,\n",
       "   1.0405124425888062,\n",
       "   0.7697185277938843,\n",
       "   1.806045413017273,\n",
       "   0.28159379959106445,\n",
       "   0.37384724617004395,\n",
       "   1.5089799165725708,\n",
       "   1.564184546470642,\n",
       "   0.8734556436538696,\n",
       "   1.5183913707733154,\n",
       "   0.9310295581817627,\n",
       "   1.4593063592910767,\n",
       "   1.3668607473373413,\n",
       "   0.972544252872467,\n",
       "   1.5635838508605957,\n",
       "   1.0151392221450806,\n",
       "   1.1038062572479248,\n",
       "   0.8763695955276489,\n",
       "   1.105269432067871,\n",
       "   0.9164173603057861,\n",
       "   1.3801841735839844,\n",
       "   1.2901407480239868,\n",
       "   1.194495439529419,\n",
       "   1.7944554090499878,\n",
       "   1.106999158859253,\n",
       "   0.7378019094467163,\n",
       "   0.4646390974521637,\n",
       "   1.9392387866973877,\n",
       "   1.334472894668579,\n",
       "   0.9237787127494812,\n",
       "   1.8918644189834595,\n",
       "   1.356784701347351,\n",
       "   1.2898448705673218,\n",
       "   1.0745054483413696,\n",
       "   0.7371869087219238,\n",
       "   1.0956346988677979,\n",
       "   1.2212332487106323,\n",
       "   1.1712058782577515,\n",
       "   1.9416913986206055,\n",
       "   0.9917358756065369,\n",
       "   1.2519469261169434,\n",
       "   0.770611047744751,\n",
       "   1.013461709022522,\n",
       "   1.214957594871521,\n",
       "   1.2658154964447021,\n",
       "   1.4807665348052979,\n",
       "   1.0480753183364868,\n",
       "   1.3254815340042114,\n",
       "   0.3821962773799896,\n",
       "   1.02863347530365,\n",
       "   1.1181175708770752,\n",
       "   1.9395835399627686,\n",
       "   0.910814642906189,\n",
       "   1.4941294193267822,\n",
       "   1.2350326776504517,\n",
       "   1.243335247039795,\n",
       "   1.5849707126617432,\n",
       "   0.7439599633216858,\n",
       "   1.119431734085083,\n",
       "   1.1290366649627686,\n",
       "   0.6702587008476257,\n",
       "   0.6171886324882507,\n",
       "   0.8932256102561951,\n",
       "   1.0619980096817017,\n",
       "   0.9605345726013184,\n",
       "   1.0680954456329346,\n",
       "   1.1480027437210083,\n",
       "   1.0133445262908936,\n",
       "   1.2223122119903564,\n",
       "   0.40218010544776917,\n",
       "   0.9993916749954224,\n",
       "   0.9709662795066833,\n",
       "   1.14167058467865,\n",
       "   0.6245896816253662,\n",
       "   0.8994702100753784,\n",
       "   1.054734230041504,\n",
       "   1.3182851076126099,\n",
       "   0.6179822683334351,\n",
       "   2.078456401824951,\n",
       "   1.2543351650238037,\n",
       "   1.0032846927642822,\n",
       "   0.6168797612190247,\n",
       "   1.0921574831008911,\n",
       "   0.9737894535064697,\n",
       "   0.6200835108757019,\n",
       "   0.5287944078445435,\n",
       "   1.1610362529754639,\n",
       "   0.9767438769340515,\n",
       "   1.6074256896972656,\n",
       "   1.0719316005706787,\n",
       "   1.0127708911895752,\n",
       "   2.24733567237854,\n",
       "   1.3771469593048096,\n",
       "   0.9503142237663269,\n",
       "   0.9915547370910645,\n",
       "   1.1096689701080322,\n",
       "   0.7709254026412964,\n",
       "   1.0555133819580078,\n",
       "   0.6568244695663452,\n",
       "   0.8354929685592651,\n",
       "   1.0609197616577148,\n",
       "   0.5008600950241089,\n",
       "   0.9619125723838806,\n",
       "   0.7445266842842102,\n",
       "   0.6396496295928955,\n",
       "   0.9828143119812012,\n",
       "   0.6929501295089722,\n",
       "   1.3729429244995117,\n",
       "   0.6793943047523499,\n",
       "   1.3423882722854614,\n",
       "   0.9470818638801575,\n",
       "   0.35973209142684937,\n",
       "   0.5721173286437988,\n",
       "   2.32694673538208,\n",
       "   1.4572722911834717,\n",
       "   1.1776137351989746,\n",
       "   1.3165444135665894,\n",
       "   1.3879313468933105,\n",
       "   0.7589142322540283,\n",
       "   0.7948922514915466,\n",
       "   0.5775115489959717,\n",
       "   1.0339525938034058,\n",
       "   1.6090385913848877,\n",
       "   0.9758915901184082,\n",
       "   0.5872058868408203,\n",
       "   0.6207445859909058,\n",
       "   1.094652771949768,\n",
       "   1.0918328762054443,\n",
       "   1.129299521446228,\n",
       "   0.8500692844390869,\n",
       "   1.8791686296463013,\n",
       "   0.9394496083259583,\n",
       "   1.0073235034942627,\n",
       "   1.0329593420028687,\n",
       "   1.0815249681472778,\n",
       "   1.1033016443252563,\n",
       "   0.8013309836387634,\n",
       "   1.2700566053390503,\n",
       "   1.0595670938491821,\n",
       "   1.1809332370758057,\n",
       "   0.32101091742515564,\n",
       "   1.4425603151321411,\n",
       "   0.9554134607315063,\n",
       "   1.1859196424484253,\n",
       "   0.9075084924697876,\n",
       "   0.6041074991226196,\n",
       "   1.680315375328064,\n",
       "   1.34792160987854,\n",
       "   0.4703965485095978,\n",
       "   0.9354411959648132,\n",
       "   0.4263688921928406,\n",
       "   1.2329384088516235,\n",
       "   1.2818979024887085,\n",
       "   1.1765903234481812,\n",
       "   0.9579613208770752,\n",
       "   0.5205895304679871,\n",
       "   0.5552979111671448,\n",
       "   0.5027489066123962,\n",
       "   1.1374139785766602,\n",
       "   1.260235071182251,\n",
       "   1.4556947946548462,\n",
       "   1.3009965419769287,\n",
       "   0.9772916436195374,\n",
       "   1.4818618297576904,\n",
       "   1.115249752998352,\n",
       "   0.6446115970611572,\n",
       "   0.7868250608444214,\n",
       "   0.7757962346076965,\n",
       "   0.8497729301452637,\n",
       "   0.8746163845062256,\n",
       "   0.538780152797699,\n",
       "   1.0599029064178467,\n",
       "   1.1480411291122437,\n",
       "   1.3546783924102783,\n",
       "   0.7242763638496399,\n",
       "   1.0933215618133545,\n",
       "   0.9253625869750977,\n",
       "   1.7403899431228638,\n",
       "   0.8893136382102966,\n",
       "   0.8702868819236755,\n",
       "   1.177790880203247,\n",
       "   1.842831015586853,\n",
       "   0.9894136190414429,\n",
       "   1.0173611640930176,\n",
       "   0.5827203392982483,\n",
       "   2.739781141281128,\n",
       "   1.0621219873428345,\n",
       "   0.3546964228153229,\n",
       "   0.7964432239532471,\n",
       "   0.7092249393463135,\n",
       "   1.3832318782806396,\n",
       "   0.8286048173904419,\n",
       "   1.181286334991455,\n",
       "   1.2767425775527954,\n",
       "   0.9550459384918213,\n",
       "   0.8950381278991699,\n",
       "   0.41670212149620056,\n",
       "   0.43172013759613037,\n",
       "   1.1689574718475342,\n",
       "   1.281458854675293,\n",
       "   0.8341317176818848,\n",
       "   1.0751326084136963,\n",
       "   1.080210566520691,\n",
       "   1.088750958442688,\n",
       "   0.4222773611545563,\n",
       "   0.8358373045921326,\n",
       "   1.3126177787780762,\n",
       "   1.339455008506775,\n",
       "   0.7906724214553833,\n",
       "   0.7460266947746277,\n",
       "   0.9158972501754761,\n",
       "   0.4802040159702301,\n",
       "   0.9839234352111816,\n",
       "   0.8752758502960205,\n",
       "   1.109940528869629],\n",
       "  ('euclidean', True, False): [0.7143795490264893,\n",
       "   0.5021238923072815,\n",
       "   0.6114131212234497,\n",
       "   0.8684605360031128,\n",
       "   1.0415184497833252,\n",
       "   1.2875847816467285,\n",
       "   0.6854116916656494,\n",
       "   1.6190013885498047,\n",
       "   0.56849205493927,\n",
       "   0.5602032542228699,\n",
       "   0.4199957847595215,\n",
       "   1.330314040184021,\n",
       "   0.5830634236335754,\n",
       "   0.9387333989143372,\n",
       "   1.5181550979614258,\n",
       "   1.0357599258422852,\n",
       "   1.0558077096939087,\n",
       "   1.4163217544555664,\n",
       "   0.6761932373046875,\n",
       "   0.5909723043441772,\n",
       "   1.1388654708862305,\n",
       "   1.0900824069976807,\n",
       "   0.42910513281822205,\n",
       "   0.8804395198822021,\n",
       "   0.9598318338394165,\n",
       "   0.8915172219276428,\n",
       "   0.5070298910140991,\n",
       "   0.8755490779876709,\n",
       "   1.2237770557403564,\n",
       "   2.5259900093078613,\n",
       "   0.7407388687133789,\n",
       "   1.0435503721237183,\n",
       "   0.6026571989059448,\n",
       "   1.2703646421432495,\n",
       "   1.242123007774353,\n",
       "   0.8121435642242432,\n",
       "   0.9583043456077576,\n",
       "   0.9988673329353333,\n",
       "   0.8410684466362,\n",
       "   1.043367862701416,\n",
       "   0.8060486912727356,\n",
       "   1.1293302774429321,\n",
       "   0.5295521020889282,\n",
       "   1.259642481803894,\n",
       "   1.3888019323349,\n",
       "   1.1814088821411133,\n",
       "   1.175126314163208,\n",
       "   0.6015508770942688,\n",
       "   1.5607343912124634,\n",
       "   1.0896292924880981,\n",
       "   0.5654734969139099,\n",
       "   1.0989232063293457,\n",
       "   1.0278338193893433,\n",
       "   0.6657267212867737,\n",
       "   0.6532213687896729,\n",
       "   1.1678396463394165,\n",
       "   1.3298289775848389,\n",
       "   1.3978056907653809,\n",
       "   0.682880163192749,\n",
       "   0.9246453046798706,\n",
       "   0.8774654269218445,\n",
       "   0.5294790863990784,\n",
       "   0.9093836545944214,\n",
       "   1.2090712785720825,\n",
       "   0.7579584717750549,\n",
       "   1.1059247255325317,\n",
       "   0.8375662565231323,\n",
       "   0.9990156292915344,\n",
       "   0.9599695801734924,\n",
       "   1.397815465927124,\n",
       "   0.937384307384491,\n",
       "   0.626751184463501,\n",
       "   1.1213301420211792,\n",
       "   1.441455364227295,\n",
       "   1.1542284488677979,\n",
       "   0.827220618724823,\n",
       "   0.8263816833496094,\n",
       "   1.185045838356018,\n",
       "   1.004349708557129,\n",
       "   0.6533411145210266,\n",
       "   1.276679515838623,\n",
       "   0.9987414479255676,\n",
       "   1.4461650848388672,\n",
       "   0.5249539017677307,\n",
       "   1.8979501724243164,\n",
       "   0.9279270172119141,\n",
       "   0.9519197344779968,\n",
       "   0.7566988468170166,\n",
       "   0.5948549509048462,\n",
       "   0.4201643466949463,\n",
       "   0.6286112070083618,\n",
       "   0.4524233341217041,\n",
       "   1.5225690603256226,\n",
       "   1.0732159614562988,\n",
       "   0.9513686299324036,\n",
       "   0.6966418623924255,\n",
       "   0.5088862776756287,\n",
       "   1.2295156717300415,\n",
       "   1.122549057006836,\n",
       "   1.4747763872146606,\n",
       "   0.8173114061355591,\n",
       "   1.2853131294250488,\n",
       "   1.3616751432418823,\n",
       "   1.0919395685195923,\n",
       "   0.7950189113616943,\n",
       "   0.7674658894538879,\n",
       "   1.4400484561920166,\n",
       "   1.4900654554367065,\n",
       "   0.7573294639587402,\n",
       "   0.8964840173721313,\n",
       "   0.6214456558227539,\n",
       "   0.7944455146789551,\n",
       "   0.563060462474823,\n",
       "   0.8350969552993774,\n",
       "   1.5173695087432861,\n",
       "   0.8648431897163391,\n",
       "   1.0708591938018799,\n",
       "   0.738801896572113,\n",
       "   0.8146902322769165,\n",
       "   0.42712822556495667,\n",
       "   0.6985875368118286,\n",
       "   0.4136819541454315,\n",
       "   0.5602870583534241,\n",
       "   0.7884316444396973,\n",
       "   1.0246025323867798,\n",
       "   0.7948068976402283,\n",
       "   1.0738624334335327,\n",
       "   0.5020357370376587,\n",
       "   1.370536208152771,\n",
       "   1.3329657316207886,\n",
       "   1.0180072784423828,\n",
       "   0.6200760006904602,\n",
       "   0.7640388011932373,\n",
       "   0.8252086639404297,\n",
       "   0.847873330116272,\n",
       "   0.7096139788627625,\n",
       "   1.1763356924057007,\n",
       "   2.1723971366882324,\n",
       "   1.154887080192566,\n",
       "   1.614071011543274,\n",
       "   1.1620099544525146,\n",
       "   1.0774571895599365,\n",
       "   0.6250519752502441,\n",
       "   0.43022745847702026,\n",
       "   1.0196701288223267,\n",
       "   1.6922425031661987,\n",
       "   0.9780077338218689,\n",
       "   1.2229382991790771,\n",
       "   0.9445559978485107,\n",
       "   0.633535623550415,\n",
       "   1.6156026124954224,\n",
       "   1.538551926612854,\n",
       "   1.0625865459442139,\n",
       "   0.970371425151825,\n",
       "   0.569377601146698,\n",
       "   0.9598231315612793,\n",
       "   1.0025196075439453,\n",
       "   0.9290689826011658,\n",
       "   0.8814334869384766,\n",
       "   0.9586864709854126,\n",
       "   1.0274020433425903,\n",
       "   1.0333912372589111,\n",
       "   0.4648769199848175,\n",
       "   1.3285638093948364,\n",
       "   1.0094892978668213,\n",
       "   0.6564489603042603,\n",
       "   1.1546332836151123,\n",
       "   0.7061633467674255,\n",
       "   1.5408743619918823,\n",
       "   0.7931813597679138,\n",
       "   1.6004067659378052,\n",
       "   0.5314857959747314,\n",
       "   0.8066897392272949,\n",
       "   0.9215958714485168,\n",
       "   1.0052694082260132,\n",
       "   0.5097213983535767,\n",
       "   1.2587687969207764,\n",
       "   1.2286320924758911,\n",
       "   1.1953907012939453,\n",
       "   1.3913134336471558,\n",
       "   0.8895879983901978,\n",
       "   0.5863370895385742,\n",
       "   0.6652410626411438,\n",
       "   0.8254496455192566,\n",
       "   1.3112138509750366,\n",
       "   0.9623852968215942,\n",
       "   1.1862751245498657,\n",
       "   0.662094235420227,\n",
       "   0.2603133022785187,\n",
       "   0.33821311593055725,\n",
       "   0.4846671521663666,\n",
       "   1.3622595071792603,\n",
       "   0.28144416213035583,\n",
       "   1.1578017473220825,\n",
       "   1.1576958894729614,\n",
       "   1.6485614776611328,\n",
       "   0.7168542742729187,\n",
       "   0.9632861018180847,\n",
       "   1.1104892492294312,\n",
       "   1.2178642749786377,\n",
       "   1.9762442111968994,\n",
       "   1.1664128303527832,\n",
       "   1.6997499465942383,\n",
       "   0.9649580717086792,\n",
       "   0.7073659896850586,\n",
       "   1.2461475133895874,\n",
       "   1.5168681144714355,\n",
       "   0.9794954657554626,\n",
       "   1.595813512802124,\n",
       "   0.5994313955307007,\n",
       "   0.7093684077262878,\n",
       "   0.8790495991706848,\n",
       "   1.673345685005188,\n",
       "   0.9205840229988098,\n",
       "   1.3386178016662598,\n",
       "   1.065274715423584,\n",
       "   1.033108115196228,\n",
       "   1.291916847229004,\n",
       "   0.5707327723503113,\n",
       "   0.7943844795227051,\n",
       "   0.595657467842102,\n",
       "   0.8082894682884216,\n",
       "   0.5865368843078613,\n",
       "   1.1794999837875366,\n",
       "   0.4605647623538971,\n",
       "   0.4330156445503235,\n",
       "   0.5359658598899841,\n",
       "   0.7285118699073792,\n",
       "   0.7513284087181091,\n",
       "   0.650568962097168,\n",
       "   1.3356722593307495,\n",
       "   1.185323715209961,\n",
       "   0.423422634601593,\n",
       "   0.6740128993988037,\n",
       "   0.5832854509353638,\n",
       "   0.7938284277915955,\n",
       "   2.1791982650756836,\n",
       "   0.41187819838523865,\n",
       "   1.0844371318817139,\n",
       "   0.6499854326248169,\n",
       "   1.143075704574585,\n",
       "   0.7986905574798584,\n",
       "   1.1826395988464355,\n",
       "   1.0494000911712646,\n",
       "   0.548233151435852,\n",
       "   1.571967601776123,\n",
       "   0.9002591371536255,\n",
       "   0.8888660073280334,\n",
       "   1.2011737823486328,\n",
       "   1.1377307176589966,\n",
       "   1.1624480485916138,\n",
       "   1.1700448989868164,\n",
       "   0.5779945850372314,\n",
       "   0.8912354707717896,\n",
       "   0.6079592704772949,\n",
       "   1.0698280334472656,\n",
       "   1.0500653982162476,\n",
       "   0.8224267363548279,\n",
       "   1.0764715671539307,\n",
       "   0.46810418367385864,\n",
       "   1.0063073635101318,\n",
       "   0.37682104110717773,\n",
       "   0.6897507905960083,\n",
       "   1.009497880935669,\n",
       "   1.0076924562454224,\n",
       "   0.9591763615608215,\n",
       "   0.6251963376998901,\n",
       "   0.6854182481765747,\n",
       "   1.4167616367340088,\n",
       "   0.7776881456375122,\n",
       "   1.4977582693099976,\n",
       "   0.6386289000511169,\n",
       "   0.46885859966278076,\n",
       "   1.4902712106704712,\n",
       "   0.9272862076759338,\n",
       "   1.3369581699371338,\n",
       "   0.9554023146629333,\n",
       "   1.2365540266036987,\n",
       "   0.8991690278053284,\n",
       "   0.7384783625602722,\n",
       "   1.3724541664123535,\n",
       "   0.7036521434783936,\n",
       "   1.5440499782562256,\n",
       "   0.6589822173118591,\n",
       "   0.8012831211090088,\n",
       "   1.226539969444275,\n",
       "   1.2895647287368774,\n",
       "   0.8973757028579712,\n",
       "   0.6042536497116089,\n",
       "   0.5439770221710205,\n",
       "   0.7643627524375916,\n",
       "   0.6220262050628662,\n",
       "   0.7374849319458008,\n",
       "   1.3474171161651611,\n",
       "   1.7503938674926758,\n",
       "   1.0642809867858887,\n",
       "   0.7037811875343323,\n",
       "   1.1479295492172241,\n",
       "   1.0405123233795166,\n",
       "   0.5741134881973267,\n",
       "   1.8060451745986938,\n",
       "   0.28159376978874207,\n",
       "   0.37384727597236633,\n",
       "   1.5089800357818604,\n",
       "   1.5368826389312744,\n",
       "   0.9187477827072144,\n",
       "   1.5072777271270752,\n",
       "   0.5786068439483643,\n",
       "   1.436676025390625,\n",
       "   1.0933148860931396,\n",
       "   0.9329139590263367,\n",
       "   1.5635838508605957,\n",
       "   1.330955982208252,\n",
       "   1.1038061380386353,\n",
       "   0.7994239926338196,\n",
       "   0.9054354429244995,\n",
       "   0.6344835758209229,\n",
       "   1.2192330360412598,\n",
       "   1.2178910970687866,\n",
       "   1.2585370540618896,\n",
       "   1.794455885887146,\n",
       "   1.1069988012313843,\n",
       "   0.670505940914154,\n",
       "   0.548460066318512,\n",
       "   1.9392387866973877,\n",
       "   1.3936609029769897,\n",
       "   0.4977451264858246,\n",
       "   1.2114065885543823,\n",
       "   1.356784701347351,\n",
       "   1.1057201623916626,\n",
       "   0.9315276145935059,\n",
       "   0.7371869087219238,\n",
       "   1.0956346988677979,\n",
       "   1.2533929347991943,\n",
       "   0.9174066185951233,\n",
       "   0.9778071045875549,\n",
       "   0.8282232284545898,\n",
       "   1.2519469261169434,\n",
       "   0.7706113457679749,\n",
       "   1.013461709022522,\n",
       "   1.214957594871521,\n",
       "   1.0906217098236084,\n",
       "   1.8092120885849,\n",
       "   1.0480754375457764,\n",
       "   0.9255815148353577,\n",
       "   0.3821963369846344,\n",
       "   0.9583552479743958,\n",
       "   1.1181175708770752,\n",
       "   1.9395835399627686,\n",
       "   0.9679279923439026,\n",
       "   1.4124610424041748,\n",
       "   0.889158308506012,\n",
       "   1.2570722103118896,\n",
       "   1.6278592348098755,\n",
       "   0.7439601421356201,\n",
       "   1.5335747003555298,\n",
       "   1.1290366649627686,\n",
       "   0.6901897192001343,\n",
       "   0.6171887516975403,\n",
       "   1.3493549823760986,\n",
       "   1.0543432235717773,\n",
       "   0.9605346322059631,\n",
       "   0.7800183892250061,\n",
       "   0.6559705138206482,\n",
       "   1.013344407081604,\n",
       "   1.7303776741027832,\n",
       "   0.7912631034851074,\n",
       "   0.7683905363082886,\n",
       "   0.9709663987159729,\n",
       "   1.018369197845459,\n",
       "   0.6245896816253662,\n",
       "   0.9182674288749695,\n",
       "   1.0547343492507935,\n",
       "   1.1179336309432983,\n",
       "   0.6179822683334351,\n",
       "   2.0859086513519287,\n",
       "   1.2590199708938599,\n",
       "   1.0032848119735718,\n",
       "   0.6168796420097351,\n",
       "   0.9611958265304565,\n",
       "   0.924741268157959,\n",
       "   0.6200835108757019,\n",
       "   0.5287943482398987,\n",
       "   1.2440673112869263,\n",
       "   0.9272529482841492,\n",
       "   1.6259229183197021,\n",
       "   1.0042630434036255,\n",
       "   1.0127710103988647,\n",
       "   1.844434380531311,\n",
       "   1.3771469593048096,\n",
       "   0.9503141045570374,\n",
       "   0.6075925230979919,\n",
       "   1.1096689701080322,\n",
       "   0.7709253430366516,\n",
       "   0.8872712850570679,\n",
       "   0.6416109204292297,\n",
       "   0.8354930877685547,\n",
       "   0.45420217514038086,\n",
       "   0.8781744837760925,\n",
       "   0.9619125723838806,\n",
       "   0.7445268034934998,\n",
       "   0.6396495699882507,\n",
       "   0.7847762107849121,\n",
       "   0.6929501295089722,\n",
       "   1.3729431629180908,\n",
       "   0.6793943047523499,\n",
       "   1.1757961511611938,\n",
       "   0.9470818638801575,\n",
       "   0.35973209142684937,\n",
       "   0.7722275853157043,\n",
       "   0.6000588536262512,\n",
       "   0.8281031847000122,\n",
       "   1.1535872220993042,\n",
       "   0.8786497116088867,\n",
       "   1.4066342115402222,\n",
       "   0.7589142322540283,\n",
       "   0.7476467490196228,\n",
       "   0.5601391792297363,\n",
       "   0.8158648014068604,\n",
       "   1.6090388298034668,\n",
       "   0.9758917093276978,\n",
       "   0.5872058868408203,\n",
       "   0.6207445859909058,\n",
       "   0.4578607380390167,\n",
       "   1.0918331146240234,\n",
       "   1.1109542846679688,\n",
       "   0.6235964298248291,\n",
       "   1.919608235359192,\n",
       "   1.101292610168457,\n",
       "   1.0073235034942627,\n",
       "   0.9311394691467285,\n",
       "   0.9428112506866455,\n",
       "   1.9499800205230713,\n",
       "   0.6013820171356201,\n",
       "   1.1422735452651978,\n",
       "   1.2884329557418823,\n",
       "   1.2050477266311646,\n",
       "   0.2067512571811676,\n",
       "   1.2165011167526245,\n",
       "   0.9307525753974915,\n",
       "   1.0257028341293335,\n",
       "   0.8057767748832703,\n",
       "   0.5569894909858704,\n",
       "   1.8059771060943604,\n",
       "   1.3479217290878296,\n",
       "   0.47039660811424255,\n",
       "   0.9325206279754639,\n",
       "   0.42636895179748535,\n",
       "   1.2007719278335571,\n",
       "   1.2818981409072876,\n",
       "   1.2847087383270264,\n",
       "   0.7604895234107971,\n",
       "   0.5109505653381348,\n",
       "   0.5552979111671448,\n",
       "   0.48429179191589355,\n",
       "   1.0381250381469727,\n",
       "   1.4339832067489624,\n",
       "   1.2704311609268188,\n",
       "   1.3009960651397705,\n",
       "   0.9570552110671997,\n",
       "   1.4818618297576904,\n",
       "   1.1152499914169312,\n",
       "   0.44208207726478577,\n",
       "   0.6709885001182556,\n",
       "   0.7653680443763733,\n",
       "   0.8497728705406189,\n",
       "   0.8746164441108704,\n",
       "   0.5513306856155396,\n",
       "   1.0599029064178467,\n",
       "   1.1480411291122437,\n",
       "   1.1842509508132935,\n",
       "   0.7223531007766724,\n",
       "   0.9551370143890381,\n",
       "   0.7836491465568542,\n",
       "   1.572471261024475,\n",
       "   1.177528977394104,\n",
       "   0.8702868819236755,\n",
       "   1.177790880203247,\n",
       "   1.8428304195404053,\n",
       "   1.0216318368911743,\n",
       "   1.0173609256744385,\n",
       "   0.5827203392982483,\n",
       "   2.8257462978363037,\n",
       "   1.0621219873428345,\n",
       "   0.25867578387260437,\n",
       "   0.7964430451393127,\n",
       "   0.7092251181602478,\n",
       "   1.3832318782806396,\n",
       "   0.8286049365997314,\n",
       "   1.3436912298202515,\n",
       "   0.8791331648826599,\n",
       "   0.9550459980964661,\n",
       "   0.8240612149238586,\n",
       "   0.41670212149620056,\n",
       "   0.4317200779914856,\n",
       "   1.5614378452301025,\n",
       "   1.0877406597137451,\n",
       "   0.4808935225009918,\n",
       "   1.0751328468322754,\n",
       "   0.9681613445281982,\n",
       "   1.1409430503845215,\n",
       "   0.787135899066925,\n",
       "   0.7986880540847778,\n",
       "   1.3491442203521729,\n",
       "   1.3394546508789062,\n",
       "   0.7906727194786072,\n",
       "   0.7460265159606934,\n",
       "   0.5724489092826843,\n",
       "   0.4802040755748749,\n",
       "   1.0240558385849,\n",
       "   0.8752758502960205,\n",
       "   1.4129724502563477],\n",
       "  ('euclidean', False, True): [1.3290493488311768,\n",
       "   0.48803573846817017,\n",
       "   0.6453565359115601,\n",
       "   1.1354767084121704,\n",
       "   1.0431921482086182,\n",
       "   1.32480788230896,\n",
       "   0.6492551565170288,\n",
       "   2.0292587280273438,\n",
       "   0.5934149026870728,\n",
       "   0.5867093801498413,\n",
       "   0.43995678424835205,\n",
       "   1.2306090593338013,\n",
       "   0.6188496947288513,\n",
       "   0.9873499870300293,\n",
       "   1.3332483768463135,\n",
       "   1.0626466274261475,\n",
       "   1.0762447118759155,\n",
       "   1.436589241027832,\n",
       "   0.7611667513847351,\n",
       "   0.9401780962944031,\n",
       "   0.9772928953170776,\n",
       "   1.0400460958480835,\n",
       "   0.43869590759277344,\n",
       "   0.8883656859397888,\n",
       "   1.1035487651824951,\n",
       "   0.967556357383728,\n",
       "   1.0847662687301636,\n",
       "   0.924457848072052,\n",
       "   1.2383167743682861,\n",
       "   2.1097335815429688,\n",
       "   0.9826453328132629,\n",
       "   1.1957489252090454,\n",
       "   0.6164361834526062,\n",
       "   1.02711820602417,\n",
       "   1.1945981979370117,\n",
       "   0.8089553713798523,\n",
       "   1.450698733329773,\n",
       "   0.9904277920722961,\n",
       "   0.8495883941650391,\n",
       "   1.0562812089920044,\n",
       "   0.7787638306617737,\n",
       "   1.139135718345642,\n",
       "   0.5636454224586487,\n",
       "   1.3105663061141968,\n",
       "   1.4611420631408691,\n",
       "   1.4533052444458008,\n",
       "   1.195666790008545,\n",
       "   0.605747401714325,\n",
       "   1.6005656719207764,\n",
       "   1.0843843221664429,\n",
       "   0.6067209839820862,\n",
       "   1.0770057439804077,\n",
       "   1.0422871112823486,\n",
       "   0.6911741495132446,\n",
       "   1.0695455074310303,\n",
       "   1.1826196908950806,\n",
       "   1.3961668014526367,\n",
       "   1.4418085813522339,\n",
       "   1.1284226179122925,\n",
       "   0.7364707589149475,\n",
       "   0.9255737662315369,\n",
       "   0.9397125840187073,\n",
       "   1.2144569158554077,\n",
       "   0.8865338563919067,\n",
       "   0.79136061668396,\n",
       "   1.1598187685012817,\n",
       "   0.8234757781028748,\n",
       "   1.0616549253463745,\n",
       "   1.3956414461135864,\n",
       "   1.4395675659179688,\n",
       "   1.0742595195770264,\n",
       "   1.0456947088241577,\n",
       "   1.1182942390441895,\n",
       "   1.46973717212677,\n",
       "   1.0632802248001099,\n",
       "   0.9622275829315186,\n",
       "   0.9505959749221802,\n",
       "   1.2181404829025269,\n",
       "   1.017237663269043,\n",
       "   1.5581241846084595,\n",
       "   1.6028101444244385,\n",
       "   1.1698566675186157,\n",
       "   1.534238338470459,\n",
       "   0.5177028179168701,\n",
       "   1.1655985116958618,\n",
       "   1.0383254289627075,\n",
       "   1.5255937576293945,\n",
       "   0.8119974136352539,\n",
       "   1.0067611932754517,\n",
       "   0.5845653414726257,\n",
       "   1.3713765144348145,\n",
       "   0.6873136758804321,\n",
       "   1.1843249797821045,\n",
       "   1.101110577583313,\n",
       "   0.925308883190155,\n",
       "   0.7106990218162537,\n",
       "   1.4569079875946045,\n",
       "   1.479009747505188,\n",
       "   1.1682214736938477,\n",
       "   0.8957505822181702,\n",
       "   0.4817329943180084,\n",
       "   0.9949772953987122,\n",
       "   1.3628191947937012,\n",
       "   1.093274474143982,\n",
       "   0.8255612254142761,\n",
       "   0.7787869572639465,\n",
       "   1.4298354387283325,\n",
       "   1.061341643333435,\n",
       "   0.7993767261505127,\n",
       "   1.1094229221343994,\n",
       "   0.5446328520774841,\n",
       "   0.8064033389091492,\n",
       "   0.5512708425521851,\n",
       "   0.9572291374206543,\n",
       "   0.7669832706451416,\n",
       "   0.921016275882721,\n",
       "   1.360334873199463,\n",
       "   1.1409441232681274,\n",
       "   0.7606733441352844,\n",
       "   0.8329249620437622,\n",
       "   0.8428972363471985,\n",
       "   0.4945549964904785,\n",
       "   0.7050581574440002,\n",
       "   0.9017654061317444,\n",
       "   1.2835609912872314,\n",
       "   0.8358641266822815,\n",
       "   1.017013669013977,\n",
       "   0.5132135152816772,\n",
       "   1.4278799295425415,\n",
       "   1.0803784132003784,\n",
       "   1.050466775894165,\n",
       "   0.6440085172653198,\n",
       "   0.7752447724342346,\n",
       "   0.8660035729408264,\n",
       "   0.8929504156112671,\n",
       "   0.7817716002464294,\n",
       "   1.2829489707946777,\n",
       "   1.8926399946212769,\n",
       "   1.4091265201568604,\n",
       "   1.590163230895996,\n",
       "   1.1559345722198486,\n",
       "   1.1007641553878784,\n",
       "   0.8846616148948669,\n",
       "   0.5425903797149658,\n",
       "   1.0766150951385498,\n",
       "   1.9282952547073364,\n",
       "   1.287406086921692,\n",
       "   1.2393757104873657,\n",
       "   0.951297402381897,\n",
       "   0.6518909931182861,\n",
       "   2.177933931350708,\n",
       "   1.0894838571548462,\n",
       "   1.1297162771224976,\n",
       "   0.9282724261283875,\n",
       "   0.8294017910957336,\n",
       "   1.3640207052230835,\n",
       "   0.9641858339309692,\n",
       "   0.965997040271759,\n",
       "   1.2042268514633179,\n",
       "   1.2348260879516602,\n",
       "   1.139912724494934,\n",
       "   1.456302285194397,\n",
       "   0.4498883783817291,\n",
       "   1.2750535011291504,\n",
       "   0.8042992353439331,\n",
       "   0.6832665801048279,\n",
       "   1.1219067573547363,\n",
       "   0.6761078238487244,\n",
       "   1.5671688318252563,\n",
       "   0.7970515489578247,\n",
       "   1.9769083261489868,\n",
       "   1.2472773790359497,\n",
       "   0.8063352108001709,\n",
       "   1.166635274887085,\n",
       "   0.9950751066207886,\n",
       "   1.1859456300735474,\n",
       "   1.34232759475708,\n",
       "   1.1315600872039795,\n",
       "   1.132879614830017,\n",
       "   1.0560142993927002,\n",
       "   1.0825151205062866,\n",
       "   0.6048102974891663,\n",
       "   0.9652956128120422,\n",
       "   1.0410363674163818,\n",
       "   1.4576722383499146,\n",
       "   0.6613807678222656,\n",
       "   1.2081379890441895,\n",
       "   0.8752878308296204,\n",
       "   0.36254021525382996,\n",
       "   0.7202388048171997,\n",
       "   0.5197972059249878,\n",
       "   0.8202899694442749,\n",
       "   0.4759654104709625,\n",
       "   1.1655007600784302,\n",
       "   0.9597654938697815,\n",
       "   1.8657654523849487,\n",
       "   0.7367733716964722,\n",
       "   1.121610164642334,\n",
       "   0.9530923962593079,\n",
       "   1.2672892808914185,\n",
       "   1.9465017318725586,\n",
       "   1.2211415767669678,\n",
       "   1.4006497859954834,\n",
       "   0.9757500290870667,\n",
       "   0.9135273098945618,\n",
       "   1.2397034168243408,\n",
       "   1.497835397720337,\n",
       "   1.0612596273422241,\n",
       "   1.5182429552078247,\n",
       "   0.6272202134132385,\n",
       "   0.7072553634643555,\n",
       "   0.9635926485061646,\n",
       "   1.6750080585479736,\n",
       "   1.191438913345337,\n",
       "   1.3028786182403564,\n",
       "   1.0963051319122314,\n",
       "   1.1392338275909424,\n",
       "   1.2637754678726196,\n",
       "   0.576737642288208,\n",
       "   0.8327711224555969,\n",
       "   0.9775242805480957,\n",
       "   1.3135610818862915,\n",
       "   1.1413227319717407,\n",
       "   1.235217571258545,\n",
       "   0.41249141097068787,\n",
       "   0.4538455605506897,\n",
       "   0.5647525787353516,\n",
       "   1.2979053258895874,\n",
       "   0.7527673840522766,\n",
       "   0.6430290937423706,\n",
       "   1.4245802164077759,\n",
       "   1.5822416543960571,\n",
       "   0.6959332823753357,\n",
       "   1.0140928030014038,\n",
       "   1.1637070178985596,\n",
       "   0.8077800869941711,\n",
       "   2.124967098236084,\n",
       "   0.4126182496547699,\n",
       "   1.1213209629058838,\n",
       "   0.5433652997016907,\n",
       "   1.0895670652389526,\n",
       "   0.8110906481742859,\n",
       "   1.1804560422897339,\n",
       "   1.2063459157943726,\n",
       "   1.519315242767334,\n",
       "   1.648144006729126,\n",
       "   1.095097303390503,\n",
       "   1.3113524913787842,\n",
       "   0.5691643953323364,\n",
       "   1.727636456489563,\n",
       "   2.379607677459717,\n",
       "   1.1679235696792603,\n",
       "   1.0554403066635132,\n",
       "   1.111691951751709,\n",
       "   1.029220461845398,\n",
       "   0.8685899376869202,\n",
       "   1.0872684717178345,\n",
       "   0.816681981086731,\n",
       "   1.2341281175613403,\n",
       "   1.4849646091461182,\n",
       "   1.1138911247253418,\n",
       "   0.4441181421279907,\n",
       "   0.8852047920227051,\n",
       "   1.1622427701950073,\n",
       "   1.017984390258789,\n",
       "   1.012037992477417,\n",
       "   1.2294800281524658,\n",
       "   0.6976022720336914,\n",
       "   1.260324478149414,\n",
       "   0.7609757781028748,\n",
       "   1.56647527217865,\n",
       "   0.7117826342582703,\n",
       "   0.44914710521698,\n",
       "   1.3930995464324951,\n",
       "   0.9729466438293457,\n",
       "   0.7487942576408386,\n",
       "   0.9614759087562561,\n",
       "   1.0356910228729248,\n",
       "   0.9177959561347961,\n",
       "   0.7446802854537964,\n",
       "   1.4175177812576294,\n",
       "   0.8514276742935181,\n",
       "   1.606197476387024,\n",
       "   1.0264105796813965,\n",
       "   0.850036084651947,\n",
       "   1.0650590658187866,\n",
       "   1.3199515342712402,\n",
       "   1.0040558576583862,\n",
       "   0.6032285094261169,\n",
       "   0.5741688013076782,\n",
       "   0.9267052412033081,\n",
       "   1.1736594438552856,\n",
       "   0.791443407535553,\n",
       "   1.4178111553192139,\n",
       "   1.7470226287841797,\n",
       "   1.0875829458236694,\n",
       "   1.7647827863693237,\n",
       "   1.1817563772201538,\n",
       "   1.0142693519592285,\n",
       "   0.7599177956581116,\n",
       "   1.829912543296814,\n",
       "   0.28376802802085876,\n",
       "   0.374896764755249,\n",
       "   1.5174459218978882,\n",
       "   1.5806487798690796,\n",
       "   0.8590254783630371,\n",
       "   1.5414202213287354,\n",
       "   0.9220764636993408,\n",
       "   1.4812028408050537,\n",
       "   1.3597332239151,\n",
       "   0.9528014063835144,\n",
       "   1.610815405845642,\n",
       "   1.0333364009857178,\n",
       "   1.1404997110366821,\n",
       "   0.86439448595047,\n",
       "   1.139190673828125,\n",
       "   0.969256579875946,\n",
       "   1.394913911819458,\n",
       "   1.3045973777770996,\n",
       "   1.2052792310714722,\n",
       "   1.8178454637527466,\n",
       "   1.1249366998672485,\n",
       "   0.7424858808517456,\n",
       "   0.4581380784511566,\n",
       "   2.0167670249938965,\n",
       "   1.3568484783172607,\n",
       "   0.8863705992698669,\n",
       "   1.9053593873977661,\n",
       "   1.4392201900482178,\n",
       "   1.2646725177764893,\n",
       "   1.0631523132324219,\n",
       "   0.713965654373169,\n",
       "   1.076250672340393,\n",
       "   1.2775956392288208,\n",
       "   1.17988121509552,\n",
       "   1.9080963134765625,\n",
       "   0.9872052073478699,\n",
       "   1.255110502243042,\n",
       "   0.7304087281227112,\n",
       "   1.0312328338623047,\n",
       "   1.2942007780075073,\n",
       "   1.2291016578674316,\n",
       "   1.4857923984527588,\n",
       "   1.0624037981033325,\n",
       "   1.356594443321228,\n",
       "   0.35358232259750366,\n",
       "   1.0278711318969727,\n",
       "   1.1443848609924316,\n",
       "   2.020521402359009,\n",
       "   0.9390771985054016,\n",
       "   1.5083115100860596,\n",
       "   1.277174949645996,\n",
       "   1.2310478687286377,\n",
       "   1.6416569948196411,\n",
       "   0.7631275653839111,\n",
       "   1.1332643032073975,\n",
       "   1.1534334421157837,\n",
       "   0.6665853261947632,\n",
       "   0.6505305767059326,\n",
       "   0.8952150344848633,\n",
       "   1.1029301881790161,\n",
       "   0.9501767158508301,\n",
       "   1.116232991218567,\n",
       "   1.1196902990341187,\n",
       "   1.0370324850082397,\n",
       "   1.2675552368164062,\n",
       "   0.41091686487197876,\n",
       "   1.0177178382873535,\n",
       "   1.0025311708450317,\n",
       "   1.185813307762146,\n",
       "   0.6370245218276978,\n",
       "   0.8903838992118835,\n",
       "   1.0870821475982666,\n",
       "   1.3660130500793457,\n",
       "   0.5942610502243042,\n",
       "   2.0680925846099854,\n",
       "   1.2590562105178833,\n",
       "   1.050979733467102,\n",
       "   0.6289880871772766,\n",
       "   1.1228893995285034,\n",
       "   0.9778293371200562,\n",
       "   0.6145413517951965,\n",
       "   0.5533517599105835,\n",
       "   1.1871975660324097,\n",
       "   0.9816753268241882,\n",
       "   1.689353585243225,\n",
       "   1.0614463090896606,\n",
       "   1.0222728252410889,\n",
       "   2.320491075515747,\n",
       "   1.3866591453552246,\n",
       "   0.9440883994102478,\n",
       "   1.0081638097763062,\n",
       "   1.1193915605545044,\n",
       "   0.7548038363456726,\n",
       "   1.0782179832458496,\n",
       "   0.632302463054657,\n",
       "   0.8362539410591125,\n",
       "   1.0891683101654053,\n",
       "   0.5016294121742249,\n",
       "   0.9407784342765808,\n",
       "   0.7546153664588928,\n",
       "   0.6454989910125732,\n",
       "   1.0043367147445679,\n",
       "   0.7196277976036072,\n",
       "   1.371882677078247,\n",
       "   0.6889867782592773,\n",
       "   1.3259750604629517,\n",
       "   0.9630756378173828,\n",
       "   0.3502277135848999,\n",
       "   0.5613824129104614,\n",
       "   2.366812229156494,\n",
       "   1.5177401304244995,\n",
       "   1.2471939325332642,\n",
       "   1.3989155292510986,\n",
       "   1.4479355812072754,\n",
       "   0.7736963033676147,\n",
       "   0.7730625867843628,\n",
       "   0.5634881258010864,\n",
       "   1.0416266918182373,\n",
       "   1.6296157836914062,\n",
       "   1.0055820941925049,\n",
       "   0.5748178362846375,\n",
       "   0.64415442943573,\n",
       "   1.0722252130508423,\n",
       "   1.074823021888733,\n",
       "   1.169988989830017,\n",
       "   0.8805789947509766,\n",
       "   1.8665019273757935,\n",
       "   1.0020071268081665,\n",
       "   1.0326603651046753,\n",
       "   0.9965140223503113,\n",
       "   1.0689903497695923,\n",
       "   1.1535139083862305,\n",
       "   0.8477924466133118,\n",
       "   1.3101164102554321,\n",
       "   1.072274923324585,\n",
       "   1.2399178743362427,\n",
       "   0.3140801191329956,\n",
       "   1.5015040636062622,\n",
       "   0.9892352819442749,\n",
       "   1.2186568975448608,\n",
       "   0.9125878810882568,\n",
       "   0.5909678339958191,\n",
       "   1.663847804069519,\n",
       "   1.3795264959335327,\n",
       "   0.43472766876220703,\n",
       "   0.9469645023345947,\n",
       "   0.41738584637641907,\n",
       "   1.3104556798934937,\n",
       "   1.3364776372909546,\n",
       "   1.2445229291915894,\n",
       "   0.9159974455833435,\n",
       "   0.5127562880516052,\n",
       "   0.5709663033485413,\n",
       "   0.4902903735637665,\n",
       "   1.1167011260986328,\n",
       "   1.3087420463562012,\n",
       "   1.4760812520980835,\n",
       "   1.3436496257781982,\n",
       "   0.9967966675758362,\n",
       "   1.553194522857666,\n",
       "   1.1175122261047363,\n",
       "   0.6414938569068909,\n",
       "   0.7597129344940186,\n",
       "   0.7962716817855835,\n",
       "   0.8701732754707336,\n",
       "   0.9074320793151855,\n",
       "   0.5364099144935608,\n",
       "   1.103999137878418,\n",
       "   1.1550503969192505,\n",
       "   1.3560069799423218,\n",
       "   0.7215349078178406,\n",
       "   1.1693470478057861,\n",
       "   0.9351305961608887,\n",
       "   1.7153310775756836,\n",
       "   0.8806575536727905,\n",
       "   0.894768476486206,\n",
       "   1.1508830785751343,\n",
       "   1.8232533931732178,\n",
       "   0.9847176671028137,\n",
       "   1.0359896421432495,\n",
       "   0.6164765954017639,\n",
       "   2.827063798904419,\n",
       "   1.1245065927505493,\n",
       "   0.34313249588012695,\n",
       "   0.802250862121582,\n",
       "   0.692003071308136,\n",
       "   1.443615198135376,\n",
       "   0.869139552116394,\n",
       "   1.20658278465271,\n",
       "   1.2733865976333618,\n",
       "   0.958255410194397,\n",
       "   0.8664673566818237,\n",
       "   0.4300503134727478,\n",
       "   0.4605838656425476,\n",
       "   1.2005000114440918,\n",
       "   1.3463819026947021,\n",
       "   0.8387665152549744,\n",
       "   1.110202431678772,\n",
       "   1.1187845468521118,\n",
       "   1.0637232065200806,\n",
       "   0.4382367730140686,\n",
       "   0.842512845993042,\n",
       "   1.3921960592269897,\n",
       "   1.3442198038101196,\n",
       "   0.8385160565376282,\n",
       "   0.7481510639190674,\n",
       "   0.9164004325866699,\n",
       "   0.48568445444107056,\n",
       "   0.9916783571243286,\n",
       "   0.9143572449684143,\n",
       "   1.1000152826309204],\n",
       "  ('euclidean', False, False): [0.7257487177848816,\n",
       "   0.48205432295799255,\n",
       "   0.6453565955162048,\n",
       "   0.9080066680908203,\n",
       "   1.0431921482086182,\n",
       "   1.3306705951690674,\n",
       "   0.649255096912384,\n",
       "   1.6442484855651855,\n",
       "   0.5934147834777832,\n",
       "   0.5867094993591309,\n",
       "   0.43995678424835205,\n",
       "   1.3869483470916748,\n",
       "   0.6030696034431458,\n",
       "   0.9649344086647034,\n",
       "   1.5422765016555786,\n",
       "   1.0626466274261475,\n",
       "   1.0510908365249634,\n",
       "   1.4301787614822388,\n",
       "   0.6895089745521545,\n",
       "   0.6156513690948486,\n",
       "   1.1313344240188599,\n",
       "   1.1040321588516235,\n",
       "   0.4386959373950958,\n",
       "   0.8524176478385925,\n",
       "   0.9346792697906494,\n",
       "   0.9129509329795837,\n",
       "   0.4987688958644867,\n",
       "   0.8825361132621765,\n",
       "   1.2383167743682861,\n",
       "   2.5871551036834717,\n",
       "   0.77365642786026,\n",
       "   1.0858441591262817,\n",
       "   0.6164363026618958,\n",
       "   1.279254674911499,\n",
       "   1.2508633136749268,\n",
       "   0.8089555501937866,\n",
       "   1.0042332410812378,\n",
       "   1.0065590143203735,\n",
       "   0.8503647446632385,\n",
       "   1.0562810897827148,\n",
       "   0.7780671119689941,\n",
       "   1.1336878538131714,\n",
       "   0.5636454820632935,\n",
       "   1.3105663061141968,\n",
       "   1.4611420631408691,\n",
       "   1.1808284521102905,\n",
       "   1.195667028427124,\n",
       "   0.5815336108207703,\n",
       "   1.6005656719207764,\n",
       "   1.0843843221664429,\n",
       "   0.6067209839820862,\n",
       "   1.0770057439804077,\n",
       "   1.0422873497009277,\n",
       "   0.6911743879318237,\n",
       "   0.6938118934631348,\n",
       "   1.1826199293136597,\n",
       "   1.3640635013580322,\n",
       "   1.493897557258606,\n",
       "   0.7111855149269104,\n",
       "   0.912358820438385,\n",
       "   0.8999414443969727,\n",
       "   0.5695912837982178,\n",
       "   0.9450628757476807,\n",
       "   1.2344353199005127,\n",
       "   0.7913607954978943,\n",
       "   1.1192787885665894,\n",
       "   0.8234758377075195,\n",
       "   0.9948785305023193,\n",
       "   0.923610508441925,\n",
       "   1.4395675659179688,\n",
       "   0.9309679269790649,\n",
       "   0.6560062170028687,\n",
       "   1.192036509513855,\n",
       "   1.4908514022827148,\n",
       "   1.1410999298095703,\n",
       "   0.864327609539032,\n",
       "   0.8453505635261536,\n",
       "   1.2181404829025269,\n",
       "   1.017237663269043,\n",
       "   0.6942553520202637,\n",
       "   1.2473770380020142,\n",
       "   1.002907395362854,\n",
       "   1.4598621129989624,\n",
       "   0.5177028179168701,\n",
       "   2.0648062229156494,\n",
       "   0.9366132616996765,\n",
       "   0.9300557374954224,\n",
       "   0.7579699754714966,\n",
       "   0.6214337944984436,\n",
       "   0.3868655264377594,\n",
       "   0.6410427689552307,\n",
       "   0.4720689058303833,\n",
       "   1.656075119972229,\n",
       "   1.1220730543136597,\n",
       "   0.9918795228004456,\n",
       "   0.7106990814208984,\n",
       "   0.5382604002952576,\n",
       "   1.2099303007125854,\n",
       "   1.168221354484558,\n",
       "   1.438991904258728,\n",
       "   0.8381471633911133,\n",
       "   1.31948983669281,\n",
       "   1.3628193140029907,\n",
       "   1.0932745933532715,\n",
       "   0.8255612254142761,\n",
       "   0.7787874341011047,\n",
       "   1.4877333641052246,\n",
       "   1.525119423866272,\n",
       "   0.7832344770431519,\n",
       "   0.8948136568069458,\n",
       "   0.6348063349723816,\n",
       "   0.7606815695762634,\n",
       "   0.5512709617614746,\n",
       "   0.8356380462646484,\n",
       "   1.542075276374817,\n",
       "   0.9063023924827576,\n",
       "   1.1206258535385132,\n",
       "   0.7633217573165894,\n",
       "   0.7927337884902954,\n",
       "   0.4515154957771301,\n",
       "   0.6996731758117676,\n",
       "   0.3985472619533539,\n",
       "   0.5369274616241455,\n",
       "   0.7849823236465454,\n",
       "   1.0302175283432007,\n",
       "   0.785457968711853,\n",
       "   1.0717445611953735,\n",
       "   0.5132135152816772,\n",
       "   1.3606637716293335,\n",
       "   1.342932105064392,\n",
       "   1.0504668951034546,\n",
       "   0.6440086364746094,\n",
       "   0.7752448916435242,\n",
       "   0.8309125304222107,\n",
       "   0.8248189687728882,\n",
       "   0.7354249954223633,\n",
       "   1.1891671419143677,\n",
       "   2.2094106674194336,\n",
       "   1.1759158372879028,\n",
       "   1.6128144264221191,\n",
       "   1.1716153621673584,\n",
       "   1.100764513015747,\n",
       "   0.6419044733047485,\n",
       "   0.42096585035324097,\n",
       "   1.043225884437561,\n",
       "   1.7701530456542969,\n",
       "   0.9607288837432861,\n",
       "   1.2393757104873657,\n",
       "   0.9456012845039368,\n",
       "   0.6518911123275757,\n",
       "   1.7037683725357056,\n",
       "   1.5310331583023071,\n",
       "   1.1297160387039185,\n",
       "   0.9771249890327454,\n",
       "   0.5789437294006348,\n",
       "   0.9413353800773621,\n",
       "   0.9940454959869385,\n",
       "   0.965997040271759,\n",
       "   0.8634175658226013,\n",
       "   0.9764233827590942,\n",
       "   1.0488852262496948,\n",
       "   1.043392300605774,\n",
       "   0.45280522108078003,\n",
       "   1.275053858757019,\n",
       "   0.9964560866355896,\n",
       "   0.6538158059120178,\n",
       "   1.1797515153884888,\n",
       "   0.6761077642440796,\n",
       "   1.5671682357788086,\n",
       "   0.7970518469810486,\n",
       "   1.640622615814209,\n",
       "   0.5490754246711731,\n",
       "   0.8063351511955261,\n",
       "   0.9365218281745911,\n",
       "   0.9950748085975647,\n",
       "   0.5332232117652893,\n",
       "   1.34232759475708,\n",
       "   1.2442657947540283,\n",
       "   1.19347083568573,\n",
       "   1.4412890672683716,\n",
       "   0.8713201284408569,\n",
       "   0.604810357093811,\n",
       "   0.6454970240592957,\n",
       "   0.8556984663009644,\n",
       "   1.341431736946106,\n",
       "   0.963072657585144,\n",
       "   1.2081379890441895,\n",
       "   0.6464024782180786,\n",
       "   0.25978147983551025,\n",
       "   0.34872111678123474,\n",
       "   0.47099021077156067,\n",
       "   1.3988522291183472,\n",
       "   0.2730385959148407,\n",
       "   1.165500521659851,\n",
       "   1.171708345413208,\n",
       "   1.6411159038543701,\n",
       "   0.7367734313011169,\n",
       "   1.0080136060714722,\n",
       "   1.109235167503357,\n",
       "   1.2672892808914185,\n",
       "   1.982731580734253,\n",
       "   1.2193665504455566,\n",
       "   1.6488854885101318,\n",
       "   0.9328491687774658,\n",
       "   0.6845386624336243,\n",
       "   1.3224234580993652,\n",
       "   1.497835397720337,\n",
       "   1.015506625175476,\n",
       "   1.6341511011123657,\n",
       "   0.6272203326225281,\n",
       "   0.7072553634643555,\n",
       "   0.876838743686676,\n",
       "   1.7932177782058716,\n",
       "   0.9687962532043457,\n",
       "   1.36543607711792,\n",
       "   1.0963053703308105,\n",
       "   1.0406444072723389,\n",
       "   1.2637754678726196,\n",
       "   0.576737642288208,\n",
       "   0.8327709436416626,\n",
       "   0.5941737294197083,\n",
       "   0.792121410369873,\n",
       "   0.6110135912895203,\n",
       "   1.1928561925888062,\n",
       "   0.44321635365486145,\n",
       "   0.4538455009460449,\n",
       "   0.5647526383399963,\n",
       "   0.7388958930969238,\n",
       "   0.7403479218482971,\n",
       "   0.643028974533081,\n",
       "   1.4245802164077759,\n",
       "   1.161844253540039,\n",
       "   0.42397341132164,\n",
       "   0.6809793710708618,\n",
       "   0.5955188274383545,\n",
       "   0.7923254370689392,\n",
       "   2.2372446060180664,\n",
       "   0.4126182496547699,\n",
       "   1.1152452230453491,\n",
       "   0.6179642677307129,\n",
       "   1.146822452545166,\n",
       "   0.8110905289649963,\n",
       "   1.1930303573608398,\n",
       "   1.0872180461883545,\n",
       "   0.5727230906486511,\n",
       "   1.648144006729126,\n",
       "   0.892006516456604,\n",
       "   0.9278247952461243,\n",
       "   1.2634285688400269,\n",
       "   1.1382315158843994,\n",
       "   1.164921760559082,\n",
       "   1.1955839395523071,\n",
       "   0.6043505668640137,\n",
       "   0.9090730547904968,\n",
       "   0.6465762257575989,\n",
       "   1.1243717670440674,\n",
       "   1.020896553993225,\n",
       "   0.816681981086731,\n",
       "   1.1063755750656128,\n",
       "   0.4788944125175476,\n",
       "   0.9947536587715149,\n",
       "   0.34769895672798157,\n",
       "   0.7034338712692261,\n",
       "   1.0568383932113647,\n",
       "   1.0179845094680786,\n",
       "   0.9754185676574707,\n",
       "   0.6649101376533508,\n",
       "   0.697602391242981,\n",
       "   1.5017253160476685,\n",
       "   0.7760801911354065,\n",
       "   1.5231622457504272,\n",
       "   0.6447684168815613,\n",
       "   0.4491470158100128,\n",
       "   1.451282262802124,\n",
       "   0.9304625391960144,\n",
       "   1.3593099117279053,\n",
       "   0.9579864740371704,\n",
       "   1.2174181938171387,\n",
       "   0.9186320304870605,\n",
       "   0.7446802854537964,\n",
       "   1.417517900466919,\n",
       "   0.7177698612213135,\n",
       "   1.6211344003677368,\n",
       "   0.6958662867546082,\n",
       "   0.7930017709732056,\n",
       "   1.283183217048645,\n",
       "   1.3519737720489502,\n",
       "   0.9222463369369507,\n",
       "   0.6032286286354065,\n",
       "   0.5741687417030334,\n",
       "   0.781254231929779,\n",
       "   0.6605789065361023,\n",
       "   0.7612675428390503,\n",
       "   1.4178112745285034,\n",
       "   1.840222954750061,\n",
       "   1.0875831842422485,\n",
       "   0.7252652645111084,\n",
       "   1.1817563772201538,\n",
       "   1.0142691135406494,\n",
       "   0.5785868167877197,\n",
       "   1.829912543296814,\n",
       "   0.28376802802085876,\n",
       "   0.3748968243598938,\n",
       "   1.5174459218978882,\n",
       "   1.5463314056396484,\n",
       "   0.9200019240379333,\n",
       "   1.5631721019744873,\n",
       "   0.5917835831642151,\n",
       "   1.4690849781036377,\n",
       "   1.0884578227996826,\n",
       "   0.9366722702980042,\n",
       "   1.6273701190948486,\n",
       "   1.3786115646362305,\n",
       "   1.140499472618103,\n",
       "   0.7740903496742249,\n",
       "   0.9076623916625977,\n",
       "   0.6421217918395996,\n",
       "   1.249083161354065,\n",
       "   1.225487470626831,\n",
       "   1.2653918266296387,\n",
       "   1.8178457021713257,\n",
       "   1.1249363422393799,\n",
       "   0.6867389678955078,\n",
       "   0.560429573059082,\n",
       "   2.0167670249938965,\n",
       "   1.4307663440704346,\n",
       "   0.5021461844444275,\n",
       "   1.2094981670379639,\n",
       "   1.4329229593276978,\n",
       "   1.107998251914978,\n",
       "   0.9203616380691528,\n",
       "   0.713965654373169,\n",
       "   1.072016954421997,\n",
       "   1.3169385194778442,\n",
       "   0.8979415893554688,\n",
       "   0.9872974753379822,\n",
       "   0.8364834785461426,\n",
       "   1.255110263824463,\n",
       "   0.7304087281227112,\n",
       "   1.0312330722808838,\n",
       "   1.2942006587982178,\n",
       "   1.1514850854873657,\n",
       "   1.8127503395080566,\n",
       "   1.0624040365219116,\n",
       "   0.931770920753479,\n",
       "   0.35358235239982605,\n",
       "   0.9595180749893188,\n",
       "   1.1443848609924316,\n",
       "   2.0205211639404297,\n",
       "   1.0090618133544922,\n",
       "   1.4040812253952026,\n",
       "   0.8867432475090027,\n",
       "   1.2310478687286377,\n",
       "   1.7280856370925903,\n",
       "   0.7631276249885559,\n",
       "   1.581282377243042,\n",
       "   1.1534335613250732,\n",
       "   0.6925910711288452,\n",
       "   0.6505306959152222,\n",
       "   1.3700772523880005,\n",
       "   1.102929949760437,\n",
       "   0.9501767158508301,\n",
       "   0.7756385207176208,\n",
       "   0.6444460153579712,\n",
       "   1.037032127380371,\n",
       "   1.794566035270691,\n",
       "   0.801369845867157,\n",
       "   0.7919710874557495,\n",
       "   1.0025311708450317,\n",
       "   1.0889281034469604,\n",
       "   0.637024462223053,\n",
       "   0.9124682545661926,\n",
       "   1.087082028388977,\n",
       "   1.1566470861434937,\n",
       "   0.5942609906196594,\n",
       "   2.0786585807800293,\n",
       "   1.2707759141921997,\n",
       "   1.0509799718856812,\n",
       "   0.6289880871772766,\n",
       "   0.9604330062866211,\n",
       "   0.9066869020462036,\n",
       "   0.6145413517951965,\n",
       "   0.5533517003059387,\n",
       "   1.277924656867981,\n",
       "   0.928317129611969,\n",
       "   1.7154690027236938,\n",
       "   0.9683672189712524,\n",
       "   1.0222728252410889,\n",
       "   1.9059487581253052,\n",
       "   1.3866591453552246,\n",
       "   0.9440883994102478,\n",
       "   0.6311006546020508,\n",
       "   1.119391679763794,\n",
       "   0.7548037767410278,\n",
       "   0.9048622250556946,\n",
       "   0.6103817820549011,\n",
       "   0.8329412341117859,\n",
       "   0.4758192002773285,\n",
       "   0.9061571955680847,\n",
       "   0.9482629895210266,\n",
       "   0.7546156048774719,\n",
       "   0.6454989910125732,\n",
       "   0.8043586015701294,\n",
       "   0.7196279168128967,\n",
       "   1.3718829154968262,\n",
       "   0.6889867782592773,\n",
       "   1.1319166421890259,\n",
       "   0.9630756378173828,\n",
       "   0.3502277135848999,\n",
       "   0.7651064991950989,\n",
       "   0.6138139367103577,\n",
       "   0.841385543346405,\n",
       "   1.2148340940475464,\n",
       "   0.9212376475334167,\n",
       "   1.4479353427886963,\n",
       "   0.7736963629722595,\n",
       "   0.7380043864250183,\n",
       "   0.5547738671302795,\n",
       "   0.8278443813323975,\n",
       "   1.629616379737854,\n",
       "   1.0055822134017944,\n",
       "   0.5748178362846375,\n",
       "   0.64415442943573,\n",
       "   0.4774395227432251,\n",
       "   1.0748233795166016,\n",
       "   1.1335535049438477,\n",
       "   0.6429732441902161,\n",
       "   1.9331086874008179,\n",
       "   1.1750413179397583,\n",
       "   1.0326603651046753,\n",
       "   0.8991049528121948,\n",
       "   0.9223732948303223,\n",
       "   2.0839545726776123,\n",
       "   0.6165514588356018,\n",
       "   1.1520044803619385,\n",
       "   1.3085033893585205,\n",
       "   1.2564064264297485,\n",
       "   0.20116282999515533,\n",
       "   1.2410469055175781,\n",
       "   0.9588341116905212,\n",
       "   1.05350661277771,\n",
       "   0.8109923601150513,\n",
       "   0.5557007193565369,\n",
       "   1.7968000173568726,\n",
       "   1.3795267343521118,\n",
       "   0.4347276985645294,\n",
       "   0.9455733299255371,\n",
       "   0.41738584637641907,\n",
       "   1.283168077468872,\n",
       "   1.3364779949188232,\n",
       "   1.3712602853775024,\n",
       "   0.7102882266044617,\n",
       "   0.48652854561805725,\n",
       "   0.5709663033485413,\n",
       "   0.48168787360191345,\n",
       "   1.0267170667648315,\n",
       "   1.483940601348877,\n",
       "   1.24919855594635,\n",
       "   1.34364914894104,\n",
       "   0.9621988534927368,\n",
       "   1.553194522857666,\n",
       "   1.117512583732605,\n",
       "   0.4328037202358246,\n",
       "   0.6648975610733032,\n",
       "   0.7876068353652954,\n",
       "   0.8740829229354858,\n",
       "   0.9074321389198303,\n",
       "   0.5490792989730835,\n",
       "   1.103999137878418,\n",
       "   1.1550506353378296,\n",
       "   1.2247322797775269,\n",
       "   0.7215348482131958,\n",
       "   0.9643011093139648,\n",
       "   0.7939773201942444,\n",
       "   1.548844337463379,\n",
       "   1.2077202796936035,\n",
       "   0.8947685360908508,\n",
       "   1.1503162384033203,\n",
       "   1.8232530355453491,\n",
       "   1.0139648914337158,\n",
       "   1.03598952293396,\n",
       "   0.6164764761924744,\n",
       "   2.9112658500671387,\n",
       "   1.139655351638794,\n",
       "   0.25059351325035095,\n",
       "   0.8022508025169373,\n",
       "   0.6920030117034912,\n",
       "   1.443615198135376,\n",
       "   0.8691397905349731,\n",
       "   1.425360083580017,\n",
       "   0.9493547677993774,\n",
       "   0.9582555294036865,\n",
       "   0.804008960723877,\n",
       "   0.4300503730773926,\n",
       "   0.4605838656425476,\n",
       "   1.5516035556793213,\n",
       "   1.10513436794281,\n",
       "   0.49678835272789,\n",
       "   1.1102025508880615,\n",
       "   0.968582034111023,\n",
       "   1.1378371715545654,\n",
       "   0.7665537595748901,\n",
       "   0.8033029437065125,\n",
       "   1.4225510358810425,\n",
       "   1.34421968460083,\n",
       "   0.838516354560852,\n",
       "   0.7481507658958435,\n",
       "   0.585038959980011,\n",
       "   0.48568445444107056,\n",
       "   1.0735268592834473,\n",
       "   0.9143571853637695,\n",
       "   1.3906406164169312]}}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"ratio_results.pkl\"\n",
    "if not os.path.exists(path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(ratio_results, f)\n",
    "else:\n",
    "    with open(path, 'rb') as f:\n",
    "        ratio_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('cosine', True, True): [1.215024709701538,\n",
       "  1.1801878213882446,\n",
       "  1.5343092679977417,\n",
       "  0.9335689544677734,\n",
       "  1.3937071561813354,\n",
       "  0.9440323710441589,\n",
       "  1.2586134672164917,\n",
       "  1.456436038017273,\n",
       "  0.9222041964530945,\n",
       "  1.7389726638793945,\n",
       "  1.6034759283065796,\n",
       "  1.3851253986358643,\n",
       "  1.064948558807373,\n",
       "  0.9437777400016785,\n",
       "  1.535832405090332,\n",
       "  0.9963194727897644,\n",
       "  1.5824706554412842,\n",
       "  1.024376392364502,\n",
       "  0.9975019693374634,\n",
       "  1.5580897331237793,\n",
       "  0.9825649261474609,\n",
       "  1.0361788272857666,\n",
       "  0.975216805934906,\n",
       "  1.615065097808838,\n",
       "  1.5632058382034302,\n",
       "  1.5235546827316284,\n",
       "  1.2201893329620361,\n",
       "  1.0456717014312744,\n",
       "  0.9650365710258484,\n",
       "  0.9124303460121155,\n",
       "  1.270965814590454,\n",
       "  1.0830395221710205,\n",
       "  1.3693957328796387,\n",
       "  1.6603580713272095,\n",
       "  1.536556601524353,\n",
       "  1.25315523147583,\n",
       "  1.6403770446777344,\n",
       "  1.399306297302246,\n",
       "  0.9636114835739136,\n",
       "  0.9664019346237183,\n",
       "  1.7461392879486084,\n",
       "  1.0109235048294067,\n",
       "  1.0111559629440308,\n",
       "  1.0289230346679688,\n",
       "  1.033232569694519,\n",
       "  1.5856809616088867,\n",
       "  1.5556163787841797,\n",
       "  1.0840706825256348,\n",
       "  0.9174164533615112,\n",
       "  0.9765291810035706,\n",
       "  1.5553375482559204,\n",
       "  1.6512117385864258,\n",
       "  0.9172935485839844,\n",
       "  1.0672276020050049,\n",
       "  1.4702306985855103,\n",
       "  1.0043152570724487,\n",
       "  0.9427614808082581,\n",
       "  1.444201111793518,\n",
       "  1.2373621463775635,\n",
       "  1.0260041952133179,\n",
       "  1.634823203086853,\n",
       "  1.0046159029006958,\n",
       "  0.9910733699798584,\n",
       "  1.0599061250686646,\n",
       "  1.4243083000183105,\n",
       "  1.0294798612594604,\n",
       "  1.4967061281204224,\n",
       "  0.9821077585220337,\n",
       "  0.954333484172821,\n",
       "  1.0298042297363281,\n",
       "  1.0658373832702637,\n",
       "  1.5040290355682373,\n",
       "  0.9799648523330688,\n",
       "  1.5588816404342651,\n",
       "  1.4514578580856323,\n",
       "  1.0052011013031006,\n",
       "  1.6740214824676514,\n",
       "  1.6875942945480347,\n",
       "  1.3646769523620605,\n",
       "  1.4602471590042114,\n",
       "  0.9992771148681641,\n",
       "  0.9961969256401062,\n",
       "  0.8880087733268738,\n",
       "  1.1544687747955322,\n",
       "  0.9357317686080933,\n",
       "  1.027686357498169,\n",
       "  1.5949811935424805,\n",
       "  1.0754565000534058,\n",
       "  1.6031770706176758,\n",
       "  1.2161437273025513,\n",
       "  1.7884362936019897,\n",
       "  1.0193766355514526,\n",
       "  0.9611062407493591,\n",
       "  1.6424155235290527,\n",
       "  1.0105451345443726,\n",
       "  0.9897129535675049,\n",
       "  1.5712571144104004,\n",
       "  0.8639070987701416,\n",
       "  1.2198729515075684,\n",
       "  1.4108673334121704,\n",
       "  1.0240814685821533,\n",
       "  1.8195825815200806,\n",
       "  1.5826646089553833,\n",
       "  0.9912734031677246,\n",
       "  0.9414337873458862,\n",
       "  0.9266393184661865,\n",
       "  0.9672094583511353,\n",
       "  1.7844548225402832,\n",
       "  1.502553105354309,\n",
       "  1.6601004600524902,\n",
       "  1.275862693786621,\n",
       "  1.5921235084533691,\n",
       "  0.959517776966095,\n",
       "  0.9721444249153137,\n",
       "  1.5796929597854614,\n",
       "  1.2984871864318848,\n",
       "  1.4517372846603394,\n",
       "  1.145400047302246,\n",
       "  1.0954457521438599,\n",
       "  1.6103730201721191,\n",
       "  1.6546175479888916,\n",
       "  1.834252119064331,\n",
       "  1.2227760553359985,\n",
       "  0.9893301129341125,\n",
       "  0.8850424289703369,\n",
       "  1.0636379718780518,\n",
       "  1.4014708995819092,\n",
       "  1.4967138767242432,\n",
       "  1.5230703353881836,\n",
       "  1.622737169265747,\n",
       "  0.9952871203422546,\n",
       "  0.9956913590431213,\n",
       "  0.9300427436828613,\n",
       "  1.3171435594558716,\n",
       "  1.0244718790054321,\n",
       "  1.0183824300765991,\n",
       "  1.5561782121658325,\n",
       "  0.9401514530181885,\n",
       "  0.9055017828941345,\n",
       "  1.5095000267028809,\n",
       "  0.9390719532966614,\n",
       "  1.4726989269256592,\n",
       "  1.6805918216705322,\n",
       "  1.147100806236267,\n",
       "  0.9146212935447693,\n",
       "  0.9835090637207031,\n",
       "  1.5451968908309937,\n",
       "  1.5528463125228882,\n",
       "  1.5728394985198975,\n",
       "  1.0471082925796509,\n",
       "  1.0008535385131836,\n",
       "  1.5761851072311401,\n",
       "  0.9772958159446716,\n",
       "  1.5577458143234253,\n",
       "  1.5593878030776978,\n",
       "  0.9700985550880432,\n",
       "  0.9927766919136047,\n",
       "  0.9736664891242981,\n",
       "  0.9520086646080017,\n",
       "  0.9779797792434692,\n",
       "  1.0688261985778809,\n",
       "  1.5707679986953735,\n",
       "  1.728203535079956,\n",
       "  1.0365440845489502,\n",
       "  0.9236395359039307,\n",
       "  1.0314486026763916,\n",
       "  0.9858591556549072,\n",
       "  1.0523009300231934,\n",
       "  0.9844624400138855,\n",
       "  0.8979872465133667,\n",
       "  1.520505666732788,\n",
       "  0.9428144693374634,\n",
       "  1.0827891826629639,\n",
       "  0.9706099033355713,\n",
       "  0.9633612036705017,\n",
       "  0.9184370040893555,\n",
       "  1.0117616653442383,\n",
       "  1.5569227933883667,\n",
       "  1.4556940793991089,\n",
       "  1.0058842897415161,\n",
       "  1.5265393257141113,\n",
       "  1.4279769659042358,\n",
       "  1.02170729637146,\n",
       "  1.3188592195510864,\n",
       "  0.9193068146705627,\n",
       "  1.1883771419525146,\n",
       "  0.9719161987304688,\n",
       "  1.162802815437317,\n",
       "  1.1085634231567383,\n",
       "  1.0742318630218506,\n",
       "  1.7783232927322388,\n",
       "  0.9617831110954285,\n",
       "  1.913895845413208,\n",
       "  0.8835693001747131,\n",
       "  0.9726011157035828,\n",
       "  1.5268502235412598,\n",
       "  1.8085238933563232,\n",
       "  1.0696223974227905,\n",
       "  1.5994945764541626,\n",
       "  1.6315592527389526,\n",
       "  0.8313369154930115,\n",
       "  1.1707359552383423,\n",
       "  1.5051119327545166,\n",
       "  1.3198518753051758,\n",
       "  1.0116735696792603,\n",
       "  0.9268602728843689,\n",
       "  1.5767357349395752,\n",
       "  1.0071314573287964,\n",
       "  0.9958072900772095,\n",
       "  1.5773133039474487,\n",
       "  1.034435510635376,\n",
       "  1.0158355236053467,\n",
       "  0.9252803325653076,\n",
       "  1.0765998363494873,\n",
       "  1.4428085088729858,\n",
       "  1.6080708503723145,\n",
       "  0.9536820650100708,\n",
       "  1.5956039428710938,\n",
       "  1.0839077234268188,\n",
       "  1.046086311340332,\n",
       "  1.0567079782485962,\n",
       "  0.9725559949874878,\n",
       "  0.9709985852241516,\n",
       "  0.9425356388092041,\n",
       "  1.1355173587799072,\n",
       "  0.9970365166664124,\n",
       "  1.6123262643814087,\n",
       "  0.9491892457008362,\n",
       "  1.7078107595443726,\n",
       "  1.0994008779525757,\n",
       "  1.7232671976089478,\n",
       "  1.01065993309021,\n",
       "  1.0889872312545776,\n",
       "  1.5573203563690186,\n",
       "  0.9874385595321655,\n",
       "  1.5860685110092163,\n",
       "  0.9238314032554626,\n",
       "  1.7573201656341553,\n",
       "  1.3027074337005615,\n",
       "  1.7461392879486084,\n",
       "  1.0556707382202148,\n",
       "  1.2072051763534546,\n",
       "  1.495665431022644,\n",
       "  1.5661208629608154,\n",
       "  1.5345962047576904,\n",
       "  0.9723284840583801,\n",
       "  0.9970983862876892,\n",
       "  0.9586696028709412,\n",
       "  1.1391940116882324,\n",
       "  0.8847947716712952,\n",
       "  0.9623484015464783,\n",
       "  1.0930156707763672,\n",
       "  1.343626856803894,\n",
       "  1.5206916332244873,\n",
       "  1.5623196363449097,\n",
       "  0.96750408411026,\n",
       "  0.9987722635269165,\n",
       "  0.9911842942237854,\n",
       "  1.551658272743225,\n",
       "  0.9923023581504822,\n",
       "  1.3646769523620605,\n",
       "  2.046804189682007,\n",
       "  1.0536959171295166,\n",
       "  1.0517990589141846,\n",
       "  1.529292345046997,\n",
       "  1.5909336805343628,\n",
       "  0.9439467191696167,\n",
       "  1.5084030628204346,\n",
       "  1.4978573322296143,\n",
       "  1.0477920770645142,\n",
       "  0.9573593735694885,\n",
       "  1.7214757204055786,\n",
       "  1.8181055784225464,\n",
       "  0.8700271248817444,\n",
       "  1.054687738418579,\n",
       "  0.9194582104682922,\n",
       "  1.0092291831970215,\n",
       "  0.9402467608451843,\n",
       "  1.722203016281128,\n",
       "  1.0563527345657349,\n",
       "  1.3851253986358643,\n",
       "  1.0251036882400513,\n",
       "  1.7086403369903564,\n",
       "  0.9155793786048889,\n",
       "  1.0698461532592773,\n",
       "  0.9923543334007263,\n",
       "  0.9741048812866211,\n",
       "  1.107139229774475,\n",
       "  0.9337311387062073,\n",
       "  1.566564917564392,\n",
       "  1.6928960084915161,\n",
       "  1.5394642353057861,\n",
       "  1.6078734397888184,\n",
       "  0.9348987936973572,\n",
       "  1.5359539985656738,\n",
       "  0.9559752345085144,\n",
       "  1.5860786437988281,\n",
       "  1.4731545448303223,\n",
       "  1.5939860343933105,\n",
       "  1.0404390096664429,\n",
       "  1.1439114809036255,\n",
       "  1.7927926778793335,\n",
       "  1.0630412101745605,\n",
       "  0.818041980266571,\n",
       "  0.95777827501297,\n",
       "  1.3362047672271729,\n",
       "  1.0464574098587036,\n",
       "  1.5848993062973022,\n",
       "  0.9579952359199524,\n",
       "  1.0669771432876587,\n",
       "  0.932231068611145,\n",
       "  0.9786744713783264,\n",
       "  0.9723433256149292,\n",
       "  0.9551302790641785,\n",
       "  1.3312560319900513,\n",
       "  1.06786048412323,\n",
       "  0.9573044180870056,\n",
       "  0.9588267207145691,\n",
       "  1.0071139335632324,\n",
       "  0.9554629325866699,\n",
       "  1.434101939201355,\n",
       "  1.6437431573867798,\n",
       "  0.9985105991363525,\n",
       "  1.8275765180587769,\n",
       "  0.9389759302139282,\n",
       "  1.5810036659240723,\n",
       "  0.9966833591461182,\n",
       "  1.5112062692642212,\n",
       "  1.5534536838531494,\n",
       "  0.947834312915802,\n",
       "  0.9296550154685974,\n",
       "  1.0699639320373535,\n",
       "  1.4191713333129883,\n",
       "  0.9664839506149292,\n",
       "  0.9668486714363098,\n",
       "  0.9346640706062317,\n",
       "  0.8814444541931152,\n",
       "  1.3437786102294922,\n",
       "  1.621201753616333,\n",
       "  1.3522356748580933,\n",
       "  1.5598000288009644,\n",
       "  0.938273549079895,\n",
       "  1.5652663707733154,\n",
       "  1.0212743282318115,\n",
       "  1.6118615865707397,\n",
       "  1.2283681631088257,\n",
       "  1.1825312376022339,\n",
       "  1.0126757621765137,\n",
       "  1.5747089385986328,\n",
       "  1.7858110666275024,\n",
       "  0.9422730803489685,\n",
       "  0.9793405532836914,\n",
       "  1.5795763731002808,\n",
       "  0.9836630821228027,\n",
       "  0.9943992495536804,\n",
       "  1.55598783493042,\n",
       "  1.5899325609207153,\n",
       "  1.7912931442260742,\n",
       "  0.9637202024459839,\n",
       "  1.5527198314666748,\n",
       "  1.567886471748352,\n",
       "  1.5376360416412354,\n",
       "  1.0355671644210815,\n",
       "  1.7077492475509644,\n",
       "  1.4685697555541992,\n",
       "  1.370864987373352,\n",
       "  1.0707136392593384,\n",
       "  1.3300079107284546,\n",
       "  1.070365071296692,\n",
       "  0.9838764667510986,\n",
       "  1.1278506517410278,\n",
       "  1.0773167610168457,\n",
       "  1.479321002960205,\n",
       "  1.0770305395126343,\n",
       "  1.8026010990142822,\n",
       "  0.8667320013046265,\n",
       "  0.9677326679229736,\n",
       "  1.0179232358932495,\n",
       "  1.6143200397491455,\n",
       "  0.9577766060829163,\n",
       "  1.25900137424469,\n",
       "  1.005363941192627,\n",
       "  1.6055766344070435,\n",
       "  1.0282753705978394,\n",
       "  1.118257999420166,\n",
       "  0.9603552222251892,\n",
       "  1.663164734840393,\n",
       "  1.1118768453598022,\n",
       "  0.9213732481002808,\n",
       "  1.5259181261062622,\n",
       "  1.6061012744903564,\n",
       "  1.5567725896835327,\n",
       "  1.343626856803894,\n",
       "  1.0595083236694336,\n",
       "  0.9147692918777466,\n",
       "  1.0890177488327026,\n",
       "  1.628177523612976,\n",
       "  1.0149226188659668,\n",
       "  1.0777027606964111,\n",
       "  0.969658374786377,\n",
       "  1.296104907989502,\n",
       "  1.2048462629318237,\n",
       "  1.1022452116012573,\n",
       "  1.0171712636947632,\n",
       "  1.6015387773513794,\n",
       "  0.9917083978652954,\n",
       "  0.8999290466308594,\n",
       "  1.5886014699935913,\n",
       "  1.0556042194366455,\n",
       "  1.2461496591567993,\n",
       "  1.454301118850708,\n",
       "  1.6267292499542236,\n",
       "  0.9998563528060913,\n",
       "  1.0013877153396606,\n",
       "  1.6748417615890503,\n",
       "  0.8771500587463379,\n",
       "  1.1076829433441162,\n",
       "  1.8113682270050049,\n",
       "  1.6093310117721558,\n",
       "  1.4436798095703125,\n",
       "  1.5361634492874146,\n",
       "  1.895896315574646,\n",
       "  1.030733585357666,\n",
       "  1.4753323793411255,\n",
       "  1.4714807271957397,\n",
       "  1.095598816871643,\n",
       "  1.125461459159851,\n",
       "  1.4688677787780762,\n",
       "  1.7121362686157227,\n",
       "  1.5615627765655518,\n",
       "  1.4228187799453735,\n",
       "  1.0334559679031372,\n",
       "  1.5902096033096313,\n",
       "  0.9615867137908936,\n",
       "  0.9446974396705627,\n",
       "  1.3138728141784668,\n",
       "  1.0229949951171875,\n",
       "  1.13666570186615,\n",
       "  0.9698854684829712,\n",
       "  1.6411455869674683,\n",
       "  1.5409624576568604,\n",
       "  1.014225721359253,\n",
       "  1.6950355768203735,\n",
       "  0.9868803024291992,\n",
       "  1.5933128595352173,\n",
       "  1.7421373128890991,\n",
       "  1.2764681577682495,\n",
       "  1.030734896659851,\n",
       "  1.0470004081726074,\n",
       "  1.4727492332458496,\n",
       "  1.0330044031143188,\n",
       "  1.6037094593048096,\n",
       "  1.8829319477081299,\n",
       "  1.7029685974121094,\n",
       "  1.811219573020935,\n",
       "  1.4442813396453857,\n",
       "  0.9678835272789001,\n",
       "  0.9365245699882507,\n",
       "  1.0038974285125732,\n",
       "  0.9812369346618652,\n",
       "  1.6316829919815063,\n",
       "  1.051087498664856,\n",
       "  1.5681016445159912,\n",
       "  1.1105877161026,\n",
       "  1.0384751558303833,\n",
       "  1.5994837284088135,\n",
       "  1.4240925312042236,\n",
       "  1.097865343093872,\n",
       "  0.9362594485282898,\n",
       "  1.6854625940322876,\n",
       "  0.9158271551132202,\n",
       "  1.572202205657959,\n",
       "  1.034712314605713,\n",
       "  1.012986421585083,\n",
       "  0.9989782571792603,\n",
       "  0.9942393898963928,\n",
       "  1.6875288486480713,\n",
       "  0.950086772441864,\n",
       "  0.8928606510162354,\n",
       "  1.208999752998352,\n",
       "  1.1477075815200806,\n",
       "  1.3787678480148315,\n",
       "  0.9811475276947021,\n",
       "  1.006426215171814,\n",
       "  1.0883256196975708,\n",
       "  1.126257300376892,\n",
       "  1.1691001653671265,\n",
       "  0.9554484486579895,\n",
       "  1.005495548248291,\n",
       "  1.0383546352386475,\n",
       "  0.9683875441551208,\n",
       "  0.9501832723617554,\n",
       "  1.0139095783233643,\n",
       "  1.1113260984420776,\n",
       "  0.9765726923942566,\n",
       "  0.9393401145935059,\n",
       "  1.6764520406723022,\n",
       "  1.0189732313156128,\n",
       "  1.0004942417144775,\n",
       "  1.0823004245758057,\n",
       "  0.970511794090271,\n",
       "  1.6043062210083008,\n",
       "  1.0459802150726318,\n",
       "  1.062394142150879,\n",
       "  1.4220041036605835,\n",
       "  1.4742629528045654,\n",
       "  1.0105730295181274,\n",
       "  0.9447066187858582,\n",
       "  1.0037775039672852,\n",
       "  1.0824410915374756,\n",
       "  1.6311078071594238,\n",
       "  0.9544586539268494],\n",
       " ('cosine', True, False): [1.215024709701538,\n",
       "  1.1801878213882446,\n",
       "  1.5343092679977417,\n",
       "  0.903221845626831,\n",
       "  1.0117932558059692,\n",
       "  0.9683955907821655,\n",
       "  1.0416202545166016,\n",
       "  0.8878309726715088,\n",
       "  0.9223654270172119,\n",
       "  1.7389726638793945,\n",
       "  1.6034759283065796,\n",
       "  1.3851255178451538,\n",
       "  1.064948558807373,\n",
       "  0.943777859210968,\n",
       "  1.5358322858810425,\n",
       "  0.9963194727897644,\n",
       "  0.9975734949111938,\n",
       "  1.024376392364502,\n",
       "  0.9975019097328186,\n",
       "  1.5580897331237793,\n",
       "  0.9825649857521057,\n",
       "  1.0361788272857666,\n",
       "  0.9752167463302612,\n",
       "  1.615065097808838,\n",
       "  1.5632058382034302,\n",
       "  1.5235546827316284,\n",
       "  1.0856753587722778,\n",
       "  1.0456717014312744,\n",
       "  0.9641268849372864,\n",
       "  0.8678069710731506,\n",
       "  1.2709660530090332,\n",
       "  1.0830395221710205,\n",
       "  1.3693957328796387,\n",
       "  1.6603580713272095,\n",
       "  1.536556601524353,\n",
       "  1.2531551122665405,\n",
       "  1.640377163887024,\n",
       "  1.399306297302246,\n",
       "  0.9636114835739136,\n",
       "  0.9851092100143433,\n",
       "  1.7461392879486084,\n",
       "  1.0109235048294067,\n",
       "  1.0295262336730957,\n",
       "  0.9273030161857605,\n",
       "  1.0332326889038086,\n",
       "  1.5856810808181763,\n",
       "  1.5556163787841797,\n",
       "  1.0840706825256348,\n",
       "  0.8737190961837769,\n",
       "  1.564797043800354,\n",
       "  1.5553375482559204,\n",
       "  1.6512117385864258,\n",
       "  0.8998662233352661,\n",
       "  1.0672276020050049,\n",
       "  1.4702306985855103,\n",
       "  0.9774971008300781,\n",
       "  0.9427614808082581,\n",
       "  1.4442009925842285,\n",
       "  0.9271070957183838,\n",
       "  1.0192872285842896,\n",
       "  1.634823203086853,\n",
       "  0.9801622033119202,\n",
       "  0.991073489189148,\n",
       "  1.015936017036438,\n",
       "  1.4243083000183105,\n",
       "  1.0275403261184692,\n",
       "  1.4967058897018433,\n",
       "  0.9915468096733093,\n",
       "  0.9211553335189819,\n",
       "  1.0974080562591553,\n",
       "  1.0018409490585327,\n",
       "  0.9221382141113281,\n",
       "  1.0466272830963135,\n",
       "  1.5588816404342651,\n",
       "  0.9600731730461121,\n",
       "  0.9318429827690125,\n",
       "  1.674021601676941,\n",
       "  1.0257855653762817,\n",
       "  0.9658502340316772,\n",
       "  1.4602470397949219,\n",
       "  0.9992771148681641,\n",
       "  1.5539908409118652,\n",
       "  0.8440041542053223,\n",
       "  1.0507971048355103,\n",
       "  0.9149311780929565,\n",
       "  1.0276864767074585,\n",
       "  1.5949811935424805,\n",
       "  1.0754565000534058,\n",
       "  1.6031770706176758,\n",
       "  1.2234885692596436,\n",
       "  1.7884362936019897,\n",
       "  1.0089640617370605,\n",
       "  0.9611062407493591,\n",
       "  1.6424155235290527,\n",
       "  1.010545015335083,\n",
       "  0.9897129535675049,\n",
       "  1.57125723361969,\n",
       "  0.8639071583747864,\n",
       "  1.0277882814407349,\n",
       "  1.1036406755447388,\n",
       "  1.0240815877914429,\n",
       "  1.8195827007293701,\n",
       "  1.5826643705368042,\n",
       "  0.9822062253952026,\n",
       "  0.9414340257644653,\n",
       "  0.9230279326438904,\n",
       "  0.9228522181510925,\n",
       "  1.7844550609588623,\n",
       "  1.502553105354309,\n",
       "  1.6601004600524902,\n",
       "  1.275862693786621,\n",
       "  1.5921236276626587,\n",
       "  0.9559922814369202,\n",
       "  0.9684054851531982,\n",
       "  1.5796929597854614,\n",
       "  1.2984871864318848,\n",
       "  1.4517372846603394,\n",
       "  1.145400047302246,\n",
       "  1.0954457521438599,\n",
       "  0.9910772442817688,\n",
       "  1.6546175479888916,\n",
       "  1.834252119064331,\n",
       "  1.0420156717300415,\n",
       "  0.986642599105835,\n",
       "  0.8833627104759216,\n",
       "  1.0636379718780518,\n",
       "  1.4014707803726196,\n",
       "  1.4967141151428223,\n",
       "  1.5230703353881836,\n",
       "  0.9376459717750549,\n",
       "  0.9952870011329651,\n",
       "  0.9803332686424255,\n",
       "  0.9300427436828613,\n",
       "  1.3171435594558716,\n",
       "  0.9876964688301086,\n",
       "  0.945036768913269,\n",
       "  1.00873863697052,\n",
       "  0.937954306602478,\n",
       "  0.9055016040802002,\n",
       "  1.5095000267028809,\n",
       "  0.9390718340873718,\n",
       "  0.8953657746315002,\n",
       "  1.6805918216705322,\n",
       "  1.1683192253112793,\n",
       "  0.9146212935447693,\n",
       "  0.9299865365028381,\n",
       "  1.5451968908309937,\n",
       "  0.9355311393737793,\n",
       "  1.5728394985198975,\n",
       "  1.0471082925796509,\n",
       "  0.9455510973930359,\n",
       "  0.9812451004981995,\n",
       "  0.9717985987663269,\n",
       "  1.5577458143234253,\n",
       "  1.559388279914856,\n",
       "  0.9583392143249512,\n",
       "  0.9927766919136047,\n",
       "  0.9810827374458313,\n",
       "  0.8767619132995605,\n",
       "  0.9344195127487183,\n",
       "  0.9833101630210876,\n",
       "  1.570767879486084,\n",
       "  1.728203535079956,\n",
       "  0.948112964630127,\n",
       "  0.8862253427505493,\n",
       "  1.1095507144927979,\n",
       "  0.9858590960502625,\n",
       "  1.0523009300231934,\n",
       "  1.5605930089950562,\n",
       "  0.8979872465133667,\n",
       "  1.520505666732788,\n",
       "  0.9562863707542419,\n",
       "  1.0827891826629639,\n",
       "  0.9687350988388062,\n",
       "  0.9415143132209778,\n",
       "  0.9279553294181824,\n",
       "  1.0117616653442383,\n",
       "  1.5569227933883667,\n",
       "  1.4556939601898193,\n",
       "  0.9617518782615662,\n",
       "  1.5265393257141113,\n",
       "  1.4279768466949463,\n",
       "  1.1874712705612183,\n",
       "  1.3188592195510864,\n",
       "  0.9193068146705627,\n",
       "  1.1883772611618042,\n",
       "  0.8710188269615173,\n",
       "  1.109413504600525,\n",
       "  1.1085633039474487,\n",
       "  1.001477599143982,\n",
       "  1.7783232927322388,\n",
       "  0.9617830514907837,\n",
       "  1.9138963222503662,\n",
       "  0.876066267490387,\n",
       "  0.9726012349128723,\n",
       "  1.5268502235412598,\n",
       "  1.8085238933563232,\n",
       "  1.0599608421325684,\n",
       "  1.5994945764541626,\n",
       "  1.6315592527389526,\n",
       "  0.8313368558883667,\n",
       "  1.1707360744476318,\n",
       "  1.5051119327545166,\n",
       "  1.013127088546753,\n",
       "  1.0139859914779663,\n",
       "  1.1392993927001953,\n",
       "  0.9980624914169312,\n",
       "  1.007131576538086,\n",
       "  0.9958072900772095,\n",
       "  1.5773133039474487,\n",
       "  0.9000405073165894,\n",
       "  0.8891431093215942,\n",
       "  0.8769994378089905,\n",
       "  1.0765999555587769,\n",
       "  1.4428085088729858,\n",
       "  1.03731369972229,\n",
       "  1.2244033813476562,\n",
       "  1.5956040620803833,\n",
       "  1.0839076042175293,\n",
       "  1.007020354270935,\n",
       "  1.0567080974578857,\n",
       "  0.9785428047180176,\n",
       "  1.5945661067962646,\n",
       "  0.9425356984138489,\n",
       "  1.1296045780181885,\n",
       "  0.9970365166664124,\n",
       "  1.6123260259628296,\n",
       "  0.9491892457008362,\n",
       "  1.7078102827072144,\n",
       "  1.1213321685791016,\n",
       "  1.7232673168182373,\n",
       "  1.01065993309021,\n",
       "  1.0665156841278076,\n",
       "  0.9774854183197021,\n",
       "  0.9874385595321655,\n",
       "  1.5860682725906372,\n",
       "  0.9238315224647522,\n",
       "  1.7573201656341553,\n",
       "  1.3027074337005615,\n",
       "  1.7461392879486084,\n",
       "  0.9406553506851196,\n",
       "  0.9926390647888184,\n",
       "  1.495665431022644,\n",
       "  1.566120982170105,\n",
       "  1.5345964431762695,\n",
       "  0.9288236498832703,\n",
       "  1.49824059009552,\n",
       "  0.957863450050354,\n",
       "  1.0233501195907593,\n",
       "  0.8231244087219238,\n",
       "  0.9698854684829712,\n",
       "  1.0930157899856567,\n",
       "  0.954444169998169,\n",
       "  1.0219647884368896,\n",
       "  1.5623195171356201,\n",
       "  0.9946677684783936,\n",
       "  0.9987724423408508,\n",
       "  0.9607810378074646,\n",
       "  0.9726779460906982,\n",
       "  0.9829646944999695,\n",
       "  1.3646767139434814,\n",
       "  2.046804189682007,\n",
       "  1.0069044828414917,\n",
       "  1.0517990589141846,\n",
       "  1.5292925834655762,\n",
       "  1.5909335613250732,\n",
       "  0.9439467787742615,\n",
       "  1.508402943611145,\n",
       "  1.4978573322296143,\n",
       "  1.6500874757766724,\n",
       "  0.9492559432983398,\n",
       "  1.156669020652771,\n",
       "  1.8181053400039673,\n",
       "  0.8700270652770996,\n",
       "  1.6671040058135986,\n",
       "  0.9194583296775818,\n",
       "  1.009229302406311,\n",
       "  0.9402467012405396,\n",
       "  1.7222028970718384,\n",
       "  0.9973982572555542,\n",
       "  1.3851255178451538,\n",
       "  0.9959258437156677,\n",
       "  1.7086403369903564,\n",
       "  0.9258524179458618,\n",
       "  1.0002377033233643,\n",
       "  0.9923543334007263,\n",
       "  0.9741050004959106,\n",
       "  1.107139229774475,\n",
       "  1.1302552223205566,\n",
       "  1.566564917564392,\n",
       "  1.6928960084915161,\n",
       "  1.5394642353057861,\n",
       "  1.6078734397888184,\n",
       "  0.934898853302002,\n",
       "  1.5359537601470947,\n",
       "  0.9559752345085144,\n",
       "  1.5860788822174072,\n",
       "  1.4731546640396118,\n",
       "  1.5939862728118896,\n",
       "  1.0389487743377686,\n",
       "  1.143911600112915,\n",
       "  1.7927926778793335,\n",
       "  1.088530421257019,\n",
       "  0.808058500289917,\n",
       "  0.8766415119171143,\n",
       "  1.6860111951828003,\n",
       "  1.046457290649414,\n",
       "  0.8729693293571472,\n",
       "  1.504265308380127,\n",
       "  1.0732855796813965,\n",
       "  0.9318742752075195,\n",
       "  0.9763776659965515,\n",
       "  0.9723431468009949,\n",
       "  0.9551303386688232,\n",
       "  1.3312559127807617,\n",
       "  1.06786048412323,\n",
       "  0.917057991027832,\n",
       "  0.9700889587402344,\n",
       "  0.9563620686531067,\n",
       "  0.9541972875595093,\n",
       "  0.7626109719276428,\n",
       "  1.6437431573867798,\n",
       "  0.9985106587409973,\n",
       "  1.8275765180587769,\n",
       "  0.9389759302139282,\n",
       "  1.002087950706482,\n",
       "  1.166717529296875,\n",
       "  1.5112063884735107,\n",
       "  1.5534536838531494,\n",
       "  0.9204469919204712,\n",
       "  0.9246217012405396,\n",
       "  1.0158635377883911,\n",
       "  0.9228422045707703,\n",
       "  0.9486513137817383,\n",
       "  0.9750100374221802,\n",
       "  0.9094845652580261,\n",
       "  0.8811466097831726,\n",
       "  1.3437786102294922,\n",
       "  1.621201753616333,\n",
       "  1.3522355556488037,\n",
       "  1.5598000288009644,\n",
       "  0.9467933177947998,\n",
       "  1.5652663707733154,\n",
       "  1.044372797012329,\n",
       "  0.9582842588424683,\n",
       "  1.2214635610580444,\n",
       "  0.9796605110168457,\n",
       "  0.9209861755371094,\n",
       "  1.5747089385986328,\n",
       "  1.7858108282089233,\n",
       "  0.9528385996818542,\n",
       "  0.9793403148651123,\n",
       "  1.046554446220398,\n",
       "  1.4880001544952393,\n",
       "  0.9943994283676147,\n",
       "  1.55598783493042,\n",
       "  1.5899324417114258,\n",
       "  1.3717080354690552,\n",
       "  0.9487929344177246,\n",
       "  1.5527198314666748,\n",
       "  1.5678865909576416,\n",
       "  1.537636160850525,\n",
       "  1.0355671644210815,\n",
       "  1.7077492475509644,\n",
       "  1.4685698747634888,\n",
       "  1.370864987373352,\n",
       "  1.0707135200500488,\n",
       "  0.9912613034248352,\n",
       "  1.083155870437622,\n",
       "  0.9775459170341492,\n",
       "  1.0602880716323853,\n",
       "  1.0385209321975708,\n",
       "  1.4793211221694946,\n",
       "  1.236742377281189,\n",
       "  1.8026008605957031,\n",
       "  0.8462927937507629,\n",
       "  0.9599657654762268,\n",
       "  1.0179232358932495,\n",
       "  1.6143200397491455,\n",
       "  0.9577765464782715,\n",
       "  1.25900137424469,\n",
       "  1.0000630617141724,\n",
       "  1.6055766344070435,\n",
       "  1.0712398290634155,\n",
       "  0.9864272475242615,\n",
       "  1.404087781906128,\n",
       "  1.663164734840393,\n",
       "  1.1118768453598022,\n",
       "  0.9122037291526794,\n",
       "  1.5259182453155518,\n",
       "  1.6061010360717773,\n",
       "  0.9305139183998108,\n",
       "  1.3436269760131836,\n",
       "  1.0574430227279663,\n",
       "  0.9364653825759888,\n",
       "  1.0890177488327026,\n",
       "  1.0179444551467896,\n",
       "  0.9872037768363953,\n",
       "  1.072979211807251,\n",
       "  0.9696584939956665,\n",
       "  0.9827507138252258,\n",
       "  1.8485292196273804,\n",
       "  1.113207221031189,\n",
       "  0.9229304194450378,\n",
       "  1.601538896560669,\n",
       "  0.9214668273925781,\n",
       "  0.8999289274215698,\n",
       "  1.5886014699935913,\n",
       "  1.0556042194366455,\n",
       "  1.2461496591567993,\n",
       "  0.8811407685279846,\n",
       "  1.6267292499542236,\n",
       "  0.9602566361427307,\n",
       "  1.0013877153396606,\n",
       "  1.6748418807983398,\n",
       "  1.563759207725525,\n",
       "  1.8823217153549194,\n",
       "  1.8113682270050049,\n",
       "  1.6093310117721558,\n",
       "  1.4436798095703125,\n",
       "  1.5361634492874146,\n",
       "  1.895896315574646,\n",
       "  1.0307334661483765,\n",
       "  0.9819294214248657,\n",
       "  0.9694508910179138,\n",
       "  1.095598816871643,\n",
       "  1.1254615783691406,\n",
       "  1.4688677787780762,\n",
       "  1.7121362686157227,\n",
       "  1.5615627765655518,\n",
       "  1.4228187799453735,\n",
       "  0.9919072389602661,\n",
       "  1.590209722518921,\n",
       "  1.6299389600753784,\n",
       "  0.944697380065918,\n",
       "  1.0356639623641968,\n",
       "  1.0229949951171875,\n",
       "  1.1538995504379272,\n",
       "  0.969885528087616,\n",
       "  1.6411455869674683,\n",
       "  1.54096257686615,\n",
       "  1.014225721359253,\n",
       "  1.6950352191925049,\n",
       "  0.979583203792572,\n",
       "  0.9841192960739136,\n",
       "  1.7421373128890991,\n",
       "  1.27646803855896,\n",
       "  1.012453317642212,\n",
       "  1.047000527381897,\n",
       "  1.0161620378494263,\n",
       "  1.0330045223236084,\n",
       "  1.6037095785140991,\n",
       "  1.1538418531417847,\n",
       "  1.7029685974121094,\n",
       "  1.0661742687225342,\n",
       "  1.4442812204360962,\n",
       "  1.0207349061965942,\n",
       "  0.9248790144920349,\n",
       "  0.9314736723899841,\n",
       "  0.9398676753044128,\n",
       "  1.6316829919815063,\n",
       "  0.9749537110328674,\n",
       "  1.5681016445159912,\n",
       "  1.1573212146759033,\n",
       "  1.013862133026123,\n",
       "  1.5994839668273926,\n",
       "  0.9581359624862671,\n",
       "  0.991782546043396,\n",
       "  0.9316478967666626,\n",
       "  1.685462474822998,\n",
       "  0.9158271551132202,\n",
       "  1.0277035236358643,\n",
       "  0.9465233087539673,\n",
       "  0.9887605905532837,\n",
       "  0.9989780187606812,\n",
       "  0.9765586256980896,\n",
       "  1.6875289678573608,\n",
       "  1.3867110013961792,\n",
       "  0.8928606510162354,\n",
       "  1.208999752998352,\n",
       "  0.9806333780288696,\n",
       "  1.378767728805542,\n",
       "  0.9811476469039917,\n",
       "  1.006426215171814,\n",
       "  1.0591065883636475,\n",
       "  0.9582107663154602,\n",
       "  1.1691001653671265,\n",
       "  1.5178183317184448,\n",
       "  1.0406969785690308,\n",
       "  1.0631442070007324,\n",
       "  1.0635546445846558,\n",
       "  0.9359728097915649,\n",
       "  1.0139096975326538,\n",
       "  1.1113260984420776,\n",
       "  0.9751112461090088,\n",
       "  0.9393400549888611,\n",
       "  1.6764520406723022,\n",
       "  0.9682612419128418,\n",
       "  0.942036509513855,\n",
       "  1.0833687782287598,\n",
       "  0.9705116152763367,\n",
       "  1.6043063402175903,\n",
       "  1.0459802150726318,\n",
       "  0.934005856513977,\n",
       "  1.422003984451294,\n",
       "  1.4742629528045654,\n",
       "  1.0105730295181274,\n",
       "  0.9191710948944092,\n",
       "  0.9826745986938477,\n",
       "  1.0824410915374756,\n",
       "  1.6311078071594238,\n",
       "  0.9467483758926392],\n",
       " ('cosine', False, True): [1.2150251865386963,\n",
       "  1.1801875829696655,\n",
       "  1.5343093872070312,\n",
       "  0.9335688352584839,\n",
       "  1.393707036972046,\n",
       "  0.9440323114395142,\n",
       "  1.258613109588623,\n",
       "  1.456436276435852,\n",
       "  0.9222040772438049,\n",
       "  1.7389732599258423,\n",
       "  1.6034762859344482,\n",
       "  1.3851255178451538,\n",
       "  1.0649484395980835,\n",
       "  0.9437777996063232,\n",
       "  1.535832405090332,\n",
       "  0.9963192343711853,\n",
       "  1.5824706554412842,\n",
       "  1.024376392364502,\n",
       "  0.9975020885467529,\n",
       "  1.5580899715423584,\n",
       "  0.9825646877288818,\n",
       "  1.0361788272857666,\n",
       "  0.975216805934906,\n",
       "  1.6150652170181274,\n",
       "  1.5632058382034302,\n",
       "  1.523554801940918,\n",
       "  1.2201893329620361,\n",
       "  1.0456717014312744,\n",
       "  0.9650366306304932,\n",
       "  0.9124304056167603,\n",
       "  1.2709659337997437,\n",
       "  1.083039402961731,\n",
       "  1.3693958520889282,\n",
       "  1.660358190536499,\n",
       "  1.5365567207336426,\n",
       "  1.25315523147583,\n",
       "  1.6403770446777344,\n",
       "  1.3993065357208252,\n",
       "  0.9636117815971375,\n",
       "  0.9664022922515869,\n",
       "  1.7461395263671875,\n",
       "  1.0109233856201172,\n",
       "  1.0111562013626099,\n",
       "  1.0289229154586792,\n",
       "  1.0332324504852295,\n",
       "  1.5856810808181763,\n",
       "  1.5556163787841797,\n",
       "  1.0840704441070557,\n",
       "  0.9174163937568665,\n",
       "  0.9765293598175049,\n",
       "  1.55533766746521,\n",
       "  1.6512117385864258,\n",
       "  0.9172934889793396,\n",
       "  1.0672273635864258,\n",
       "  1.4702305793762207,\n",
       "  1.0043152570724487,\n",
       "  0.9427615404129028,\n",
       "  1.4442012310028076,\n",
       "  1.2373621463775635,\n",
       "  1.0260045528411865,\n",
       "  1.6348235607147217,\n",
       "  1.0046159029006958,\n",
       "  0.9910733699798584,\n",
       "  1.0599061250686646,\n",
       "  1.4243083000183105,\n",
       "  1.0294800996780396,\n",
       "  1.4967066049575806,\n",
       "  0.9821076393127441,\n",
       "  0.9543336033821106,\n",
       "  1.0298041105270386,\n",
       "  1.0658375024795532,\n",
       "  1.5040289163589478,\n",
       "  0.9799647331237793,\n",
       "  1.5588818788528442,\n",
       "  1.451458215713501,\n",
       "  1.0052011013031006,\n",
       "  1.6740217208862305,\n",
       "  1.6875942945480347,\n",
       "  1.3646769523620605,\n",
       "  1.460247278213501,\n",
       "  0.9992772340774536,\n",
       "  0.9961968064308167,\n",
       "  0.8880088329315186,\n",
       "  1.1544685363769531,\n",
       "  0.9357317686080933,\n",
       "  1.027686595916748,\n",
       "  1.5949811935424805,\n",
       "  1.0754563808441162,\n",
       "  1.6031768321990967,\n",
       "  1.2161437273025513,\n",
       "  1.7884361743927002,\n",
       "  1.0193763971328735,\n",
       "  0.9611065983772278,\n",
       "  1.6424158811569214,\n",
       "  1.010545253753662,\n",
       "  0.9897129535675049,\n",
       "  1.5712568759918213,\n",
       "  0.8639072775840759,\n",
       "  1.2198729515075684,\n",
       "  1.410867691040039,\n",
       "  1.0240815877914429,\n",
       "  1.8195827007293701,\n",
       "  1.5826646089553833,\n",
       "  0.9912733435630798,\n",
       "  0.9414337873458862,\n",
       "  0.9266391396522522,\n",
       "  0.9672093987464905,\n",
       "  1.7844548225402832,\n",
       "  1.50255286693573,\n",
       "  1.6601002216339111,\n",
       "  1.275862693786621,\n",
       "  1.5921239852905273,\n",
       "  0.959517776966095,\n",
       "  0.9721441864967346,\n",
       "  1.579693078994751,\n",
       "  1.2984870672225952,\n",
       "  1.451737403869629,\n",
       "  1.1454001665115356,\n",
       "  1.095445990562439,\n",
       "  1.6103731393814087,\n",
       "  1.6546176671981812,\n",
       "  1.834252119064331,\n",
       "  1.2227760553359985,\n",
       "  0.9893299341201782,\n",
       "  0.8850424885749817,\n",
       "  1.0636379718780518,\n",
       "  1.4014710187911987,\n",
       "  1.4967142343521118,\n",
       "  1.5230704545974731,\n",
       "  1.622737169265747,\n",
       "  0.9952871203422546,\n",
       "  0.9956912398338318,\n",
       "  0.9300425052642822,\n",
       "  1.3171439170837402,\n",
       "  1.0244718790054321,\n",
       "  1.01838219165802,\n",
       "  1.5561779737472534,\n",
       "  0.9401514530181885,\n",
       "  0.905501663684845,\n",
       "  1.5094999074935913,\n",
       "  0.9390717148780823,\n",
       "  1.4726992845535278,\n",
       "  1.6805917024612427,\n",
       "  1.1471006870269775,\n",
       "  0.9146213531494141,\n",
       "  0.983508825302124,\n",
       "  1.545196771621704,\n",
       "  1.5528464317321777,\n",
       "  1.5728397369384766,\n",
       "  1.0471080541610718,\n",
       "  1.0008533000946045,\n",
       "  1.5761852264404297,\n",
       "  0.9772956967353821,\n",
       "  1.5577455759048462,\n",
       "  1.5593878030776978,\n",
       "  0.9700986742973328,\n",
       "  0.9927765727043152,\n",
       "  0.9736666679382324,\n",
       "  0.9520083069801331,\n",
       "  0.9779796600341797,\n",
       "  1.0688261985778809,\n",
       "  1.5707679986953735,\n",
       "  1.728203535079956,\n",
       "  1.0365442037582397,\n",
       "  0.9236394762992859,\n",
       "  1.0314486026763916,\n",
       "  0.9858593940734863,\n",
       "  1.0523011684417725,\n",
       "  0.9844624400138855,\n",
       "  0.8979872465133667,\n",
       "  1.5205057859420776,\n",
       "  0.9428145289421082,\n",
       "  1.0827891826629639,\n",
       "  0.9706099629402161,\n",
       "  0.9633612036705017,\n",
       "  0.9184370040893555,\n",
       "  1.0117616653442383,\n",
       "  1.5569225549697876,\n",
       "  1.4556941986083984,\n",
       "  1.0058842897415161,\n",
       "  1.5265392065048218,\n",
       "  1.4279770851135254,\n",
       "  1.02170729637146,\n",
       "  1.3188594579696655,\n",
       "  0.9193069934844971,\n",
       "  1.1883769035339355,\n",
       "  0.9719163179397583,\n",
       "  1.162803053855896,\n",
       "  1.1085636615753174,\n",
       "  1.0742316246032715,\n",
       "  1.7783235311508179,\n",
       "  0.9617830514907837,\n",
       "  1.9138964414596558,\n",
       "  0.8835692405700684,\n",
       "  0.9726009368896484,\n",
       "  1.5268505811691284,\n",
       "  1.8085240125656128,\n",
       "  1.0696223974227905,\n",
       "  1.5994945764541626,\n",
       "  1.6315592527389526,\n",
       "  0.8313369154930115,\n",
       "  1.1707361936569214,\n",
       "  1.5051120519638062,\n",
       "  1.3198517560958862,\n",
       "  1.0116733312606812,\n",
       "  0.9268600344657898,\n",
       "  1.5767358541488647,\n",
       "  1.0071316957473755,\n",
       "  0.9958072304725647,\n",
       "  1.5773131847381592,\n",
       "  1.034435510635376,\n",
       "  1.0158357620239258,\n",
       "  0.9252803921699524,\n",
       "  1.0765997171401978,\n",
       "  1.442808747291565,\n",
       "  1.6080708503723145,\n",
       "  0.9536820650100708,\n",
       "  1.5956041812896729,\n",
       "  1.0839077234268188,\n",
       "  1.046086072921753,\n",
       "  1.0567079782485962,\n",
       "  0.9725558757781982,\n",
       "  0.9709985256195068,\n",
       "  0.9425356984138489,\n",
       "  1.1355171203613281,\n",
       "  0.9970364570617676,\n",
       "  1.61232590675354,\n",
       "  0.9491894245147705,\n",
       "  1.707810878753662,\n",
       "  1.0994006395339966,\n",
       "  1.7232671976089478,\n",
       "  1.0106598138809204,\n",
       "  1.0889872312545776,\n",
       "  1.5573203563690186,\n",
       "  0.987438440322876,\n",
       "  1.5860685110092163,\n",
       "  0.9238315224647522,\n",
       "  1.7573201656341553,\n",
       "  1.3027076721191406,\n",
       "  1.7461395263671875,\n",
       "  1.0556706190109253,\n",
       "  1.2072051763534546,\n",
       "  1.495665192604065,\n",
       "  1.5661206245422363,\n",
       "  1.5345964431762695,\n",
       "  0.9723287224769592,\n",
       "  0.9970985651016235,\n",
       "  0.9586697816848755,\n",
       "  1.1391940116882324,\n",
       "  0.8847947716712952,\n",
       "  0.962348222732544,\n",
       "  1.0930155515670776,\n",
       "  1.3436270952224731,\n",
       "  1.5206921100616455,\n",
       "  1.5623198747634888,\n",
       "  0.9675042033195496,\n",
       "  0.9987723231315613,\n",
       "  0.9911841750144958,\n",
       "  1.5516587495803833,\n",
       "  0.9923022389411926,\n",
       "  1.3646769523620605,\n",
       "  2.046804189682007,\n",
       "  1.053695797920227,\n",
       "  1.051798939704895,\n",
       "  1.5292929410934448,\n",
       "  1.5909340381622314,\n",
       "  0.9439467191696167,\n",
       "  1.5084030628204346,\n",
       "  1.4978575706481934,\n",
       "  1.047791838645935,\n",
       "  0.9573593735694885,\n",
       "  1.7214760780334473,\n",
       "  1.8181052207946777,\n",
       "  0.8700273036956787,\n",
       "  1.0546878576278687,\n",
       "  0.9194584488868713,\n",
       "  1.0092294216156006,\n",
       "  0.9402467608451843,\n",
       "  1.7222031354904175,\n",
       "  1.056352972984314,\n",
       "  1.3851255178451538,\n",
       "  1.0251035690307617,\n",
       "  1.708640456199646,\n",
       "  0.9155793786048889,\n",
       "  1.0698463916778564,\n",
       "  0.9923545122146606,\n",
       "  0.9741049408912659,\n",
       "  1.107139229774475,\n",
       "  0.9337311387062073,\n",
       "  1.5665652751922607,\n",
       "  1.6928958892822266,\n",
       "  1.5394642353057861,\n",
       "  1.607873558998108,\n",
       "  0.9348984956741333,\n",
       "  1.5359538793563843,\n",
       "  0.9559754133224487,\n",
       "  1.5860788822174072,\n",
       "  1.4731546640396118,\n",
       "  1.5939861536026,\n",
       "  1.0404390096664429,\n",
       "  1.143911600112915,\n",
       "  1.7927929162979126,\n",
       "  1.063041090965271,\n",
       "  0.818041980266571,\n",
       "  0.9577780961990356,\n",
       "  1.3362047672271729,\n",
       "  1.0464575290679932,\n",
       "  1.5848994255065918,\n",
       "  0.9579951763153076,\n",
       "  1.0669770240783691,\n",
       "  0.9322310090065002,\n",
       "  0.9786745309829712,\n",
       "  0.972343385219574,\n",
       "  0.9551301002502441,\n",
       "  1.33125638961792,\n",
       "  1.067860722541809,\n",
       "  0.9573043584823608,\n",
       "  0.9588267207145691,\n",
       "  1.0071139335632324,\n",
       "  0.9554629921913147,\n",
       "  1.4341020584106445,\n",
       "  1.6437429189682007,\n",
       "  0.9985106587409973,\n",
       "  1.8275763988494873,\n",
       "  0.938975989818573,\n",
       "  1.5810035467147827,\n",
       "  0.9966831803321838,\n",
       "  1.5112063884735107,\n",
       "  1.553453803062439,\n",
       "  0.9478344321250916,\n",
       "  0.9296549558639526,\n",
       "  1.069964051246643,\n",
       "  1.4191714525222778,\n",
       "  0.9664841294288635,\n",
       "  0.9668486714363098,\n",
       "  0.9346640706062317,\n",
       "  0.8814443945884705,\n",
       "  1.3437786102294922,\n",
       "  1.6212018728256226,\n",
       "  1.3522356748580933,\n",
       "  1.5598000288009644,\n",
       "  0.9382736086845398,\n",
       "  1.565266489982605,\n",
       "  1.0212743282318115,\n",
       "  1.6118619441986084,\n",
       "  1.2283681631088257,\n",
       "  1.182531476020813,\n",
       "  1.0126756429672241,\n",
       "  1.5747092962265015,\n",
       "  1.785811424255371,\n",
       "  0.9422730803489685,\n",
       "  0.9793404340744019,\n",
       "  1.5795762538909912,\n",
       "  0.9836631417274475,\n",
       "  0.9943992495536804,\n",
       "  1.5559879541397095,\n",
       "  1.589932918548584,\n",
       "  1.7912931442260742,\n",
       "  0.9637203216552734,\n",
       "  1.5527198314666748,\n",
       "  1.5678863525390625,\n",
       "  1.537636160850525,\n",
       "  1.0355671644210815,\n",
       "  1.707749605178833,\n",
       "  1.4685697555541992,\n",
       "  1.3708651065826416,\n",
       "  1.0707135200500488,\n",
       "  1.3300079107284546,\n",
       "  1.0703649520874023,\n",
       "  0.9838762879371643,\n",
       "  1.127850890159607,\n",
       "  1.0773165225982666,\n",
       "  1.479320764541626,\n",
       "  1.0770305395126343,\n",
       "  1.8026009798049927,\n",
       "  0.8667320609092712,\n",
       "  0.9677327275276184,\n",
       "  1.0179232358932495,\n",
       "  1.614319920539856,\n",
       "  0.9577766060829163,\n",
       "  1.25900137424469,\n",
       "  1.005364179611206,\n",
       "  1.6055763959884644,\n",
       "  1.0282752513885498,\n",
       "  1.118257761001587,\n",
       "  0.9603552222251892,\n",
       "  1.663164734840393,\n",
       "  1.1118768453598022,\n",
       "  0.9213731288909912,\n",
       "  1.5259182453155518,\n",
       "  1.606101155281067,\n",
       "  1.5567724704742432,\n",
       "  1.3436270952224731,\n",
       "  1.0595083236694336,\n",
       "  0.9147691130638123,\n",
       "  1.0890178680419922,\n",
       "  1.628177523612976,\n",
       "  1.0149226188659668,\n",
       "  1.0777028799057007,\n",
       "  0.9696581959724426,\n",
       "  1.2961050271987915,\n",
       "  1.2048463821411133,\n",
       "  1.102245569229126,\n",
       "  1.0171712636947632,\n",
       "  1.6015387773513794,\n",
       "  0.9917082190513611,\n",
       "  0.8999292254447937,\n",
       "  1.5886013507843018,\n",
       "  1.0556044578552246,\n",
       "  1.2461495399475098,\n",
       "  1.454301118850708,\n",
       "  1.6267292499542236,\n",
       "  0.9998564720153809,\n",
       "  1.0013874769210815,\n",
       "  1.6748418807983398,\n",
       "  0.8771501779556274,\n",
       "  1.107682704925537,\n",
       "  1.8113682270050049,\n",
       "  1.6093312501907349,\n",
       "  1.443679690361023,\n",
       "  1.5361634492874146,\n",
       "  1.8958964347839355,\n",
       "  1.0307334661483765,\n",
       "  1.475332498550415,\n",
       "  1.4714809656143188,\n",
       "  1.095598816871643,\n",
       "  1.125461459159851,\n",
       "  1.4688678979873657,\n",
       "  1.712136149406433,\n",
       "  1.5615628957748413,\n",
       "  1.4228190183639526,\n",
       "  1.0334559679031372,\n",
       "  1.5902096033096313,\n",
       "  0.9615867137908936,\n",
       "  0.9446973204612732,\n",
       "  1.313873052597046,\n",
       "  1.022994875907898,\n",
       "  1.1366658210754395,\n",
       "  0.9698853492736816,\n",
       "  1.6411457061767578,\n",
       "  1.5409629344940186,\n",
       "  1.014225721359253,\n",
       "  1.6950355768203735,\n",
       "  0.9868800640106201,\n",
       "  1.593313217163086,\n",
       "  1.7421373128890991,\n",
       "  1.2764681577682495,\n",
       "  1.030734896659851,\n",
       "  1.0470006465911865,\n",
       "  1.4727492332458496,\n",
       "  1.0330040454864502,\n",
       "  1.6037094593048096,\n",
       "  1.882932186126709,\n",
       "  1.7029683589935303,\n",
       "  1.8112194538116455,\n",
       "  1.4442813396453857,\n",
       "  0.967883825302124,\n",
       "  0.9365243911743164,\n",
       "  1.0038974285125732,\n",
       "  0.9812367558479309,\n",
       "  1.6316828727722168,\n",
       "  1.051087498664856,\n",
       "  1.5681016445159912,\n",
       "  1.1105878353118896,\n",
       "  1.0384747982025146,\n",
       "  1.5994839668273926,\n",
       "  1.4240925312042236,\n",
       "  1.097865343093872,\n",
       "  0.9362596273422241,\n",
       "  1.6854627132415771,\n",
       "  0.915827214717865,\n",
       "  1.5722023248672485,\n",
       "  1.0347124338150024,\n",
       "  1.0129863023757935,\n",
       "  0.9989782571792603,\n",
       "  0.9942393898963928,\n",
       "  1.6875290870666504,\n",
       "  0.9500868320465088,\n",
       "  0.8928607106208801,\n",
       "  1.2089998722076416,\n",
       "  1.1477078199386597,\n",
       "  1.378767728805542,\n",
       "  0.981147825717926,\n",
       "  1.006426215171814,\n",
       "  1.0883257389068604,\n",
       "  1.126257061958313,\n",
       "  1.1690999269485474,\n",
       "  0.9554484486579895,\n",
       "  1.0054954290390015,\n",
       "  1.0383548736572266,\n",
       "  0.9683876633644104,\n",
       "  0.9501833319664001,\n",
       "  1.0139096975326538,\n",
       "  1.1113263368606567,\n",
       "  0.9765726327896118,\n",
       "  0.9393400549888611,\n",
       "  1.6764522790908813,\n",
       "  1.0189731121063232,\n",
       "  1.0004942417144775,\n",
       "  1.0823004245758057,\n",
       "  0.970511794090271,\n",
       "  1.6043063402175903,\n",
       "  1.0459802150726318,\n",
       "  1.062394142150879,\n",
       "  1.422004222869873,\n",
       "  1.4742631912231445,\n",
       "  1.0105730295181274,\n",
       "  0.9447066187858582,\n",
       "  1.0037776231765747,\n",
       "  1.082440972328186,\n",
       "  1.6311075687408447,\n",
       "  0.9544587135314941],\n",
       " ('cosine', False, False): [1.2150249481201172,\n",
       "  1.180187702178955,\n",
       "  1.5343091487884521,\n",
       "  0.903221845626831,\n",
       "  1.0117932558059692,\n",
       "  0.9683955907821655,\n",
       "  1.041619896888733,\n",
       "  0.8878310918807983,\n",
       "  0.9223654866218567,\n",
       "  1.7389732599258423,\n",
       "  1.6034762859344482,\n",
       "  1.3851256370544434,\n",
       "  1.064948558807373,\n",
       "  0.9437779188156128,\n",
       "  1.535832405090332,\n",
       "  0.9963192939758301,\n",
       "  0.9975733160972595,\n",
       "  1.024376392364502,\n",
       "  0.9975019097328186,\n",
       "  1.5580897331237793,\n",
       "  0.9825649261474609,\n",
       "  1.0361789464950562,\n",
       "  0.9752166867256165,\n",
       "  1.6150652170181274,\n",
       "  1.5632058382034302,\n",
       "  1.523554801940918,\n",
       "  1.0856754779815674,\n",
       "  1.0456717014312744,\n",
       "  0.9641269445419312,\n",
       "  0.8678067922592163,\n",
       "  1.2709660530090332,\n",
       "  1.0830392837524414,\n",
       "  1.3693958520889282,\n",
       "  1.660358190536499,\n",
       "  1.5365567207336426,\n",
       "  1.25315523147583,\n",
       "  1.640377163887024,\n",
       "  1.3993065357208252,\n",
       "  0.9636118412017822,\n",
       "  0.9851093292236328,\n",
       "  1.7461395263671875,\n",
       "  1.0109233856201172,\n",
       "  1.0295265913009644,\n",
       "  0.927302896976471,\n",
       "  1.033232569694519,\n",
       "  1.5856808423995972,\n",
       "  1.5556163787841797,\n",
       "  1.0840704441070557,\n",
       "  0.8737190365791321,\n",
       "  1.564797282218933,\n",
       "  1.55533766746521,\n",
       "  1.6512117385864258,\n",
       "  0.8998661637306213,\n",
       "  1.0672273635864258,\n",
       "  1.4702305793762207,\n",
       "  0.9774969816207886,\n",
       "  0.9427614808082581,\n",
       "  1.444201111793518,\n",
       "  0.9271070957183838,\n",
       "  1.0192874670028687,\n",
       "  1.6348235607147217,\n",
       "  0.9801623821258545,\n",
       "  0.991073489189148,\n",
       "  1.0159358978271484,\n",
       "  1.4243084192276,\n",
       "  1.0275403261184692,\n",
       "  1.496706247329712,\n",
       "  0.9915467500686646,\n",
       "  0.9211555123329163,\n",
       "  1.0974081754684448,\n",
       "  1.0018409490585327,\n",
       "  0.922137975692749,\n",
       "  1.0466272830963135,\n",
       "  1.5588818788528442,\n",
       "  0.9600731730461121,\n",
       "  0.9318428635597229,\n",
       "  1.67402184009552,\n",
       "  1.0257855653762817,\n",
       "  0.9658500552177429,\n",
       "  1.460247278213501,\n",
       "  0.9992771148681641,\n",
       "  1.5539908409118652,\n",
       "  0.8440042734146118,\n",
       "  1.0507968664169312,\n",
       "  0.9149312973022461,\n",
       "  1.0276864767074585,\n",
       "  1.5949811935424805,\n",
       "  1.0754563808441162,\n",
       "  1.6031768321990967,\n",
       "  1.2234883308410645,\n",
       "  1.7884361743927002,\n",
       "  1.0089640617370605,\n",
       "  0.9611064791679382,\n",
       "  1.6424158811569214,\n",
       "  1.010545253753662,\n",
       "  0.9897129535675049,\n",
       "  1.5712568759918213,\n",
       "  0.8639072775840759,\n",
       "  1.027788519859314,\n",
       "  1.1036409139633179,\n",
       "  1.0240815877914429,\n",
       "  1.8195827007293701,\n",
       "  1.5826647281646729,\n",
       "  0.9822060465812683,\n",
       "  0.9414337873458862,\n",
       "  0.9230279922485352,\n",
       "  0.9228523373603821,\n",
       "  1.7844550609588623,\n",
       "  1.5025527477264404,\n",
       "  1.6601002216339111,\n",
       "  1.275862693786621,\n",
       "  1.5921236276626587,\n",
       "  0.9559923410415649,\n",
       "  0.968405544757843,\n",
       "  1.5796929597854614,\n",
       "  1.2984868288040161,\n",
       "  1.451737403869629,\n",
       "  1.145399808883667,\n",
       "  1.0954457521438599,\n",
       "  0.9910771250724792,\n",
       "  1.654617428779602,\n",
       "  1.834251880645752,\n",
       "  1.0420156717300415,\n",
       "  0.9866428375244141,\n",
       "  0.8833624720573425,\n",
       "  1.0636379718780518,\n",
       "  1.4014711380004883,\n",
       "  1.4967139959335327,\n",
       "  1.5230704545974731,\n",
       "  0.9376458525657654,\n",
       "  0.9952871203422546,\n",
       "  0.9803333282470703,\n",
       "  0.9300424456596375,\n",
       "  1.3171439170837402,\n",
       "  0.9876963496208191,\n",
       "  0.9450368285179138,\n",
       "  1.0087388753890991,\n",
       "  0.9379541873931885,\n",
       "  0.9055016040802002,\n",
       "  1.5094999074935913,\n",
       "  0.9390717148780823,\n",
       "  0.8953660726547241,\n",
       "  1.6805917024612427,\n",
       "  1.1683193445205688,\n",
       "  0.9146210551261902,\n",
       "  0.9299867153167725,\n",
       "  1.545196771621704,\n",
       "  0.9355310797691345,\n",
       "  1.5728397369384766,\n",
       "  1.0471080541610718,\n",
       "  0.9455506205558777,\n",
       "  0.9812452793121338,\n",
       "  0.9717985987663269,\n",
       "  1.5577458143234253,\n",
       "  1.5593880414962769,\n",
       "  0.9583394527435303,\n",
       "  0.9927763938903809,\n",
       "  0.9810827374458313,\n",
       "  0.8767619132995605,\n",
       "  0.9344195127487183,\n",
       "  0.983310341835022,\n",
       "  1.570767879486084,\n",
       "  1.728203535079956,\n",
       "  0.9481130242347717,\n",
       "  0.8862253427505493,\n",
       "  1.1095505952835083,\n",
       "  0.9858593344688416,\n",
       "  1.052301049232483,\n",
       "  1.5605930089950562,\n",
       "  0.8979873657226562,\n",
       "  1.5205057859420776,\n",
       "  0.9562864303588867,\n",
       "  1.0827891826629639,\n",
       "  0.9687349796295166,\n",
       "  0.9415143728256226,\n",
       "  0.9279553294181824,\n",
       "  1.0117616653442383,\n",
       "  1.5569225549697876,\n",
       "  1.4556941986083984,\n",
       "  0.9617518186569214,\n",
       "  1.5265394449234009,\n",
       "  1.4279769659042358,\n",
       "  1.1874713897705078,\n",
       "  1.3188594579696655,\n",
       "  0.919306755065918,\n",
       "  1.1883772611618042,\n",
       "  0.8710188269615173,\n",
       "  1.109413504600525,\n",
       "  1.1085635423660278,\n",
       "  1.001477599143982,\n",
       "  1.7783235311508179,\n",
       "  0.9617829918861389,\n",
       "  1.9138962030410767,\n",
       "  0.8760660290718079,\n",
       "  0.9726011753082275,\n",
       "  1.5268503427505493,\n",
       "  1.8085240125656128,\n",
       "  1.0599606037139893,\n",
       "  1.5994945764541626,\n",
       "  1.6315592527389526,\n",
       "  0.8313367962837219,\n",
       "  1.170736312866211,\n",
       "  1.5051120519638062,\n",
       "  1.0131269693374634,\n",
       "  1.0139861106872559,\n",
       "  1.1392992734909058,\n",
       "  0.9980625510215759,\n",
       "  1.0071313381195068,\n",
       "  0.9958072304725647,\n",
       "  1.5773130655288696,\n",
       "  0.9000406861305237,\n",
       "  0.8891432285308838,\n",
       "  0.8769994974136353,\n",
       "  1.0765997171401978,\n",
       "  1.442808747291565,\n",
       "  1.0373135805130005,\n",
       "  1.2244032621383667,\n",
       "  1.5956041812896729,\n",
       "  1.0839077234268188,\n",
       "  1.007020354270935,\n",
       "  1.0567079782485962,\n",
       "  0.9785430431365967,\n",
       "  1.5945662260055542,\n",
       "  0.9425356388092041,\n",
       "  1.1296045780181885,\n",
       "  0.9970365166664124,\n",
       "  1.61232590675354,\n",
       "  0.949189305305481,\n",
       "  1.7078102827072144,\n",
       "  1.1213324069976807,\n",
       "  1.7232674360275269,\n",
       "  1.0106598138809204,\n",
       "  1.0665156841278076,\n",
       "  0.9774854779243469,\n",
       "  0.9874384999275208,\n",
       "  1.5860685110092163,\n",
       "  0.9238314032554626,\n",
       "  1.7573201656341553,\n",
       "  1.3027076721191406,\n",
       "  1.7461395263671875,\n",
       "  0.9406553506851196,\n",
       "  0.9926391243934631,\n",
       "  1.495665192604065,\n",
       "  1.5661206245422363,\n",
       "  1.5345964431762695,\n",
       "  0.928823709487915,\n",
       "  1.4982408285140991,\n",
       "  0.957863450050354,\n",
       "  1.0233501195907593,\n",
       "  0.8231245279312134,\n",
       "  0.969885528087616,\n",
       "  1.0930160284042358,\n",
       "  0.954444169998169,\n",
       "  1.0219647884368896,\n",
       "  1.5623196363449097,\n",
       "  0.9946678280830383,\n",
       "  0.9987723231315613,\n",
       "  0.960780918598175,\n",
       "  0.9726778268814087,\n",
       "  0.9829646944999695,\n",
       "  1.3646767139434814,\n",
       "  2.046804428100586,\n",
       "  1.0069042444229126,\n",
       "  1.051798939704895,\n",
       "  1.5292928218841553,\n",
       "  1.5909337997436523,\n",
       "  0.9439467787742615,\n",
       "  1.508402943611145,\n",
       "  1.4978575706481934,\n",
       "  1.6500874757766724,\n",
       "  0.9492558240890503,\n",
       "  1.1566691398620605,\n",
       "  1.8181055784225464,\n",
       "  0.8700272440910339,\n",
       "  1.6671043634414673,\n",
       "  0.9194583892822266,\n",
       "  1.0092294216156006,\n",
       "  0.9402467012405396,\n",
       "  1.722203016281128,\n",
       "  0.997398316860199,\n",
       "  1.3851256370544434,\n",
       "  0.9959259629249573,\n",
       "  1.708640456199646,\n",
       "  0.9258524775505066,\n",
       "  1.0002374649047852,\n",
       "  0.9923545122146606,\n",
       "  0.9741050004959106,\n",
       "  1.1071391105651855,\n",
       "  1.1302549839019775,\n",
       "  1.5665655136108398,\n",
       "  1.6928958892822266,\n",
       "  1.5394642353057861,\n",
       "  1.6078733205795288,\n",
       "  0.9348987340927124,\n",
       "  1.5359541177749634,\n",
       "  0.9559755325317383,\n",
       "  1.5860788822174072,\n",
       "  1.4731547832489014,\n",
       "  1.5939863920211792,\n",
       "  1.0389485359191895,\n",
       "  1.1439114809036255,\n",
       "  1.7927929162979126,\n",
       "  1.0885303020477295,\n",
       "  0.808058500289917,\n",
       "  0.8766416311264038,\n",
       "  1.6860111951828003,\n",
       "  1.046457290649414,\n",
       "  0.872969388961792,\n",
       "  1.5042651891708374,\n",
       "  1.0732852220535278,\n",
       "  0.93187415599823,\n",
       "  0.9763776659965515,\n",
       "  0.9723432660102844,\n",
       "  0.9551301598548889,\n",
       "  1.3312562704086304,\n",
       "  1.06786048412323,\n",
       "  0.9170579314231873,\n",
       "  0.9700890183448792,\n",
       "  0.9563620090484619,\n",
       "  0.9541974663734436,\n",
       "  0.7626109719276428,\n",
       "  1.6437429189682007,\n",
       "  0.9985107779502869,\n",
       "  1.8275763988494873,\n",
       "  0.938975989818573,\n",
       "  1.002087950706482,\n",
       "  1.166717290878296,\n",
       "  1.5112063884735107,\n",
       "  1.553453803062439,\n",
       "  0.9204469919204712,\n",
       "  0.9246218204498291,\n",
       "  1.015863299369812,\n",
       "  0.922842264175415,\n",
       "  0.9486514329910278,\n",
       "  0.9750099778175354,\n",
       "  0.9094846248626709,\n",
       "  0.8811466097831726,\n",
       "  1.3437786102294922,\n",
       "  1.6212018728256226,\n",
       "  1.3522354364395142,\n",
       "  1.5598000288009644,\n",
       "  0.9467935562133789,\n",
       "  1.565266489982605,\n",
       "  1.044372797012329,\n",
       "  0.9582843780517578,\n",
       "  1.2214637994766235,\n",
       "  0.9796607494354248,\n",
       "  0.9209862351417542,\n",
       "  1.5747090578079224,\n",
       "  1.7858113050460815,\n",
       "  0.9528385996818542,\n",
       "  0.9793404340744019,\n",
       "  1.046554446220398,\n",
       "  1.4880002737045288,\n",
       "  0.9943994283676147,\n",
       "  1.555988073348999,\n",
       "  1.589932918548584,\n",
       "  1.3717079162597656,\n",
       "  0.9487929344177246,\n",
       "  1.5527198314666748,\n",
       "  1.567886471748352,\n",
       "  1.537636160850525,\n",
       "  1.0355671644210815,\n",
       "  1.707749605178833,\n",
       "  1.4685697555541992,\n",
       "  1.370864987373352,\n",
       "  1.0707135200500488,\n",
       "  0.9912614822387695,\n",
       "  1.083155870437622,\n",
       "  0.9775458574295044,\n",
       "  1.0602880716323853,\n",
       "  1.0385208129882812,\n",
       "  1.4793212413787842,\n",
       "  1.2367420196533203,\n",
       "  1.8026010990142822,\n",
       "  0.8462929129600525,\n",
       "  0.9599657654762268,\n",
       "  1.0179232358932495,\n",
       "  1.6143200397491455,\n",
       "  0.9577764868736267,\n",
       "  1.25900137424469,\n",
       "  1.000063180923462,\n",
       "  1.6055763959884644,\n",
       "  1.0712398290634155,\n",
       "  0.9864272475242615,\n",
       "  1.404087781906128,\n",
       "  1.663164734840393,\n",
       "  1.1118768453598022,\n",
       "  0.9122034907341003,\n",
       "  1.5259180068969727,\n",
       "  1.606101155281067,\n",
       "  0.9305139780044556,\n",
       "  1.3436272144317627,\n",
       "  1.0574430227279663,\n",
       "  0.9364652037620544,\n",
       "  1.0890178680419922,\n",
       "  1.0179444551467896,\n",
       "  0.9872036576271057,\n",
       "  1.072979211807251,\n",
       "  0.9696582555770874,\n",
       "  0.9827504754066467,\n",
       "  1.8485294580459595,\n",
       "  1.1132069826126099,\n",
       "  0.9229304790496826,\n",
       "  1.6015386581420898,\n",
       "  0.9214667677879333,\n",
       "  0.8999292254447937,\n",
       "  1.5886013507843018,\n",
       "  1.0556042194366455,\n",
       "  1.2461495399475098,\n",
       "  0.8811407089233398,\n",
       "  1.6267292499542236,\n",
       "  0.9602566957473755,\n",
       "  1.001387596130371,\n",
       "  1.6748418807983398,\n",
       "  1.5637593269348145,\n",
       "  1.882321834564209,\n",
       "  1.8113682270050049,\n",
       "  1.6093312501907349,\n",
       "  1.4436798095703125,\n",
       "  1.536163330078125,\n",
       "  1.895896315574646,\n",
       "  1.030733585357666,\n",
       "  0.9819294214248657,\n",
       "  0.9694508910179138,\n",
       "  1.0955989360809326,\n",
       "  1.1254616975784302,\n",
       "  1.4688677787780762,\n",
       "  1.712136149406433,\n",
       "  1.5615628957748413,\n",
       "  1.4228190183639526,\n",
       "  0.9919072985649109,\n",
       "  1.5902098417282104,\n",
       "  1.6299387216567993,\n",
       "  0.9446973204612732,\n",
       "  1.0356639623641968,\n",
       "  1.022994875907898,\n",
       "  1.1538997888565063,\n",
       "  0.9698852300643921,\n",
       "  1.6411457061767578,\n",
       "  1.540962815284729,\n",
       "  1.014225721359253,\n",
       "  1.6950350999832153,\n",
       "  0.9795830249786377,\n",
       "  0.984119176864624,\n",
       "  1.7421374320983887,\n",
       "  1.27646803855896,\n",
       "  1.0124534368515015,\n",
       "  1.047000527381897,\n",
       "  1.0161617994308472,\n",
       "  1.0330042839050293,\n",
       "  1.6037095785140991,\n",
       "  1.1538419723510742,\n",
       "  1.7029683589935303,\n",
       "  1.0661739110946655,\n",
       "  1.4442811012268066,\n",
       "  1.0207349061965942,\n",
       "  0.9248790144920349,\n",
       "  0.9314736723899841,\n",
       "  0.9398674964904785,\n",
       "  1.6316828727722168,\n",
       "  0.9749534726142883,\n",
       "  1.5681016445159912,\n",
       "  1.1573212146759033,\n",
       "  1.013862133026123,\n",
       "  1.5994842052459717,\n",
       "  0.9581360220909119,\n",
       "  0.991782546043396,\n",
       "  0.9316480755805969,\n",
       "  1.6854627132415771,\n",
       "  0.9158270955085754,\n",
       "  1.0277035236358643,\n",
       "  0.9465231895446777,\n",
       "  0.9887609481811523,\n",
       "  0.9989780783653259,\n",
       "  0.9765588045120239,\n",
       "  1.6875290870666504,\n",
       "  1.3867110013961792,\n",
       "  0.8928606510162354,\n",
       "  1.2089998722076416,\n",
       "  0.9806333780288696,\n",
       "  1.378767728805542,\n",
       "  0.9811475872993469,\n",
       "  1.006426215171814,\n",
       "  1.0591063499450684,\n",
       "  0.9582105278968811,\n",
       "  1.1690999269485474,\n",
       "  1.5178186893463135,\n",
       "  1.0406967401504517,\n",
       "  1.0631442070007324,\n",
       "  1.0635546445846558,\n",
       "  0.9359728097915649,\n",
       "  1.0139096975326538,\n",
       "  1.1113263368606567,\n",
       "  0.975111186504364,\n",
       "  0.9393400549888611,\n",
       "  1.6764522790908813,\n",
       "  0.9682613015174866,\n",
       "  0.942036509513855,\n",
       "  1.0833690166473389,\n",
       "  0.970511794090271,\n",
       "  1.6043065786361694,\n",
       "  1.045980453491211,\n",
       "  0.9340055584907532,\n",
       "  1.422004222869873,\n",
       "  1.4742631912231445,\n",
       "  1.0105730295181274,\n",
       "  0.9191712737083435,\n",
       "  0.9826745390892029,\n",
       "  1.082440972328186,\n",
       "  1.6311075687408447,\n",
       "  0.9467484354972839],\n",
       " ('euclidean', True, True): [1.2973521947860718,\n",
       "  0.500749945640564,\n",
       "  0.6114131212234497,\n",
       "  1.313923716545105,\n",
       "  0.7191166877746582,\n",
       "  1.2992444038391113,\n",
       "  0.6731559634208679,\n",
       "  0.6788283586502075,\n",
       "  1.6265437602996826,\n",
       "  0.31603172421455383,\n",
       "  0.4199957847595215,\n",
       "  0.6013990640640259,\n",
       "  0.7129268646240234,\n",
       "  1.450329065322876,\n",
       "  0.6281730532646179,\n",
       "  1.0166139602661133,\n",
       "  0.632472813129425,\n",
       "  1.058840274810791,\n",
       "  1.134447693824768,\n",
       "  0.590972363948822,\n",
       "  1.1120413541793823,\n",
       "  0.909715473651886,\n",
       "  1.3366711139678955,\n",
       "  0.8768725991249084,\n",
       "  1.4756816625595093,\n",
       "  0.6450247764587402,\n",
       "  0.5374057292938232,\n",
       "  0.8892667889595032,\n",
       "  1.3250672817230225,\n",
       "  2.0218098163604736,\n",
       "  0.9475119709968567,\n",
       "  0.8614165186882019,\n",
       "  0.6026570200920105,\n",
       "  0.5260066390037537,\n",
       "  0.5818139910697937,\n",
       "  0.7566788792610168,\n",
       "  1.3230226039886475,\n",
       "  0.6064263582229614,\n",
       "  1.217475414276123,\n",
       "  1.1582800149917603,\n",
       "  0.7753462791442871,\n",
       "  1.1316953897476196,\n",
       "  0.9957082867622375,\n",
       "  1.0481246709823608,\n",
       "  0.9036412835121155,\n",
       "  0.4431091248989105,\n",
       "  0.6062033176422119,\n",
       "  0.5643165707588196,\n",
       "  1.5229827165603638,\n",
       "  1.171074628829956,\n",
       "  0.5654734969139099,\n",
       "  0.5134574174880981,\n",
       "  1.6986991167068481,\n",
       "  0.8301032185554504,\n",
       "  0.6532213687896729,\n",
       "  0.9815480709075928,\n",
       "  1.3158892393112183,\n",
       "  0.4214310944080353,\n",
       "  1.2059788703918457,\n",
       "  0.9286588430404663,\n",
       "  0.4986882507801056,\n",
       "  1.0659040212631226,\n",
       "  1.3201937675476074,\n",
       "  0.9387556910514832,\n",
       "  0.9122641682624817,\n",
       "  0.9381088018417358,\n",
       "  0.2925907075405121,\n",
       "  1.213332176208496,\n",
       "  1.315551519393921,\n",
       "  1.2708818912506104,\n",
       "  0.9869577884674072,\n",
       "  1.2797991037368774,\n",
       "  1.121809720993042,\n",
       "  0.9062955379486084,\n",
       "  0.9610432982444763,\n",
       "  1.220751404762268,\n",
       "  0.5684919953346252,\n",
       "  0.4243881404399872,\n",
       "  0.7141544818878174,\n",
       "  0.6533411145210266,\n",
       "  1.0023123025894165,\n",
       "  1.136392593383789,\n",
       "  1.8256921768188477,\n",
       "  0.5406605005264282,\n",
       "  1.3882955312728882,\n",
       "  0.8495745062828064,\n",
       "  0.4798940122127533,\n",
       "  0.8399457335472107,\n",
       "  0.5948549509048462,\n",
       "  0.6126391887664795,\n",
       "  0.45046690106391907,\n",
       "  0.9314275979995728,\n",
       "  1.1636643409729004,\n",
       "  0.6629031300544739,\n",
       "  0.9679962992668152,\n",
       "  1.0292117595672607,\n",
       "  0.5088862180709839,\n",
       "  1.4717475175857544,\n",
       "  0.9220908880233765,\n",
       "  0.8979020118713379,\n",
       "  1.2114872932434082,\n",
       "  0.8240376114845276,\n",
       "  0.5487624406814575,\n",
       "  1.039416790008545,\n",
       "  1.2989389896392822,\n",
       "  1.6545820236206055,\n",
       "  1.3156489133834839,\n",
       "  0.4576045274734497,\n",
       "  0.5045343041419983,\n",
       "  0.8951026201248169,\n",
       "  0.25747379660606384,\n",
       "  0.44737112522125244,\n",
       "  1.4566630125045776,\n",
       "  1.1457160711288452,\n",
       "  0.6511096358299255,\n",
       "  0.5789405107498169,\n",
       "  0.5767989754676819,\n",
       "  0.8024064302444458,\n",
       "  0.5763835310935974,\n",
       "  0.8771660327911377,\n",
       "  0.5977327823638916,\n",
       "  0.25493690371513367,\n",
       "  0.5464672446250916,\n",
       "  1.0826616287231445,\n",
       "  1.3311301469802856,\n",
       "  0.977048397064209,\n",
       "  0.5268230438232422,\n",
       "  0.5020357370376587,\n",
       "  0.5199544429779053,\n",
       "  1.4203088283538818,\n",
       "  1.0284209251403809,\n",
       "  1.0609363317489624,\n",
       "  1.1863703727722168,\n",
       "  0.5440505743026733,\n",
       "  1.0449659824371338,\n",
       "  1.1875876188278198,\n",
       "  0.956189751625061,\n",
       "  1.4949018955230713,\n",
       "  1.4372918605804443,\n",
       "  1.2993489503860474,\n",
       "  1.1675962209701538,\n",
       "  0.6341074109077454,\n",
       "  0.5926850438117981,\n",
       "  0.6021186709403992,\n",
       "  1.7354484796524048,\n",
       "  1.0943981409072876,\n",
       "  0.6274691224098206,\n",
       "  0.8788830637931824,\n",
       "  0.5192031264305115,\n",
       "  1.4290745258331299,\n",
       "  1.3523764610290527,\n",
       "  1.046266794204712,\n",
       "  1.1863471269607544,\n",
       "  0.5479485392570496,\n",
       "  0.5693778395652771,\n",
       "  1.3863983154296875,\n",
       "  1.023733139038086,\n",
       "  1.100366234779358,\n",
       "  1.205214500427246,\n",
       "  1.1495118141174316,\n",
       "  1.046718716621399,\n",
       "  0.5000373125076294,\n",
       "  0.2667524814605713,\n",
       "  1.1888586282730103,\n",
       "  1.4236341714859009,\n",
       "  0.8515644073486328,\n",
       "  1.0877115726470947,\n",
       "  0.8681358098983765,\n",
       "  1.1890085935592651,\n",
       "  1.4176995754241943,\n",
       "  0.6563737392425537,\n",
       "  1.3115214109420776,\n",
       "  1.0474952459335327,\n",
       "  1.1539994478225708,\n",
       "  1.5362690687179565,\n",
       "  1.8252216577529907,\n",
       "  0.9834091067314148,\n",
       "  0.4335990846157074,\n",
       "  0.6774958968162537,\n",
       "  1.0060539245605469,\n",
       "  0.6007581353187561,\n",
       "  0.5863369703292847,\n",
       "  1.0905067920684814,\n",
       "  0.4863303601741791,\n",
       "  1.6364637613296509,\n",
       "  0.6578720211982727,\n",
       "  1.87824285030365,\n",
       "  0.6662425994873047,\n",
       "  0.3515886068344116,\n",
       "  0.6983234882354736,\n",
       "  0.30918267369270325,\n",
       "  1.1921637058258057,\n",
       "  0.2814442217350006,\n",
       "  1.3939827680587769,\n",
       "  1.2354062795639038,\n",
       "  0.6196567416191101,\n",
       "  0.6884530186653137,\n",
       "  0.8343212604522705,\n",
       "  1.1379387378692627,\n",
       "  0.5861575603485107,\n",
       "  1.5770777463912964,\n",
       "  0.8026384711265564,\n",
       "  0.49716421961784363,\n",
       "  0.5362706780433655,\n",
       "  0.9139044284820557,\n",
       "  1.2568320035934448,\n",
       "  1.0069137811660767,\n",
       "  0.9719423055648804,\n",
       "  1.0218924283981323,\n",
       "  0.5994313359260559,\n",
       "  1.421533226966858,\n",
       "  1.524773359298706,\n",
       "  1.6489336490631104,\n",
       "  0.8777902722358704,\n",
       "  0.6595761179924011,\n",
       "  0.9010079503059387,\n",
       "  1.1362472772598267,\n",
       "  0.5927116274833679,\n",
       "  0.7797667384147644,\n",
       "  0.937569797039032,\n",
       "  0.8257730007171631,\n",
       "  1.257988691329956,\n",
       "  1.3468266725540161,\n",
       "  1.4117685556411743,\n",
       "  0.4685826301574707,\n",
       "  1.0932575464248657,\n",
       "  0.7876633405685425,\n",
       "  1.1763659715652466,\n",
       "  0.47521889209747314,\n",
       "  0.5959550738334656,\n",
       "  1.0437424182891846,\n",
       "  0.9443128108978271,\n",
       "  0.7791494727134705,\n",
       "  1.0850574970245361,\n",
       "  1.2269275188446045,\n",
       "  0.511427104473114,\n",
       "  1.4316799640655518,\n",
       "  0.41187819838523865,\n",
       "  0.38095784187316895,\n",
       "  0.3493649661540985,\n",
       "  1.1377536058425903,\n",
       "  0.47462373971939087,\n",
       "  0.5785130262374878,\n",
       "  0.5333126783370972,\n",
       "  1.4486000537872314,\n",
       "  1.2136321067810059,\n",
       "  1.018164038658142,\n",
       "  1.1336209774017334,\n",
       "  0.9199398756027222,\n",
       "  1.5344772338867188,\n",
       "  1.1404240131378174,\n",
       "  0.8951122760772705,\n",
       "  0.5779944062232971,\n",
       "  0.6399624347686768,\n",
       "  0.6079592704772949,\n",
       "  1.2371630668640137,\n",
       "  1.0058300495147705,\n",
       "  1.4045443534851074,\n",
       "  0.5366741418838501,\n",
       "  1.1458425521850586,\n",
       "  0.5122089982032776,\n",
       "  0.5076634287834167,\n",
       "  0.8129353523254395,\n",
       "  0.903827965259552,\n",
       "  0.6451364159584045,\n",
       "  0.5954299569129944,\n",
       "  1.2379024028778076,\n",
       "  1.6049299240112305,\n",
       "  0.9193138480186462,\n",
       "  0.7436355352401733,\n",
       "  1.7088488340377808,\n",
       "  0.617749810218811,\n",
       "  0.2749914228916168,\n",
       "  1.374599575996399,\n",
       "  0.8619998097419739,\n",
       "  1.364905595779419,\n",
       "  0.9163434505462646,\n",
       "  1.2447402477264404,\n",
       "  0.5339562892913818,\n",
       "  1.0057624578475952,\n",
       "  0.6013990640640259,\n",
       "  1.016886591911316,\n",
       "  0.5194184184074402,\n",
       "  1.397834300994873,\n",
       "  0.999192476272583,\n",
       "  1.0744366645812988,\n",
       "  1.087193250656128,\n",
       "  0.8383756875991821,\n",
       "  1.516154170036316,\n",
       "  0.5439769625663757,\n",
       "  0.5551496744155884,\n",
       "  0.6220262050628662,\n",
       "  0.3687913119792938,\n",
       "  1.3207125663757324,\n",
       "  0.6163486242294312,\n",
       "  1.1741634607315063,\n",
       "  0.3999767601490021,\n",
       "  1.064760684967041,\n",
       "  0.49060118198394775,\n",
       "  0.8676745891571045,\n",
       "  0.700320839881897,\n",
       "  0.28159379959106445,\n",
       "  0.6479864120483398,\n",
       "  1.5640548467636108,\n",
       "  1.266335129737854,\n",
       "  0.8617305755615234,\n",
       "  0.8928510546684265,\n",
       "  0.5786069631576538,\n",
       "  1.2586448192596436,\n",
       "  0.7489709258079529,\n",
       "  1.509485125541687,\n",
       "  1.2712070941925049,\n",
       "  1.166442632675171,\n",
       "  1.3119560480117798,\n",
       "  0.6379470825195312,\n",
       "  0.8013652563095093,\n",
       "  1.410161018371582,\n",
       "  1.2597862482070923,\n",
       "  1.1841069459915161,\n",
       "  1.348751187324524,\n",
       "  0.725148618221283,\n",
       "  0.4279569089412689,\n",
       "  1.0373526811599731,\n",
       "  0.2716687023639679,\n",
       "  1.6979864835739136,\n",
       "  0.9858086109161377,\n",
       "  1.426497459411621,\n",
       "  0.5528342723846436,\n",
       "  0.571631133556366,\n",
       "  1.3849891424179077,\n",
       "  1.570154070854187,\n",
       "  0.7451101541519165,\n",
       "  1.247788667678833,\n",
       "  1.2009638547897339,\n",
       "  1.2784162759780884,\n",
       "  1.2416280508041382,\n",
       "  1.3593645095825195,\n",
       "  0.9964612722396851,\n",
       "  0.43272310495376587,\n",
       "  0.5796219706535339,\n",
       "  0.6157219409942627,\n",
       "  1.3223938941955566,\n",
       "  0.5417237281799316,\n",
       "  0.9503518342971802,\n",
       "  1.0985716581344604,\n",
       "  0.3998855650424957,\n",
       "  1.011560320854187,\n",
       "  1.1308903694152832,\n",
       "  1.3189349174499512,\n",
       "  0.5160050988197327,\n",
       "  1.4445933103561401,\n",
       "  1.0692952871322632,\n",
       "  0.8781406283378601,\n",
       "  1.4498090744018555,\n",
       "  1.0117900371551514,\n",
       "  0.46479207277297974,\n",
       "  0.3115498125553131,\n",
       "  0.4805417060852051,\n",
       "  1.1528892517089844,\n",
       "  0.8847672939300537,\n",
       "  0.5330682992935181,\n",
       "  0.4933285415172577,\n",
       "  1.0155307054519653,\n",
       "  0.3298211991786957,\n",
       "  0.6233955025672913,\n",
       "  1.1627312898635864,\n",
       "  0.6525892019271851,\n",
       "  1.0590041875839233,\n",
       "  0.8319193124771118,\n",
       "  1.1773499250411987,\n",
       "  0.7083535194396973,\n",
       "  0.9076920747756958,\n",
       "  0.4975515902042389,\n",
       "  1.0481866598129272,\n",
       "  0.5123801231384277,\n",
       "  1.7418885231018066,\n",
       "  1.3280856609344482,\n",
       "  0.9878702163696289,\n",
       "  1.264738917350769,\n",
       "  1.3543540239334106,\n",
       "  0.6666419506072998,\n",
       "  0.9993591904640198,\n",
       "  0.5287944078445435,\n",
       "  0.9730116724967957,\n",
       "  1.075568437576294,\n",
       "  1.2756226062774658,\n",
       "  0.44588810205459595,\n",
       "  0.7332125902175903,\n",
       "  2.03547739982605,\n",
       "  0.5122362375259399,\n",
       "  0.9459331035614014,\n",
       "  1.0135717391967773,\n",
       "  0.5779944062232971,\n",
       "  0.7136263251304626,\n",
       "  1.4853469133377075,\n",
       "  0.594596803188324,\n",
       "  0.9307412505149841,\n",
       "  0.9630773067474365,\n",
       "  0.6636391878128052,\n",
       "  1.3357661962509155,\n",
       "  1.076360821723938,\n",
       "  0.6177409887313843,\n",
       "  0.8355518579483032,\n",
       "  1.0056536197662354,\n",
       "  1.238423466682434,\n",
       "  1.210911750793457,\n",
       "  1.2008123397827148,\n",
       "  0.5240064263343811,\n",
       "  0.698732852935791,\n",
       "  0.5739012360572815,\n",
       "  1.8403478860855103,\n",
       "  0.413173645734787,\n",
       "  1.0513572692871094,\n",
       "  1.0697050094604492,\n",
       "  0.9636999368667603,\n",
       "  1.6646811962127686,\n",
       "  0.6498799920082092,\n",
       "  0.6725305914878845,\n",
       "  0.6314675211906433,\n",
       "  0.5607325434684753,\n",
       "  0.5334206223487854,\n",
       "  0.392753928899765,\n",
       "  0.9503070116043091,\n",
       "  1.1289175748825073,\n",
       "  1.0648225545883179,\n",
       "  0.8379549384117126,\n",
       "  0.730649471282959,\n",
       "  1.7091625928878784,\n",
       "  0.40502139925956726,\n",
       "  0.4367012083530426,\n",
       "  0.5046741962432861,\n",
       "  1.0592252016067505,\n",
       "  0.5643791556358337,\n",
       "  1.383909821510315,\n",
       "  1.2020736932754517,\n",
       "  0.7287406921386719,\n",
       "  1.0205802917480469,\n",
       "  0.32101091742515564,\n",
       "  1.1839889287948608,\n",
       "  0.9307115077972412,\n",
       "  0.5853955149650574,\n",
       "  1.1678646802902222,\n",
       "  0.35250529646873474,\n",
       "  1.1217690706253052,\n",
       "  0.4396202266216278,\n",
       "  0.4554162919521332,\n",
       "  0.7316470742225647,\n",
       "  0.8476479053497314,\n",
       "  1.026926040649414,\n",
       "  0.8614345788955688,\n",
       "  1.0420385599136353,\n",
       "  0.36810368299484253,\n",
       "  0.5483102202415466,\n",
       "  0.5552979111671448,\n",
       "  0.32456448674201965,\n",
       "  0.4980268180370331,\n",
       "  1.55780029296875,\n",
       "  1.455452561378479,\n",
       "  1.3938817977905273,\n",
       "  1.1820714473724365,\n",
       "  1.3304883241653442,\n",
       "  1.1259047985076904,\n",
       "  0.3515801429748535,\n",
       "  0.791807234287262,\n",
       "  0.8940550088882446,\n",
       "  0.39461272954940796,\n",
       "  1.2170593738555908,\n",
       "  0.5510610938072205,\n",
       "  1.6645222902297974,\n",
       "  0.5272719264030457,\n",
       "  1.2543103694915771,\n",
       "  0.8156132102012634,\n",
       "  1.0353469848632812,\n",
       "  1.0330390930175781,\n",
       "  1.003594160079956,\n",
       "  1.1008970737457275,\n",
       "  0.4882814884185791,\n",
       "  1.325806975364685,\n",
       "  2.2107174396514893,\n",
       "  0.9057076573371887,\n",
       "  1.1576215028762817,\n",
       "  0.9901672601699829,\n",
       "  1.07698655128479,\n",
       "  1.0170841217041016,\n",
       "  0.5256263017654419,\n",
       "  0.7752575874328613,\n",
       "  0.6422412991523743,\n",
       "  1.3945072889328003,\n",
       "  1.0391004085540771,\n",
       "  1.2225134372711182,\n",
       "  1.2944616079330444,\n",
       "  1.4269050359725952,\n",
       "  0.9912737607955933,\n",
       "  0.6657747626304626,\n",
       "  1.2463618516921997,\n",
       "  1.1797420978546143,\n",
       "  0.3535586893558502,\n",
       "  1.0602222681045532,\n",
       "  1.4759654998779297,\n",
       "  0.7644215226173401,\n",
       "  1.1263725757598877,\n",
       "  0.4222773611545563,\n",
       "  0.8934417963027954,\n",
       "  1.3423629999160767,\n",
       "  0.9686961770057678,\n",
       "  0.3501431941986084,\n",
       "  0.9335525631904602,\n",
       "  1.2981956005096436,\n",
       "  1.1800518035888672,\n",
       "  0.7404255270957947,\n",
       "  0.7957745790481567,\n",
       "  1.1902899742126465],\n",
       " ('euclidean', True, False): [0.714379608631134,\n",
       "  0.4545353949069977,\n",
       "  0.6114131808280945,\n",
       "  1.6519627571105957,\n",
       "  0.9354960918426514,\n",
       "  1.123713493347168,\n",
       "  0.699371874332428,\n",
       "  1.3758301734924316,\n",
       "  1.52510404586792,\n",
       "  0.3160317838191986,\n",
       "  0.4199957847595215,\n",
       "  0.6013990044593811,\n",
       "  0.7129269242286682,\n",
       "  1.27279531955719,\n",
       "  0.6281731128692627,\n",
       "  1.0166141986846924,\n",
       "  1.004993200302124,\n",
       "  0.9452072978019714,\n",
       "  1.0290749073028564,\n",
       "  0.5909723043441772,\n",
       "  1.0933550596237183,\n",
       "  0.909715473651886,\n",
       "  1.2228187322616577,\n",
       "  0.4675908088684082,\n",
       "  0.5377312302589417,\n",
       "  0.6450247764587402,\n",
       "  0.5374057292938232,\n",
       "  0.8892670273780823,\n",
       "  1.3387900590896606,\n",
       "  2.0218098163604736,\n",
       "  0.6663951277732849,\n",
       "  0.8614165186882019,\n",
       "  0.6026571393013,\n",
       "  0.5260066390037537,\n",
       "  0.5818139910697937,\n",
       "  0.6264737844467163,\n",
       "  0.6079186201095581,\n",
       "  0.6064263582229614,\n",
       "  1.217475414276123,\n",
       "  1.0578879117965698,\n",
       "  0.3493649661540985,\n",
       "  0.951553225517273,\n",
       "  0.9185819625854492,\n",
       "  1.2596423625946045,\n",
       "  0.9036412835121155,\n",
       "  0.4431091547012329,\n",
       "  0.6062032580375671,\n",
       "  0.5643165707588196,\n",
       "  1.9226295948028564,\n",
       "  0.6308254599571228,\n",
       "  0.5654734969139099,\n",
       "  0.5134573578834534,\n",
       "  1.7539970874786377,\n",
       "  0.7728427052497864,\n",
       "  0.6532213687896729,\n",
       "  1.1203687191009521,\n",
       "  1.3158894777297974,\n",
       "  0.4214309751987457,\n",
       "  1.372389793395996,\n",
       "  0.9286590218544006,\n",
       "  0.4986882507801056,\n",
       "  1.078951120376587,\n",
       "  1.0218135118484497,\n",
       "  0.9567151665687561,\n",
       "  0.6200234889984131,\n",
       "  0.9381088018417358,\n",
       "  0.2925906777381897,\n",
       "  1.0521271228790283,\n",
       "  1.431847095489502,\n",
       "  0.6933616399765015,\n",
       "  0.9869577884674072,\n",
       "  1.2797991037368774,\n",
       "  0.8350124359130859,\n",
       "  0.3865146338939667,\n",
       "  1.2987041473388672,\n",
       "  1.220751166343689,\n",
       "  0.5684919953346252,\n",
       "  0.8835874795913696,\n",
       "  1.2618931531906128,\n",
       "  0.6533411145210266,\n",
       "  1.0023123025894165,\n",
       "  0.5495225191116333,\n",
       "  1.831047773361206,\n",
       "  0.5406605005264282,\n",
       "  1.4034755229949951,\n",
       "  0.7863985300064087,\n",
       "  0.4798940122127533,\n",
       "  0.733292281627655,\n",
       "  0.5948549509048462,\n",
       "  0.4336276948451996,\n",
       "  0.45046690106391907,\n",
       "  0.9314275979995728,\n",
       "  1.1628292798995972,\n",
       "  0.6629031300544739,\n",
       "  0.9679965972900391,\n",
       "  1.0292117595672607,\n",
       "  0.5088862776756287,\n",
       "  1.2931307554244995,\n",
       "  0.9220905900001526,\n",
       "  0.8979020118713379,\n",
       "  0.8870658278465271,\n",
       "  0.5123007893562317,\n",
       "  0.5487624406814575,\n",
       "  1.039416790008545,\n",
       "  1.121720314025879,\n",
       "  1.6545816659927368,\n",
       "  1.348574161529541,\n",
       "  0.4576045274734497,\n",
       "  0.3610616624355316,\n",
       "  0.5292666554450989,\n",
       "  0.25747376680374146,\n",
       "  0.44737106561660767,\n",
       "  1.4566624164581299,\n",
       "  1.1457163095474243,\n",
       "  0.6511095762252808,\n",
       "  0.5789404511451721,\n",
       "  0.5767989754676819,\n",
       "  0.8024064898490906,\n",
       "  0.5388897657394409,\n",
       "  1.0578879117965698,\n",
       "  0.5977327823638916,\n",
       "  0.2549368739128113,\n",
       "  0.8066396713256836,\n",
       "  1.082661509513855,\n",
       "  1.3311302661895752,\n",
       "  0.7865053415298462,\n",
       "  0.5268230438232422,\n",
       "  0.5020357370376587,\n",
       "  0.5199544429779053,\n",
       "  1.5196475982666016,\n",
       "  1.02842116355896,\n",
       "  1.0908573865890503,\n",
       "  1.1863704919815063,\n",
       "  0.5440505743026733,\n",
       "  1.048758625984192,\n",
       "  1.1875877380371094,\n",
       "  0.9561896920204163,\n",
       "  1.4949018955230713,\n",
       "  1.3690437078475952,\n",
       "  0.6218453645706177,\n",
       "  1.1675963401794434,\n",
       "  1.4766517877578735,\n",
       "  0.40498417615890503,\n",
       "  0.5797373652458191,\n",
       "  1.6195096969604492,\n",
       "  1.4160916805267334,\n",
       "  0.6274691224098206,\n",
       "  1.494632363319397,\n",
       "  0.5192031860351562,\n",
       "  0.8616518974304199,\n",
       "  1.3523764610290527,\n",
       "  1.046266794204712,\n",
       "  1.1863471269607544,\n",
       "  0.5479485392570496,\n",
       "  0.569377601146698,\n",
       "  1.4171719551086426,\n",
       "  1.023733139038086,\n",
       "  1.0664488077163696,\n",
       "  1.5438168048858643,\n",
       "  1.1767292022705078,\n",
       "  1.076615333557129,\n",
       "  0.5000373721122742,\n",
       "  0.2667524814605713,\n",
       "  1.1888587474822998,\n",
       "  1.4507876634597778,\n",
       "  0.6664867997169495,\n",
       "  1.0877113342285156,\n",
       "  0.833763062953949,\n",
       "  0.6142350435256958,\n",
       "  1.402494192123413,\n",
       "  0.6563737392425537,\n",
       "  1.204745888710022,\n",
       "  0.8610116839408875,\n",
       "  1.1539989709854126,\n",
       "  1.7205034494400024,\n",
       "  1.5707316398620605,\n",
       "  0.970184326171875,\n",
       "  0.4335990846157074,\n",
       "  0.6774959564208984,\n",
       "  1.3110311031341553,\n",
       "  0.6007581353187561,\n",
       "  0.5863370895385742,\n",
       "  0.6524088978767395,\n",
       "  0.4863303601741791,\n",
       "  1.6364644765853882,\n",
       "  0.6578719615936279,\n",
       "  1.878243088722229,\n",
       "  0.7188016176223755,\n",
       "  0.351588636636734,\n",
       "  0.98687344789505,\n",
       "  0.30918267369270325,\n",
       "  1.1921637058258057,\n",
       "  0.28144416213035583,\n",
       "  1.3939825296401978,\n",
       "  1.2354055643081665,\n",
       "  0.6196567416191101,\n",
       "  0.45542922616004944,\n",
       "  0.8343209028244019,\n",
       "  0.6101762652397156,\n",
       "  0.5861575603485107,\n",
       "  1.5005834102630615,\n",
       "  0.8026383519172668,\n",
       "  0.4971643090248108,\n",
       "  0.9397037625312805,\n",
       "  0.8814800977706909,\n",
       "  0.8159072399139404,\n",
       "  1.012898325920105,\n",
       "  0.9719423651695251,\n",
       "  1.0208181142807007,\n",
       "  0.5994313955307007,\n",
       "  1.5938400030136108,\n",
       "  2.016460657119751,\n",
       "  1.703571081161499,\n",
       "  0.8777902126312256,\n",
       "  0.6595761179924011,\n",
       "  0.9010081887245178,\n",
       "  0.7436603903770447,\n",
       "  0.5927115678787231,\n",
       "  0.7797666788101196,\n",
       "  0.9375696182250977,\n",
       "  0.7854539155960083,\n",
       "  1.1423876285552979,\n",
       "  0.5865368843078613,\n",
       "  1.2529189586639404,\n",
       "  0.4685825705528259,\n",
       "  1.0164518356323242,\n",
       "  0.5359658598899841,\n",
       "  1.1666871309280396,\n",
       "  0.47521886229515076,\n",
       "  0.5038289427757263,\n",
       "  0.5318754315376282,\n",
       "  0.9443128108978271,\n",
       "  0.7791493535041809,\n",
       "  1.085057258605957,\n",
       "  1.054722785949707,\n",
       "  0.5114270448684692,\n",
       "  1.3982861042022705,\n",
       "  0.41187819838523865,\n",
       "  0.38095784187316895,\n",
       "  0.3493649661540985,\n",
       "  1.1377536058425903,\n",
       "  1.0837509632110596,\n",
       "  0.578512966632843,\n",
       "  0.5333126783370972,\n",
       "  0.548233151435852,\n",
       "  1.2136316299438477,\n",
       "  0.6090651154518127,\n",
       "  1.133621096611023,\n",
       "  0.9199398756027222,\n",
       "  1.7008366584777832,\n",
       "  1.1066149473190308,\n",
       "  0.8951120376586914,\n",
       "  1.2619019746780396,\n",
       "  0.9576089978218079,\n",
       "  0.6079592704772949,\n",
       "  1.0288671255111694,\n",
       "  1.0058298110961914,\n",
       "  1.4918227195739746,\n",
       "  1.1147689819335938,\n",
       "  1.1458425521850586,\n",
       "  0.5122090578079224,\n",
       "  0.25142526626586914,\n",
       "  0.967165470123291,\n",
       "  0.903827965259552,\n",
       "  0.6451364755630493,\n",
       "  0.5954299569129944,\n",
       "  1.1648328304290771,\n",
       "  0.5635419487953186,\n",
       "  0.6272189617156982,\n",
       "  0.35075974464416504,\n",
       "  1.720840573310852,\n",
       "  0.617749810218811,\n",
       "  0.27499139308929443,\n",
       "  1.374599575996399,\n",
       "  0.5058689713478088,\n",
       "  1.3296390771865845,\n",
       "  0.9163433909416199,\n",
       "  1.189754605293274,\n",
       "  0.5339562892913818,\n",
       "  1.0057623386383057,\n",
       "  0.6013990044593811,\n",
       "  1.016886591911316,\n",
       "  0.5194184184074402,\n",
       "  1.2038164138793945,\n",
       "  0.9991925954818726,\n",
       "  1.0744366645812988,\n",
       "  1.0871928930282593,\n",
       "  0.8383756875991821,\n",
       "  0.7320988774299622,\n",
       "  0.5439770817756653,\n",
       "  0.5551496744155884,\n",
       "  0.6220262050628662,\n",
       "  0.3687913417816162,\n",
       "  1.2106300592422485,\n",
       "  0.6163486242294312,\n",
       "  1.1373586654663086,\n",
       "  0.39997678995132446,\n",
       "  0.5196924209594727,\n",
       "  0.49060115218162537,\n",
       "  0.867674708366394,\n",
       "  0.700320839881897,\n",
       "  0.28159379959106445,\n",
       "  0.5715048909187317,\n",
       "  1.5640548467636108,\n",
       "  1.646742820739746,\n",
       "  0.5401174426078796,\n",
       "  0.8928513526916504,\n",
       "  2.1490421295166016,\n",
       "  0.666918933391571,\n",
       "  0.7335543036460876,\n",
       "  1.5094859600067139,\n",
       "  1.2712070941925049,\n",
       "  1.1003119945526123,\n",
       "  1.3119558095932007,\n",
       "  0.6379470825195312,\n",
       "  0.801365315914154,\n",
       "  1.4967541694641113,\n",
       "  1.1450252532958984,\n",
       "  1.5167691707611084,\n",
       "  1.3487510681152344,\n",
       "  3.708575487136841,\n",
       "  0.42795687913894653,\n",
       "  1.0202425718307495,\n",
       "  0.2716687023639679,\n",
       "  1.2555644512176514,\n",
       "  0.9861928224563599,\n",
       "  0.6417422890663147,\n",
       "  0.5528342127799988,\n",
       "  0.571631133556366,\n",
       "  1.3849891424179077,\n",
       "  1.570154070854187,\n",
       "  0.8983557224273682,\n",
       "  1.247788667678833,\n",
       "  1.330431342124939,\n",
       "  1.085133671760559,\n",
       "  1.4145188331604004,\n",
       "  1.3593645095825195,\n",
       "  0.607358992099762,\n",
       "  0.43272313475608826,\n",
       "  0.5796219706535339,\n",
       "  0.6157219409942627,\n",
       "  1.2469314336776733,\n",
       "  0.5417237281799316,\n",
       "  0.8438480496406555,\n",
       "  1.098571538925171,\n",
       "  0.3998856246471405,\n",
       "  1.0983467102050781,\n",
       "  1.4268689155578613,\n",
       "  0.5973930358886719,\n",
       "  0.5160052180290222,\n",
       "  1.1537548303604126,\n",
       "  1.0692954063415527,\n",
       "  0.8781406283378601,\n",
       "  0.6398770809173584,\n",
       "  1.0117902755737305,\n",
       "  0.4647921323776245,\n",
       "  0.3115498423576355,\n",
       "  0.48054176568984985,\n",
       "  1.1528894901275635,\n",
       "  0.513697624206543,\n",
       "  0.5330682992935181,\n",
       "  0.49332860112190247,\n",
       "  0.8877553343772888,\n",
       "  0.3298211991786957,\n",
       "  0.6233955025672913,\n",
       "  0.708656907081604,\n",
       "  0.652588963508606,\n",
       "  1.059004306793213,\n",
       "  0.7590620517730713,\n",
       "  1.1773498058319092,\n",
       "  0.7083535194396973,\n",
       "  0.9076920747756958,\n",
       "  0.4975515604019165,\n",
       "  0.6537977457046509,\n",
       "  0.2581122815608978,\n",
       "  1.8266618251800537,\n",
       "  1.3280855417251587,\n",
       "  0.9437224864959717,\n",
       "  0.4619102478027344,\n",
       "  1.2786396741867065,\n",
       "  0.6666419506072998,\n",
       "  0.9993586540222168,\n",
       "  0.5287944078445435,\n",
       "  0.8691349029541016,\n",
       "  1.0755685567855835,\n",
       "  0.5480851531028748,\n",
       "  0.44588810205459595,\n",
       "  0.6946094632148743,\n",
       "  2.2926673889160156,\n",
       "  0.5122362375259399,\n",
       "  0.60255366563797,\n",
       "  1.2468758821487427,\n",
       "  0.5779945850372314,\n",
       "  0.7136263251304626,\n",
       "  1.2126749753952026,\n",
       "  0.566439688205719,\n",
       "  0.9307413697242737,\n",
       "  1.091477870941162,\n",
       "  0.6744166016578674,\n",
       "  1.2779468297958374,\n",
       "  1.0763611793518066,\n",
       "  0.40853193402290344,\n",
       "  0.7905926704406738,\n",
       "  1.3404159545898438,\n",
       "  0.6066832542419434,\n",
       "  1.3863575458526611,\n",
       "  1.2008124589920044,\n",
       "  0.5240064263343811,\n",
       "  0.6661778688430786,\n",
       "  0.48055124282836914,\n",
       "  2.0736265182495117,\n",
       "  0.413173645734787,\n",
       "  1.376552939414978,\n",
       "  0.9964095950126648,\n",
       "  0.48439672589302063,\n",
       "  0.6348481178283691,\n",
       "  0.3351445198059082,\n",
       "  0.3475206196308136,\n",
       "  0.6314674615859985,\n",
       "  0.5607326030731201,\n",
       "  0.5334206223487854,\n",
       "  0.392753928899765,\n",
       "  0.9026569128036499,\n",
       "  1.1289175748825073,\n",
       "  1.0648226737976074,\n",
       "  0.8379548788070679,\n",
       "  0.7306494116783142,\n",
       "  0.7068439722061157,\n",
       "  0.40502139925956726,\n",
       "  0.4367012083530426,\n",
       "  0.5046741962432861,\n",
       "  1.062818169593811,\n",
       "  0.5643791556358337,\n",
       "  0.5011104941368103,\n",
       "  1.170231819152832,\n",
       "  0.8037750720977783,\n",
       "  0.9532610177993774,\n",
       "  0.30634745955467224,\n",
       "  1.1839890480041504,\n",
       "  0.628287672996521,\n",
       "  0.5853955149650574,\n",
       "  0.9528712630271912,\n",
       "  0.35250526666641235,\n",
       "  1.1217693090438843,\n",
       "  1.1049154996871948,\n",
       "  0.24734386801719666,\n",
       "  0.7316470742225647,\n",
       "  0.8476479649543762,\n",
       "  0.7427042126655579,\n",
       "  0.9417489767074585,\n",
       "  0.9322466254234314,\n",
       "  0.3681037127971649,\n",
       "  0.5483102798461914,\n",
       "  0.5552979111671448,\n",
       "  0.6777642369270325,\n",
       "  0.49802690744400024,\n",
       "  0.9083859920501709,\n",
       "  1.4554526805877686,\n",
       "  1.3938816785812378,\n",
       "  1.194098711013794,\n",
       "  0.5427887439727783,\n",
       "  1.1259047985076904,\n",
       "  0.3515801429748535,\n",
       "  0.5416588187217712,\n",
       "  0.9553187489509583,\n",
       "  0.3946128189563751,\n",
       "  1.2170593738555908,\n",
       "  1.2642337083816528,\n",
       "  1.7133243083953857,\n",
       "  0.5272719264030457,\n",
       "  1.254310965538025,\n",
       "  0.8156130313873291,\n",
       "  1.2012611627578735,\n",
       "  1.0428880453109741,\n",
       "  1.0035945177078247,\n",
       "  1.100897192955017,\n",
       "  0.4882814884185791,\n",
       "  0.5514556169509888,\n",
       "  1.5713993310928345,\n",
       "  0.5226085782051086,\n",
       "  1.157621145248413,\n",
       "  0.5827203392982483,\n",
       "  1.0757251977920532,\n",
       "  0.9760836362838745,\n",
       "  0.5256262421607971,\n",
       "  1.1611047983169556,\n",
       "  0.6058744788169861,\n",
       "  0.6402468681335449,\n",
       "  0.8088845610618591,\n",
       "  0.8667681813240051,\n",
       "  0.773481011390686,\n",
       "  1.5743439197540283,\n",
       "  0.9331403374671936,\n",
       "  0.6657748818397522,\n",
       "  1.2472237348556519,\n",
       "  1.1797419786453247,\n",
       "  0.3535586893558502,\n",
       "  1.2278977632522583,\n",
       "  1.5199633836746216,\n",
       "  0.7587765455245972,\n",
       "  1.1263726949691772,\n",
       "  0.4222773611545563,\n",
       "  0.8644235730171204,\n",
       "  1.3423627614974976,\n",
       "  0.5993313789367676,\n",
       "  0.35014328360557556,\n",
       "  0.9335524439811707,\n",
       "  1.6032238006591797,\n",
       "  1.180051565170288,\n",
       "  0.7404255867004395,\n",
       "  0.5483139753341675,\n",
       "  1.1902899742126465],\n",
       " ('euclidean', False, True): [1.3687469959259033,\n",
       "  0.4866729974746704,\n",
       "  0.6453565359115601,\n",
       "  1.3607302904129028,\n",
       "  0.7498835325241089,\n",
       "  1.3560644388198853,\n",
       "  0.6736453175544739,\n",
       "  0.6996161341667175,\n",
       "  1.719299554824829,\n",
       "  0.34027087688446045,\n",
       "  0.43995678424835205,\n",
       "  0.6422197818756104,\n",
       "  0.7244364619255066,\n",
       "  1.5265666246414185,\n",
       "  0.64933180809021,\n",
       "  1.0922436714172363,\n",
       "  0.6579777002334595,\n",
       "  1.0869077444076538,\n",
       "  1.1734274625778198,\n",
       "  0.6156514286994934,\n",
       "  1.093596339225769,\n",
       "  0.9282931685447693,\n",
       "  1.378570318222046,\n",
       "  0.8783392310142517,\n",
       "  1.5247509479522705,\n",
       "  0.6690059304237366,\n",
       "  0.5310549139976501,\n",
       "  0.8979835510253906,\n",
       "  1.308821201324463,\n",
       "  2.018754005432129,\n",
       "  0.9863123297691345,\n",
       "  0.895882248878479,\n",
       "  0.6164361834526062,\n",
       "  0.5490787625312805,\n",
       "  0.5996875166893005,\n",
       "  0.7578603029251099,\n",
       "  1.3847805261611938,\n",
       "  0.6365237832069397,\n",
       "  1.236127495765686,\n",
       "  1.2059754133224487,\n",
       "  0.7591766715049744,\n",
       "  1.1652859449386597,\n",
       "  1.0311896800994873,\n",
       "  1.0615990161895752,\n",
       "  0.9226699471473694,\n",
       "  0.46766436100006104,\n",
       "  0.6247072815895081,\n",
       "  0.5779039859771729,\n",
       "  1.5460124015808105,\n",
       "  1.1905261278152466,\n",
       "  0.6067209839820862,\n",
       "  0.5185645818710327,\n",
       "  1.7478852272033691,\n",
       "  0.9097851514816284,\n",
       "  0.6938118934631348,\n",
       "  0.9848330020904541,\n",
       "  1.3912086486816406,\n",
       "  0.45251819491386414,\n",
       "  1.2600284814834595,\n",
       "  0.9152933955192566,\n",
       "  0.5250971913337708,\n",
       "  1.1297776699066162,\n",
       "  1.449847936630249,\n",
       "  0.9696104526519775,\n",
       "  0.9521344900131226,\n",
       "  0.9959749579429626,\n",
       "  0.29863157868385315,\n",
       "  1.2376024723052979,\n",
       "  1.3284155130386353,\n",
       "  1.319785237312317,\n",
       "  1.0065926313400269,\n",
       "  1.3507146835327148,\n",
       "  1.2073020935058594,\n",
       "  0.9687860012054443,\n",
       "  0.939206600189209,\n",
       "  1.2816131114959717,\n",
       "  0.5954527258872986,\n",
       "  0.44106006622314453,\n",
       "  0.7420947551727295,\n",
       "  0.6942553520202637,\n",
       "  1.0715974569320679,\n",
       "  1.1690236330032349,\n",
       "  1.8976600170135498,\n",
       "  0.5385289788246155,\n",
       "  1.478903889656067,\n",
       "  0.8530473113059998,\n",
       "  0.4897020757198334,\n",
       "  0.8428699970245361,\n",
       "  0.6214337944984436,\n",
       "  0.6106318235397339,\n",
       "  0.46444037556648254,\n",
       "  1.013098120689392,\n",
       "  1.27866792678833,\n",
       "  0.6941745281219482,\n",
       "  1.0192053318023682,\n",
       "  1.0506937503814697,\n",
       "  0.5382604002952576,\n",
       "  1.5202209949493408,\n",
       "  0.9807860255241394,\n",
       "  0.8957507610321045,\n",
       "  1.2850004434585571,\n",
       "  0.8114187121391296,\n",
       "  0.5611280202865601,\n",
       "  1.0549272298812866,\n",
       "  1.3462642431259155,\n",
       "  1.6475147008895874,\n",
       "  1.3697772026062012,\n",
       "  0.46787410974502563,\n",
       "  0.5373542904853821,\n",
       "  0.8717549443244934,\n",
       "  0.26111799478530884,\n",
       "  0.4414054751396179,\n",
       "  1.4283612966537476,\n",
       "  1.2139921188354492,\n",
       "  0.6675499677658081,\n",
       "  0.6078843474388123,\n",
       "  0.6022205948829651,\n",
       "  0.8238072395324707,\n",
       "  0.5656227469444275,\n",
       "  0.9079517126083374,\n",
       "  0.6119208931922913,\n",
       "  0.2525971233844757,\n",
       "  0.5286299586296082,\n",
       "  1.0927748680114746,\n",
       "  1.4059202671051025,\n",
       "  1.016430377960205,\n",
       "  0.5438300967216492,\n",
       "  0.5132135152816772,\n",
       "  0.5329561233520508,\n",
       "  1.4492361545562744,\n",
       "  1.0110305547714233,\n",
       "  1.0709964036941528,\n",
       "  1.172343373298645,\n",
       "  0.5652436017990112,\n",
       "  1.0166949033737183,\n",
       "  1.2187057733535767,\n",
       "  0.9604068994522095,\n",
       "  1.5190349817276,\n",
       "  1.4512698650360107,\n",
       "  1.3456969261169434,\n",
       "  1.2231372594833374,\n",
       "  0.663276195526123,\n",
       "  0.5907922983169556,\n",
       "  0.5797970294952393,\n",
       "  1.823311448097229,\n",
       "  1.1453121900558472,\n",
       "  0.6382508873939514,\n",
       "  0.8897542357444763,\n",
       "  0.5391364097595215,\n",
       "  1.5130335092544556,\n",
       "  1.4383939504623413,\n",
       "  1.0351158380508423,\n",
       "  1.2477550506591797,\n",
       "  0.5745441317558289,\n",
       "  0.5789439678192139,\n",
       "  1.4552886486053467,\n",
       "  1.0407891273498535,\n",
       "  1.1750894784927368,\n",
       "  1.2741564512252808,\n",
       "  1.1482445001602173,\n",
       "  1.0530345439910889,\n",
       "  0.5252816677093506,\n",
       "  0.2629193961620331,\n",
       "  1.2394953966140747,\n",
       "  1.4994624853134155,\n",
       "  0.88392573595047,\n",
       "  1.1219069957733154,\n",
       "  0.8478237986564636,\n",
       "  1.2362987995147705,\n",
       "  1.4675939083099365,\n",
       "  0.6755331754684448,\n",
       "  1.3336752653121948,\n",
       "  1.0945860147476196,\n",
       "  1.1913923025131226,\n",
       "  1.5557163953781128,\n",
       "  1.9199789762496948,\n",
       "  1.0139352083206177,\n",
       "  0.4481181204319,\n",
       "  0.6961821913719177,\n",
       "  1.008503794670105,\n",
       "  0.6106361150741577,\n",
       "  0.6048102974891663,\n",
       "  1.0944050550460815,\n",
       "  0.5218933820724487,\n",
       "  1.6739389896392822,\n",
       "  0.6831259727478027,\n",
       "  1.9606437683105469,\n",
       "  0.6486172080039978,\n",
       "  0.3413001298904419,\n",
       "  0.7235002517700195,\n",
       "  0.306024432182312,\n",
       "  1.1873679161071777,\n",
       "  0.2730386257171631,\n",
       "  1.4809643030166626,\n",
       "  1.2391911745071411,\n",
       "  0.643815815448761,\n",
       "  0.7149121165275574,\n",
       "  0.8986683487892151,\n",
       "  1.1817548274993896,\n",
       "  0.6204314827919006,\n",
       "  1.5506625175476074,\n",
       "  0.838530957698822,\n",
       "  0.5215109586715698,\n",
       "  0.5460297465324402,\n",
       "  0.9201298952102661,\n",
       "  1.3330973386764526,\n",
       "  0.9815062284469604,\n",
       "  1.0136419534683228,\n",
       "  1.0602450370788574,\n",
       "  0.6272202134132385,\n",
       "  1.4042465686798096,\n",
       "  1.5636380910873413,\n",
       "  1.845255732536316,\n",
       "  0.9186741709709167,\n",
       "  0.6863878965377808,\n",
       "  0.9139853715896606,\n",
       "  1.1821364164352417,\n",
       "  0.603299617767334,\n",
       "  0.7705227136611938,\n",
       "  0.9770325422286987,\n",
       "  0.8363766670227051,\n",
       "  1.2812178134918213,\n",
       "  1.3705954551696777,\n",
       "  1.4366215467453003,\n",
       "  0.4531106948852539,\n",
       "  1.159193992614746,\n",
       "  0.8321234583854675,\n",
       "  1.15962815284729,\n",
       "  0.48720869421958923,\n",
       "  0.602769672870636,\n",
       "  1.0822702646255493,\n",
       "  0.9668511748313904,\n",
       "  0.786427915096283,\n",
       "  1.120030164718628,\n",
       "  1.2498329877853394,\n",
       "  0.52206951379776,\n",
       "  1.5090688467025757,\n",
       "  0.4126182496547699,\n",
       "  0.3773142099380493,\n",
       "  0.3500087261199951,\n",
       "  1.140869140625,\n",
       "  0.46633389592170715,\n",
       "  0.596400797367096,\n",
       "  0.564470648765564,\n",
       "  1.4919390678405762,\n",
       "  1.2670645713806152,\n",
       "  1.0163670778274536,\n",
       "  1.2259941101074219,\n",
       "  0.910998523235321,\n",
       "  1.612134337425232,\n",
       "  1.1647971868515015,\n",
       "  0.9347596168518066,\n",
       "  0.6043503284454346,\n",
       "  0.6789218783378601,\n",
       "  0.6465762257575989,\n",
       "  1.2703638076782227,\n",
       "  1.0057212114334106,\n",
       "  1.466318964958191,\n",
       "  0.5493578910827637,\n",
       "  1.162831425666809,\n",
       "  0.528076171875,\n",
       "  0.494754821062088,\n",
       "  0.8399355411529541,\n",
       "  0.9142575860023499,\n",
       "  0.6787635684013367,\n",
       "  0.6173227429389954,\n",
       "  1.2972499132156372,\n",
       "  1.670602798461914,\n",
       "  1.0065988302230835,\n",
       "  0.7327443361282349,\n",
       "  1.7814861536026,\n",
       "  0.615924596786499,\n",
       "  0.27496209740638733,\n",
       "  1.3828141689300537,\n",
       "  0.8221216201782227,\n",
       "  1.3855640888214111,\n",
       "  0.919066309928894,\n",
       "  1.242087960243225,\n",
       "  0.5552262663841248,\n",
       "  0.9997557997703552,\n",
       "  0.6422197818756104,\n",
       "  1.0587773323059082,\n",
       "  0.5431973934173584,\n",
       "  1.4936842918395996,\n",
       "  1.027328610420227,\n",
       "  1.1269339323043823,\n",
       "  1.1313704252243042,\n",
       "  0.8590257167816162,\n",
       "  1.5351063013076782,\n",
       "  0.5741688013076782,\n",
       "  0.5880258083343506,\n",
       "  0.6605789661407471,\n",
       "  0.38201114535331726,\n",
       "  1.3925310373306274,\n",
       "  0.6465386748313904,\n",
       "  1.1965978145599365,\n",
       "  0.42216214537620544,\n",
       "  1.0822062492370605,\n",
       "  0.498617947101593,\n",
       "  0.8652104139328003,\n",
       "  0.6945610642433167,\n",
       "  0.28376802802085876,\n",
       "  0.6550620198249817,\n",
       "  1.568176507949829,\n",
       "  1.3118867874145508,\n",
       "  0.8758221864700317,\n",
       "  0.8968632817268372,\n",
       "  0.5917835831642151,\n",
       "  1.290953516960144,\n",
       "  0.7838027477264404,\n",
       "  1.5627013444900513,\n",
       "  1.358622670173645,\n",
       "  1.1988465785980225,\n",
       "  1.3657690286636353,\n",
       "  0.6459164619445801,\n",
       "  0.7976279258728027,\n",
       "  1.5042757987976074,\n",
       "  1.2826200723648071,\n",
       "  1.2033486366271973,\n",
       "  1.4015004634857178,\n",
       "  0.7512814402580261,\n",
       "  0.45043668150901794,\n",
       "  1.0734624862670898,\n",
       "  0.274008572101593,\n",
       "  1.760499358177185,\n",
       "  1.0209999084472656,\n",
       "  1.4413890838623047,\n",
       "  0.5690121650695801,\n",
       "  0.6133047342300415,\n",
       "  1.4236845970153809,\n",
       "  1.5739322900772095,\n",
       "  0.7500831484794617,\n",
       "  1.220069408416748,\n",
       "  1.2621355056762695,\n",
       "  1.3151713609695435,\n",
       "  1.2587077617645264,\n",
       "  1.3278595209121704,\n",
       "  1.0399137735366821,\n",
       "  0.4279339015483856,\n",
       "  0.6017356514930725,\n",
       "  0.6513342261314392,\n",
       "  1.3541477918624878,\n",
       "  0.5600175261497498,\n",
       "  0.9453433156013489,\n",
       "  1.135129451751709,\n",
       "  0.37289783358573914,\n",
       "  1.0023045539855957,\n",
       "  1.1862438917160034,\n",
       "  1.4104114770889282,\n",
       "  0.5499657988548279,\n",
       "  1.4634044170379639,\n",
       "  1.094742774963379,\n",
       "  0.8780617713928223,\n",
       "  1.5441867113113403,\n",
       "  1.0417180061340332,\n",
       "  0.4848816692829132,\n",
       "  0.32354307174682617,\n",
       "  0.49324730038642883,\n",
       "  1.1838494539260864,\n",
       "  0.8788110613822937,\n",
       "  0.5641840696334839,\n",
       "  0.5016558766365051,\n",
       "  1.0568333864212036,\n",
       "  0.3346823751926422,\n",
       "  0.6458671689033508,\n",
       "  1.1924521923065186,\n",
       "  0.6681726574897766,\n",
       "  1.1220344305038452,\n",
       "  0.8621901273727417,\n",
       "  1.2544621229171753,\n",
       "  0.695724606513977,\n",
       "  0.9341983795166016,\n",
       "  0.5251232981681824,\n",
       "  1.066947340965271,\n",
       "  0.5136777758598328,\n",
       "  1.759800672531128,\n",
       "  1.341850757598877,\n",
       "  1.0407001972198486,\n",
       "  1.249726414680481,\n",
       "  1.397818684577942,\n",
       "  0.6844727993011475,\n",
       "  1.0193167924880981,\n",
       "  0.5533517599105835,\n",
       "  0.9360746145248413,\n",
       "  1.1055333614349365,\n",
       "  1.3381409645080566,\n",
       "  0.44512978196144104,\n",
       "  0.7391529083251953,\n",
       "  2.0778422355651855,\n",
       "  0.5364860892295837,\n",
       "  0.955412745475769,\n",
       "  1.0201029777526855,\n",
       "  0.6043503284454346,\n",
       "  0.7161015272140503,\n",
       "  1.569469928741455,\n",
       "  0.5833660364151001,\n",
       "  0.9816334247589111,\n",
       "  1.0124015808105469,\n",
       "  0.6865732669830322,\n",
       "  1.3861949443817139,\n",
       "  1.0997810363769531,\n",
       "  0.6134110689163208,\n",
       "  0.836477518081665,\n",
       "  1.0520942211151123,\n",
       "  1.2505563497543335,\n",
       "  1.2586404085159302,\n",
       "  1.1958763599395752,\n",
       "  0.546877920627594,\n",
       "  0.6998218297958374,\n",
       "  0.5655149221420288,\n",
       "  1.876149296760559,\n",
       "  0.4306924641132355,\n",
       "  1.1063851118087769,\n",
       "  1.11307692527771,\n",
       "  1.020797848701477,\n",
       "  1.7372382879257202,\n",
       "  0.6553639769554138,\n",
       "  0.6819436550140381,\n",
       "  0.6515609622001648,\n",
       "  0.5720989108085632,\n",
       "  0.5505211353302002,\n",
       "  0.3921316862106323,\n",
       "  1.008102297782898,\n",
       "  1.1383954286575317,\n",
       "  1.0464316606521606,\n",
       "  0.8769935369491577,\n",
       "  0.7458680868148804,\n",
       "  1.7697011232376099,\n",
       "  0.423494428396225,\n",
       "  0.4577988386154175,\n",
       "  0.5066663026809692,\n",
       "  1.0587027072906494,\n",
       "  0.5913937091827393,\n",
       "  1.4448089599609375,\n",
       "  1.2388280630111694,\n",
       "  0.7231143116950989,\n",
       "  1.0568536520004272,\n",
       "  0.3140801191329956,\n",
       "  1.2076642513275146,\n",
       "  0.9606320858001709,\n",
       "  0.6075804233551025,\n",
       "  1.25347101688385,\n",
       "  0.3622612953186035,\n",
       "  1.0997506380081177,\n",
       "  0.47112396359443665,\n",
       "  0.4223342835903168,\n",
       "  0.7655949592590332,\n",
       "  0.8429760932922363,\n",
       "  1.1046298742294312,\n",
       "  0.8856599926948547,\n",
       "  1.0910767316818237,\n",
       "  0.3634016513824463,\n",
       "  0.5282787680625916,\n",
       "  0.5709663033485413,\n",
       "  0.3296886682510376,\n",
       "  0.5043754577636719,\n",
       "  1.6248470544815063,\n",
       "  1.4902372360229492,\n",
       "  1.4166507720947266,\n",
       "  1.2365012168884277,\n",
       "  1.3900772333145142,\n",
       "  1.089422583580017,\n",
       "  0.35140591859817505,\n",
       "  0.7973366379737854,\n",
       "  0.9172472953796387,\n",
       "  0.4114294648170471,\n",
       "  1.3112828731536865,\n",
       "  0.5657700896263123,\n",
       "  1.6388652324676514,\n",
       "  0.5464658737182617,\n",
       "  1.2480332851409912,\n",
       "  0.8441849946975708,\n",
       "  1.049652338027954,\n",
       "  1.0512257814407349,\n",
       "  1.0007339715957642,\n",
       "  1.1323603391647339,\n",
       "  0.5119727253913879,\n",
       "  1.3736834526062012,\n",
       "  2.2595345973968506,\n",
       "  0.9031075835227966,\n",
       "  1.192334771156311,\n",
       "  1.000667691230774,\n",
       "  1.0892667770385742,\n",
       "  1.0604451894760132,\n",
       "  0.5234282612800598,\n",
       "  0.7730786800384521,\n",
       "  0.6328429579734802,\n",
       "  1.4631510972976685,\n",
       "  1.1343423128128052,\n",
       "  1.2526222467422485,\n",
       "  1.3093534708023071,\n",
       "  1.431511402130127,\n",
       "  1.0006266832351685,\n",
       "  0.6599087715148926,\n",
       "  1.32050621509552,\n",
       "  1.2086375951766968,\n",
       "  0.37555474042892456,\n",
       "  1.039521336555481,\n",
       "  1.4852224588394165,\n",
       "  0.805689811706543,\n",
       "  1.10277259349823,\n",
       "  0.4382367730140686,\n",
       "  0.9037628173828125,\n",
       "  1.4402027130126953,\n",
       "  1.0254684686660767,\n",
       "  0.36822715401649475,\n",
       "  0.9418348670005798,\n",
       "  1.2862790822982788,\n",
       "  1.1780918836593628,\n",
       "  0.7630501389503479,\n",
       "  0.8278738856315613,\n",
       "  1.2070749998092651],\n",
       " ('euclidean', False, False): [0.7257487177848816,\n",
       "  0.44079044461250305,\n",
       "  0.6453565955162048,\n",
       "  1.737955927848816,\n",
       "  0.9368528127670288,\n",
       "  1.1823526620864868,\n",
       "  0.6736451983451843,\n",
       "  1.4422634840011597,\n",
       "  1.6149108409881592,\n",
       "  0.3402709364891052,\n",
       "  0.43995678424835205,\n",
       "  0.6422197818756104,\n",
       "  0.724436342716217,\n",
       "  1.3212701082229614,\n",
       "  0.6493316888809204,\n",
       "  1.092244029045105,\n",
       "  0.9969993829727173,\n",
       "  0.9590622782707214,\n",
       "  1.0773221254348755,\n",
       "  0.6156513690948486,\n",
       "  1.0849958658218384,\n",
       "  0.9282934069633484,\n",
       "  1.2618006467819214,\n",
       "  0.48216190934181213,\n",
       "  0.5486834645271301,\n",
       "  0.6690059304237366,\n",
       "  0.5310548543930054,\n",
       "  0.8979836702346802,\n",
       "  1.3165591955184937,\n",
       "  2.018754005432129,\n",
       "  0.6833351254463196,\n",
       "  0.895882248878479,\n",
       "  0.6164363026618958,\n",
       "  0.5490787625312805,\n",
       "  0.5996875166893005,\n",
       "  0.6394430994987488,\n",
       "  0.6380056738853455,\n",
       "  0.6365237832069397,\n",
       "  1.2361271381378174,\n",
       "  1.0866247415542603,\n",
       "  0.3500087261199951,\n",
       "  0.9476178288459778,\n",
       "  0.9414106607437134,\n",
       "  1.3105663061141968,\n",
       "  0.9226700067520142,\n",
       "  0.46766436100006104,\n",
       "  0.6247072219848633,\n",
       "  0.5779039859771729,\n",
       "  1.9991027116775513,\n",
       "  0.65824294090271,\n",
       "  0.6067209839820862,\n",
       "  0.5185645222663879,\n",
       "  1.8220971822738647,\n",
       "  0.8507455587387085,\n",
       "  0.6938118934631348,\n",
       "  1.1416360139846802,\n",
       "  1.3912087678909302,\n",
       "  0.45251813530921936,\n",
       "  1.4443155527114868,\n",
       "  0.9152935743331909,\n",
       "  0.5250971913337708,\n",
       "  1.1850038766860962,\n",
       "  1.046406865119934,\n",
       "  0.9732990264892578,\n",
       "  0.662108838558197,\n",
       "  0.9853987693786621,\n",
       "  0.29863154888153076,\n",
       "  1.006240725517273,\n",
       "  1.486116647720337,\n",
       "  0.7413421273231506,\n",
       "  1.0065926313400269,\n",
       "  1.3507146835327148,\n",
       "  0.9110603332519531,\n",
       "  0.40597596764564514,\n",
       "  1.3307653665542603,\n",
       "  1.2816129922866821,\n",
       "  0.5954527258872986,\n",
       "  0.9004970788955688,\n",
       "  1.2953940629959106,\n",
       "  0.6942553520202637,\n",
       "  1.071597695350647,\n",
       "  0.5693202018737793,\n",
       "  1.8976600170135498,\n",
       "  0.5385289788246155,\n",
       "  1.4789036512374878,\n",
       "  0.7972621321678162,\n",
       "  0.4897020757198334,\n",
       "  0.7520859241485596,\n",
       "  0.6214337944984436,\n",
       "  0.4002279043197632,\n",
       "  0.46444037556648254,\n",
       "  1.013098120689392,\n",
       "  1.27866792678833,\n",
       "  0.6941745281219482,\n",
       "  1.0192055702209473,\n",
       "  1.0506936311721802,\n",
       "  0.5382604002952576,\n",
       "  1.3129496574401855,\n",
       "  0.9807860255241394,\n",
       "  0.8957507610321045,\n",
       "  0.9196656346321106,\n",
       "  0.5235675573348999,\n",
       "  0.5611280798912048,\n",
       "  1.054927110671997,\n",
       "  1.159218430519104,\n",
       "  1.6475143432617188,\n",
       "  1.3697772026062012,\n",
       "  0.46787410974502563,\n",
       "  0.3831605613231659,\n",
       "  0.5358534455299377,\n",
       "  0.26111793518066406,\n",
       "  0.44140544533729553,\n",
       "  1.428360939025879,\n",
       "  1.2139924764633179,\n",
       "  0.6675499677658081,\n",
       "  0.607884407043457,\n",
       "  0.6022205948829651,\n",
       "  0.823807418346405,\n",
       "  0.5174877643585205,\n",
       "  1.1263928413391113,\n",
       "  0.6119208931922913,\n",
       "  0.2525971233844757,\n",
       "  0.8339874148368835,\n",
       "  1.0927748680114746,\n",
       "  1.3842384815216064,\n",
       "  0.7972207069396973,\n",
       "  0.5438300967216492,\n",
       "  0.5132135152816772,\n",
       "  0.5329561233520508,\n",
       "  1.521005630493164,\n",
       "  1.011030673980713,\n",
       "  1.0981853008270264,\n",
       "  1.1723434925079346,\n",
       "  0.5652436017990112,\n",
       "  1.0166949033737183,\n",
       "  1.2187057733535767,\n",
       "  0.9604068994522095,\n",
       "  1.5190349817276,\n",
       "  1.3956047296524048,\n",
       "  0.6481265425682068,\n",
       "  1.223137378692627,\n",
       "  1.5710997581481934,\n",
       "  0.4142124354839325,\n",
       "  0.5630947351455688,\n",
       "  1.6820188760757446,\n",
       "  1.4598417282104492,\n",
       "  0.6382508873939514,\n",
       "  1.563968300819397,\n",
       "  0.5391364693641663,\n",
       "  0.8739398717880249,\n",
       "  1.4383939504623413,\n",
       "  1.0351157188415527,\n",
       "  1.2477548122406006,\n",
       "  0.5745441317558289,\n",
       "  0.5789437294006348,\n",
       "  1.4929440021514893,\n",
       "  1.0407891273498535,\n",
       "  1.1334072351455688,\n",
       "  1.6428871154785156,\n",
       "  1.1807348728179932,\n",
       "  1.1242400407791138,\n",
       "  0.5252817869186401,\n",
       "  0.2629193961620331,\n",
       "  1.2394953966140747,\n",
       "  1.4994628429412842,\n",
       "  0.6544301509857178,\n",
       "  1.1219068765640259,\n",
       "  0.8080441355705261,\n",
       "  0.641869306564331,\n",
       "  1.4615188837051392,\n",
       "  0.6755331754684448,\n",
       "  1.2123916149139404,\n",
       "  0.8855928778648376,\n",
       "  1.1860815286636353,\n",
       "  1.7406855821609497,\n",
       "  1.6539734601974487,\n",
       "  1.0139352083206177,\n",
       "  0.4481181204319,\n",
       "  0.6961823105812073,\n",
       "  1.3109827041625977,\n",
       "  0.6106360554695129,\n",
       "  0.604810357093811,\n",
       "  0.6566122174263,\n",
       "  0.5218933820724487,\n",
       "  1.6739393472671509,\n",
       "  0.6831259727478027,\n",
       "  1.9606437683105469,\n",
       "  0.7015284299850464,\n",
       "  0.34130018949508667,\n",
       "  1.0430984497070312,\n",
       "  0.306024432182312,\n",
       "  1.1873679161071777,\n",
       "  0.2730385959148407,\n",
       "  1.480963945388794,\n",
       "  1.2391902208328247,\n",
       "  0.643815815448761,\n",
       "  0.4611203372478485,\n",
       "  0.8986679911613464,\n",
       "  0.6381512880325317,\n",
       "  0.6204314827919006,\n",
       "  1.4755663871765137,\n",
       "  0.8385308980941772,\n",
       "  0.5215109586715698,\n",
       "  0.935821533203125,\n",
       "  0.8822345733642578,\n",
       "  0.8511415719985962,\n",
       "  0.9861621260643005,\n",
       "  1.0136421918869019,\n",
       "  1.043243646621704,\n",
       "  0.6272203326225281,\n",
       "  1.5574320554733276,\n",
       "  2.0972678661346436,\n",
       "  1.89162278175354,\n",
       "  0.9186741709709167,\n",
       "  0.6863878965377808,\n",
       "  0.9139856696128845,\n",
       "  0.7714884877204895,\n",
       "  0.603299617767334,\n",
       "  0.7705226540565491,\n",
       "  0.977032482624054,\n",
       "  0.7830610871315002,\n",
       "  1.1533393859863281,\n",
       "  0.6110135912895203,\n",
       "  1.2522941827774048,\n",
       "  0.45311060547828674,\n",
       "  1.045925498008728,\n",
       "  0.5647526383399963,\n",
       "  1.204813838005066,\n",
       "  0.48720869421958923,\n",
       "  0.4863552451133728,\n",
       "  0.5604802966117859,\n",
       "  0.9668511748313904,\n",
       "  0.7864277958869934,\n",
       "  1.1200299263000488,\n",
       "  1.063832402229309,\n",
       "  0.5220695734024048,\n",
       "  1.4489445686340332,\n",
       "  0.4126182496547699,\n",
       "  0.3773142099380493,\n",
       "  0.3500087261199951,\n",
       "  1.140869379043579,\n",
       "  1.086063027381897,\n",
       "  0.596400797367096,\n",
       "  0.564470648765564,\n",
       "  0.5727230906486511,\n",
       "  1.2670644521713257,\n",
       "  0.633052408695221,\n",
       "  1.2259942293167114,\n",
       "  0.9109984636306763,\n",
       "  1.767066478729248,\n",
       "  1.1140283346176147,\n",
       "  0.9347593784332275,\n",
       "  1.3275020122528076,\n",
       "  0.9998618364334106,\n",
       "  0.6465762257575989,\n",
       "  1.066281795501709,\n",
       "  1.005721092224121,\n",
       "  1.548637866973877,\n",
       "  1.136651635169983,\n",
       "  1.162831425666809,\n",
       "  0.5280762314796448,\n",
       "  0.24335096776485443,\n",
       "  0.9707783460617065,\n",
       "  0.9142575860023499,\n",
       "  0.6787635684013367,\n",
       "  0.6173226833343506,\n",
       "  1.204202651977539,\n",
       "  0.5853298306465149,\n",
       "  0.6797995567321777,\n",
       "  0.35597798228263855,\n",
       "  1.7906724214553833,\n",
       "  0.6159246563911438,\n",
       "  0.27496203780174255,\n",
       "  1.3828141689300537,\n",
       "  0.5070896148681641,\n",
       "  1.3283179998397827,\n",
       "  0.919066309928894,\n",
       "  1.197161078453064,\n",
       "  0.5552262663841248,\n",
       "  0.9997556209564209,\n",
       "  0.6422197818756104,\n",
       "  1.0587773323059082,\n",
       "  0.5431973934173584,\n",
       "  1.2749134302139282,\n",
       "  1.027328610420227,\n",
       "  1.1269339323043823,\n",
       "  1.1313703060150146,\n",
       "  0.8590256571769714,\n",
       "  0.7381551265716553,\n",
       "  0.5741688013076782,\n",
       "  0.5880258083343506,\n",
       "  0.6605789661407471,\n",
       "  0.38201114535331726,\n",
       "  1.2598192691802979,\n",
       "  0.6465386152267456,\n",
       "  1.1153732538223267,\n",
       "  0.42216211557388306,\n",
       "  0.5429599285125732,\n",
       "  0.49861788749694824,\n",
       "  0.8652104139328003,\n",
       "  0.6945611238479614,\n",
       "  0.28376802802085876,\n",
       "  0.5973643064498901,\n",
       "  1.5681766271591187,\n",
       "  1.716565728187561,\n",
       "  0.5592262744903564,\n",
       "  0.8968635201454163,\n",
       "  2.258617877960205,\n",
       "  0.6866031289100647,\n",
       "  0.7668276429176331,\n",
       "  1.5519992113113403,\n",
       "  1.358622670173645,\n",
       "  1.1256020069122314,\n",
       "  1.3657687902450562,\n",
       "  0.6459165215492249,\n",
       "  0.7976279854774475,\n",
       "  1.594462275505066,\n",
       "  1.1533297300338745,\n",
       "  1.5806043148040771,\n",
       "  1.4015003442764282,\n",
       "  3.883368730545044,\n",
       "  0.45043665170669556,\n",
       "  1.0734620094299316,\n",
       "  0.274008572101593,\n",
       "  1.2503248453140259,\n",
       "  1.0553709268569946,\n",
       "  0.6435633301734924,\n",
       "  0.5690121650695801,\n",
       "  0.6133047342300415,\n",
       "  1.42368483543396,\n",
       "  1.5739322900772095,\n",
       "  0.9290655255317688,\n",
       "  1.220069408416748,\n",
       "  1.3991334438323975,\n",
       "  1.1061110496520996,\n",
       "  1.447482943534851,\n",
       "  1.3278595209121704,\n",
       "  0.6324245929718018,\n",
       "  0.427933931350708,\n",
       "  0.6017356514930725,\n",
       "  0.6513341665267944,\n",
       "  1.2442529201507568,\n",
       "  0.5600175261497498,\n",
       "  0.8432410359382629,\n",
       "  1.1351290941238403,\n",
       "  0.3710305988788605,\n",
       "  1.1098499298095703,\n",
       "  1.4785983562469482,\n",
       "  0.6271103024482727,\n",
       "  0.5499659180641174,\n",
       "  1.1637212038040161,\n",
       "  1.0947428941726685,\n",
       "  0.8780617713928223,\n",
       "  0.6651003956794739,\n",
       "  1.0417181253433228,\n",
       "  0.484881728887558,\n",
       "  0.32354313135147095,\n",
       "  0.4932474195957184,\n",
       "  1.1838496923446655,\n",
       "  0.5372762084007263,\n",
       "  0.5641840696334839,\n",
       "  0.5016559362411499,\n",
       "  0.8807199001312256,\n",
       "  0.3346823751926422,\n",
       "  0.645867109298706,\n",
       "  0.7357816696166992,\n",
       "  0.6681725978851318,\n",
       "  1.1220345497131348,\n",
       "  0.783340573310852,\n",
       "  1.2544620037078857,\n",
       "  0.695724606513977,\n",
       "  0.9341985583305359,\n",
       "  0.5251232385635376,\n",
       "  0.6883984804153442,\n",
       "  0.2551255226135254,\n",
       "  1.8787119388580322,\n",
       "  1.341850757598877,\n",
       "  0.9833834171295166,\n",
       "  0.4680776000022888,\n",
       "  1.293298363685608,\n",
       "  0.6844727993011475,\n",
       "  1.01931631565094,\n",
       "  0.5533517599105835,\n",
       "  0.8539395332336426,\n",
       "  1.105533480644226,\n",
       "  0.5791457891464233,\n",
       "  0.44512978196144104,\n",
       "  0.7135886549949646,\n",
       "  2.3681893348693848,\n",
       "  0.5364860892295837,\n",
       "  0.6157494187355042,\n",
       "  1.3007359504699707,\n",
       "  0.6043505668640137,\n",
       "  0.7161015272140503,\n",
       "  1.2365655899047852,\n",
       "  0.5623366236686707,\n",
       "  0.9816334247589111,\n",
       "  1.1215792894363403,\n",
       "  0.6749280095100403,\n",
       "  1.3339096307754517,\n",
       "  1.0997812747955322,\n",
       "  0.40783584117889404,\n",
       "  0.8190032839775085,\n",
       "  1.4242279529571533,\n",
       "  0.6220806241035461,\n",
       "  1.39885675907135,\n",
       "  1.1958763599395752,\n",
       "  0.546877920627594,\n",
       "  0.6683913469314575,\n",
       "  0.4733003079891205,\n",
       "  2.1253325939178467,\n",
       "  0.4306924641132355,\n",
       "  1.4435077905654907,\n",
       "  1.037014365196228,\n",
       "  0.5018161535263062,\n",
       "  0.6625329256057739,\n",
       "  0.33888718485832214,\n",
       "  0.35625115036964417,\n",
       "  0.6515610218048096,\n",
       "  0.5720990300178528,\n",
       "  0.5505211353302002,\n",
       "  0.3921316862106323,\n",
       "  0.9669743180274963,\n",
       "  1.1383954286575317,\n",
       "  1.0464317798614502,\n",
       "  0.8769934773445129,\n",
       "  0.745867908000946,\n",
       "  0.7313107848167419,\n",
       "  0.423494428396225,\n",
       "  0.4577988386154175,\n",
       "  0.5066663026809692,\n",
       "  1.0621432065963745,\n",
       "  0.5913936495780945,\n",
       "  0.5253543853759766,\n",
       "  1.1792700290679932,\n",
       "  0.797559380531311,\n",
       "  0.973879337310791,\n",
       "  0.2862391173839569,\n",
       "  1.2076646089553833,\n",
       "  0.6632099151611328,\n",
       "  0.6075804829597473,\n",
       "  0.9645853042602539,\n",
       "  0.36226126551628113,\n",
       "  1.0997509956359863,\n",
       "  1.1516443490982056,\n",
       "  0.24555645883083344,\n",
       "  0.765595018863678,\n",
       "  0.8429761528968811,\n",
       "  0.7777427434921265,\n",
       "  0.9653602242469788,\n",
       "  0.9700911641120911,\n",
       "  0.3634016513824463,\n",
       "  0.5282787680625916,\n",
       "  0.5709663033485413,\n",
       "  0.6711762547492981,\n",
       "  0.5043755173683167,\n",
       "  0.9251598119735718,\n",
       "  1.4902373552322388,\n",
       "  1.4166507720947266,\n",
       "  1.2543138265609741,\n",
       "  0.5664067268371582,\n",
       "  1.0894227027893066,\n",
       "  0.35140591859817505,\n",
       "  0.562394917011261,\n",
       "  0.9881948232650757,\n",
       "  0.41142961382865906,\n",
       "  1.311282753944397,\n",
       "  1.2795064449310303,\n",
       "  1.7117441892623901,\n",
       "  0.5464659929275513,\n",
       "  1.248034119606018,\n",
       "  0.8441846966743469,\n",
       "  1.2422335147857666,\n",
       "  1.0630840063095093,\n",
       "  1.0007342100143433,\n",
       "  1.1323604583740234,\n",
       "  0.5119727849960327,\n",
       "  0.5736777186393738,\n",
       "  1.5309107303619385,\n",
       "  0.5151401162147522,\n",
       "  1.1923344135284424,\n",
       "  0.6164764761924744,\n",
       "  1.0892670154571533,\n",
       "  1.0166716575622559,\n",
       "  0.523428201675415,\n",
       "  1.2109835147857666,\n",
       "  0.6125954985618591,\n",
       "  0.6936299800872803,\n",
       "  0.8364459872245789,\n",
       "  0.9097495079040527,\n",
       "  0.8409786820411682,\n",
       "  1.5700651407241821,\n",
       "  0.9282135367393494,\n",
       "  0.6599088907241821,\n",
       "  1.32050621509552,\n",
       "  1.2086373567581177,\n",
       "  0.37555474042892456,\n",
       "  1.2811018228530884,\n",
       "  1.5266985893249512,\n",
       "  0.8056895732879639,\n",
       "  1.102772831916809,\n",
       "  0.438236802816391,\n",
       "  0.8701990842819214,\n",
       "  1.4402025938034058,\n",
       "  0.627324104309082,\n",
       "  0.36822718381881714,\n",
       "  0.9418346285820007,\n",
       "  1.5703338384628296,\n",
       "  1.1780916452407837,\n",
       "  0.7630502581596375,\n",
       "  0.5806347727775574,\n",
       "  1.2070748805999756]}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(ratio_results['checkpoints/finetuned_0']))\n",
    "print(len(ratio_results['google/codegemma-2b']))\n",
    "\n",
    "ratio_results['checkpoints/finetuned_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matthews_correlation(tp: int, tn: int, fp: int, fn: int) -> float:\n",
    "    numerator = tp * tn - fp * fn\n",
    "    denominator = ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "    return numerator / denominator\n",
    "\n",
    "def p4_score(tp: int, tn: int, fp: int, fn: int) -> float:\n",
    "    return (4 * tp * tn) / (4 * tp * tn + (tp + tn) * (fp + fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 24.54it/s]\n"
     ]
    }
   ],
   "source": [
    "search_results = {}\n",
    "\n",
    "for distance_function, normalize, sample_many in tqdm(itertools.product(\n",
    "    ['cosine', 'euclidean'],\n",
    "    [True, False],\n",
    "    [True, False]\n",
    "), total=2*2*2):\n",
    "    for mid in eval_data.keys():\n",
    "        ratios = ratio_results[mid][(distance_function, normalize, sample_many)]\n",
    "        labels = [d['l'] for d in eval_data[mid]]\n",
    "        p4s = []\n",
    "        thresholds = []\n",
    "        confusion_matrices = []\n",
    "\n",
    "        for threshold in np.linspace(0, 3, 200):\n",
    "            classifications = [r <= threshold for r in ratios]\n",
    "            tp = sum(\n",
    "                (1 if (l and c) else 0) \n",
    "                for l, c in zip(labels, classifications)\n",
    "            )\n",
    "            tn = sum(\n",
    "                (1 if (not l and not c) else 0) \n",
    "                for l, c in zip(labels, classifications)\n",
    "            )\n",
    "            fp = sum(\n",
    "                (1 if (not l and c) else 0) \n",
    "                for l, c in zip(labels, classifications)\n",
    "            )\n",
    "            fn = sum(\n",
    "                (1 if (l and not c) else 0) \n",
    "                for l, c in zip(labels, classifications)\n",
    "            )\n",
    "            p4 = p4_score(tp, tn, fp, fn)\n",
    "            p4s.append(p4)\n",
    "            thresholds.append(threshold)\n",
    "            confusion_matrices.append((tp, tn, fp, fn))\n",
    "\n",
    "        search_results[(mid, distance_function, normalize, sample_many)] = (\n",
    "            thresholds, p4s, confusion_matrices\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:01,  8.26it/s]                      \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid</th>\n",
       "      <th>distance_function</th>\n",
       "      <th>normalize</th>\n",
       "      <th>sample_many</th>\n",
       "      <th>test_threshold</th>\n",
       "      <th>p4</th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>checkpoints/finetuned_0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>checkpoints/finetuned_0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.015075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>checkpoints/finetuned_0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>checkpoints/finetuned_0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.045226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>checkpoints/finetuned_0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.060302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>google/codegemma-2b</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.939698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>google/codegemma-2b</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.954774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>google/codegemma-2b</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.969849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>google/codegemma-2b</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.984925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>google/codegemma-2b</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          mid distance_function  normalize  sample_many  \\\n",
       "0     checkpoints/finetuned_0            cosine       True         True   \n",
       "1     checkpoints/finetuned_0            cosine       True         True   \n",
       "2     checkpoints/finetuned_0            cosine       True         True   \n",
       "3     checkpoints/finetuned_0            cosine       True         True   \n",
       "4     checkpoints/finetuned_0            cosine       True         True   \n",
       "...                       ...               ...        ...          ...   \n",
       "3195      google/codegemma-2b         euclidean      False        False   \n",
       "3196      google/codegemma-2b         euclidean      False        False   \n",
       "3197      google/codegemma-2b         euclidean      False        False   \n",
       "3198      google/codegemma-2b         euclidean      False        False   \n",
       "3199      google/codegemma-2b         euclidean      False        False   \n",
       "\n",
       "      test_threshold   p4   tp   tn   fp   fn  \n",
       "0           0.000000  0.0    0  243    0  269  \n",
       "1           0.015075  0.0    0  243    0  269  \n",
       "2           0.030151  0.0    0  243    0  269  \n",
       "3           0.045226  0.0    0  243    0  269  \n",
       "4           0.060302  0.0    0  243    0  269  \n",
       "...              ...  ...  ...  ...  ...  ...  \n",
       "3195        2.939698  0.0  269    0  243    0  \n",
       "3196        2.954774  0.0  269    0  243    0  \n",
       "3197        2.969849  0.0  269    0  243    0  \n",
       "3198        2.984925  0.0  269    0  243    0  \n",
       "3199        3.000000  0.0  269    0  243    0  \n",
       "\n",
       "[3200 rows x 10 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[\n",
    "    'mid', 'distance_function', 'normalize', 'sample_many', 'test_threshold', 'p4',\n",
    "    'tp', 'tn', 'fp', 'fn'\n",
    "])\n",
    "\n",
    "for mid, distance_function, normalize, sample_many in tqdm(itertools.product(\n",
    "    ['checkpoints/finetuned_0', 'google/codegemma-2b'],\n",
    "    ['cosine', 'euclidean'],\n",
    "    [True, False],\n",
    "    [True, False]\n",
    "), total=2*2*2):\n",
    "    for threshold, p4, (tp, tn, fp, fn) in zip(*search_results[(mid, distance_function, normalize, sample_many)]):\n",
    "        df.loc[len(df)] = {\n",
    "            'mid': mid,\n",
    "            'distance_function': distance_function,\n",
    "            'normalize': normalize,\n",
    "            'sample_many': sample_many,\n",
    "            'test_threshold': threshold,\n",
    "            'p4': p4,\n",
    "            'tp': tp,\n",
    "            'tn': tn,\n",
    "            'fp': fp,\n",
    "            'fn': fn\n",
    "        }\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3869346733668342 0.5335448285806986\n"
     ]
    }
   ],
   "source": [
    "optimum_df = df.nlargest(1, 'p4')\n",
    "X_opt = optimum_df['test_threshold'].values[0]\n",
    "Y_opt = optimum_df['p4'].values[0]\n",
    "\n",
    "print(X_opt, Y_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_362504/4082372899.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_opt = df.groupby(['mid', 'distance_function', 'normalize', 'sample_many']).apply(lambda x: x.nlargest(1, 'p4')).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid</th>\n",
       "      <th>distance_function</th>\n",
       "      <th>normalize</th>\n",
       "      <th>sample_many</th>\n",
       "      <th>test_threshold</th>\n",
       "      <th>p4</th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>checkpoints/finetuned_0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.341709</td>\n",
       "      <td>0.509823</td>\n",
       "      <td>186</td>\n",
       "      <td>91</td>\n",
       "      <td>152</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>checkpoints/finetuned_0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.386935</td>\n",
       "      <td>0.533545</td>\n",
       "      <td>185</td>\n",
       "      <td>100</td>\n",
       "      <td>143</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>checkpoints/finetuned_0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1.341709</td>\n",
       "      <td>0.509823</td>\n",
       "      <td>186</td>\n",
       "      <td>91</td>\n",
       "      <td>152</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>checkpoints/finetuned_0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.386935</td>\n",
       "      <td>0.533545</td>\n",
       "      <td>185</td>\n",
       "      <td>100</td>\n",
       "      <td>143</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>checkpoints/finetuned_0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.889447</td>\n",
       "      <td>0.504510</td>\n",
       "      <td>139</td>\n",
       "      <td>120</td>\n",
       "      <td>123</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>checkpoints/finetuned_0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.904523</td>\n",
       "      <td>0.530913</td>\n",
       "      <td>131</td>\n",
       "      <td>141</td>\n",
       "      <td>102</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>checkpoints/finetuned_0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.502399</td>\n",
       "      <td>139</td>\n",
       "      <td>119</td>\n",
       "      <td>124</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>checkpoints/finetuned_0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.783920</td>\n",
       "      <td>0.530355</td>\n",
       "      <td>115</td>\n",
       "      <td>160</td>\n",
       "      <td>83</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>google/codegemma-2b</td>\n",
       "      <td>cosine</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.010050</td>\n",
       "      <td>0.491935</td>\n",
       "      <td>130</td>\n",
       "      <td>122</td>\n",
       "      <td>121</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>google/codegemma-2b</td>\n",
       "      <td>cosine</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.010050</td>\n",
       "      <td>0.495100</td>\n",
       "      <td>119</td>\n",
       "      <td>135</td>\n",
       "      <td>108</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>google/codegemma-2b</td>\n",
       "      <td>cosine</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1.010050</td>\n",
       "      <td>0.491935</td>\n",
       "      <td>130</td>\n",
       "      <td>122</td>\n",
       "      <td>121</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>google/codegemma-2b</td>\n",
       "      <td>cosine</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.010050</td>\n",
       "      <td>0.495100</td>\n",
       "      <td>119</td>\n",
       "      <td>135</td>\n",
       "      <td>108</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>google/codegemma-2b</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.514382</td>\n",
       "      <td>114</td>\n",
       "      <td>152</td>\n",
       "      <td>91</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>google/codegemma-2b</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.085427</td>\n",
       "      <td>0.525802</td>\n",
       "      <td>156</td>\n",
       "      <td>116</td>\n",
       "      <td>127</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>google/codegemma-2b</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.518369</td>\n",
       "      <td>115</td>\n",
       "      <td>153</td>\n",
       "      <td>90</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>google/codegemma-2b</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.100503</td>\n",
       "      <td>0.526173</td>\n",
       "      <td>169</td>\n",
       "      <td>107</td>\n",
       "      <td>136</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        mid distance_function  normalize  sample_many  \\\n",
       "0   checkpoints/finetuned_0            cosine      False        False   \n",
       "1   checkpoints/finetuned_0            cosine      False         True   \n",
       "2   checkpoints/finetuned_0            cosine       True        False   \n",
       "3   checkpoints/finetuned_0            cosine       True         True   \n",
       "4   checkpoints/finetuned_0         euclidean      False        False   \n",
       "5   checkpoints/finetuned_0         euclidean      False         True   \n",
       "6   checkpoints/finetuned_0         euclidean       True        False   \n",
       "7   checkpoints/finetuned_0         euclidean       True         True   \n",
       "8       google/codegemma-2b            cosine      False        False   \n",
       "9       google/codegemma-2b            cosine      False         True   \n",
       "10      google/codegemma-2b            cosine       True        False   \n",
       "11      google/codegemma-2b            cosine       True         True   \n",
       "12      google/codegemma-2b         euclidean      False        False   \n",
       "13      google/codegemma-2b         euclidean      False         True   \n",
       "14      google/codegemma-2b         euclidean       True        False   \n",
       "15      google/codegemma-2b         euclidean       True         True   \n",
       "\n",
       "    test_threshold        p4   tp   tn   fp   fn  \n",
       "0         1.341709  0.509823  186   91  152   83  \n",
       "1         1.386935  0.533545  185  100  143   84  \n",
       "2         1.341709  0.509823  186   91  152   83  \n",
       "3         1.386935  0.533545  185  100  143   84  \n",
       "4         0.889447  0.504510  139  120  123  130  \n",
       "5         0.904523  0.530913  131  141  102  138  \n",
       "6         0.874372  0.502399  139  119  124  130  \n",
       "7         0.783920  0.530355  115  160   83  154  \n",
       "8         1.010050  0.491935  130  122  121  139  \n",
       "9         1.010050  0.495100  119  135  108  150  \n",
       "10        1.010050  0.491935  130  122  121  139  \n",
       "11        1.010050  0.495100  119  135  108  150  \n",
       "12        0.874372  0.514382  114  152   91  155  \n",
       "13        1.085427  0.525802  156  116  127  113  \n",
       "14        0.874372  0.518369  115  153   90  154  \n",
       "15        1.100503  0.526173  169  107  136  100  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_opt = df.groupby(['mid', 'distance_function', 'normalize', 'sample_many']).apply(lambda x: x.nlargest(1, 'p4')).reset_index(drop=True)\n",
    "df_opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_362504/733005911.py:45: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
      "  ax.scatter(X_best, Y_best, c=color, marker=marker, s=200, label=label)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mids = ['checkpoints/finetuned_0', 'google/codegemma-2b']\n",
    "distance_functions = ['cosine', 'euclidean']\n",
    "normalize_values = [False, True]\n",
    "sample_many_values = [False, True]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10), sharey=True, sharex=True)\n",
    "colors = [plt.get_cmap('bwr')(i) for i in (0, 1 - 1e-6)]\n",
    "line_styles = ['-', '--']\n",
    "\n",
    "for y, normalize in enumerate(normalize_values):\n",
    "    for x, sample_many in enumerate(sample_many_values):\n",
    "        ax = axs[x, y]\n",
    "        for (color, line_style), (mid, distance_function) in zip(itertools.product(colors, line_styles), itertools.product(mids, distance_functions)):\n",
    "            df_ = df[\n",
    "                (df['mid'] == mid) &\n",
    "                (df['distance_function'] == distance_function) &\n",
    "                (df['normalize'] == normalize) &\n",
    "                (df['sample_many'] == sample_many)\n",
    "            ]\n",
    "            X = df_['test_threshold']\n",
    "            Y = df_['p4']\n",
    "            if len(X) == 0 or len(Y) == 0:\n",
    "                continue\n",
    "            XY = list(zip(X, Y))\n",
    "            XY.sort(key=lambda x: x[0])\n",
    "            X, Y = zip(*XY)\n",
    "\n",
    "            XY_best = max(XY, key=lambda x: x[1])\n",
    "            X_best, Y_best = XY_best\n",
    "\n",
    "            title = \"\"\n",
    "            title += \"Normalized, \" if normalize else \"Not normalized, \"\n",
    "            title += \"Multisampling\" if sample_many else \"Single sampling\"\n",
    "            ax.set_title(title)\n",
    "            \n",
    "            ax.plot(X, Y, c=color, linestyle=line_style, label=f'Model: {mid}; Distance metric: {distance_function}')\n",
    "\n",
    "            is_best = X_best == X_opt and Y_best == Y_opt\n",
    "\n",
    "            marker = mpl.markers.MarkerStyle(\n",
    "                marker='*' if is_best else '.',\n",
    "                fillstyle='full'# if distance_function == 'cosine' else 'none'\n",
    "            )\n",
    "            label = 'Global optimum' if is_best else 'Local optimum*'\n",
    "            ax.scatter(X_best, Y_best, c=color, marker=marker, s=200, label=label)\n",
    "            if is_best:\n",
    "                ax.annotate(f'({X_best:.2f}, {Y_best:.2f})', (X_best, Y_best), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
    "\n",
    "            ax.set_xlabel('Threshold')\n",
    "            ax.set_ylabel('P4 score')\n",
    "\n",
    "            ax.set_ylim([0, 0.7])\n",
    "\n",
    "legend_elements = [\n",
    "    mpl.lines.Line2D([0], [0], color='black', linestyle='-', label='Euclidean metric'),\n",
    "    mpl.lines.Line2D([0], [0], color='black', linestyle='--', label='Cosine metric'),\n",
    "    mpl.lines.Line2D([0], [0], color=colors[0], label='Finetuned model'),\n",
    "    mpl.lines.Line2D([0], [0], color=colors[1], label='Codegemma 2b'),\n",
    "    mpl.lines.Line2D([0], [0], marker='*', color='k', label='Global optimum', markersize=10, linestyle='None'),\n",
    "    mpl.lines.Line2D([0], [0], marker='.', color='k', label='Local optimum', markersize=10, linestyle='None')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncol=3, bbox_to_anchor=(0.5, 0))\n",
    "fig.tight_layout(rect=[0, 0.05, 1, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../cache/updated_docstrings_1.json', 'r') as f:\n",
    "    updated_docstrings_1 = json.load(f)\n",
    "\n",
    "updated_docstrings_1 = {\n",
    "    mid: [list(x) for x in zip(*data)]\n",
    "    for mid, data in updated_docstrings_1.items()\n",
    "}\n",
    "\n",
    "seed = 1337\n",
    "sample_size=512\n",
    "\n",
    "with open(\"test_data.json\", 'r') as f:\n",
    "    test_data_2 = Random(seed).sample(\n",
    "        [\n",
    "            {'c': item['c'], 'd': item['d'], 'l': item['l']}\n",
    "            for item in json.load(f)\n",
    "        ],\n",
    "        k=sample_size\n",
    "    )\n",
    "\n",
    "eval_data_2 = {\n",
    "    mid: [td | {'g': gd} for td, gd in zip(test_data_2, updated_docstrings_0[mid])]\n",
    "    for mid in updated_docstrings_0.keys()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid = \"checkpoints/finetuned_0\"\n",
    "\n",
    "data = eval_data_2[mid]\n",
    "\n",
    "codes = [d['c'] for d in data]\n",
    "docstrings = [d['d'] for d in data]\n",
    "generated_docstrings = [d['g'] for d in data]\n",
    "labels = [d['l'] for d in data]\n",
    "\n",
    "ratios_2 = perform_test(\n",
    "    codes, docstrings, generated_docstrings, \n",
    "    distance_function='euclidean',\n",
    "    normalize=False, \n",
    "    sample_many=True\n",
    ")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"ratios_2.pkl\"\n",
    "if not os.path.exists(path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(ratios_2, f)\n",
    "else:\n",
    "    with open(path, 'rb') as f:\n",
    "        ratios_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 212, TN: 39, FP: 220, FN: 41\n",
      "P4 score: 0.3354736617875293\n"
     ]
    }
   ],
   "source": [
    "threshold = 1.39\n",
    "labels = [d['l'] for d in eval_data_2[mid]]\n",
    "classifications = [r <= threshold for r in ratios_2]\n",
    "tp = sum(\n",
    "    (1 if (l and c) else 0) \n",
    "    for l, c in zip(labels, classifications)\n",
    ")\n",
    "tn = sum(\n",
    "    (1 if (not l and not c) else 0) \n",
    "    for l, c in zip(labels, classifications)\n",
    ")\n",
    "fp = sum(\n",
    "    (1 if (not l and c) else 0) \n",
    "    for l, c in zip(labels, classifications)\n",
    ")\n",
    "fn = sum(\n",
    "    (1 if (l and not c) else 0) \n",
    "    for l, c in zip(labels, classifications)\n",
    ")\n",
    "p4 = p4_score(tp, tn, fp, fn)\n",
    "\n",
    "print(f\"TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")\n",
    "print(f\"P4 score: {p4}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
